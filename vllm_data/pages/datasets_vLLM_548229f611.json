{
  "url": "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/datasets/",
  "title": "datasets - vLLM",
  "content": "This module defines a framework for sampling benchmark requests from various datasets. Each dataset subclass of BenchmarkDataset must implement sample generation. Supported dataset types include: - ShareGPT - Random (synthetic) - Sonnet - BurstGPT - HuggingFace - VisionArena\n\nBases: HuggingFaceDataset\n\nDataset class for processing a AIMO dataset with reasoning questions.\n\nBases: HuggingFaceDataset\n\nDataset class for processing a ASR dataset for transcription. Tested on the following set:\n\n+----------------+----------------------------------------+--------------------------+-----------------------------+ | Dataset | Domain | Speaking Style | hf-subset | +----------------+----------------------------------------+--------------------------+-----------------------------+ | TED-LIUM | TED talks | Oratory | release1, release2, release3| | | | | release3-speaker-adaptation | | VoxPopuli | European Parliament | Oratory | en, de, it, fr, ... | | LibriSpeech | Audiobook | Narrated | \"LIUM/tedlium\" | | GigaSpeech | Audiobook, podcast, YouTube | Narrated, spontaneous | xs, s, m, l, xl, dev, test | | SPGISpeech | Financial meetings | Oratory, spontaneous | S, M, L, dev, test | | AMI | Meetings | Spontaneous | ihm, sdm | +----------------+----------------------------------------+--------------------------+-----------------------------+\n\nInitialize the BenchmarkDataset with an optional dataset path and random seed.\n\nPath to the dataset. If None, it indicates that a default or random dataset might be used.\n\nSeed value for reproducible shuffling or sampling. Defaults to DEFAULT_SEED.\n\nTransform a prompt and optional multimodal content into a chat format. This method is used for chat models that expect a specific conversation format.\n\nOptionally select a random LoRA request.\n\nThis method is used when LoRA parameters are provided. It randomly selects a LoRA based on max_loras.\n\nThe maximum number of LoRAs available. If None, LoRA is not used.\n\nPath to the LoRA parameters on disk. If None, LoRA is not used.\n\n(or None if not applicable).\n\nLoad data from the dataset path into self.data.\n\nThis method must be overridden by subclasses since the method to load data will vary depending on the dataset format and source.\n\nIf a subclass does not implement this method.\n\nOversamples the list of requests if its size is less than the desired number.\n\nThe current list of sampled requests.\n\nThe target number of requests.\n\nThe prefix applied to generated request identifiers.\n\nAbstract method to generate sample requests from the dataset.\n\nSubclasses must override this method to implement dataset-specific logic for generating a list of SampleRequest objects.\n\nThe tokenizer to be used for processing the dataset's text.\n\nThe number of sample requests to generate.\n\nThe prefix of request_id.\n\nlist[SampleRequest]: A list of sample requests generated from the\n\nBases: HuggingFaceDataset\n\nBlazedit Dataset. https://github.com/ise-uiuc/blazedit\n\n5k char version: vdaita/edit_5k_char 10k char version: vdaita/edit_10k_char\n\nBases: BenchmarkDataset\n\nImplements the BurstGPT dataset. Loads data from a CSV file and generates sample requests based on synthetic prompt generation. Only rows with Model \"GPT-4\" and positive response tokens are used.\n\nBases: HuggingFaceDataset\n\nDataset for text-only conversation data.\n\nBases: BenchmarkDataset\n\nImplements the Custom dataset. Loads data from a JSONL file and generates sample requests based on conversation turns. E.g.,\n\nBases: BenchmarkDataset\n\nBase class for datasets hosted on HuggingFace.\n\nLoad data from HuggingFace datasets.\n\nBases: HuggingFaceDataset\n\nInstructCoder Dataset. https://huggingface.co/datasets/likaixin/InstructCoder\n\nInstructCoder is the dataset designed for general code editing. It consists of 114,239 instruction-input-output triplets, and covers multiple distinct code editing scenario.\n\nBases: HuggingFaceDataset\n\nMLPerf Inference Dataset.\n\nDataset on HF: https://huggingface.co/datasets/mgoin/mlperf-inference-llama2-data https://huggingface.co/datasets/mgoin/mlperf-inference-llama3.1-data\n\nWe combine the system prompt and question into a chat-formatted prompt (using the tokenizer's chat template) and set the expected output length to the tokenized length of the provided reference answer.\n\nBases: HuggingFaceDataset\n\nLin-Chen/MMStar: https://huggingface.co/datasets/Lin-Chen/MMStar refer to: https://github.com/sgl-project/SpecForge/pull/106\n\nBases: HuggingFaceDataset\n\nMMVU Dataset. https://huggingface.co/datasets/yale-nlp/MMVU\n\nBases: HuggingFaceDataset\n\nMT-Bench Dataset. https://huggingface.co/datasets/philschmid/mt-bench\n\nWe create a single turn dataset for MT-Bench. This is similar to Spec decoding benchmark setup in vLLM https://github.com/vllm-project/vllm/blob/9d98ab5ec/examples/offline_inference/eagle.py#L14-L18\n\nBases: HuggingFaceDataset\n\nDataset for multimodal conversation data.\n\nBases: HuggingFaceDataset\n\nDataset class for processing a Next Edit Prediction dataset.\n\nBases: BenchmarkDataset\n\nBases: BenchmarkDataset\n\nSynthetic text-only dataset for serving/throughput benchmarks.\n\nStrategy: - Sample input/output token lengths per request from integer-uniform ranges around configured means (controlled by range_ratio). - Prepend a fixed random prefix of length prefix_len. - Generate the remaining tokens as a reproducible sequence: (offset + index + arange(input_len)) % vocab_size. - Decode then re-encode/truncate to ensure prompt token counts match. - Uses numpy.default_rng seeded with random_seed for reproducible sampling.\n\nReturns (prompt, total_input_len).\n\nNOTE: After decoding the prompt we have to encode and decode it again. This is done because in some cases N consecutive tokens give a string tokenized into != N number of tokens. For example for GPT2Tokenizer: [6880, 6881] -> ['Ġcalls', 'here'] -> [1650, 939, 486] -> ['Ġcall', 'sh', 'ere'] To avoid uncontrolled change of the prompt length, the encoded sequence is truncated before being decoded again.\n\nGet the prefix for the dataset.\n\nGet the sampling parameters for the dataset.\n\nRandom dataset specialized for the needs of scoring: - Batches of inputs - Inputs composed of pairs\n\nSynthetic multimodal dataset (text + images) that extends RandomDataset.\n\nStatus: - Images: supported via synthetic RGB data. - Video: supported via synthetic RGB data. - Audio: not yet supported.\n\nSampling overview: 1) Number of items per request is sampled uniformly from the integer range [floor(n·(1−r)), ceil(n·(1+r))], where n is the base count and r is num_mm_items_range_ratio in [0, 1]. r=0 keeps it fixed; r=1 allows 0. The maximum is further clamped to the sum of per-modality limits. 2) Each item’s modality and shape is sampled from bucket_config, a dict mapping (height, width, num_frames) → probability. We treat num_frames=1 as image and num_frames > 1 as video. Entries with zero probability are removed and the rest are renormalized to sum to 1. 3) Per-modality hard caps are enforced via limit_mm_per_prompt. When a modality reaches its cap, all of its buckets are excluded and the remaining probabilities are renormalized.\n\nExample bucket configuration: {(256, 256, 1): 0.5, (720, 1280, 1): 0.4, (720, 1280, 16): 0.1} - Two image buckets (num_frames=1) and one video bucket (num_frames=16). OBS.: Only image sampling is supported for now.\n\nCreate synthetic images and videos and apply process_image/process_video respectively. This follows the OpenAI API chat completions https://github.com/openai/openai-python\n\nGenerate synthetic PIL image with random RGB values.\n\nNOTE: iid pixel sampling results in worst-case compression (good for stressing I/O), but very unlike real photos. We could consider a “low-freq” mode (e.g., noise blur) to emulate network realism instead of max stress.\n\nGenerate synthetic video with random values.\n\nCreates a video with random pixel values, encodes it to MP4 format, and returns the content as bytes.\n\nIterator over the multimodal items for each request whose size is between min_num_mm_items and max_num_mm_items.\n\nLoop over the bucket config and sample a multimodal item. Loop until the number of multimodal items sampled is equal to request_num_mm_items or limit of multimodal items per prompt for all modalities is reached.\n\nNote: - This function operates on a per-request shallow copy of bucket_config (tuple->float). The original dict passed to sample is not mutated. If this ever changes, a test is implemented and will fail.\n\nGet the sampling parameters for the multimodal items.\n\nMap the configuration to the modality.\n\nRemove zero probability entries and normalize the bucket config to sum to 1.\n\nRepresents a single inference request for benchmarking.\n\nBases: BenchmarkDataset\n\nImplements the ShareGPT dataset. Loads data from a JSON file and generates sample requests based on conversation turns.\n\nBases: BenchmarkDataset\n\nSimplified implementation of the Sonnet dataset. Loads poem lines from a text file and generates sample requests. Default values here copied from benchmark_serving.py for the sonnet dataset.\n\nImplements the SpecBench dataset: https://github.com/hemingkx/Spec-Bench Download the dataset using: wget https://raw.githubusercontent.com/hemingkx/Spec-Bench/refs/heads/main/data/spec_bench/question.jsonl\n\nBases: HuggingFaceDataset\n\nVision Arena Dataset.\n\nArgparse action to validate dataset name and path compatibility.\n\nFormat the zeta prompt for the Next Edit Prediction (NEP) dataset.\n\nThis function formats examples from the NEP dataset into prompts and expected outputs. It could be further extended to support more NEP datasets.\n\nThe dataset sample containing events, inputs, and outputs.\n\nThe marker indicating the start of the editable region. Defaults to \"<|editable_region_start|>\".\n\nA dictionary with the formatted prompts and expected outputs.\n\nEnsure decoded-then-encoded prompt length matches the target token length.\n\nThis function decodes an initial token sequence to text and re-encodes it , iteratively adjusting the token sequence length to match a target. This is necessary because some tokenizers do not guarantee a 1:1 mapping between consecutive tokens and the decoded-then-encoded sequence length. For example, for GPT2Tokenizer: [6880, 6881] -> ['Ġcalls', 'here'] -> [1650, 939, 486] -> ['Ġcall', 'sh', 'ere']\n\nReturns a tuple of the final prompt string and the adjusted token sequence.\n\nValidate a sequence based on prompt and output lengths.\n\nDefault pruning criteria are copied from the original sample_hf_requests and sample_sharegpt_requests functions in benchmark_serving.py, as well as from sample_requests in benchmark_throughput.py.\n\nProcess a single image input and return a multimedia content dictionary.\n\nSupports the following input types:\n\nDictionary with raw image bytes: - Expects a dict with a 'bytes' key containing raw image data. - Loads the bytes as a PIL.Image.Image.\n\nPIL.Image.Image input: - Converts the image to RGB. - Saves the image as a JPEG in memory. - Encodes the JPEG data as a base64 string. - Returns a dictionary with the image as a base64 data URL.\n\nString input: - Treats the string as a URL or local file path. - Prepends \"file://\" if the string doesn't start with \"http://\" or \"file://\". - Returns a dictionary with the image URL.\n\nIf the input is not a supported type.\n\nProcess a single video input and return a multimedia content dictionary.\n\nSupports the following input types:\n\nDictionary with raw video bytes: - Expects a dict with a 'bytes' key containing raw video data.\n\nString input: - Treats the string as a URL or local file path. - Prepends \"file://\" if the string doesn't start with \"http://\" or \"file://\". - Returns a dictionary with the image URL.\n\nIf the input is not a supported type.",
  "headings": [
    {
      "level": "h1",
      "text": "vllm.benchmarks.datasets ¶",
      "id": "vllm.benchmarks.datasets"
    },
    {
      "level": "h2",
      "text": "datasets module-attribute ¶",
      "id": "vllm.benchmarks.datasets.datasets"
    },
    {
      "level": "h2",
      "text": "logger module-attribute ¶",
      "id": "vllm.benchmarks.datasets.logger"
    },
    {
      "level": "h2",
      "text": "lora_tokenizer_cache module-attribute ¶",
      "id": "vllm.benchmarks.datasets.lora_tokenizer_cache"
    },
    {
      "level": "h2",
      "text": "zeta_prompt module-attribute ¶",
      "id": "vllm.benchmarks.datasets.zeta_prompt"
    },
    {
      "level": "h2",
      "text": "AIMODataset ¶",
      "id": "vllm.benchmarks.datasets.AIMODataset"
    },
    {
      "level": "h3",
      "text": "SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.AIMODataset.SUPPORTED_DATASET_PATHS"
    },
    {
      "level": "h3",
      "text": "sample ¶",
      "id": "vllm.benchmarks.datasets.AIMODataset.sample"
    },
    {
      "level": "h2",
      "text": "ASRDataset ¶",
      "id": "vllm.benchmarks.datasets.ASRDataset"
    },
    {
      "level": "h3",
      "text": "DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.ASRDataset.DEFAULT_OUTPUT_LEN"
    },
    {
      "level": "h3",
      "text": "IS_MULTIMODAL class-attribute instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.ASRDataset.IS_MULTIMODAL"
    },
    {
      "level": "h3",
      "text": "SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.ASRDataset.SUPPORTED_DATASET_PATHS"
    },
    {
      "level": "h3",
      "text": "TRANSCRIPTION_PREAMBLE class-attribute instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.ASRDataset.TRANSCRIPTION_PREAMBLE"
    },
    {
      "level": "h3",
      "text": "skip_long_audios class-attribute instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.ASRDataset.skip_long_audios"
    },
    {
      "level": "h3",
      "text": "sample ¶",
      "id": "vllm.benchmarks.datasets.ASRDataset.sample"
    },
    {
      "level": "h2",
      "text": "BenchmarkDataset ¶",
      "id": "vllm.benchmarks.datasets.BenchmarkDataset"
    },
    {
      "level": "h3",
      "text": "DEFAULT_SEED class-attribute instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.BenchmarkDataset.DEFAULT_SEED"
    },
    {
      "level": "h3",
      "text": "IS_MULTIMODAL class-attribute instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.BenchmarkDataset.IS_MULTIMODAL"
    },
    {
      "level": "h3",
      "text": "data instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.BenchmarkDataset.data"
    },
    {
      "level": "h3",
      "text": "dataset_path instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.BenchmarkDataset.dataset_path"
    },
    {
      "level": "h3",
      "text": "disable_shuffle instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.BenchmarkDataset.disable_shuffle"
    },
    {
      "level": "h3",
      "text": "random_seed instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.BenchmarkDataset.random_seed"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.benchmarks.datasets.BenchmarkDataset.__init__"
    },
    {
      "level": "h3",
      "text": "apply_multimodal_chat_transformation ¶",
      "id": "vllm.benchmarks.datasets.BenchmarkDataset.apply_multimodal_chat_transformation"
    },
    {
      "level": "h3",
      "text": "get_random_lora_request ¶",
      "id": "vllm.benchmarks.datasets.BenchmarkDataset.get_random_lora_request"
    },
    {
      "level": "h3",
      "text": "load_data ¶",
      "id": "vllm.benchmarks.datasets.BenchmarkDataset.load_data"
    },
    {
      "level": "h3",
      "text": "maybe_oversample_requests ¶",
      "id": "vllm.benchmarks.datasets.BenchmarkDataset.maybe_oversample_requests"
    },
    {
      "level": "h3",
      "text": "sample abstractmethod ¶",
      "id": "vllm.benchmarks.datasets.BenchmarkDataset.sample"
    },
    {
      "level": "h2",
      "text": "BlazeditDataset ¶",
      "id": "vllm.benchmarks.datasets.BlazeditDataset"
    },
    {
      "level": "h3",
      "text": "DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.BlazeditDataset.DEFAULT_OUTPUT_LEN"
    },
    {
      "level": "h3",
      "text": "SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.BlazeditDataset.SUPPORTED_DATASET_PATHS"
    },
    {
      "level": "h3",
      "text": "sample ¶",
      "id": "vllm.benchmarks.datasets.BlazeditDataset.sample"
    },
    {
      "level": "h2",
      "text": "BurstGPTDataset ¶",
      "id": "vllm.benchmarks.datasets.BurstGPTDataset"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.benchmarks.datasets.BurstGPTDataset.__init__"
    },
    {
      "level": "h3",
      "text": "_sample_loaded_data ¶",
      "id": "vllm.benchmarks.datasets.BurstGPTDataset._sample_loaded_data"
    },
    {
      "level": "h3",
      "text": "load_data ¶",
      "id": "vllm.benchmarks.datasets.BurstGPTDataset.load_data"
    },
    {
      "level": "h3",
      "text": "sample ¶",
      "id": "vllm.benchmarks.datasets.BurstGPTDataset.sample"
    },
    {
      "level": "h2",
      "text": "ConversationDataset ¶",
      "id": "vllm.benchmarks.datasets.ConversationDataset"
    },
    {
      "level": "h3",
      "text": "IS_MULTIMODAL class-attribute instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.ConversationDataset.IS_MULTIMODAL"
    },
    {
      "level": "h3",
      "text": "SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.ConversationDataset.SUPPORTED_DATASET_PATHS"
    },
    {
      "level": "h3",
      "text": "sample ¶",
      "id": "vllm.benchmarks.datasets.ConversationDataset.sample"
    },
    {
      "level": "h2",
      "text": "CustomDataset ¶",
      "id": "vllm.benchmarks.datasets.CustomDataset"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.benchmarks.datasets.CustomDataset.__init__"
    },
    {
      "level": "h3",
      "text": "load_data ¶",
      "id": "vllm.benchmarks.datasets.CustomDataset.load_data"
    },
    {
      "level": "h3",
      "text": "sample ¶",
      "id": "vllm.benchmarks.datasets.CustomDataset.sample"
    },
    {
      "level": "h2",
      "text": "HuggingFaceDataset ¶",
      "id": "vllm.benchmarks.datasets.HuggingFaceDataset"
    },
    {
      "level": "h3",
      "text": "SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.HuggingFaceDataset.SUPPORTED_DATASET_PATHS"
    },
    {
      "level": "h3",
      "text": "dataset_split instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.HuggingFaceDataset.dataset_split"
    },
    {
      "level": "h3",
      "text": "dataset_subset instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.HuggingFaceDataset.dataset_subset"
    },
    {
      "level": "h3",
      "text": "hf_name instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.HuggingFaceDataset.hf_name"
    },
    {
      "level": "h3",
      "text": "load_stream instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.HuggingFaceDataset.load_stream"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.benchmarks.datasets.HuggingFaceDataset.__init__"
    },
    {
      "level": "h3",
      "text": "load_data ¶",
      "id": "vllm.benchmarks.datasets.HuggingFaceDataset.load_data"
    },
    {
      "level": "h2",
      "text": "InstructCoderDataset ¶",
      "id": "vllm.benchmarks.datasets.InstructCoderDataset"
    },
    {
      "level": "h3",
      "text": "DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.InstructCoderDataset.DEFAULT_OUTPUT_LEN"
    },
    {
      "level": "h3",
      "text": "SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.InstructCoderDataset.SUPPORTED_DATASET_PATHS"
    },
    {
      "level": "h3",
      "text": "sample ¶",
      "id": "vllm.benchmarks.datasets.InstructCoderDataset.sample"
    },
    {
      "level": "h2",
      "text": "MLPerfDataset ¶",
      "id": "vllm.benchmarks.datasets.MLPerfDataset"
    },
    {
      "level": "h3",
      "text": "SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.MLPerfDataset.SUPPORTED_DATASET_PATHS"
    },
    {
      "level": "h3",
      "text": "sample ¶",
      "id": "vllm.benchmarks.datasets.MLPerfDataset.sample"
    },
    {
      "level": "h2",
      "text": "MMStarDataset ¶",
      "id": "vllm.benchmarks.datasets.MMStarDataset"
    },
    {
      "level": "h3",
      "text": "DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.MMStarDataset.DEFAULT_OUTPUT_LEN"
    },
    {
      "level": "h3",
      "text": "IS_MULTIMODAL class-attribute instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.MMStarDataset.IS_MULTIMODAL"
    },
    {
      "level": "h3",
      "text": "SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.MMStarDataset.SUPPORTED_DATASET_PATHS"
    },
    {
      "level": "h3",
      "text": "sample ¶",
      "id": "vllm.benchmarks.datasets.MMStarDataset.sample"
    },
    {
      "level": "h2",
      "text": "MMVUDataset ¶",
      "id": "vllm.benchmarks.datasets.MMVUDataset"
    },
    {
      "level": "h3",
      "text": "DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.MMVUDataset.DEFAULT_OUTPUT_LEN"
    },
    {
      "level": "h3",
      "text": "SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.MMVUDataset.SUPPORTED_DATASET_PATHS"
    },
    {
      "level": "h3",
      "text": "sample ¶",
      "id": "vllm.benchmarks.datasets.MMVUDataset.sample"
    },
    {
      "level": "h2",
      "text": "MTBenchDataset ¶",
      "id": "vllm.benchmarks.datasets.MTBenchDataset"
    },
    {
      "level": "h3",
      "text": "DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.MTBenchDataset.DEFAULT_OUTPUT_LEN"
    },
    {
      "level": "h3",
      "text": "SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.MTBenchDataset.SUPPORTED_DATASET_PATHS"
    },
    {
      "level": "h3",
      "text": "sample ¶",
      "id": "vllm.benchmarks.datasets.MTBenchDataset.sample"
    },
    {
      "level": "h2",
      "text": "MultiModalConversationDataset ¶",
      "id": "vllm.benchmarks.datasets.MultiModalConversationDataset"
    },
    {
      "level": "h3",
      "text": "IS_MULTIMODAL class-attribute instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.MultiModalConversationDataset.IS_MULTIMODAL"
    },
    {
      "level": "h3",
      "text": "SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.MultiModalConversationDataset.SUPPORTED_DATASET_PATHS"
    },
    {
      "level": "h3",
      "text": "sample ¶",
      "id": "vllm.benchmarks.datasets.MultiModalConversationDataset.sample"
    },
    {
      "level": "h2",
      "text": "NextEditPredictionDataset ¶",
      "id": "vllm.benchmarks.datasets.NextEditPredictionDataset"
    },
    {
      "level": "h3",
      "text": "MAPPING_PROMPT_FUNCS class-attribute instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.NextEditPredictionDataset.MAPPING_PROMPT_FUNCS"
    },
    {
      "level": "h3",
      "text": "SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.NextEditPredictionDataset.SUPPORTED_DATASET_PATHS"
    },
    {
      "level": "h3",
      "text": "sample ¶",
      "id": "vllm.benchmarks.datasets.NextEditPredictionDataset.sample"
    },
    {
      "level": "h2",
      "text": "PrefixRepetitionRandomDataset ¶",
      "id": "vllm.benchmarks.datasets.PrefixRepetitionRandomDataset"
    },
    {
      "level": "h3",
      "text": "DEFAULT_NUM_PREFIXES class-attribute instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.PrefixRepetitionRandomDataset.DEFAULT_NUM_PREFIXES"
    },
    {
      "level": "h3",
      "text": "DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.PrefixRepetitionRandomDataset.DEFAULT_OUTPUT_LEN"
    },
    {
      "level": "h3",
      "text": "DEFAULT_PREFIX_LEN class-attribute instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.PrefixRepetitionRandomDataset.DEFAULT_PREFIX_LEN"
    },
    {
      "level": "h3",
      "text": "DEFAULT_SUFFIX_LEN class-attribute instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.PrefixRepetitionRandomDataset.DEFAULT_SUFFIX_LEN"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.benchmarks.datasets.PrefixRepetitionRandomDataset.__init__"
    },
    {
      "level": "h3",
      "text": "sample ¶",
      "id": "vllm.benchmarks.datasets.PrefixRepetitionRandomDataset.sample"
    },
    {
      "level": "h2",
      "text": "RandomDataset ¶",
      "id": "vllm.benchmarks.datasets.RandomDataset"
    },
    {
      "level": "h3",
      "text": "DEFAULT_INPUT_LEN class-attribute instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.RandomDataset.DEFAULT_INPUT_LEN"
    },
    {
      "level": "h3",
      "text": "DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.RandomDataset.DEFAULT_OUTPUT_LEN"
    },
    {
      "level": "h3",
      "text": "DEFAULT_PREFIX_LEN class-attribute instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.RandomDataset.DEFAULT_PREFIX_LEN"
    },
    {
      "level": "h3",
      "text": "DEFAULT_RANGE_RATIO class-attribute instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.RandomDataset.DEFAULT_RANGE_RATIO"
    },
    {
      "level": "h3",
      "text": "_rng instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.RandomDataset._rng"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.benchmarks.datasets.RandomDataset.__init__"
    },
    {
      "level": "h3",
      "text": "generate_token_sequence ¶",
      "id": "vllm.benchmarks.datasets.RandomDataset.generate_token_sequence"
    },
    {
      "level": "h3",
      "text": "get_prefix ¶",
      "id": "vllm.benchmarks.datasets.RandomDataset.get_prefix"
    },
    {
      "level": "h3",
      "text": "get_sampling_params ¶",
      "id": "vllm.benchmarks.datasets.RandomDataset.get_sampling_params"
    },
    {
      "level": "h3",
      "text": "sample ¶",
      "id": "vllm.benchmarks.datasets.RandomDataset.sample"
    },
    {
      "level": "h2",
      "text": "RandomDatasetForReranking ¶",
      "id": "vllm.benchmarks.datasets.RandomDatasetForReranking"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.benchmarks.datasets.RandomDatasetForReranking.__init__"
    },
    {
      "level": "h3",
      "text": "sample ¶",
      "id": "vllm.benchmarks.datasets.RandomDatasetForReranking.sample"
    },
    {
      "level": "h2",
      "text": "RandomMultiModalDataset ¶",
      "id": "vllm.benchmarks.datasets.RandomMultiModalDataset"
    },
    {
      "level": "h3",
      "text": "DEFAULT_BASE_ITEMS_PER_REQUEST class-attribute instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.RandomMultiModalDataset.DEFAULT_BASE_ITEMS_PER_REQUEST"
    },
    {
      "level": "h3",
      "text": "DEFAULT_ENABLE_MULTIMODAL_CHAT class-attribute instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.RandomMultiModalDataset.DEFAULT_ENABLE_MULTIMODAL_CHAT"
    },
    {
      "level": "h3",
      "text": "DEFAULT_LIMIT_MM_PER_PROMPT class-attribute instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.RandomMultiModalDataset.DEFAULT_LIMIT_MM_PER_PROMPT"
    },
    {
      "level": "h3",
      "text": "DEFAULT_MM_ITEM_BUCKET_CONFIG class-attribute instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.RandomMultiModalDataset.DEFAULT_MM_ITEM_BUCKET_CONFIG"
    },
    {
      "level": "h3",
      "text": "DEFAULT_NUM_MM_ITEMS_RANGE_RATIO class-attribute instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.RandomMultiModalDataset.DEFAULT_NUM_MM_ITEMS_RANGE_RATIO"
    },
    {
      "level": "h3",
      "text": "IS_MULTIMODAL class-attribute instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.RandomMultiModalDataset.IS_MULTIMODAL"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.benchmarks.datasets.RandomMultiModalDataset.__init__"
    },
    {
      "level": "h3",
      "text": "generate_mm_item ¶",
      "id": "vllm.benchmarks.datasets.RandomMultiModalDataset.generate_mm_item"
    },
    {
      "level": "h3",
      "text": "generate_synthetic_image ¶",
      "id": "vllm.benchmarks.datasets.RandomMultiModalDataset.generate_synthetic_image"
    },
    {
      "level": "h3",
      "text": "generate_synthetic_video ¶",
      "id": "vllm.benchmarks.datasets.RandomMultiModalDataset.generate_synthetic_video"
    },
    {
      "level": "h3",
      "text": "get_mm_item_iterator ¶",
      "id": "vllm.benchmarks.datasets.RandomMultiModalDataset.get_mm_item_iterator"
    },
    {
      "level": "h3",
      "text": "get_mm_item_sampling_params ¶",
      "id": "vllm.benchmarks.datasets.RandomMultiModalDataset.get_mm_item_sampling_params"
    },
    {
      "level": "h3",
      "text": "map_config_to_modality ¶",
      "id": "vllm.benchmarks.datasets.RandomMultiModalDataset.map_config_to_modality"
    },
    {
      "level": "h3",
      "text": "normalize_bucket_config ¶",
      "id": "vllm.benchmarks.datasets.RandomMultiModalDataset.normalize_bucket_config"
    },
    {
      "level": "h3",
      "text": "sample ¶",
      "id": "vllm.benchmarks.datasets.RandomMultiModalDataset.sample"
    },
    {
      "level": "h2",
      "text": "SampleRequest dataclass ¶",
      "id": "vllm.benchmarks.datasets.SampleRequest"
    },
    {
      "level": "h3",
      "text": "expected_output_len instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.SampleRequest.expected_output_len"
    },
    {
      "level": "h3",
      "text": "lora_request class-attribute instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.SampleRequest.lora_request"
    },
    {
      "level": "h3",
      "text": "multi_modal_data class-attribute instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.SampleRequest.multi_modal_data"
    },
    {
      "level": "h3",
      "text": "prompt instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.SampleRequest.prompt"
    },
    {
      "level": "h3",
      "text": "prompt_len instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.SampleRequest.prompt_len"
    },
    {
      "level": "h3",
      "text": "request_id class-attribute instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.SampleRequest.request_id"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.benchmarks.datasets.SampleRequest.__init__"
    },
    {
      "level": "h2",
      "text": "ShareGPTDataset ¶",
      "id": "vllm.benchmarks.datasets.ShareGPTDataset"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.benchmarks.datasets.ShareGPTDataset.__init__"
    },
    {
      "level": "h3",
      "text": "load_data ¶",
      "id": "vllm.benchmarks.datasets.ShareGPTDataset.load_data"
    },
    {
      "level": "h3",
      "text": "sample ¶",
      "id": "vllm.benchmarks.datasets.ShareGPTDataset.sample"
    },
    {
      "level": "h2",
      "text": "SonnetDataset ¶",
      "id": "vllm.benchmarks.datasets.SonnetDataset"
    },
    {
      "level": "h3",
      "text": "DEFAULT_INPUT_LEN class-attribute instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.SonnetDataset.DEFAULT_INPUT_LEN"
    },
    {
      "level": "h3",
      "text": "DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.SonnetDataset.DEFAULT_OUTPUT_LEN"
    },
    {
      "level": "h3",
      "text": "DEFAULT_PREFIX_LEN class-attribute instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.SonnetDataset.DEFAULT_PREFIX_LEN"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.benchmarks.datasets.SonnetDataset.__init__"
    },
    {
      "level": "h3",
      "text": "load_data ¶",
      "id": "vllm.benchmarks.datasets.SonnetDataset.load_data"
    },
    {
      "level": "h3",
      "text": "sample ¶",
      "id": "vllm.benchmarks.datasets.SonnetDataset.sample"
    },
    {
      "level": "h2",
      "text": "SpecBench ¶",
      "id": "vllm.benchmarks.datasets.SpecBench"
    },
    {
      "level": "h3",
      "text": "category instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.SpecBench.category"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.benchmarks.datasets.SpecBench.__init__"
    },
    {
      "level": "h3",
      "text": "load_data ¶",
      "id": "vllm.benchmarks.datasets.SpecBench.load_data"
    },
    {
      "level": "h3",
      "text": "sample ¶",
      "id": "vllm.benchmarks.datasets.SpecBench.sample"
    },
    {
      "level": "h2",
      "text": "VisionArenaDataset ¶",
      "id": "vllm.benchmarks.datasets.VisionArenaDataset"
    },
    {
      "level": "h3",
      "text": "DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.VisionArenaDataset.DEFAULT_OUTPUT_LEN"
    },
    {
      "level": "h3",
      "text": "IS_MULTIMODAL class-attribute instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.VisionArenaDataset.IS_MULTIMODAL"
    },
    {
      "level": "h3",
      "text": "SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶",
      "id": "vllm.benchmarks.datasets.VisionArenaDataset.SUPPORTED_DATASET_PATHS"
    },
    {
      "level": "h3",
      "text": "sample ¶",
      "id": "vllm.benchmarks.datasets.VisionArenaDataset.sample"
    },
    {
      "level": "h2",
      "text": "_ValidateDatasetArgs ¶",
      "id": "vllm.benchmarks.datasets._ValidateDatasetArgs"
    },
    {
      "level": "h3",
      "text": "__call__ ¶",
      "id": "vllm.benchmarks.datasets._ValidateDatasetArgs.__call__"
    },
    {
      "level": "h2",
      "text": "_format_zeta_prompt ¶",
      "id": "vllm.benchmarks.datasets._format_zeta_prompt"
    },
    {
      "level": "h2",
      "text": "add_dataset_parser ¶",
      "id": "vllm.benchmarks.datasets.add_dataset_parser"
    },
    {
      "level": "h2",
      "text": "gen_prompt_decode_to_target_len ¶",
      "id": "vllm.benchmarks.datasets.gen_prompt_decode_to_target_len"
    },
    {
      "level": "h2",
      "text": "get_samples ¶",
      "id": "vllm.benchmarks.datasets.get_samples"
    },
    {
      "level": "h2",
      "text": "is_valid_sequence ¶",
      "id": "vllm.benchmarks.datasets.is_valid_sequence"
    },
    {
      "level": "h2",
      "text": "lora_path_on_disk cached ¶",
      "id": "vllm.benchmarks.datasets.lora_path_on_disk"
    },
    {
      "level": "h2",
      "text": "process_image ¶",
      "id": "vllm.benchmarks.datasets.process_image"
    },
    {
      "level": "h2",
      "text": "process_video ¶",
      "id": "vllm.benchmarks.datasets.process_video"
    }
  ],
  "code_samples": [
    {
      "code": "datasets = PlaceholderModule('datasets')",
      "language": "unknown"
    },
    {
      "code": "datasets = PlaceholderModule('datasets')",
      "language": "unknown"
    },
    {
      "code": "logger = getLogger(__name__)",
      "language": "unknown"
    },
    {
      "code": "logger = getLogger(__name__)",
      "language": "unknown"
    },
    {
      "code": "lora_tokenizer_cache: dict[int, TokenizerLike] = {}",
      "language": "yaml"
    },
    {
      "code": "lora_tokenizer_cache: dict[int, TokenizerLike] = {}",
      "language": "yaml"
    },
    {
      "code": "zeta_prompt = \"### Instruction:\\nYou are a code completion assistant and your task is to analyze user edits and then rewrite an excerpt that the user provides, suggesting the appropriate edits within the excerpt, taking into account the cursor location.\\n\\n### User Edits:\\n\\n{}\\n\\n### User Excerpt:\\n\\n{}\\n\\n### Response:\\n\\n\"",
      "language": "unknown"
    },
    {
      "code": "zeta_prompt = \"### Instruction:\\nYou are a code completion assistant and your task is to analyze user edits and then rewrite an excerpt that the user provides, suggesting the appropriate edits within the excerpt, taking into account the cursor location.\\n\\n### User Edits:\\n\\n{}\\n\\n### User Excerpt:\\n\\n{}\\n\\n### Response:\\n\\n\"",
      "language": "unknown"
    },
    {
      "code": "2736\n2737\n2738\n2739\n2740\n2741\n2742\n2743\n2744\n2745\n2746\n2747\n2748\n2749\n2750\n2751\n2752\n2753\n2754\n2755\n2756\n2757\n2758\n2759\n2760\n2761\n2762\n2763\n2764\n2765\n2766\n2767\n2768\n2769\n2770\n2771\n2772\n2773\n2774\n2775\n2776\n2777\n2778\n2779\n2780\n2781\n2782\n2783\n2784\n2785\n2786\n2787\n2788",
      "language": "unknown"
    },
    {
      "code": "class AIMODataset(HuggingFaceDataset):\n    \"\"\"\n    Dataset class for processing a AIMO dataset with reasoning questions.\n    \"\"\"\n\n    SUPPORTED_DATASET_PATHS = {\n        \"AI-MO/aimo-validation-aime\",\n        \"AI-MO/NuminaMath-1.5\",\n        \"AI-MO/NuminaMath-CoT\",\n    }\n\n    def sample(\n        self,\n        tokenizer: TokenizerLike,\n        num_requests: int,\n        output_len: int | None = None,\n        request_id_prefix: str = \"\",\n        no_oversample: bool = False,\n        **kwargs,\n    ) -> list:\n        sampled_requests = []\n        ind = 0\n        dynamic_output = output_len is None\n\n        for item in self.data:\n            if len(sampled_requests) >= num_requests:\n                break\n            prompt, completion = item[\"problem\"], item[\"solution\"]\n\n            prompt_ids = tokenizer(prompt).input_ids\n            completion_ids = tokenizer(completion).input_ids\n            prompt_len = len(prompt_ids)\n            completion_len = len(completion_ids)\n            output_len = completion_len if dynamic_output else output_len\n            assert isinstance(output_len, int) and output_len > 0\n            if dynamic_output and not is_valid_sequence(\n                prompt_len, completion_len, max_prompt_len=2048, max_total_len=32000\n            ):\n                continue\n            sampled_requests.append(\n                SampleRequest(\n                    prompt=prompt,\n                    prompt_len=prompt_len,\n                    expected_output_len=output_len,\n                    multi_modal_data=None,\n                    request_id=request_id_prefix + str(ind),\n                )\n            )\n            ind += 1\n        self.maybe_oversample_requests(\n            sampled_requests, num_requests, request_id_prefix, no_oversample\n        )\n        return sampled_requests",
      "language": "python"
    },
    {
      "code": "class AIMODataset(HuggingFaceDataset):\n    \"\"\"\n    Dataset class for processing a AIMO dataset with reasoning questions.\n    \"\"\"\n\n    SUPPORTED_DATASET_PATHS = {\n        \"AI-MO/aimo-validation-aime\",\n        \"AI-MO/NuminaMath-1.5\",\n        \"AI-MO/NuminaMath-CoT\",\n    }\n\n    def sample(\n        self,\n        tokenizer: TokenizerLike,\n        num_requests: int,\n        output_len: int | None = None,\n        request_id_prefix: str = \"\",\n        no_oversample: bool = False,\n        **kwargs,\n    ) -> list:\n        sampled_requests = []\n        ind = 0\n        dynamic_output = output_len is None\n\n        for item in self.data:\n            if len(sampled_requests) >= num_requests:\n                break\n            prompt, completion = item[\"problem\"], item[\"solution\"]\n\n            prompt_ids = tokenizer(prompt).input_ids\n            completion_ids = tokenizer(completion).input_ids\n            prompt_len = len(prompt_ids)\n            completion_len = len(completion_ids)\n            output_len = completion_len if dynamic_output else output_len\n            assert isinstance(output_len, int) and output_len > 0\n            if dynamic_output and not is_valid_sequence(\n                prompt_len, completion_len, max_prompt_len=2048, max_total_len=32000\n            ):\n                continue\n            sampled_requests.append(\n                SampleRequest(\n                    prompt=prompt,\n                    prompt_len=prompt_len,\n                    expected_output_len=output_len,\n                    multi_modal_data=None,\n                    request_id=request_id_prefix + str(ind),\n                )\n            )\n            ind += 1\n        self.maybe_oversample_requests(\n            sampled_requests, num_requests, request_id_prefix, no_oversample\n        )\n        return sampled_requests",
      "language": "python"
    },
    {
      "code": "SUPPORTED_DATASET_PATHS = {\n    \"AI-MO/aimo-validation-aime\",\n    \"AI-MO/NuminaMath-1.5\",\n    \"AI-MO/NuminaMath-CoT\",\n}",
      "language": "unknown"
    },
    {
      "code": "SUPPORTED_DATASET_PATHS = {\n    \"AI-MO/aimo-validation-aime\",\n    \"AI-MO/NuminaMath-1.5\",\n    \"AI-MO/NuminaMath-CoT\",\n}",
      "language": "unknown"
    },
    {
      "code": "sample(\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    output_len: int | None = None,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list",
      "language": "typescript"
    },
    {
      "code": "sample(\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    output_len: int | None = None,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list",
      "language": "typescript"
    },
    {
      "code": "2747\n2748\n2749\n2750\n2751\n2752\n2753\n2754\n2755\n2756\n2757\n2758\n2759\n2760\n2761\n2762\n2763\n2764\n2765\n2766\n2767\n2768\n2769\n2770\n2771\n2772\n2773\n2774\n2775\n2776\n2777\n2778\n2779\n2780\n2781\n2782\n2783\n2784\n2785\n2786\n2787\n2788",
      "language": "unknown"
    },
    {
      "code": "def sample(\n    self,\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    output_len: int | None = None,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list:\n    sampled_requests = []\n    ind = 0\n    dynamic_output = output_len is None\n\n    for item in self.data:\n        if len(sampled_requests) >= num_requests:\n            break\n        prompt, completion = item[\"problem\"], item[\"solution\"]\n\n        prompt_ids = tokenizer(prompt).input_ids\n        completion_ids = tokenizer(completion).input_ids\n        prompt_len = len(prompt_ids)\n        completion_len = len(completion_ids)\n        output_len = completion_len if dynamic_output else output_len\n        assert isinstance(output_len, int) and output_len > 0\n        if dynamic_output and not is_valid_sequence(\n            prompt_len, completion_len, max_prompt_len=2048, max_total_len=32000\n        ):\n            continue\n        sampled_requests.append(\n            SampleRequest(\n                prompt=prompt,\n                prompt_len=prompt_len,\n                expected_output_len=output_len,\n                multi_modal_data=None,\n                request_id=request_id_prefix + str(ind),\n            )\n        )\n        ind += 1\n    self.maybe_oversample_requests(\n        sampled_requests, num_requests, request_id_prefix, no_oversample\n    )\n    return sampled_requests",
      "language": "python"
    },
    {
      "code": "def sample(\n    self,\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    output_len: int | None = None,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list:\n    sampled_requests = []\n    ind = 0\n    dynamic_output = output_len is None\n\n    for item in self.data:\n        if len(sampled_requests) >= num_requests:\n            break\n        prompt, completion = item[\"problem\"], item[\"solution\"]\n\n        prompt_ids = tokenizer(prompt).input_ids\n        completion_ids = tokenizer(completion).input_ids\n        prompt_len = len(prompt_ids)\n        completion_len = len(completion_ids)\n        output_len = completion_len if dynamic_output else output_len\n        assert isinstance(output_len, int) and output_len > 0\n        if dynamic_output and not is_valid_sequence(\n            prompt_len, completion_len, max_prompt_len=2048, max_total_len=32000\n        ):\n            continue\n        sampled_requests.append(\n            SampleRequest(\n                prompt=prompt,\n                prompt_len=prompt_len,\n                expected_output_len=output_len,\n                multi_modal_data=None,\n                request_id=request_id_prefix + str(ind),\n            )\n        )\n        ind += 1\n    self.maybe_oversample_requests(\n        sampled_requests, num_requests, request_id_prefix, no_oversample\n    )\n    return sampled_requests",
      "language": "python"
    },
    {
      "code": "2894\n2895\n2896\n2897\n2898\n2899\n2900\n2901\n2902\n2903\n2904\n2905\n2906\n2907\n2908\n2909\n2910\n2911\n2912\n2913\n2914\n2915\n2916\n2917\n2918\n2919\n2920\n2921\n2922\n2923\n2924\n2925\n2926\n2927\n2928\n2929\n2930\n2931\n2932\n2933\n2934\n2935\n2936\n2937\n2938\n2939\n2940\n2941\n2942\n2943\n2944\n2945\n2946\n2947\n2948\n2949\n2950\n2951\n2952\n2953\n2954\n2955\n2956\n2957\n2958\n2959\n2960\n2961\n2962\n2963\n2964\n2965\n2966\n2967\n2968\n2969\n2970\n2971\n2972\n2973\n2974\n2975\n2976",
      "language": "unknown"
    },
    {
      "code": "class ASRDataset(HuggingFaceDataset):\n    \"\"\"\n    Dataset class for processing a ASR dataset for transcription.\n    Tested on the following set:\n\n    +----------------+----------------------------------------+--------------------------+-----------------------------+\n    | Dataset        | Domain                                 | Speaking Style           | hf-subset                   |\n    +----------------+----------------------------------------+--------------------------+-----------------------------+\n    | TED-LIUM       | TED talks                              | Oratory                  | release1, release2, release3|\n    |                |                                        |                          | release3-speaker-adaptation |\n    | VoxPopuli      | European Parliament                    | Oratory                  | en, de, it, fr,  ...        |\n    | LibriSpeech    | Audiobook                              | Narrated                 | \"LIUM/tedlium\"              |\n    | GigaSpeech     | Audiobook, podcast, YouTube            | Narrated, spontaneous    | xs, s, m, l, xl, dev, test  |\n    | SPGISpeech     | Financial meetings                     | Oratory, spontaneous     | S, M, L, dev, test          |\n    | AMI            | Meetings                               | Spontaneous              | ihm, sdm                    |\n    +----------------+----------------------------------------+--------------------------+-----------------------------+\n\n    \"\"\"  # noqa: E501\n\n    SUPPORTED_DATASET_PATHS = {\n        \"openslr/librispeech_asr\",\n        \"facebook/voxpopuli\",\n        \"LIUM/tedlium\",\n        \"edinburghcstr/ami\",\n        \"speechcolab/gigaspeech\",\n        \"kensho/spgispeech\",\n    }\n\n    DEFAULT_OUTPUT_LEN = 128\n    IS_MULTIMODAL = True\n\n    # TODO Whisper-specific. Abstract interface when more models are supported.\n    TRANSCRIPTION_PREAMBLE = \"<|startoftranscript|><|en|><|transcribe|><|notimestamps|>\"\n    skip_long_audios: bool = True\n\n    def sample(\n        self,\n        tokenizer: TokenizerLike,\n        num_requests: int,\n        output_len: int | None = None,\n        request_id_prefix: str = \"\",\n        no_oversample: bool = False,\n        **kwargs,\n    ) -> list:\n        output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN\n        prompt = ASRDataset.TRANSCRIPTION_PREAMBLE\n        prompt_len = len(tokenizer(prompt).input_ids)\n        sampled_requests = []\n        ind = 0\n        skipped = 0\n        for item in self.data:\n            if len(sampled_requests) >= num_requests:\n                break\n            audio = item[\"audio\"]\n            y, sr = audio[\"array\"], audio[\"sampling_rate\"]\n            duration_s = librosa.get_duration(y=y, sr=sr)\n            # Whisper max supported duration\n            if self.skip_long_audios and duration_s > 30:\n                skipped += 1\n                continue\n\n            mm_content = {\"audio\": (y, sr)}\n            sampled_requests.append(\n                SampleRequest(\n                    prompt=prompt,\n                    prompt_len=prompt_len,\n                    expected_output_len=output_len,\n                    multi_modal_data=mm_content,\n                    request_id=request_id_prefix + str(ind),\n                )\n            )\n            ind += 1\n        if skipped:\n            logger.warning(\n                \"%d samples discarded from dataset due to\"\n                \" their length being greater than\"\n                \" what Whisper supports.\",\n                skipped,\n            )\n        self.maybe_oversample_requests(\n            sampled_requests, num_requests, request_id_prefix, no_oversample\n        )\n        return sampled_requests",
      "language": "python"
    },
    {
      "code": "class ASRDataset(HuggingFaceDataset):\n    \"\"\"\n    Dataset class for processing a ASR dataset for transcription.\n    Tested on the following set:\n\n    +----------------+----------------------------------------+--------------------------+-----------------------------+\n    | Dataset        | Domain                                 | Speaking Style           | hf-subset                   |\n    +----------------+----------------------------------------+--------------------------+-----------------------------+\n    | TED-LIUM       | TED talks                              | Oratory                  | release1, release2, release3|\n    |                |                                        |                          | release3-speaker-adaptation |\n    | VoxPopuli      | European Parliament                    | Oratory                  | en, de, it, fr,  ...        |\n    | LibriSpeech    | Audiobook                              | Narrated                 | \"LIUM/tedlium\"              |\n    | GigaSpeech     | Audiobook, podcast, YouTube            | Narrated, spontaneous    | xs, s, m, l, xl, dev, test  |\n    | SPGISpeech     | Financial meetings                     | Oratory, spontaneous     | S, M, L, dev, test          |\n    | AMI            | Meetings                               | Spontaneous              | ihm, sdm                    |\n    +----------------+----------------------------------------+--------------------------+-----------------------------+\n\n    \"\"\"  # noqa: E501\n\n    SUPPORTED_DATASET_PATHS = {\n        \"openslr/librispeech_asr\",\n        \"facebook/voxpopuli\",\n        \"LIUM/tedlium\",\n        \"edinburghcstr/ami\",\n        \"speechcolab/gigaspeech\",\n        \"kensho/spgispeech\",\n    }\n\n    DEFAULT_OUTPUT_LEN = 128\n    IS_MULTIMODAL = True\n\n    # TODO Whisper-specific. Abstract interface when more models are supported.\n    TRANSCRIPTION_PREAMBLE = \"<|startoftranscript|><|en|><|transcribe|><|notimestamps|>\"\n    skip_long_audios: bool = True\n\n    def sample(\n        self,\n        tokenizer: TokenizerLike,\n        num_requests: int,\n        output_len: int | None = None,\n        request_id_prefix: str = \"\",\n        no_oversample: bool = False,\n        **kwargs,\n    ) -> list:\n        output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN\n        prompt = ASRDataset.TRANSCRIPTION_PREAMBLE\n        prompt_len = len(tokenizer(prompt).input_ids)\n        sampled_requests = []\n        ind = 0\n        skipped = 0\n        for item in self.data:\n            if len(sampled_requests) >= num_requests:\n                break\n            audio = item[\"audio\"]\n            y, sr = audio[\"array\"], audio[\"sampling_rate\"]\n            duration_s = librosa.get_duration(y=y, sr=sr)\n            # Whisper max supported duration\n            if self.skip_long_audios and duration_s > 30:\n                skipped += 1\n                continue\n\n            mm_content = {\"audio\": (y, sr)}\n            sampled_requests.append(\n                SampleRequest(\n                    prompt=prompt,\n                    prompt_len=prompt_len,\n                    expected_output_len=output_len,\n                    multi_modal_data=mm_content,\n                    request_id=request_id_prefix + str(ind),\n                )\n            )\n            ind += 1\n        if skipped:\n            logger.warning(\n                \"%d samples discarded from dataset due to\"\n                \" their length being greater than\"\n                \" what Whisper supports.\",\n                skipped,\n            )\n        self.maybe_oversample_requests(\n            sampled_requests, num_requests, request_id_prefix, no_oversample\n        )\n        return sampled_requests",
      "language": "python"
    },
    {
      "code": "DEFAULT_OUTPUT_LEN = 128",
      "language": "unknown"
    },
    {
      "code": "DEFAULT_OUTPUT_LEN = 128",
      "language": "unknown"
    },
    {
      "code": "IS_MULTIMODAL = True",
      "language": "unknown"
    },
    {
      "code": "IS_MULTIMODAL = True",
      "language": "unknown"
    },
    {
      "code": "SUPPORTED_DATASET_PATHS = {\n    \"openslr/librispeech_asr\",\n    \"facebook/voxpopuli\",\n    \"LIUM/tedlium\",\n    \"edinburghcstr/ami\",\n    \"speechcolab/gigaspeech\",\n    \"kensho/spgispeech\",\n}",
      "language": "unknown"
    },
    {
      "code": "SUPPORTED_DATASET_PATHS = {\n    \"openslr/librispeech_asr\",\n    \"facebook/voxpopuli\",\n    \"LIUM/tedlium\",\n    \"edinburghcstr/ami\",\n    \"speechcolab/gigaspeech\",\n    \"kensho/spgispeech\",\n}",
      "language": "unknown"
    },
    {
      "code": "TRANSCRIPTION_PREAMBLE = \"<|startoftranscript|><|en|><|transcribe|><|notimestamps|>\"",
      "language": "unknown"
    },
    {
      "code": "TRANSCRIPTION_PREAMBLE = \"<|startoftranscript|><|en|><|transcribe|><|notimestamps|>\"",
      "language": "unknown"
    },
    {
      "code": "skip_long_audios: bool = True",
      "language": "typescript"
    },
    {
      "code": "skip_long_audios: bool = True",
      "language": "typescript"
    },
    {
      "code": "sample(\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    output_len: int | None = None,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list",
      "language": "typescript"
    },
    {
      "code": "sample(\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    output_len: int | None = None,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list",
      "language": "typescript"
    },
    {
      "code": "2929\n2930\n2931\n2932\n2933\n2934\n2935\n2936\n2937\n2938\n2939\n2940\n2941\n2942\n2943\n2944\n2945\n2946\n2947\n2948\n2949\n2950\n2951\n2952\n2953\n2954\n2955\n2956\n2957\n2958\n2959\n2960\n2961\n2962\n2963\n2964\n2965\n2966\n2967\n2968\n2969\n2970\n2971\n2972\n2973\n2974\n2975\n2976",
      "language": "unknown"
    },
    {
      "code": "def sample(\n    self,\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    output_len: int | None = None,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list:\n    output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN\n    prompt = ASRDataset.TRANSCRIPTION_PREAMBLE\n    prompt_len = len(tokenizer(prompt).input_ids)\n    sampled_requests = []\n    ind = 0\n    skipped = 0\n    for item in self.data:\n        if len(sampled_requests) >= num_requests:\n            break\n        audio = item[\"audio\"]\n        y, sr = audio[\"array\"], audio[\"sampling_rate\"]\n        duration_s = librosa.get_duration(y=y, sr=sr)\n        # Whisper max supported duration\n        if self.skip_long_audios and duration_s > 30:\n            skipped += 1\n            continue\n\n        mm_content = {\"audio\": (y, sr)}\n        sampled_requests.append(\n            SampleRequest(\n                prompt=prompt,\n                prompt_len=prompt_len,\n                expected_output_len=output_len,\n                multi_modal_data=mm_content,\n                request_id=request_id_prefix + str(ind),\n            )\n        )\n        ind += 1\n    if skipped:\n        logger.warning(\n            \"%d samples discarded from dataset due to\"\n            \" their length being greater than\"\n            \" what Whisper supports.\",\n            skipped,\n        )\n    self.maybe_oversample_requests(\n        sampled_requests, num_requests, request_id_prefix, no_oversample\n    )\n    return sampled_requests",
      "language": "python"
    },
    {
      "code": "def sample(\n    self,\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    output_len: int | None = None,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list:\n    output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN\n    prompt = ASRDataset.TRANSCRIPTION_PREAMBLE\n    prompt_len = len(tokenizer(prompt).input_ids)\n    sampled_requests = []\n    ind = 0\n    skipped = 0\n    for item in self.data:\n        if len(sampled_requests) >= num_requests:\n            break\n        audio = item[\"audio\"]\n        y, sr = audio[\"array\"], audio[\"sampling_rate\"]\n        duration_s = librosa.get_duration(y=y, sr=sr)\n        # Whisper max supported duration\n        if self.skip_long_audios and duration_s > 30:\n            skipped += 1\n            continue\n\n        mm_content = {\"audio\": (y, sr)}\n        sampled_requests.append(\n            SampleRequest(\n                prompt=prompt,\n                prompt_len=prompt_len,\n                expected_output_len=output_len,\n                multi_modal_data=mm_content,\n                request_id=request_id_prefix + str(ind),\n            )\n        )\n        ind += 1\n    if skipped:\n        logger.warning(\n            \"%d samples discarded from dataset due to\"\n            \" their length being greater than\"\n            \" what Whisper supports.\",\n            skipped,\n        )\n    self.maybe_oversample_requests(\n        sampled_requests, num_requests, request_id_prefix, no_oversample\n    )\n    return sampled_requests",
      "language": "python"
    },
    {
      "code": "91\n 92\n 93\n 94\n 95\n 96\n 97\n 98\n 99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254",
      "language": "unknown"
    },
    {
      "code": "class BenchmarkDataset(ABC):\n    DEFAULT_SEED = 0\n    IS_MULTIMODAL = False\n\n    def __init__(\n        self,\n        dataset_path: str | None = None,\n        random_seed: int = DEFAULT_SEED,\n        disable_shuffle: bool = False,\n        **kwargs,\n    ) -> None:\n        \"\"\"\n        Initialize the BenchmarkDataset with an optional dataset path and random\n        seed.\n\n        Args:\n            dataset_path (Optional[str]): Path to the dataset. If None, it\n                indicates that a default or random dataset might be used.\n            random_seed (int): Seed value for reproducible shuffling or\n                sampling. Defaults to DEFAULT_SEED.\n        \"\"\"\n        self.dataset_path = dataset_path\n        # Set the random seed, ensuring that a None value is replaced with the\n        # default seed.\n        self.random_seed = random_seed if random_seed is not None else self.DEFAULT_SEED\n        self.disable_shuffle = disable_shuffle\n        self.data = None\n\n    def apply_multimodal_chat_transformation(\n        self,\n        prompt: str,\n        mm_content: MultiModalDataDict | dict | list[dict] | None = None,\n    ) -> list[dict]:\n        \"\"\"\n        Transform a prompt and optional multimodal content into a chat format.\n        This method is used for chat models that expect a specific conversation\n        format.\n        \"\"\"\n        content = [{\"text\": prompt, \"type\": \"text\"}]\n        if mm_content is not None:\n            if isinstance(mm_content, list):\n                content.extend(cast(list[dict[str, Any]], mm_content))\n            elif isinstance(mm_content, dict):\n                content.append(mm_content)\n            else:\n                raise TypeError(\n                    \"Could not process multimodal content of type: \"\n                    + f\"{type(mm_content)}\"\n                )\n        return [{\"role\": \"user\", \"content\": content}]\n\n    def load_data(self) -> None:\n        \"\"\"\n        Load data from the dataset path into self.data.\n\n        This method must be overridden by subclasses since the method to load\n        data will vary depending on the dataset format and source.\n\n        Raises:\n            NotImplementedError: If a subclass does not implement this method.\n        \"\"\"\n        # TODO (jenniferzhao): add support for downloading data\n        raise NotImplementedError(\"load_data must be implemented in subclasses.\")\n\n    def get_random_lora_request(\n        self,\n        max_loras: int | None = None,\n        lora_path: str | None = None,\n    ) -> LoRARequest | None:\n        \"\"\"\n        Optionally select a random LoRA request.\n\n        This method is used when LoRA parameters are provided.  It randomly\n        selects a LoRA based on max_loras.\n\n        Args:\n            max_loras (Optional[int]): The maximum number of LoRAs available.\n                If `None`, LoRA is not used.\n            lora_path (Optional[str]): Path to the LoRA parameters on disk.\n                If `None`, LoRA is not used.\n\n        Returns:\n            A new [`LoRARequest`][vllm.lora.request.LoRARequest]\n            (or `None` if not applicable).\n        \"\"\"\n        if max_loras is None or lora_path is None:\n            return None\n\n        # Generate a random LoRA ID in the range [1, max_loras].\n        lora_id = random.randint(1, max_loras)\n        lora_request = LoRARequest(\n            lora_name=str(lora_id),\n            lora_int_id=lora_id,\n            lora_path=lora_path_on_disk(lora_path),\n        )\n        return lora_request\n\n    @abstractmethod\n    def sample(\n        self,\n        tokenizer: TokenizerLike,\n        num_requests: int,\n        request_id_prefix: str = \"\",\n        no_oversample: bool = False,\n    ) -> list[SampleRequest]:\n        \"\"\"\n        Abstract method to generate sample requests from the dataset.\n\n        Subclasses must override this method to implement dataset-specific logic\n        for generating a list of SampleRequest objects.\n\n        Args:\n            tokenizer (TokenizerLike): The tokenizer to be used\n                for processing the dataset's text.\n            num_requests (int): The number of sample requests to generate.\n            request_id_prefix (str): The prefix of request_id.\n\n        Returns:\n            list[SampleRequest]: A list of sample requests generated from the\n            dataset.\n        \"\"\"\n        raise NotImplementedError(\"sample must be implemented in subclasses.\")\n\n    def maybe_oversample_requests(\n        self,\n        requests: list[SampleRequest],\n        num_requests: int,\n        request_id_prefix: str = \"\",\n        no_oversample: bool = False,\n    ) -> None:\n        \"\"\"\n        Oversamples the list of requests if its size is less than the desired\n        number.\n\n        Args:\n            requests (List[SampleRequest]): The current list of sampled\n                requests.\n            num_requests (int): The target number of requests.\n            request_id_prefix (str): The prefix applied to generated request\n                identifiers.\n\n        \"\"\"\n        if no_oversample:\n            logger.info(\"Skipping oversampling. Total samples: %d.\", len(requests))\n            return\n\n        if len(requests) < num_requests:\n            random.seed(self.random_seed)\n            needed = num_requests - len(requests)\n            additional = []\n            for i in range(needed):\n                req = deepcopy(random.choice(requests))\n                req.request_id = request_id_prefix + str(len(requests) + i)\n                additional.append(req)\n            requests.extend(additional)\n            logger.info(\"Oversampled requests to reach %d total samples.\", num_requests)\n\n        ids = [req.request_id for req in requests]\n        if len(ids) != len(set(ids)):\n            raise ValueError(\n                \"Duplicate request_id found in the sampled \"\n                \"requests. Please ensure that each request_id \"\n                \"is unique.\"\n            )",
      "language": "python"
    },
    {
      "code": "class BenchmarkDataset(ABC):\n    DEFAULT_SEED = 0\n    IS_MULTIMODAL = False\n\n    def __init__(\n        self,\n        dataset_path: str | None = None,\n        random_seed: int = DEFAULT_SEED,\n        disable_shuffle: bool = False,\n        **kwargs,\n    ) -> None:\n        \"\"\"\n        Initialize the BenchmarkDataset with an optional dataset path and random\n        seed.\n\n        Args:\n            dataset_path (Optional[str]): Path to the dataset. If None, it\n                indicates that a default or random dataset might be used.\n            random_seed (int): Seed value for reproducible shuffling or\n                sampling. Defaults to DEFAULT_SEED.\n        \"\"\"\n        self.dataset_path = dataset_path\n        # Set the random seed, ensuring that a None value is replaced with the\n        # default seed.\n        self.random_seed = random_seed if random_seed is not None else self.DEFAULT_SEED\n        self.disable_shuffle = disable_shuffle\n        self.data = None\n\n    def apply_multimodal_chat_transformation(\n        self,\n        prompt: str,\n        mm_content: MultiModalDataDict | dict | list[dict] | None = None,\n    ) -> list[dict]:\n        \"\"\"\n        Transform a prompt and optional multimodal content into a chat format.\n        This method is used for chat models that expect a specific conversation\n        format.\n        \"\"\"\n        content = [{\"text\": prompt, \"type\": \"text\"}]\n        if mm_content is not None:\n            if isinstance(mm_content, list):\n                content.extend(cast(list[dict[str, Any]], mm_content))\n            elif isinstance(mm_content, dict):\n                content.append(mm_content)\n            else:\n                raise TypeError(\n                    \"Could not process multimodal content of type: \"\n                    + f\"{type(mm_content)}\"\n                )\n        return [{\"role\": \"user\", \"content\": content}]\n\n    def load_data(self) -> None:\n        \"\"\"\n        Load data from the dataset path into self.data.\n\n        This method must be overridden by subclasses since the method to load\n        data will vary depending on the dataset format and source.\n\n        Raises:\n            NotImplementedError: If a subclass does not implement this method.\n        \"\"\"\n        # TODO (jenniferzhao): add support for downloading data\n        raise NotImplementedError(\"load_data must be implemented in subclasses.\")\n\n    def get_random_lora_request(\n        self,\n        max_loras: int | None = None,\n        lora_path: str | None = None,\n    ) -> LoRARequest | None:\n        \"\"\"\n        Optionally select a random LoRA request.\n\n        This method is used when LoRA parameters are provided.  It randomly\n        selects a LoRA based on max_loras.\n\n        Args:\n            max_loras (Optional[int]): The maximum number of LoRAs available.\n                If `None`, LoRA is not used.\n            lora_path (Optional[str]): Path to the LoRA parameters on disk.\n                If `None`, LoRA is not used.\n\n        Returns:\n            A new [`LoRARequest`][vllm.lora.request.LoRARequest]\n            (or `None` if not applicable).\n        \"\"\"\n        if max_loras is None or lora_path is None:\n            return None\n\n        # Generate a random LoRA ID in the range [1, max_loras].\n        lora_id = random.randint(1, max_loras)\n        lora_request = LoRARequest(\n            lora_name=str(lora_id),\n            lora_int_id=lora_id,\n            lora_path=lora_path_on_disk(lora_path),\n        )\n        return lora_request\n\n    @abstractmethod\n    def sample(\n        self,\n        tokenizer: TokenizerLike,\n        num_requests: int,\n        request_id_prefix: str = \"\",\n        no_oversample: bool = False,\n    ) -> list[SampleRequest]:\n        \"\"\"\n        Abstract method to generate sample requests from the dataset.\n\n        Subclasses must override this method to implement dataset-specific logic\n        for generating a list of SampleRequest objects.\n\n        Args:\n            tokenizer (TokenizerLike): The tokenizer to be used\n                for processing the dataset's text.\n            num_requests (int): The number of sample requests to generate.\n            request_id_prefix (str): The prefix of request_id.\n\n        Returns:\n            list[SampleRequest]: A list of sample requests generated from the\n            dataset.\n        \"\"\"\n        raise NotImplementedError(\"sample must be implemented in subclasses.\")\n\n    def maybe_oversample_requests(\n        self,\n        requests: list[SampleRequest],\n        num_requests: int,\n        request_id_prefix: str = \"\",\n        no_oversample: bool = False,\n    ) -> None:\n        \"\"\"\n        Oversamples the list of requests if its size is less than the desired\n        number.\n\n        Args:\n            requests (List[SampleRequest]): The current list of sampled\n                requests.\n            num_requests (int): The target number of requests.\n            request_id_prefix (str): The prefix applied to generated request\n                identifiers.\n\n        \"\"\"\n        if no_oversample:\n            logger.info(\"Skipping oversampling. Total samples: %d.\", len(requests))\n            return\n\n        if len(requests) < num_requests:\n            random.seed(self.random_seed)\n            needed = num_requests - len(requests)\n            additional = []\n            for i in range(needed):\n                req = deepcopy(random.choice(requests))\n                req.request_id = request_id_prefix + str(len(requests) + i)\n                additional.append(req)\n            requests.extend(additional)\n            logger.info(\"Oversampled requests to reach %d total samples.\", num_requests)\n\n        ids = [req.request_id for req in requests]\n        if len(ids) != len(set(ids)):\n            raise ValueError(\n                \"Duplicate request_id found in the sampled \"\n                \"requests. Please ensure that each request_id \"\n                \"is unique.\"\n            )",
      "language": "python"
    },
    {
      "code": "DEFAULT_SEED = 0",
      "language": "unknown"
    },
    {
      "code": "DEFAULT_SEED = 0",
      "language": "unknown"
    },
    {
      "code": "IS_MULTIMODAL = False",
      "language": "unknown"
    },
    {
      "code": "IS_MULTIMODAL = False",
      "language": "unknown"
    },
    {
      "code": "data = None",
      "language": "rust"
    },
    {
      "code": "data = None",
      "language": "rust"
    },
    {
      "code": "dataset_path = dataset_path",
      "language": "unknown"
    },
    {
      "code": "dataset_path = dataset_path",
      "language": "unknown"
    },
    {
      "code": "disable_shuffle = disable_shuffle",
      "language": "unknown"
    },
    {
      "code": "disable_shuffle = disable_shuffle",
      "language": "unknown"
    },
    {
      "code": "random_seed = (\n    random_seed if random_seed is not None else DEFAULT_SEED\n)",
      "language": "rust"
    },
    {
      "code": "random_seed = (\n    random_seed if random_seed is not None else DEFAULT_SEED\n)",
      "language": "rust"
    },
    {
      "code": "__init__(\n    dataset_path: str | None = None,\n    random_seed: int = DEFAULT_SEED,\n    disable_shuffle: bool = False,\n    **kwargs,\n) -> None",
      "language": "typescript"
    },
    {
      "code": "__init__(\n    dataset_path: str | None = None,\n    random_seed: int = DEFAULT_SEED,\n    disable_shuffle: bool = False,\n    **kwargs,\n) -> None",
      "language": "typescript"
    },
    {
      "code": "95\n 96\n 97\n 98\n 99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117",
      "language": "unknown"
    },
    {
      "code": "def __init__(\n    self,\n    dataset_path: str | None = None,\n    random_seed: int = DEFAULT_SEED,\n    disable_shuffle: bool = False,\n    **kwargs,\n) -> None:\n    \"\"\"\n    Initialize the BenchmarkDataset with an optional dataset path and random\n    seed.\n\n    Args:\n        dataset_path (Optional[str]): Path to the dataset. If None, it\n            indicates that a default or random dataset might be used.\n        random_seed (int): Seed value for reproducible shuffling or\n            sampling. Defaults to DEFAULT_SEED.\n    \"\"\"\n    self.dataset_path = dataset_path\n    # Set the random seed, ensuring that a None value is replaced with the\n    # default seed.\n    self.random_seed = random_seed if random_seed is not None else self.DEFAULT_SEED\n    self.disable_shuffle = disable_shuffle\n    self.data = None",
      "language": "python"
    },
    {
      "code": "def __init__(\n    self,\n    dataset_path: str | None = None,\n    random_seed: int = DEFAULT_SEED,\n    disable_shuffle: bool = False,\n    **kwargs,\n) -> None:\n    \"\"\"\n    Initialize the BenchmarkDataset with an optional dataset path and random\n    seed.\n\n    Args:\n        dataset_path (Optional[str]): Path to the dataset. If None, it\n            indicates that a default or random dataset might be used.\n        random_seed (int): Seed value for reproducible shuffling or\n            sampling. Defaults to DEFAULT_SEED.\n    \"\"\"\n    self.dataset_path = dataset_path\n    # Set the random seed, ensuring that a None value is replaced with the\n    # default seed.\n    self.random_seed = random_seed if random_seed is not None else self.DEFAULT_SEED\n    self.disable_shuffle = disable_shuffle\n    self.data = None",
      "language": "python"
    },
    {
      "code": "apply_multimodal_chat_transformation(\n    prompt: str,\n    mm_content: MultiModalDataDict\n    | dict\n    | list[dict]\n    | None = None,\n) -> list[dict]",
      "language": "rust"
    },
    {
      "code": "apply_multimodal_chat_transformation(\n    prompt: str,\n    mm_content: MultiModalDataDict\n    | dict\n    | list[dict]\n    | None = None,\n) -> list[dict]",
      "language": "rust"
    },
    {
      "code": "119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140",
      "language": "unknown"
    },
    {
      "code": "def apply_multimodal_chat_transformation(\n    self,\n    prompt: str,\n    mm_content: MultiModalDataDict | dict | list[dict] | None = None,\n) -> list[dict]:\n    \"\"\"\n    Transform a prompt and optional multimodal content into a chat format.\n    This method is used for chat models that expect a specific conversation\n    format.\n    \"\"\"\n    content = [{\"text\": prompt, \"type\": \"text\"}]\n    if mm_content is not None:\n        if isinstance(mm_content, list):\n            content.extend(cast(list[dict[str, Any]], mm_content))\n        elif isinstance(mm_content, dict):\n            content.append(mm_content)\n        else:\n            raise TypeError(\n                \"Could not process multimodal content of type: \"\n                + f\"{type(mm_content)}\"\n            )\n    return [{\"role\": \"user\", \"content\": content}]",
      "language": "json"
    },
    {
      "code": "def apply_multimodal_chat_transformation(\n    self,\n    prompt: str,\n    mm_content: MultiModalDataDict | dict | list[dict] | None = None,\n) -> list[dict]:\n    \"\"\"\n    Transform a prompt and optional multimodal content into a chat format.\n    This method is used for chat models that expect a specific conversation\n    format.\n    \"\"\"\n    content = [{\"text\": prompt, \"type\": \"text\"}]\n    if mm_content is not None:\n        if isinstance(mm_content, list):\n            content.extend(cast(list[dict[str, Any]], mm_content))\n        elif isinstance(mm_content, dict):\n            content.append(mm_content)\n        else:\n            raise TypeError(\n                \"Could not process multimodal content of type: \"\n                + f\"{type(mm_content)}\"\n            )\n    return [{\"role\": \"user\", \"content\": content}]",
      "language": "json"
    },
    {
      "code": "get_random_lora_request(\n    max_loras: int | None = None,\n    lora_path: str | None = None,\n) -> LoRARequest | None",
      "language": "rust"
    },
    {
      "code": "get_random_lora_request(\n    max_loras: int | None = None,\n    lora_path: str | None = None,\n) -> LoRARequest | None",
      "language": "rust"
    },
    {
      "code": "155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186",
      "language": "unknown"
    },
    {
      "code": "def get_random_lora_request(\n    self,\n    max_loras: int | None = None,\n    lora_path: str | None = None,\n) -> LoRARequest | None:\n    \"\"\"\n    Optionally select a random LoRA request.\n\n    This method is used when LoRA parameters are provided.  It randomly\n    selects a LoRA based on max_loras.\n\n    Args:\n        max_loras (Optional[int]): The maximum number of LoRAs available.\n            If `None`, LoRA is not used.\n        lora_path (Optional[str]): Path to the LoRA parameters on disk.\n            If `None`, LoRA is not used.\n\n    Returns:\n        A new [`LoRARequest`][vllm.lora.request.LoRARequest]\n        (or `None` if not applicable).\n    \"\"\"\n    if max_loras is None or lora_path is None:\n        return None\n\n    # Generate a random LoRA ID in the range [1, max_loras].\n    lora_id = random.randint(1, max_loras)\n    lora_request = LoRARequest(\n        lora_name=str(lora_id),\n        lora_int_id=lora_id,\n        lora_path=lora_path_on_disk(lora_path),\n    )\n    return lora_request",
      "language": "python"
    },
    {
      "code": "def get_random_lora_request(\n    self,\n    max_loras: int | None = None,\n    lora_path: str | None = None,\n) -> LoRARequest | None:\n    \"\"\"\n    Optionally select a random LoRA request.\n\n    This method is used when LoRA parameters are provided.  It randomly\n    selects a LoRA based on max_loras.\n\n    Args:\n        max_loras (Optional[int]): The maximum number of LoRAs available.\n            If `None`, LoRA is not used.\n        lora_path (Optional[str]): Path to the LoRA parameters on disk.\n            If `None`, LoRA is not used.\n\n    Returns:\n        A new [`LoRARequest`][vllm.lora.request.LoRARequest]\n        (or `None` if not applicable).\n    \"\"\"\n    if max_loras is None or lora_path is None:\n        return None\n\n    # Generate a random LoRA ID in the range [1, max_loras].\n    lora_id = random.randint(1, max_loras)\n    lora_request = LoRARequest(\n        lora_name=str(lora_id),\n        lora_int_id=lora_id,\n        lora_path=lora_path_on_disk(lora_path),\n    )\n    return lora_request",
      "language": "python"
    },
    {
      "code": "load_data() -> None",
      "language": "rust"
    },
    {
      "code": "load_data() -> None",
      "language": "rust"
    },
    {
      "code": "142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153",
      "language": "unknown"
    },
    {
      "code": "def load_data(self) -> None:\n    \"\"\"\n    Load data from the dataset path into self.data.\n\n    This method must be overridden by subclasses since the method to load\n    data will vary depending on the dataset format and source.\n\n    Raises:\n        NotImplementedError: If a subclass does not implement this method.\n    \"\"\"\n    # TODO (jenniferzhao): add support for downloading data\n    raise NotImplementedError(\"load_data must be implemented in subclasses.\")",
      "language": "python"
    },
    {
      "code": "def load_data(self) -> None:\n    \"\"\"\n    Load data from the dataset path into self.data.\n\n    This method must be overridden by subclasses since the method to load\n    data will vary depending on the dataset format and source.\n\n    Raises:\n        NotImplementedError: If a subclass does not implement this method.\n    \"\"\"\n    # TODO (jenniferzhao): add support for downloading data\n    raise NotImplementedError(\"load_data must be implemented in subclasses.\")",
      "language": "python"
    },
    {
      "code": "maybe_oversample_requests(\n    requests: list[SampleRequest],\n    num_requests: int,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n) -> None",
      "language": "typescript"
    },
    {
      "code": "maybe_oversample_requests(\n    requests: list[SampleRequest],\n    num_requests: int,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n) -> None",
      "language": "typescript"
    },
    {
      "code": "214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254",
      "language": "unknown"
    },
    {
      "code": "def maybe_oversample_requests(\n    self,\n    requests: list[SampleRequest],\n    num_requests: int,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n) -> None:\n    \"\"\"\n    Oversamples the list of requests if its size is less than the desired\n    number.\n\n    Args:\n        requests (List[SampleRequest]): The current list of sampled\n            requests.\n        num_requests (int): The target number of requests.\n        request_id_prefix (str): The prefix applied to generated request\n            identifiers.\n\n    \"\"\"\n    if no_oversample:\n        logger.info(\"Skipping oversampling. Total samples: %d.\", len(requests))\n        return\n\n    if len(requests) < num_requests:\n        random.seed(self.random_seed)\n        needed = num_requests - len(requests)\n        additional = []\n        for i in range(needed):\n            req = deepcopy(random.choice(requests))\n            req.request_id = request_id_prefix + str(len(requests) + i)\n            additional.append(req)\n        requests.extend(additional)\n        logger.info(\"Oversampled requests to reach %d total samples.\", num_requests)\n\n    ids = [req.request_id for req in requests]\n    if len(ids) != len(set(ids)):\n        raise ValueError(\n            \"Duplicate request_id found in the sampled \"\n            \"requests. Please ensure that each request_id \"\n            \"is unique.\"\n        )",
      "language": "python"
    },
    {
      "code": "def maybe_oversample_requests(\n    self,\n    requests: list[SampleRequest],\n    num_requests: int,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n) -> None:\n    \"\"\"\n    Oversamples the list of requests if its size is less than the desired\n    number.\n\n    Args:\n        requests (List[SampleRequest]): The current list of sampled\n            requests.\n        num_requests (int): The target number of requests.\n        request_id_prefix (str): The prefix applied to generated request\n            identifiers.\n\n    \"\"\"\n    if no_oversample:\n        logger.info(\"Skipping oversampling. Total samples: %d.\", len(requests))\n        return\n\n    if len(requests) < num_requests:\n        random.seed(self.random_seed)\n        needed = num_requests - len(requests)\n        additional = []\n        for i in range(needed):\n            req = deepcopy(random.choice(requests))\n            req.request_id = request_id_prefix + str(len(requests) + i)\n            additional.append(req)\n        requests.extend(additional)\n        logger.info(\"Oversampled requests to reach %d total samples.\", num_requests)\n\n    ids = [req.request_id for req in requests]\n    if len(ids) != len(set(ids)):\n        raise ValueError(\n            \"Duplicate request_id found in the sampled \"\n            \"requests. Please ensure that each request_id \"\n            \"is unique.\"\n        )",
      "language": "python"
    },
    {
      "code": "sample(\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n) -> list[SampleRequest]",
      "language": "typescript"
    },
    {
      "code": "sample(\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n) -> list[SampleRequest]",
      "language": "typescript"
    },
    {
      "code": "188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212",
      "language": "unknown"
    },
    {
      "code": "@abstractmethod\ndef sample(\n    self,\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n) -> list[SampleRequest]:\n    \"\"\"\n    Abstract method to generate sample requests from the dataset.\n\n    Subclasses must override this method to implement dataset-specific logic\n    for generating a list of SampleRequest objects.\n\n    Args:\n        tokenizer (TokenizerLike): The tokenizer to be used\n            for processing the dataset's text.\n        num_requests (int): The number of sample requests to generate.\n        request_id_prefix (str): The prefix of request_id.\n\n    Returns:\n        list[SampleRequest]: A list of sample requests generated from the\n        dataset.\n    \"\"\"\n    raise NotImplementedError(\"sample must be implemented in subclasses.\")",
      "language": "python"
    },
    {
      "code": "@abstractmethod\ndef sample(\n    self,\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n) -> list[SampleRequest]:\n    \"\"\"\n    Abstract method to generate sample requests from the dataset.\n\n    Subclasses must override this method to implement dataset-specific logic\n    for generating a list of SampleRequest objects.\n\n    Args:\n        tokenizer (TokenizerLike): The tokenizer to be used\n            for processing the dataset's text.\n        num_requests (int): The number of sample requests to generate.\n        request_id_prefix (str): The prefix of request_id.\n\n    Returns:\n        list[SampleRequest]: A list of sample requests generated from the\n        dataset.\n    \"\"\"\n    raise NotImplementedError(\"sample must be implemented in subclasses.\")",
      "language": "python"
    },
    {
      "code": "2647\n2648\n2649\n2650\n2651\n2652\n2653\n2654\n2655\n2656\n2657\n2658\n2659\n2660\n2661\n2662\n2663\n2664\n2665\n2666\n2667\n2668\n2669\n2670\n2671\n2672\n2673\n2674\n2675\n2676\n2677\n2678\n2679\n2680\n2681\n2682\n2683\n2684\n2685\n2686\n2687\n2688\n2689\n2690\n2691\n2692\n2693\n2694\n2695\n2696\n2697\n2698\n2699\n2700\n2701\n2702\n2703\n2704\n2705\n2706\n2707\n2708\n2709\n2710\n2711\n2712\n2713\n2714\n2715\n2716\n2717\n2718\n2719\n2720\n2721\n2722\n2723\n2724\n2725\n2726\n2727\n2728",
      "language": "unknown"
    },
    {
      "code": "class BlazeditDataset(HuggingFaceDataset):\n    \"\"\"\n    Blazedit Dataset.\n    https://github.com/ise-uiuc/blazedit\n\n    5k char version: vdaita/edit_5k_char\n    10k char version: vdaita/edit_10k_char\n    \"\"\"  # noqa: E501\n\n    # 5k char version will have output as ~5k chars\n    # 10k char version will have output as ~10k chars\n    # Assuming 3 char per token, 10k chars will be 3333 tokens\n    # We set default to 4000 to be safe\n    DEFAULT_OUTPUT_LEN = 4000\n    SUPPORTED_DATASET_PATHS = {\n        \"vdaita/edit_5k_char\",\n        \"vdaita/edit_10k_char\",\n    }\n\n    def sample(\n        self,\n        tokenizer: TokenizerLike,\n        num_requests: int,\n        output_len: int | None = None,\n        skip_chat_template: bool = False,\n        request_id_prefix: str = \"\",\n        no_oversample: bool = False,\n        min_distance: float = 0.0,\n        max_distance: float = 1.0,\n        **kwargs,\n    ) -> list:\n        output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN\n        sampled_requests = []\n\n        for i, item in enumerate(self.data):\n            if len(sampled_requests) >= num_requests:\n                break\n            code = item[\"code\"]\n            change_request = item[\"change_request\"]\n            norm_distance = item[\"norm_distance\"]\n\n            # compare the levenshtein distance normalized by code length\n            if norm_distance < min_distance or norm_distance > max_distance:\n                continue\n\n            # template copied from\n            # https://github.com/ise-uiuc/blazedit/blob/7765137e656fd62de877422d2e4cf8de51228054/dataset/create_refined_dataset.py#L94-L105 # noqa: E501\n            prompt = f\"\"\"Given a code file, please apply the change requests and generate the new file.\n\nOriginal file:\n```python\n{code}\n```\n\nChange request:\n{change_request}\n\nPlease generate the new code file in the \"New file\" section below.\"\"\"  # noqa: E501\n\n            # apply template\n            if not skip_chat_template:\n                prompt = tokenizer.apply_chat_template(\n                    [{\"role\": \"user\", \"content\": prompt}],\n                    add_generation_prompt=True,\n                    tokenize=False,\n                )\n\n            prompt_len = len(tokenizer(prompt).input_ids)\n\n            sampled_requests.append(\n                SampleRequest(\n                    prompt=prompt,\n                    prompt_len=prompt_len,\n                    expected_output_len=output_len,\n                    request_id=request_id_prefix + str(i),\n                )\n            )\n        self.maybe_oversample_requests(\n            sampled_requests, num_requests, request_id_prefix, no_oversample\n        )\n\n        return sampled_requests",
      "language": "json"
    },
    {
      "code": "class BlazeditDataset(HuggingFaceDataset):\n    \"\"\"\n    Blazedit Dataset.\n    https://github.com/ise-uiuc/blazedit\n\n    5k char version: vdaita/edit_5k_char\n    10k char version: vdaita/edit_10k_char\n    \"\"\"  # noqa: E501\n\n    # 5k char version will have output as ~5k chars\n    # 10k char version will have output as ~10k chars\n    # Assuming 3 char per token, 10k chars will be 3333 tokens\n    # We set default to 4000 to be safe\n    DEFAULT_OUTPUT_LEN = 4000\n    SUPPORTED_DATASET_PATHS = {\n        \"vdaita/edit_5k_char\",\n        \"vdaita/edit_10k_char\",\n    }\n\n    def sample(\n        self,\n        tokenizer: TokenizerLike,\n        num_requests: int,\n        output_len: int | None = None,\n        skip_chat_template: bool = False,\n        request_id_prefix: str = \"\",\n        no_oversample: bool = False,\n        min_distance: float = 0.0,\n        max_distance: float = 1.0,\n        **kwargs,\n    ) -> list:\n        output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN\n        sampled_requests = []\n\n        for i, item in enumerate(self.data):\n            if len(sampled_requests) >= num_requests:\n                break\n            code = item[\"code\"]\n            change_request = item[\"change_request\"]\n            norm_distance = item[\"norm_distance\"]\n\n            # compare the levenshtein distance normalized by code length\n            if norm_distance < min_distance or norm_distance > max_distance:\n                continue\n\n            # template copied from\n            # https://github.com/ise-uiuc/blazedit/blob/7765137e656fd62de877422d2e4cf8de51228054/dataset/create_refined_dataset.py#L94-L105 # noqa: E501\n            prompt = f\"\"\"Given a code file, please apply the change requests and generate the new file.\n\nOriginal file:\n```python\n{code}\n```\n\nChange request:\n{change_request}\n\nPlease generate the new code file in the \"New file\" section below.\"\"\"  # noqa: E501\n\n            # apply template\n            if not skip_chat_template:\n                prompt = tokenizer.apply_chat_template(\n                    [{\"role\": \"user\", \"content\": prompt}],\n                    add_generation_prompt=True,\n                    tokenize=False,\n                )\n\n            prompt_len = len(tokenizer(prompt).input_ids)\n\n            sampled_requests.append(\n                SampleRequest(\n                    prompt=prompt,\n                    prompt_len=prompt_len,\n                    expected_output_len=output_len,\n                    request_id=request_id_prefix + str(i),\n                )\n            )\n        self.maybe_oversample_requests(\n            sampled_requests, num_requests, request_id_prefix, no_oversample\n        )\n\n        return sampled_requests",
      "language": "json"
    },
    {
      "code": "DEFAULT_OUTPUT_LEN = 4000",
      "language": "unknown"
    },
    {
      "code": "DEFAULT_OUTPUT_LEN = 4000",
      "language": "unknown"
    },
    {
      "code": "SUPPORTED_DATASET_PATHS = {\n    \"vdaita/edit_5k_char\",\n    \"vdaita/edit_10k_char\",\n}",
      "language": "unknown"
    },
    {
      "code": "SUPPORTED_DATASET_PATHS = {\n    \"vdaita/edit_5k_char\",\n    \"vdaita/edit_10k_char\",\n}",
      "language": "unknown"
    },
    {
      "code": "sample(\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    output_len: int | None = None,\n    skip_chat_template: bool = False,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    min_distance: float = 0.0,\n    max_distance: float = 1.0,\n    **kwargs,\n) -> list",
      "language": "typescript"
    },
    {
      "code": "sample(\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    output_len: int | None = None,\n    skip_chat_template: bool = False,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    min_distance: float = 0.0,\n    max_distance: float = 1.0,\n    **kwargs,\n) -> list",
      "language": "typescript"
    },
    {
      "code": "2666\n2667\n2668\n2669\n2670\n2671\n2672\n2673\n2674\n2675\n2676\n2677\n2678\n2679\n2680\n2681\n2682\n2683\n2684\n2685\n2686\n2687\n2688\n2689\n2690\n2691\n2692\n2693\n2694\n2695\n2696\n2697\n2698\n2699\n2700\n2701\n2702\n2703\n2704\n2705\n2706\n2707\n2708\n2709\n2710\n2711\n2712\n2713\n2714\n2715\n2716\n2717\n2718\n2719\n2720\n2721\n2722\n2723\n2724\n2725\n2726\n2727\n2728",
      "language": "unknown"
    },
    {
      "code": "def sample(\n        self,\n        tokenizer: TokenizerLike,\n        num_requests: int,\n        output_len: int | None = None,\n        skip_chat_template: bool = False,\n        request_id_prefix: str = \"\",\n        no_oversample: bool = False,\n        min_distance: float = 0.0,\n        max_distance: float = 1.0,\n        **kwargs,\n    ) -> list:\n        output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN\n        sampled_requests = []\n\n        for i, item in enumerate(self.data):\n            if len(sampled_requests) >= num_requests:\n                break\n            code = item[\"code\"]\n            change_request = item[\"change_request\"]\n            norm_distance = item[\"norm_distance\"]\n\n            # compare the levenshtein distance normalized by code length\n            if norm_distance < min_distance or norm_distance > max_distance:\n                continue\n\n            # template copied from\n            # https://github.com/ise-uiuc/blazedit/blob/7765137e656fd62de877422d2e4cf8de51228054/dataset/create_refined_dataset.py#L94-L105 # noqa: E501\n            prompt = f\"\"\"Given a code file, please apply the change requests and generate the new file.\n\nOriginal file:\n```python\n{code}\n```\n\nChange request:\n{change_request}\n\nPlease generate the new code file in the \"New file\" section below.\"\"\"  # noqa: E501\n\n            # apply template\n            if not skip_chat_template:\n                prompt = tokenizer.apply_chat_template(\n                    [{\"role\": \"user\", \"content\": prompt}],\n                    add_generation_prompt=True,\n                    tokenize=False,\n                )\n\n            prompt_len = len(tokenizer(prompt).input_ids)\n\n            sampled_requests.append(\n                SampleRequest(\n                    prompt=prompt,\n                    prompt_len=prompt_len,\n                    expected_output_len=output_len,\n                    request_id=request_id_prefix + str(i),\n                )\n            )\n        self.maybe_oversample_requests(\n            sampled_requests, num_requests, request_id_prefix, no_oversample\n        )\n\n        return sampled_requests",
      "language": "json"
    },
    {
      "code": "def sample(\n        self,\n        tokenizer: TokenizerLike,\n        num_requests: int,\n        output_len: int | None = None,\n        skip_chat_template: bool = False,\n        request_id_prefix: str = \"\",\n        no_oversample: bool = False,\n        min_distance: float = 0.0,\n        max_distance: float = 1.0,\n        **kwargs,\n    ) -> list:\n        output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN\n        sampled_requests = []\n\n        for i, item in enumerate(self.data):\n            if len(sampled_requests) >= num_requests:\n                break\n            code = item[\"code\"]\n            change_request = item[\"change_request\"]\n            norm_distance = item[\"norm_distance\"]\n\n            # compare the levenshtein distance normalized by code length\n            if norm_distance < min_distance or norm_distance > max_distance:\n                continue\n\n            # template copied from\n            # https://github.com/ise-uiuc/blazedit/blob/7765137e656fd62de877422d2e4cf8de51228054/dataset/create_refined_dataset.py#L94-L105 # noqa: E501\n            prompt = f\"\"\"Given a code file, please apply the change requests and generate the new file.\n\nOriginal file:\n```python\n{code}\n```\n\nChange request:\n{change_request}\n\nPlease generate the new code file in the \"New file\" section below.\"\"\"  # noqa: E501\n\n            # apply template\n            if not skip_chat_template:\n                prompt = tokenizer.apply_chat_template(\n                    [{\"role\": \"user\", \"content\": prompt}],\n                    add_generation_prompt=True,\n                    tokenize=False,\n                )\n\n            prompt_len = len(tokenizer(prompt).input_ids)\n\n            sampled_requests.append(\n                SampleRequest(\n                    prompt=prompt,\n                    prompt_len=prompt_len,\n                    expected_output_len=output_len,\n                    request_id=request_id_prefix + str(i),\n                )\n            )\n        self.maybe_oversample_requests(\n            sampled_requests, num_requests, request_id_prefix, no_oversample\n        )\n\n        return sampled_requests",
      "language": "json"
    },
    {
      "code": "2170\n2171\n2172\n2173\n2174\n2175\n2176\n2177\n2178\n2179\n2180\n2181\n2182\n2183\n2184\n2185\n2186\n2187\n2188\n2189\n2190\n2191\n2192\n2193\n2194\n2195\n2196\n2197\n2198\n2199\n2200\n2201\n2202\n2203\n2204\n2205\n2206\n2207\n2208\n2209\n2210\n2211\n2212\n2213\n2214\n2215\n2216\n2217\n2218\n2219\n2220\n2221\n2222\n2223\n2224\n2225\n2226\n2227\n2228\n2229\n2230\n2231\n2232\n2233\n2234\n2235\n2236\n2237\n2238\n2239",
      "language": "unknown"
    },
    {
      "code": "class BurstGPTDataset(BenchmarkDataset):\n    \"\"\"\n    Implements the BurstGPT dataset.  Loads data from a CSV file and generates\n    sample requests based on synthetic prompt generation. Only rows with Model\n    \"GPT-4\" and positive response tokens are used.\n    \"\"\"\n\n    def __init__(self, **kwargs) -> None:\n        super().__init__(**kwargs)\n        self.load_data()\n\n    def load_data(\n        self,\n    ):\n        if self.dataset_path is None:\n            raise ValueError(\"dataset_path must be provided for loading data.\")\n\n        df = pd.read_csv(self.dataset_path)\n        # Filter to keep only GPT-4 rows.\n        gpt4_df = df[df[\"Model\"] == \"GPT-4\"]\n        # Remove failed requests (where Response tokens is 0 or less).\n        gpt4_df = gpt4_df[gpt4_df[\"Response tokens\"] > 0]\n        # Sample the desired number of rows.\n        self.data = gpt4_df\n\n    def _sample_loaded_data(self, num_requests: int) -> list:\n        if num_requests <= len(self.data):\n            data = self.data.sample(n=num_requests, random_state=self.random_seed)\n        else:\n            data = self.data.sample(\n                n=num_requests,\n                random_state=self.random_seed,\n                replace=True,\n            )\n        # Convert the dataframe to a list of lists.\n        return data.values.tolist()\n\n    def sample(\n        self,\n        tokenizer: TokenizerLike,\n        num_requests: int,\n        max_loras: int | None = None,\n        lora_path: str | None = None,\n        request_id_prefix: str = \"\",\n        no_oversample: bool = False,\n        **kwargs,\n    ) -> list[SampleRequest]:\n        samples = []\n        data = self._sample_loaded_data(num_requests=num_requests)\n        for i in range(num_requests):\n            input_len = int(data[i][2])\n            output_len = int(data[i][3])\n            lora_req = self.get_random_lora_request(\n                max_loras=max_loras, lora_path=lora_path\n            )\n            vocab_size = tokenizer.vocab_size\n            # Generate a synthetic prompt: a list of token IDs computed as (i +\n            # j) modulo vocab_size.\n            token_ids = [(i + j) % vocab_size for j in range(input_len)]\n            prompt = tokenizer.decode(token_ids)\n            samples.append(\n                SampleRequest(\n                    prompt=prompt,\n                    prompt_len=input_len,\n                    expected_output_len=output_len,\n                    lora_request=lora_req,\n                    request_id=request_id_prefix + str(i),\n                )\n            )\n        return samples",
      "language": "python"
    },
    {
      "code": "class BurstGPTDataset(BenchmarkDataset):\n    \"\"\"\n    Implements the BurstGPT dataset.  Loads data from a CSV file and generates\n    sample requests based on synthetic prompt generation. Only rows with Model\n    \"GPT-4\" and positive response tokens are used.\n    \"\"\"\n\n    def __init__(self, **kwargs) -> None:\n        super().__init__(**kwargs)\n        self.load_data()\n\n    def load_data(\n        self,\n    ):\n        if self.dataset_path is None:\n            raise ValueError(\"dataset_path must be provided for loading data.\")\n\n        df = pd.read_csv(self.dataset_path)\n        # Filter to keep only GPT-4 rows.\n        gpt4_df = df[df[\"Model\"] == \"GPT-4\"]\n        # Remove failed requests (where Response tokens is 0 or less).\n        gpt4_df = gpt4_df[gpt4_df[\"Response tokens\"] > 0]\n        # Sample the desired number of rows.\n        self.data = gpt4_df\n\n    def _sample_loaded_data(self, num_requests: int) -> list:\n        if num_requests <= len(self.data):\n            data = self.data.sample(n=num_requests, random_state=self.random_seed)\n        else:\n            data = self.data.sample(\n                n=num_requests,\n                random_state=self.random_seed,\n                replace=True,\n            )\n        # Convert the dataframe to a list of lists.\n        return data.values.tolist()\n\n    def sample(\n        self,\n        tokenizer: TokenizerLike,\n        num_requests: int,\n        max_loras: int | None = None,\n        lora_path: str | None = None,\n        request_id_prefix: str = \"\",\n        no_oversample: bool = False,\n        **kwargs,\n    ) -> list[SampleRequest]:\n        samples = []\n        data = self._sample_loaded_data(num_requests=num_requests)\n        for i in range(num_requests):\n            input_len = int(data[i][2])\n            output_len = int(data[i][3])\n            lora_req = self.get_random_lora_request(\n                max_loras=max_loras, lora_path=lora_path\n            )\n            vocab_size = tokenizer.vocab_size\n            # Generate a synthetic prompt: a list of token IDs computed as (i +\n            # j) modulo vocab_size.\n            token_ids = [(i + j) % vocab_size for j in range(input_len)]\n            prompt = tokenizer.decode(token_ids)\n            samples.append(\n                SampleRequest(\n                    prompt=prompt,\n                    prompt_len=input_len,\n                    expected_output_len=output_len,\n                    lora_request=lora_req,\n                    request_id=request_id_prefix + str(i),\n                )\n            )\n        return samples",
      "language": "python"
    },
    {
      "code": "__init__(**kwargs) -> None",
      "language": "python"
    },
    {
      "code": "__init__(**kwargs) -> None",
      "language": "python"
    },
    {
      "code": "2177\n2178\n2179",
      "language": "unknown"
    },
    {
      "code": "def __init__(self, **kwargs) -> None:\n    super().__init__(**kwargs)\n    self.load_data()",
      "language": "python"
    },
    {
      "code": "def __init__(self, **kwargs) -> None:\n    super().__init__(**kwargs)\n    self.load_data()",
      "language": "python"
    },
    {
      "code": "_sample_loaded_data(num_requests: int) -> list",
      "language": "php"
    },
    {
      "code": "_sample_loaded_data(num_requests: int) -> list",
      "language": "php"
    },
    {
      "code": "2195\n2196\n2197\n2198\n2199\n2200\n2201\n2202\n2203\n2204\n2205",
      "language": "unknown"
    },
    {
      "code": "def _sample_loaded_data(self, num_requests: int) -> list:\n    if num_requests <= len(self.data):\n        data = self.data.sample(n=num_requests, random_state=self.random_seed)\n    else:\n        data = self.data.sample(\n            n=num_requests,\n            random_state=self.random_seed,\n            replace=True,\n        )\n    # Convert the dataframe to a list of lists.\n    return data.values.tolist()",
      "language": "python"
    },
    {
      "code": "def _sample_loaded_data(self, num_requests: int) -> list:\n    if num_requests <= len(self.data):\n        data = self.data.sample(n=num_requests, random_state=self.random_seed)\n    else:\n        data = self.data.sample(\n            n=num_requests,\n            random_state=self.random_seed,\n            replace=True,\n        )\n    # Convert the dataframe to a list of lists.\n    return data.values.tolist()",
      "language": "python"
    },
    {
      "code": "load_data()",
      "language": "unknown"
    },
    {
      "code": "load_data()",
      "language": "unknown"
    },
    {
      "code": "2181\n2182\n2183\n2184\n2185\n2186\n2187\n2188\n2189\n2190\n2191\n2192\n2193",
      "language": "unknown"
    },
    {
      "code": "def load_data(\n    self,\n):\n    if self.dataset_path is None:\n        raise ValueError(\"dataset_path must be provided for loading data.\")\n\n    df = pd.read_csv(self.dataset_path)\n    # Filter to keep only GPT-4 rows.\n    gpt4_df = df[df[\"Model\"] == \"GPT-4\"]\n    # Remove failed requests (where Response tokens is 0 or less).\n    gpt4_df = gpt4_df[gpt4_df[\"Response tokens\"] > 0]\n    # Sample the desired number of rows.\n    self.data = gpt4_df",
      "language": "python"
    },
    {
      "code": "def load_data(\n    self,\n):\n    if self.dataset_path is None:\n        raise ValueError(\"dataset_path must be provided for loading data.\")\n\n    df = pd.read_csv(self.dataset_path)\n    # Filter to keep only GPT-4 rows.\n    gpt4_df = df[df[\"Model\"] == \"GPT-4\"]\n    # Remove failed requests (where Response tokens is 0 or less).\n    gpt4_df = gpt4_df[gpt4_df[\"Response tokens\"] > 0]\n    # Sample the desired number of rows.\n    self.data = gpt4_df",
      "language": "python"
    },
    {
      "code": "sample(\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    max_loras: int | None = None,\n    lora_path: str | None = None,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list[SampleRequest]",
      "language": "typescript"
    },
    {
      "code": "sample(\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    max_loras: int | None = None,\n    lora_path: str | None = None,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list[SampleRequest]",
      "language": "typescript"
    },
    {
      "code": "2207\n2208\n2209\n2210\n2211\n2212\n2213\n2214\n2215\n2216\n2217\n2218\n2219\n2220\n2221\n2222\n2223\n2224\n2225\n2226\n2227\n2228\n2229\n2230\n2231\n2232\n2233\n2234\n2235\n2236\n2237\n2238\n2239",
      "language": "unknown"
    },
    {
      "code": "def sample(\n    self,\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    max_loras: int | None = None,\n    lora_path: str | None = None,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list[SampleRequest]:\n    samples = []\n    data = self._sample_loaded_data(num_requests=num_requests)\n    for i in range(num_requests):\n        input_len = int(data[i][2])\n        output_len = int(data[i][3])\n        lora_req = self.get_random_lora_request(\n            max_loras=max_loras, lora_path=lora_path\n        )\n        vocab_size = tokenizer.vocab_size\n        # Generate a synthetic prompt: a list of token IDs computed as (i +\n        # j) modulo vocab_size.\n        token_ids = [(i + j) % vocab_size for j in range(input_len)]\n        prompt = tokenizer.decode(token_ids)\n        samples.append(\n            SampleRequest(\n                prompt=prompt,\n                prompt_len=input_len,\n                expected_output_len=output_len,\n                lora_request=lora_req,\n                request_id=request_id_prefix + str(i),\n            )\n        )\n    return samples",
      "language": "python"
    },
    {
      "code": "def sample(\n    self,\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    max_loras: int | None = None,\n    lora_path: str | None = None,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list[SampleRequest]:\n    samples = []\n    data = self._sample_loaded_data(num_requests=num_requests)\n    for i in range(num_requests):\n        input_len = int(data[i][2])\n        output_len = int(data[i][3])\n        lora_req = self.get_random_lora_request(\n            max_loras=max_loras, lora_path=lora_path\n        )\n        vocab_size = tokenizer.vocab_size\n        # Generate a synthetic prompt: a list of token IDs computed as (i +\n        # j) modulo vocab_size.\n        token_ids = [(i + j) % vocab_size for j in range(input_len)]\n        prompt = tokenizer.decode(token_ids)\n        samples.append(\n            SampleRequest(\n                prompt=prompt,\n                prompt_len=input_len,\n                expected_output_len=output_len,\n                lora_request=lora_req,\n                request_id=request_id_prefix + str(i),\n            )\n        )\n    return samples",
      "language": "python"
    },
    {
      "code": "2284\n2285\n2286\n2287\n2288\n2289\n2290\n2291\n2292\n2293\n2294\n2295\n2296\n2297\n2298\n2299\n2300\n2301\n2302\n2303\n2304\n2305\n2306\n2307\n2308\n2309\n2310\n2311\n2312\n2313\n2314\n2315\n2316\n2317\n2318\n2319\n2320\n2321\n2322\n2323\n2324\n2325\n2326\n2327\n2328\n2329\n2330\n2331\n2332\n2333\n2334\n2335\n2336\n2337\n2338\n2339\n2340\n2341",
      "language": "unknown"
    },
    {
      "code": "class ConversationDataset(HuggingFaceDataset):\n    \"\"\"Dataset for text-only conversation data.\"\"\"\n\n    SUPPORTED_DATASET_PATHS = {\n        \"Aeala/ShareGPT_Vicuna_unfiltered\",\n    }\n    IS_MULTIMODAL = False\n\n    def sample(\n        self,\n        tokenizer: TokenizerLike,\n        num_requests: int,\n        output_len: int | None = None,\n        enable_multimodal_chat: bool = False,\n        request_id_prefix: str = \"\",\n        no_oversample: bool = False,\n        **kwargs,\n    ) -> list:\n        # Filter examples with at least 2 conversations\n        filtered_data = self.data.filter(lambda x: len(x[\"conversations\"]) >= 2)\n        sampled_requests = []\n        ind = 0\n        dynamic_output = output_len is None\n\n        for item in filtered_data:\n            if len(sampled_requests) >= num_requests:\n                break\n            conv = item[\"conversations\"]\n            prompt, completion = conv[0][\"value\"], conv[1][\"value\"]\n\n            prompt_ids = tokenizer(prompt).input_ids\n            completion_ids = tokenizer(completion).input_ids\n            prompt_len = len(prompt_ids)\n            completion_len = len(completion_ids)\n            output_len = completion_len if dynamic_output else output_len\n            assert isinstance(output_len, int) and output_len > 0\n            if dynamic_output and not is_valid_sequence(prompt_len, completion_len):\n                continue\n            mm_content = process_image(item[\"image\"]) if \"image\" in item else None\n            if enable_multimodal_chat:\n                # Note: when chat is enabled the request prompt_len is no longer\n                # accurate and we will be using request output to count the\n                # actual prompt len and output len\n                prompt = self.apply_multimodal_chat_transformation(prompt, mm_content)\n            sampled_requests.append(\n                SampleRequest(\n                    prompt=prompt,\n                    prompt_len=prompt_len,\n                    expected_output_len=output_len,\n                    multi_modal_data=mm_content,\n                    request_id=request_id_prefix + str(ind),\n                )\n            )\n            ind += 1\n        self.maybe_oversample_requests(\n            sampled_requests, num_requests, request_id_prefix, no_oversample\n        )\n        return sampled_requests",
      "language": "python"
    },
    {
      "code": "class ConversationDataset(HuggingFaceDataset):\n    \"\"\"Dataset for text-only conversation data.\"\"\"\n\n    SUPPORTED_DATASET_PATHS = {\n        \"Aeala/ShareGPT_Vicuna_unfiltered\",\n    }\n    IS_MULTIMODAL = False\n\n    def sample(\n        self,\n        tokenizer: TokenizerLike,\n        num_requests: int,\n        output_len: int | None = None,\n        enable_multimodal_chat: bool = False,\n        request_id_prefix: str = \"\",\n        no_oversample: bool = False,\n        **kwargs,\n    ) -> list:\n        # Filter examples with at least 2 conversations\n        filtered_data = self.data.filter(lambda x: len(x[\"conversations\"]) >= 2)\n        sampled_requests = []\n        ind = 0\n        dynamic_output = output_len is None\n\n        for item in filtered_data:\n            if len(sampled_requests) >= num_requests:\n                break\n            conv = item[\"conversations\"]\n            prompt, completion = conv[0][\"value\"], conv[1][\"value\"]\n\n            prompt_ids = tokenizer(prompt).input_ids\n            completion_ids = tokenizer(completion).input_ids\n            prompt_len = len(prompt_ids)\n            completion_len = len(completion_ids)\n            output_len = completion_len if dynamic_output else output_len\n            assert isinstance(output_len, int) and output_len > 0\n            if dynamic_output and not is_valid_sequence(prompt_len, completion_len):\n                continue\n            mm_content = process_image(item[\"image\"]) if \"image\" in item else None\n            if enable_multimodal_chat:\n                # Note: when chat is enabled the request prompt_len is no longer\n                # accurate and we will be using request output to count the\n                # actual prompt len and output len\n                prompt = self.apply_multimodal_chat_transformation(prompt, mm_content)\n            sampled_requests.append(\n                SampleRequest(\n                    prompt=prompt,\n                    prompt_len=prompt_len,\n                    expected_output_len=output_len,\n                    multi_modal_data=mm_content,\n                    request_id=request_id_prefix + str(ind),\n                )\n            )\n            ind += 1\n        self.maybe_oversample_requests(\n            sampled_requests, num_requests, request_id_prefix, no_oversample\n        )\n        return sampled_requests",
      "language": "python"
    },
    {
      "code": "IS_MULTIMODAL = False",
      "language": "unknown"
    },
    {
      "code": "IS_MULTIMODAL = False",
      "language": "unknown"
    },
    {
      "code": "SUPPORTED_DATASET_PATHS = {\n    \"Aeala/ShareGPT_Vicuna_unfiltered\"\n}",
      "language": "unknown"
    },
    {
      "code": "SUPPORTED_DATASET_PATHS = {\n    \"Aeala/ShareGPT_Vicuna_unfiltered\"\n}",
      "language": "unknown"
    },
    {
      "code": "sample(\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    output_len: int | None = None,\n    enable_multimodal_chat: bool = False,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list",
      "language": "typescript"
    },
    {
      "code": "sample(\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    output_len: int | None = None,\n    enable_multimodal_chat: bool = False,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list",
      "language": "typescript"
    },
    {
      "code": "2292\n2293\n2294\n2295\n2296\n2297\n2298\n2299\n2300\n2301\n2302\n2303\n2304\n2305\n2306\n2307\n2308\n2309\n2310\n2311\n2312\n2313\n2314\n2315\n2316\n2317\n2318\n2319\n2320\n2321\n2322\n2323\n2324\n2325\n2326\n2327\n2328\n2329\n2330\n2331\n2332\n2333\n2334\n2335\n2336\n2337\n2338\n2339\n2340\n2341",
      "language": "unknown"
    },
    {
      "code": "def sample(\n    self,\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    output_len: int | None = None,\n    enable_multimodal_chat: bool = False,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list:\n    # Filter examples with at least 2 conversations\n    filtered_data = self.data.filter(lambda x: len(x[\"conversations\"]) >= 2)\n    sampled_requests = []\n    ind = 0\n    dynamic_output = output_len is None\n\n    for item in filtered_data:\n        if len(sampled_requests) >= num_requests:\n            break\n        conv = item[\"conversations\"]\n        prompt, completion = conv[0][\"value\"], conv[1][\"value\"]\n\n        prompt_ids = tokenizer(prompt).input_ids\n        completion_ids = tokenizer(completion).input_ids\n        prompt_len = len(prompt_ids)\n        completion_len = len(completion_ids)\n        output_len = completion_len if dynamic_output else output_len\n        assert isinstance(output_len, int) and output_len > 0\n        if dynamic_output and not is_valid_sequence(prompt_len, completion_len):\n            continue\n        mm_content = process_image(item[\"image\"]) if \"image\" in item else None\n        if enable_multimodal_chat:\n            # Note: when chat is enabled the request prompt_len is no longer\n            # accurate and we will be using request output to count the\n            # actual prompt len and output len\n            prompt = self.apply_multimodal_chat_transformation(prompt, mm_content)\n        sampled_requests.append(\n            SampleRequest(\n                prompt=prompt,\n                prompt_len=prompt_len,\n                expected_output_len=output_len,\n                multi_modal_data=mm_content,\n                request_id=request_id_prefix + str(ind),\n            )\n        )\n        ind += 1\n    self.maybe_oversample_requests(\n        sampled_requests, num_requests, request_id_prefix, no_oversample\n    )\n    return sampled_requests",
      "language": "python"
    },
    {
      "code": "def sample(\n    self,\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    output_len: int | None = None,\n    enable_multimodal_chat: bool = False,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list:\n    # Filter examples with at least 2 conversations\n    filtered_data = self.data.filter(lambda x: len(x[\"conversations\"]) >= 2)\n    sampled_requests = []\n    ind = 0\n    dynamic_output = output_len is None\n\n    for item in filtered_data:\n        if len(sampled_requests) >= num_requests:\n            break\n        conv = item[\"conversations\"]\n        prompt, completion = conv[0][\"value\"], conv[1][\"value\"]\n\n        prompt_ids = tokenizer(prompt).input_ids\n        completion_ids = tokenizer(completion).input_ids\n        prompt_len = len(prompt_ids)\n        completion_len = len(completion_ids)\n        output_len = completion_len if dynamic_output else output_len\n        assert isinstance(output_len, int) and output_len > 0\n        if dynamic_output and not is_valid_sequence(prompt_len, completion_len):\n            continue\n        mm_content = process_image(item[\"image\"]) if \"image\" in item else None\n        if enable_multimodal_chat:\n            # Note: when chat is enabled the request prompt_len is no longer\n            # accurate and we will be using request output to count the\n            # actual prompt len and output len\n            prompt = self.apply_multimodal_chat_transformation(prompt, mm_content)\n        sampled_requests.append(\n            SampleRequest(\n                prompt=prompt,\n                prompt_len=prompt_len,\n                expected_output_len=output_len,\n                multi_modal_data=mm_content,\n                request_id=request_id_prefix + str(ind),\n            )\n        )\n        ind += 1\n    self.maybe_oversample_requests(\n        sampled_requests, num_requests, request_id_prefix, no_oversample\n    )\n    return sampled_requests",
      "language": "python"
    },
    {
      "code": "{\"prompt\": \"What is the capital of India?\"}\n{\"prompt\": \"What is the capital of Iran?\"}\n{\"prompt\": \"What is the capital of China?\"}",
      "language": "json"
    },
    {
      "code": "{\"prompt\": \"What is the capital of India?\"}\n{\"prompt\": \"What is the capital of Iran?\"}\n{\"prompt\": \"What is the capital of China?\"}",
      "language": "json"
    },
    {
      "code": "1927\n1928\n1929\n1930\n1931\n1932\n1933\n1934\n1935\n1936\n1937\n1938\n1939\n1940\n1941\n1942\n1943\n1944\n1945\n1946\n1947\n1948\n1949\n1950\n1951\n1952\n1953\n1954\n1955\n1956\n1957\n1958\n1959\n1960\n1961\n1962\n1963\n1964\n1965\n1966\n1967\n1968\n1969\n1970\n1971\n1972\n1973\n1974\n1975\n1976\n1977\n1978\n1979\n1980\n1981\n1982\n1983\n1984\n1985\n1986\n1987\n1988\n1989\n1990\n1991\n1992\n1993\n1994\n1995\n1996\n1997\n1998\n1999\n2000\n2001\n2002\n2003\n2004\n2005\n2006\n2007\n2008\n2009\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n2024\n2025\n2026",
      "language": "unknown"
    },
    {
      "code": "class CustomDataset(BenchmarkDataset):\n    \"\"\"\n    Implements the Custom dataset.  Loads data from a JSONL file and generates\n    sample requests based on conversation turns. E.g.,\n    ```\n    {\"prompt\": \"What is the capital of India?\"}\n    {\"prompt\": \"What is the capital of Iran?\"}\n    {\"prompt\": \"What is the capital of China?\"}\n    ```\n    \"\"\"\n\n    def __init__(self, **kwargs) -> None:\n        super().__init__(**kwargs)\n        self.load_data()\n\n    def load_data(self) -> None:\n        if self.dataset_path is None:\n            raise ValueError(\"dataset_path must be provided for loading data.\")\n\n        # self.data will be a list of dictionaries\n        # e.g., [{\"prompt\": \"What is the capital of India?\"}, ...]\n        # This will be the standardized format which load_data()\n        # has to convert into depending on the filetype of dataset_path.\n        # sample() will assume this standardized format of self.data\n        self.data = []\n\n        # Load the JSONL file\n        if self.dataset_path.endswith(\".jsonl\"):\n            jsonl_data = pd.read_json(path_or_buf=self.dataset_path, lines=True)\n\n            # check if the JSONL file has a 'prompt' column\n            if \"prompt\" not in jsonl_data.columns:\n                raise ValueError(\"JSONL file must contain a 'prompt' column.\")\n\n            # Convert each row to a dictionary and append to self.data\n            # This will convert the DataFrame to a list of dictionaries\n            # where each dictionary corresponds to a row in the DataFrame.\n            # This is the standardized format we want for self.data\n            for _, row in jsonl_data.iterrows():\n                self.data.append(row.to_dict())\n        else:\n            raise NotImplementedError(\n                \"Only JSONL format is supported for CustomDataset.\"\n            )\n\n        random.seed(self.random_seed)\n        if not getattr(self, \"disable_shuffle\", False):\n            random.shuffle(self.data)\n\n    def sample(\n        self,\n        tokenizer: TokenizerLike,\n        num_requests: int,\n        lora_path: str | None = None,\n        max_loras: int | None = None,\n        output_len: int | None = None,\n        enable_multimodal_chat: bool = False,\n        skip_chat_template: bool = False,\n        request_id_prefix: str = \"\",\n        no_oversample: bool = False,\n        **kwargs,\n    ) -> list:\n        # load all data if needed\n        self.num_available_samples = len(self.data)\n        if num_requests <= 0:\n            num_requests = self.num_available_samples\n            logger.info(\n                \"num_requests is set to 0 or negative, \"\n                \"so using all available samples: %d\",\n                num_requests,\n            )\n\n        sampled_requests = []\n        for i, item in enumerate(self.data):\n            if len(sampled_requests) >= num_requests:\n                break\n            prompt = item[\"prompt\"]\n\n            # apply template\n            if not skip_chat_template:\n                prompt = tokenizer.apply_chat_template(\n                    [{\"role\": \"user\", \"content\": prompt}],\n                    add_generation_prompt=True,\n                    tokenize=False,\n                )\n\n            prompt_len = len(tokenizer(prompt).input_ids)\n            sampled_requests.append(\n                SampleRequest(\n                    prompt=prompt,\n                    prompt_len=prompt_len,\n                    expected_output_len=output_len,\n                    request_id=request_id_prefix + str(i),\n                )\n            )\n        self.maybe_oversample_requests(\n            sampled_requests, num_requests, request_id_prefix, no_oversample\n        )\n\n        return sampled_requests",
      "language": "json"
    },
    {
      "code": "class CustomDataset(BenchmarkDataset):\n    \"\"\"\n    Implements the Custom dataset.  Loads data from a JSONL file and generates\n    sample requests based on conversation turns. E.g.,\n    ```\n    {\"prompt\": \"What is the capital of India?\"}\n    {\"prompt\": \"What is the capital of Iran?\"}\n    {\"prompt\": \"What is the capital of China?\"}\n    ```\n    \"\"\"\n\n    def __init__(self, **kwargs) -> None:\n        super().__init__(**kwargs)\n        self.load_data()\n\n    def load_data(self) -> None:\n        if self.dataset_path is None:\n            raise ValueError(\"dataset_path must be provided for loading data.\")\n\n        # self.data will be a list of dictionaries\n        # e.g., [{\"prompt\": \"What is the capital of India?\"}, ...]\n        # This will be the standardized format which load_data()\n        # has to convert into depending on the filetype of dataset_path.\n        # sample() will assume this standardized format of self.data\n        self.data = []\n\n        # Load the JSONL file\n        if self.dataset_path.endswith(\".jsonl\"):\n            jsonl_data = pd.read_json(path_or_buf=self.dataset_path, lines=True)\n\n            # check if the JSONL file has a 'prompt' column\n            if \"prompt\" not in jsonl_data.columns:\n                raise ValueError(\"JSONL file must contain a 'prompt' column.\")\n\n            # Convert each row to a dictionary and append to self.data\n            # This will convert the DataFrame to a list of dictionaries\n            # where each dictionary corresponds to a row in the DataFrame.\n            # This is the standardized format we want for self.data\n            for _, row in jsonl_data.iterrows():\n                self.data.append(row.to_dict())\n        else:\n            raise NotImplementedError(\n                \"Only JSONL format is supported for CustomDataset.\"\n            )\n\n        random.seed(self.random_seed)\n        if not getattr(self, \"disable_shuffle\", False):\n            random.shuffle(self.data)\n\n    def sample(\n        self,\n        tokenizer: TokenizerLike,\n        num_requests: int,\n        lora_path: str | None = None,\n        max_loras: int | None = None,\n        output_len: int | None = None,\n        enable_multimodal_chat: bool = False,\n        skip_chat_template: bool = False,\n        request_id_prefix: str = \"\",\n        no_oversample: bool = False,\n        **kwargs,\n    ) -> list:\n        # load all data if needed\n        self.num_available_samples = len(self.data)\n        if num_requests <= 0:\n            num_requests = self.num_available_samples\n            logger.info(\n                \"num_requests is set to 0 or negative, \"\n                \"so using all available samples: %d\",\n                num_requests,\n            )\n\n        sampled_requests = []\n        for i, item in enumerate(self.data):\n            if len(sampled_requests) >= num_requests:\n                break\n            prompt = item[\"prompt\"]\n\n            # apply template\n            if not skip_chat_template:\n                prompt = tokenizer.apply_chat_template(\n                    [{\"role\": \"user\", \"content\": prompt}],\n                    add_generation_prompt=True,\n                    tokenize=False,\n                )\n\n            prompt_len = len(tokenizer(prompt).input_ids)\n            sampled_requests.append(\n                SampleRequest(\n                    prompt=prompt,\n                    prompt_len=prompt_len,\n                    expected_output_len=output_len,\n                    request_id=request_id_prefix + str(i),\n                )\n            )\n        self.maybe_oversample_requests(\n            sampled_requests, num_requests, request_id_prefix, no_oversample\n        )\n\n        return sampled_requests",
      "language": "json"
    },
    {
      "code": "__init__(**kwargs) -> None",
      "language": "python"
    },
    {
      "code": "__init__(**kwargs) -> None",
      "language": "python"
    },
    {
      "code": "1938\n1939\n1940",
      "language": "unknown"
    },
    {
      "code": "def __init__(self, **kwargs) -> None:\n    super().__init__(**kwargs)\n    self.load_data()",
      "language": "python"
    },
    {
      "code": "def __init__(self, **kwargs) -> None:\n    super().__init__(**kwargs)\n    self.load_data()",
      "language": "python"
    },
    {
      "code": "load_data() -> None",
      "language": "rust"
    },
    {
      "code": "load_data() -> None",
      "language": "rust"
    },
    {
      "code": "1942\n1943\n1944\n1945\n1946\n1947\n1948\n1949\n1950\n1951\n1952\n1953\n1954\n1955\n1956\n1957\n1958\n1959\n1960\n1961\n1962\n1963\n1964\n1965\n1966\n1967\n1968\n1969\n1970\n1971\n1972\n1973\n1974",
      "language": "unknown"
    },
    {
      "code": "def load_data(self) -> None:\n    if self.dataset_path is None:\n        raise ValueError(\"dataset_path must be provided for loading data.\")\n\n    # self.data will be a list of dictionaries\n    # e.g., [{\"prompt\": \"What is the capital of India?\"}, ...]\n    # This will be the standardized format which load_data()\n    # has to convert into depending on the filetype of dataset_path.\n    # sample() will assume this standardized format of self.data\n    self.data = []\n\n    # Load the JSONL file\n    if self.dataset_path.endswith(\".jsonl\"):\n        jsonl_data = pd.read_json(path_or_buf=self.dataset_path, lines=True)\n\n        # check if the JSONL file has a 'prompt' column\n        if \"prompt\" not in jsonl_data.columns:\n            raise ValueError(\"JSONL file must contain a 'prompt' column.\")\n\n        # Convert each row to a dictionary and append to self.data\n        # This will convert the DataFrame to a list of dictionaries\n        # where each dictionary corresponds to a row in the DataFrame.\n        # This is the standardized format we want for self.data\n        for _, row in jsonl_data.iterrows():\n            self.data.append(row.to_dict())\n    else:\n        raise NotImplementedError(\n            \"Only JSONL format is supported for CustomDataset.\"\n        )\n\n    random.seed(self.random_seed)\n    if not getattr(self, \"disable_shuffle\", False):\n        random.shuffle(self.data)",
      "language": "python"
    },
    {
      "code": "def load_data(self) -> None:\n    if self.dataset_path is None:\n        raise ValueError(\"dataset_path must be provided for loading data.\")\n\n    # self.data will be a list of dictionaries\n    # e.g., [{\"prompt\": \"What is the capital of India?\"}, ...]\n    # This will be the standardized format which load_data()\n    # has to convert into depending on the filetype of dataset_path.\n    # sample() will assume this standardized format of self.data\n    self.data = []\n\n    # Load the JSONL file\n    if self.dataset_path.endswith(\".jsonl\"):\n        jsonl_data = pd.read_json(path_or_buf=self.dataset_path, lines=True)\n\n        # check if the JSONL file has a 'prompt' column\n        if \"prompt\" not in jsonl_data.columns:\n            raise ValueError(\"JSONL file must contain a 'prompt' column.\")\n\n        # Convert each row to a dictionary and append to self.data\n        # This will convert the DataFrame to a list of dictionaries\n        # where each dictionary corresponds to a row in the DataFrame.\n        # This is the standardized format we want for self.data\n        for _, row in jsonl_data.iterrows():\n            self.data.append(row.to_dict())\n    else:\n        raise NotImplementedError(\n            \"Only JSONL format is supported for CustomDataset.\"\n        )\n\n    random.seed(self.random_seed)\n    if not getattr(self, \"disable_shuffle\", False):\n        random.shuffle(self.data)",
      "language": "python"
    },
    {
      "code": "sample(\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    lora_path: str | None = None,\n    max_loras: int | None = None,\n    output_len: int | None = None,\n    enable_multimodal_chat: bool = False,\n    skip_chat_template: bool = False,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list",
      "language": "typescript"
    },
    {
      "code": "sample(\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    lora_path: str | None = None,\n    max_loras: int | None = None,\n    output_len: int | None = None,\n    enable_multimodal_chat: bool = False,\n    skip_chat_template: bool = False,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list",
      "language": "typescript"
    },
    {
      "code": "1976\n1977\n1978\n1979\n1980\n1981\n1982\n1983\n1984\n1985\n1986\n1987\n1988\n1989\n1990\n1991\n1992\n1993\n1994\n1995\n1996\n1997\n1998\n1999\n2000\n2001\n2002\n2003\n2004\n2005\n2006\n2007\n2008\n2009\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n2024\n2025\n2026",
      "language": "unknown"
    },
    {
      "code": "def sample(\n    self,\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    lora_path: str | None = None,\n    max_loras: int | None = None,\n    output_len: int | None = None,\n    enable_multimodal_chat: bool = False,\n    skip_chat_template: bool = False,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list:\n    # load all data if needed\n    self.num_available_samples = len(self.data)\n    if num_requests <= 0:\n        num_requests = self.num_available_samples\n        logger.info(\n            \"num_requests is set to 0 or negative, \"\n            \"so using all available samples: %d\",\n            num_requests,\n        )\n\n    sampled_requests = []\n    for i, item in enumerate(self.data):\n        if len(sampled_requests) >= num_requests:\n            break\n        prompt = item[\"prompt\"]\n\n        # apply template\n        if not skip_chat_template:\n            prompt = tokenizer.apply_chat_template(\n                [{\"role\": \"user\", \"content\": prompt}],\n                add_generation_prompt=True,\n                tokenize=False,\n            )\n\n        prompt_len = len(tokenizer(prompt).input_ids)\n        sampled_requests.append(\n            SampleRequest(\n                prompt=prompt,\n                prompt_len=prompt_len,\n                expected_output_len=output_len,\n                request_id=request_id_prefix + str(i),\n            )\n        )\n    self.maybe_oversample_requests(\n        sampled_requests, num_requests, request_id_prefix, no_oversample\n    )\n\n    return sampled_requests",
      "language": "json"
    },
    {
      "code": "def sample(\n    self,\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    lora_path: str | None = None,\n    max_loras: int | None = None,\n    output_len: int | None = None,\n    enable_multimodal_chat: bool = False,\n    skip_chat_template: bool = False,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list:\n    # load all data if needed\n    self.num_available_samples = len(self.data)\n    if num_requests <= 0:\n        num_requests = self.num_available_samples\n        logger.info(\n            \"num_requests is set to 0 or negative, \"\n            \"so using all available samples: %d\",\n            num_requests,\n        )\n\n    sampled_requests = []\n    for i, item in enumerate(self.data):\n        if len(sampled_requests) >= num_requests:\n            break\n        prompt = item[\"prompt\"]\n\n        # apply template\n        if not skip_chat_template:\n            prompt = tokenizer.apply_chat_template(\n                [{\"role\": \"user\", \"content\": prompt}],\n                add_generation_prompt=True,\n                tokenize=False,\n            )\n\n        prompt_len = len(tokenizer(prompt).input_ids)\n        sampled_requests.append(\n            SampleRequest(\n                prompt=prompt,\n                prompt_len=prompt_len,\n                expected_output_len=output_len,\n                request_id=request_id_prefix + str(i),\n            )\n        )\n    self.maybe_oversample_requests(\n        sampled_requests, num_requests, request_id_prefix, no_oversample\n    )\n\n    return sampled_requests",
      "language": "json"
    },
    {
      "code": "2245\n2246\n2247\n2248\n2249\n2250\n2251\n2252\n2253\n2254\n2255\n2256\n2257\n2258\n2259\n2260\n2261\n2262\n2263\n2264\n2265\n2266\n2267\n2268\n2269\n2270\n2271\n2272\n2273\n2274\n2275\n2276",
      "language": "unknown"
    },
    {
      "code": "class HuggingFaceDataset(BenchmarkDataset):\n    \"\"\"Base class for datasets hosted on HuggingFace.\"\"\"\n\n    SUPPORTED_DATASET_PATHS: set[str] | dict[str, Callable] = set()\n\n    def __init__(\n        self,\n        dataset_path: str,\n        dataset_split: str,\n        no_stream: bool = False,\n        dataset_subset: str | None = None,\n        hf_name: str | None = None,\n        **kwargs,\n    ) -> None:\n        super().__init__(dataset_path=dataset_path, **kwargs)\n\n        self.dataset_split = dataset_split\n        self.dataset_subset = dataset_subset\n        self.load_stream = not no_stream\n        self.hf_name = hf_name or dataset_path\n        self.load_data()\n\n    def load_data(self) -> None:\n        \"\"\"Load data from HuggingFace datasets.\"\"\"\n        self.data = load_dataset(\n            self.dataset_path,\n            name=self.dataset_subset,\n            split=self.dataset_split,\n            streaming=self.load_stream,\n        )\n        if not getattr(self, \"disable_shuffle\", False):\n            self.data = self.data.shuffle(seed=self.random_seed)",
      "language": "python"
    },
    {
      "code": "class HuggingFaceDataset(BenchmarkDataset):\n    \"\"\"Base class for datasets hosted on HuggingFace.\"\"\"\n\n    SUPPORTED_DATASET_PATHS: set[str] | dict[str, Callable] = set()\n\n    def __init__(\n        self,\n        dataset_path: str,\n        dataset_split: str,\n        no_stream: bool = False,\n        dataset_subset: str | None = None,\n        hf_name: str | None = None,\n        **kwargs,\n    ) -> None:\n        super().__init__(dataset_path=dataset_path, **kwargs)\n\n        self.dataset_split = dataset_split\n        self.dataset_subset = dataset_subset\n        self.load_stream = not no_stream\n        self.hf_name = hf_name or dataset_path\n        self.load_data()\n\n    def load_data(self) -> None:\n        \"\"\"Load data from HuggingFace datasets.\"\"\"\n        self.data = load_dataset(\n            self.dataset_path,\n            name=self.dataset_subset,\n            split=self.dataset_split,\n            streaming=self.load_stream,\n        )\n        if not getattr(self, \"disable_shuffle\", False):\n            self.data = self.data.shuffle(seed=self.random_seed)",
      "language": "python"
    },
    {
      "code": "SUPPORTED_DATASET_PATHS: set[str] | dict[str, Callable] = (\n    set()\n)",
      "language": "yaml"
    },
    {
      "code": "SUPPORTED_DATASET_PATHS: set[str] | dict[str, Callable] = (\n    set()\n)",
      "language": "yaml"
    },
    {
      "code": "dataset_split = dataset_split",
      "language": "unknown"
    },
    {
      "code": "dataset_split = dataset_split",
      "language": "unknown"
    },
    {
      "code": "dataset_subset = dataset_subset",
      "language": "unknown"
    },
    {
      "code": "dataset_subset = dataset_subset",
      "language": "unknown"
    },
    {
      "code": "hf_name = hf_name or dataset_path",
      "language": "unknown"
    },
    {
      "code": "hf_name = hf_name or dataset_path",
      "language": "unknown"
    },
    {
      "code": "load_stream = not no_stream",
      "language": "unknown"
    },
    {
      "code": "load_stream = not no_stream",
      "language": "unknown"
    },
    {
      "code": "__init__(\n    dataset_path: str,\n    dataset_split: str,\n    no_stream: bool = False,\n    dataset_subset: str | None = None,\n    hf_name: str | None = None,\n    **kwargs,\n) -> None",
      "language": "typescript"
    },
    {
      "code": "__init__(\n    dataset_path: str,\n    dataset_split: str,\n    no_stream: bool = False,\n    dataset_subset: str | None = None,\n    hf_name: str | None = None,\n    **kwargs,\n) -> None",
      "language": "typescript"
    },
    {
      "code": "2250\n2251\n2252\n2253\n2254\n2255\n2256\n2257\n2258\n2259\n2260\n2261\n2262\n2263\n2264\n2265",
      "language": "unknown"
    },
    {
      "code": "def __init__(\n    self,\n    dataset_path: str,\n    dataset_split: str,\n    no_stream: bool = False,\n    dataset_subset: str | None = None,\n    hf_name: str | None = None,\n    **kwargs,\n) -> None:\n    super().__init__(dataset_path=dataset_path, **kwargs)\n\n    self.dataset_split = dataset_split\n    self.dataset_subset = dataset_subset\n    self.load_stream = not no_stream\n    self.hf_name = hf_name or dataset_path\n    self.load_data()",
      "language": "python"
    },
    {
      "code": "def __init__(\n    self,\n    dataset_path: str,\n    dataset_split: str,\n    no_stream: bool = False,\n    dataset_subset: str | None = None,\n    hf_name: str | None = None,\n    **kwargs,\n) -> None:\n    super().__init__(dataset_path=dataset_path, **kwargs)\n\n    self.dataset_split = dataset_split\n    self.dataset_subset = dataset_subset\n    self.load_stream = not no_stream\n    self.hf_name = hf_name or dataset_path\n    self.load_data()",
      "language": "python"
    },
    {
      "code": "load_data() -> None",
      "language": "rust"
    },
    {
      "code": "load_data() -> None",
      "language": "rust"
    },
    {
      "code": "2267\n2268\n2269\n2270\n2271\n2272\n2273\n2274\n2275\n2276",
      "language": "unknown"
    },
    {
      "code": "def load_data(self) -> None:\n    \"\"\"Load data from HuggingFace datasets.\"\"\"\n    self.data = load_dataset(\n        self.dataset_path,\n        name=self.dataset_subset,\n        split=self.dataset_split,\n        streaming=self.load_stream,\n    )\n    if not getattr(self, \"disable_shuffle\", False):\n        self.data = self.data.shuffle(seed=self.random_seed)",
      "language": "python"
    },
    {
      "code": "def load_data(self) -> None:\n    \"\"\"Load data from HuggingFace datasets.\"\"\"\n    self.data = load_dataset(\n        self.dataset_path,\n        name=self.dataset_subset,\n        split=self.dataset_split,\n        streaming=self.load_stream,\n    )\n    if not getattr(self, \"disable_shuffle\", False):\n        self.data = self.data.shuffle(seed=self.random_seed)",
      "language": "python"
    },
    {
      "code": "2521\n2522\n2523\n2524\n2525\n2526\n2527\n2528\n2529\n2530\n2531\n2532\n2533\n2534\n2535\n2536\n2537\n2538\n2539\n2540\n2541\n2542\n2543\n2544\n2545\n2546\n2547\n2548\n2549\n2550\n2551\n2552\n2553\n2554\n2555\n2556\n2557\n2558\n2559\n2560\n2561\n2562\n2563\n2564\n2565\n2566\n2567\n2568\n2569\n2570\n2571\n2572\n2573\n2574\n2575\n2576\n2577",
      "language": "unknown"
    },
    {
      "code": "class InstructCoderDataset(HuggingFaceDataset):\n    \"\"\"\n    InstructCoder Dataset.\n    https://huggingface.co/datasets/likaixin/InstructCoder\n\n    InstructCoder is the dataset designed for general code editing.  It consists\n    of 114,239 instruction-input-output triplets, and covers multiple distinct\n    code editing scenario.\n    \"\"\"\n\n    DEFAULT_OUTPUT_LEN = 200  # this is the average default output length\n    SUPPORTED_DATASET_PATHS = {\n        \"likaixin/InstructCoder\",\n    }\n\n    def sample(\n        self,\n        tokenizer: TokenizerLike,\n        num_requests: int,\n        output_len: int | None = None,\n        enable_multimodal_chat: bool = False,\n        skip_chat_template: bool = False,\n        request_id_prefix: str = \"\",\n        no_oversample: bool = False,\n        **kwargs,\n    ) -> list:\n        output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN\n        sampled_requests = []\n        for i, item in enumerate(self.data):\n            if len(sampled_requests) >= num_requests:\n                break\n            prompt = (\n                f\"{item['input']}\\n\\n{item['instruction']} Just output \"\n                \"the code, do not include any explanation.\"\n            )\n\n            # apply template\n            if not skip_chat_template:\n                prompt = tokenizer.apply_chat_template(\n                    [{\"role\": \"user\", \"content\": prompt}],\n                    add_generation_prompt=True,\n                    tokenize=False,\n                )\n\n            prompt_len = len(tokenizer(prompt).input_ids)\n            sampled_requests.append(\n                SampleRequest(\n                    prompt=prompt,\n                    prompt_len=prompt_len,\n                    expected_output_len=output_len,\n                    request_id=request_id_prefix + str(i),\n                )\n            )\n        self.maybe_oversample_requests(\n            sampled_requests, num_requests, request_id_prefix, no_oversample\n        )\n        return sampled_requests",
      "language": "json"
    },
    {
      "code": "class InstructCoderDataset(HuggingFaceDataset):\n    \"\"\"\n    InstructCoder Dataset.\n    https://huggingface.co/datasets/likaixin/InstructCoder\n\n    InstructCoder is the dataset designed for general code editing.  It consists\n    of 114,239 instruction-input-output triplets, and covers multiple distinct\n    code editing scenario.\n    \"\"\"\n\n    DEFAULT_OUTPUT_LEN = 200  # this is the average default output length\n    SUPPORTED_DATASET_PATHS = {\n        \"likaixin/InstructCoder\",\n    }\n\n    def sample(\n        self,\n        tokenizer: TokenizerLike,\n        num_requests: int,\n        output_len: int | None = None,\n        enable_multimodal_chat: bool = False,\n        skip_chat_template: bool = False,\n        request_id_prefix: str = \"\",\n        no_oversample: bool = False,\n        **kwargs,\n    ) -> list:\n        output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN\n        sampled_requests = []\n        for i, item in enumerate(self.data):\n            if len(sampled_requests) >= num_requests:\n                break\n            prompt = (\n                f\"{item['input']}\\n\\n{item['instruction']} Just output \"\n                \"the code, do not include any explanation.\"\n            )\n\n            # apply template\n            if not skip_chat_template:\n                prompt = tokenizer.apply_chat_template(\n                    [{\"role\": \"user\", \"content\": prompt}],\n                    add_generation_prompt=True,\n                    tokenize=False,\n                )\n\n            prompt_len = len(tokenizer(prompt).input_ids)\n            sampled_requests.append(\n                SampleRequest(\n                    prompt=prompt,\n                    prompt_len=prompt_len,\n                    expected_output_len=output_len,\n                    request_id=request_id_prefix + str(i),\n                )\n            )\n        self.maybe_oversample_requests(\n            sampled_requests, num_requests, request_id_prefix, no_oversample\n        )\n        return sampled_requests",
      "language": "json"
    },
    {
      "code": "DEFAULT_OUTPUT_LEN = 200",
      "language": "unknown"
    },
    {
      "code": "DEFAULT_OUTPUT_LEN = 200",
      "language": "unknown"
    },
    {
      "code": "SUPPORTED_DATASET_PATHS = {'likaixin/InstructCoder'}",
      "language": "unknown"
    },
    {
      "code": "SUPPORTED_DATASET_PATHS = {'likaixin/InstructCoder'}",
      "language": "unknown"
    },
    {
      "code": "sample(\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    output_len: int | None = None,\n    enable_multimodal_chat: bool = False,\n    skip_chat_template: bool = False,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list",
      "language": "typescript"
    },
    {
      "code": "sample(\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    output_len: int | None = None,\n    enable_multimodal_chat: bool = False,\n    skip_chat_template: bool = False,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list",
      "language": "typescript"
    },
    {
      "code": "2536\n2537\n2538\n2539\n2540\n2541\n2542\n2543\n2544\n2545\n2546\n2547\n2548\n2549\n2550\n2551\n2552\n2553\n2554\n2555\n2556\n2557\n2558\n2559\n2560\n2561\n2562\n2563\n2564\n2565\n2566\n2567\n2568\n2569\n2570\n2571\n2572\n2573\n2574\n2575\n2576\n2577",
      "language": "unknown"
    },
    {
      "code": "def sample(\n    self,\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    output_len: int | None = None,\n    enable_multimodal_chat: bool = False,\n    skip_chat_template: bool = False,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list:\n    output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN\n    sampled_requests = []\n    for i, item in enumerate(self.data):\n        if len(sampled_requests) >= num_requests:\n            break\n        prompt = (\n            f\"{item['input']}\\n\\n{item['instruction']} Just output \"\n            \"the code, do not include any explanation.\"\n        )\n\n        # apply template\n        if not skip_chat_template:\n            prompt = tokenizer.apply_chat_template(\n                [{\"role\": \"user\", \"content\": prompt}],\n                add_generation_prompt=True,\n                tokenize=False,\n            )\n\n        prompt_len = len(tokenizer(prompt).input_ids)\n        sampled_requests.append(\n            SampleRequest(\n                prompt=prompt,\n                prompt_len=prompt_len,\n                expected_output_len=output_len,\n                request_id=request_id_prefix + str(i),\n            )\n        )\n    self.maybe_oversample_requests(\n        sampled_requests, num_requests, request_id_prefix, no_oversample\n    )\n    return sampled_requests",
      "language": "json"
    },
    {
      "code": "def sample(\n    self,\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    output_len: int | None = None,\n    enable_multimodal_chat: bool = False,\n    skip_chat_template: bool = False,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list:\n    output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN\n    sampled_requests = []\n    for i, item in enumerate(self.data):\n        if len(sampled_requests) >= num_requests:\n            break\n        prompt = (\n            f\"{item['input']}\\n\\n{item['instruction']} Just output \"\n            \"the code, do not include any explanation.\"\n        )\n\n        # apply template\n        if not skip_chat_template:\n            prompt = tokenizer.apply_chat_template(\n                [{\"role\": \"user\", \"content\": prompt}],\n                add_generation_prompt=True,\n                tokenize=False,\n            )\n\n        prompt_len = len(tokenizer(prompt).input_ids)\n        sampled_requests.append(\n            SampleRequest(\n                prompt=prompt,\n                prompt_len=prompt_len,\n                expected_output_len=output_len,\n                request_id=request_id_prefix + str(i),\n            )\n        )\n    self.maybe_oversample_requests(\n        sampled_requests, num_requests, request_id_prefix, no_oversample\n    )\n    return sampled_requests",
      "language": "json"
    },
    {
      "code": "2984\n2985\n2986\n2987\n2988\n2989\n2990\n2991\n2992\n2993\n2994\n2995\n2996\n2997\n2998\n2999\n3000\n3001\n3002\n3003\n3004\n3005\n3006\n3007\n3008\n3009\n3010\n3011\n3012\n3013\n3014\n3015\n3016\n3017\n3018\n3019\n3020\n3021\n3022\n3023\n3024\n3025\n3026\n3027\n3028\n3029\n3030\n3031\n3032\n3033\n3034\n3035\n3036\n3037\n3038\n3039\n3040\n3041\n3042\n3043\n3044\n3045\n3046\n3047\n3048\n3049\n3050\n3051\n3052\n3053\n3054\n3055\n3056\n3057\n3058\n3059\n3060\n3061\n3062",
      "language": "unknown"
    },
    {
      "code": "class MLPerfDataset(HuggingFaceDataset):\n    \"\"\"\n    MLPerf Inference Dataset.\n\n    Dataset on HF:\n    https://huggingface.co/datasets/mgoin/mlperf-inference-llama2-data\n    https://huggingface.co/datasets/mgoin/mlperf-inference-llama3.1-data\n\n    Each record contains:\n      - \"system_prompt\": system role instruction.\n      - \"question\": user question.\n      - \"output\": reference answer.\n\n    We combine the system prompt and question into a chat-formatted prompt\n    (using the tokenizer's chat template) and set the expected output length to\n    the tokenized length of the provided reference answer.\n    \"\"\"\n\n    SUPPORTED_DATASET_PATHS = {\n        \"mgoin/mlperf-inference-llama2-data\",\n        \"mgoin/mlperf-inference-llama3.1-data\",\n    }\n\n    def sample(\n        self,\n        tokenizer: TokenizerLike,\n        num_requests: int,\n        output_len: int | None = None,\n        request_id_prefix: str = \"\",\n        no_oversample: bool = False,\n        **kwargs,\n    ) -> list[SampleRequest]:\n        # Force dynamic output length based on reference completion.\n        dynamic_output = output_len is None\n        sampled_requests: list[SampleRequest] = []\n        ind = 0\n\n        for item in self.data:\n            if len(sampled_requests) >= num_requests:\n                break\n\n            system_prompt = item[\"system_prompt\"]\n            question = item[\"question\"]\n            reference_answer = item[\"output\"]\n\n            # Build chat-style prompt using tokenizer template, if available.\n            messages = [\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": question},\n            ]\n            prompt_formatted = tokenizer.apply_chat_template(\n                messages, add_generation_prompt=True, tokenize=False\n            )\n            prompt_len = len(tokenizer(prompt_formatted).input_ids)\n\n            # Determine output length from reference answer tokens.\n            ref_out_len = len(\n                tokenizer(reference_answer, add_special_tokens=False).input_ids\n            )\n            expected_output_len = ref_out_len if dynamic_output else output_len\n\n            # Validate sequence lengths.\n            if not is_valid_sequence(prompt_len, expected_output_len):\n                continue\n\n            sampled_requests.append(\n                SampleRequest(\n                    prompt=prompt_formatted,\n                    prompt_len=prompt_len,\n                    expected_output_len=expected_output_len,\n                    request_id=request_id_prefix + str(ind),\n                )\n            )\n            ind += 1\n\n        self.maybe_oversample_requests(\n            sampled_requests, num_requests, request_id_prefix, no_oversample\n        )\n        return sampled_requests",
      "language": "json"
    },
    {
      "code": "class MLPerfDataset(HuggingFaceDataset):\n    \"\"\"\n    MLPerf Inference Dataset.\n\n    Dataset on HF:\n    https://huggingface.co/datasets/mgoin/mlperf-inference-llama2-data\n    https://huggingface.co/datasets/mgoin/mlperf-inference-llama3.1-data\n\n    Each record contains:\n      - \"system_prompt\": system role instruction.\n      - \"question\": user question.\n      - \"output\": reference answer.\n\n    We combine the system prompt and question into a chat-formatted prompt\n    (using the tokenizer's chat template) and set the expected output length to\n    the tokenized length of the provided reference answer.\n    \"\"\"\n\n    SUPPORTED_DATASET_PATHS = {\n        \"mgoin/mlperf-inference-llama2-data\",\n        \"mgoin/mlperf-inference-llama3.1-data\",\n    }\n\n    def sample(\n        self,\n        tokenizer: TokenizerLike,\n        num_requests: int,\n        output_len: int | None = None,\n        request_id_prefix: str = \"\",\n        no_oversample: bool = False,\n        **kwargs,\n    ) -> list[SampleRequest]:\n        # Force dynamic output length based on reference completion.\n        dynamic_output = output_len is None\n        sampled_requests: list[SampleRequest] = []\n        ind = 0\n\n        for item in self.data:\n            if len(sampled_requests) >= num_requests:\n                break\n\n            system_prompt = item[\"system_prompt\"]\n            question = item[\"question\"]\n            reference_answer = item[\"output\"]\n\n            # Build chat-style prompt using tokenizer template, if available.\n            messages = [\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": question},\n            ]\n            prompt_formatted = tokenizer.apply_chat_template(\n                messages, add_generation_prompt=True, tokenize=False\n            )\n            prompt_len = len(tokenizer(prompt_formatted).input_ids)\n\n            # Determine output length from reference answer tokens.\n            ref_out_len = len(\n                tokenizer(reference_answer, add_special_tokens=False).input_ids\n            )\n            expected_output_len = ref_out_len if dynamic_output else output_len\n\n            # Validate sequence lengths.\n            if not is_valid_sequence(prompt_len, expected_output_len):\n                continue\n\n            sampled_requests.append(\n                SampleRequest(\n                    prompt=prompt_formatted,\n                    prompt_len=prompt_len,\n                    expected_output_len=expected_output_len,\n                    request_id=request_id_prefix + str(ind),\n                )\n            )\n            ind += 1\n\n        self.maybe_oversample_requests(\n            sampled_requests, num_requests, request_id_prefix, no_oversample\n        )\n        return sampled_requests",
      "language": "json"
    },
    {
      "code": "SUPPORTED_DATASET_PATHS = {\n    \"mgoin/mlperf-inference-llama2-data\",\n    \"mgoin/mlperf-inference-llama3.1-data\",\n}",
      "language": "unknown"
    },
    {
      "code": "SUPPORTED_DATASET_PATHS = {\n    \"mgoin/mlperf-inference-llama2-data\",\n    \"mgoin/mlperf-inference-llama3.1-data\",\n}",
      "language": "unknown"
    },
    {
      "code": "sample(\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    output_len: int | None = None,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list[SampleRequest]",
      "language": "typescript"
    },
    {
      "code": "sample(\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    output_len: int | None = None,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list[SampleRequest]",
      "language": "typescript"
    },
    {
      "code": "3007\n3008\n3009\n3010\n3011\n3012\n3013\n3014\n3015\n3016\n3017\n3018\n3019\n3020\n3021\n3022\n3023\n3024\n3025\n3026\n3027\n3028\n3029\n3030\n3031\n3032\n3033\n3034\n3035\n3036\n3037\n3038\n3039\n3040\n3041\n3042\n3043\n3044\n3045\n3046\n3047\n3048\n3049\n3050\n3051\n3052\n3053\n3054\n3055\n3056\n3057\n3058\n3059\n3060\n3061\n3062",
      "language": "unknown"
    },
    {
      "code": "def sample(\n    self,\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    output_len: int | None = None,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list[SampleRequest]:\n    # Force dynamic output length based on reference completion.\n    dynamic_output = output_len is None\n    sampled_requests: list[SampleRequest] = []\n    ind = 0\n\n    for item in self.data:\n        if len(sampled_requests) >= num_requests:\n            break\n\n        system_prompt = item[\"system_prompt\"]\n        question = item[\"question\"]\n        reference_answer = item[\"output\"]\n\n        # Build chat-style prompt using tokenizer template, if available.\n        messages = [\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": question},\n        ]\n        prompt_formatted = tokenizer.apply_chat_template(\n            messages, add_generation_prompt=True, tokenize=False\n        )\n        prompt_len = len(tokenizer(prompt_formatted).input_ids)\n\n        # Determine output length from reference answer tokens.\n        ref_out_len = len(\n            tokenizer(reference_answer, add_special_tokens=False).input_ids\n        )\n        expected_output_len = ref_out_len if dynamic_output else output_len\n\n        # Validate sequence lengths.\n        if not is_valid_sequence(prompt_len, expected_output_len):\n            continue\n\n        sampled_requests.append(\n            SampleRequest(\n                prompt=prompt_formatted,\n                prompt_len=prompt_len,\n                expected_output_len=expected_output_len,\n                request_id=request_id_prefix + str(ind),\n            )\n        )\n        ind += 1\n\n    self.maybe_oversample_requests(\n        sampled_requests, num_requests, request_id_prefix, no_oversample\n    )\n    return sampled_requests",
      "language": "json"
    },
    {
      "code": "def sample(\n    self,\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    output_len: int | None = None,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list[SampleRequest]:\n    # Force dynamic output length based on reference completion.\n    dynamic_output = output_len is None\n    sampled_requests: list[SampleRequest] = []\n    ind = 0\n\n    for item in self.data:\n        if len(sampled_requests) >= num_requests:\n            break\n\n        system_prompt = item[\"system_prompt\"]\n        question = item[\"question\"]\n        reference_answer = item[\"output\"]\n\n        # Build chat-style prompt using tokenizer template, if available.\n        messages = [\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": question},\n        ]\n        prompt_formatted = tokenizer.apply_chat_template(\n            messages, add_generation_prompt=True, tokenize=False\n        )\n        prompt_len = len(tokenizer(prompt_formatted).input_ids)\n\n        # Determine output length from reference answer tokens.\n        ref_out_len = len(\n            tokenizer(reference_answer, add_special_tokens=False).input_ids\n        )\n        expected_output_len = ref_out_len if dynamic_output else output_len\n\n        # Validate sequence lengths.\n        if not is_valid_sequence(prompt_len, expected_output_len):\n            continue\n\n        sampled_requests.append(\n            SampleRequest(\n                prompt=prompt_formatted,\n                prompt_len=prompt_len,\n                expected_output_len=expected_output_len,\n                request_id=request_id_prefix + str(ind),\n            )\n        )\n        ind += 1\n\n    self.maybe_oversample_requests(\n        sampled_requests, num_requests, request_id_prefix, no_oversample\n    )\n    return sampled_requests",
      "language": "json"
    },
    {
      "code": "3162\n3163\n3164\n3165\n3166\n3167\n3168\n3169\n3170\n3171\n3172\n3173\n3174\n3175\n3176\n3177\n3178\n3179\n3180\n3181\n3182\n3183\n3184\n3185\n3186\n3187\n3188\n3189\n3190\n3191\n3192\n3193\n3194\n3195\n3196\n3197\n3198\n3199\n3200\n3201\n3202\n3203\n3204\n3205\n3206\n3207\n3208\n3209\n3210\n3211\n3212\n3213\n3214\n3215\n3216\n3217\n3218\n3219\n3220\n3221\n3222\n3223\n3224\n3225\n3226\n3227",
      "language": "unknown"
    },
    {
      "code": "class MMStarDataset(HuggingFaceDataset):\n    \"\"\"\n    Lin-Chen/MMStar: https://huggingface.co/datasets/Lin-Chen/MMStar\n    refer to: https://github.com/sgl-project/SpecForge/pull/106\n    \"\"\"\n\n    DEFAULT_OUTPUT_LEN = 128\n    SUPPORTED_DATASET_PATHS = {\"Lin-Chen/MMStar\"}\n    IS_MULTIMODAL = True\n\n    def sample(\n        self,\n        tokenizer: TokenizerLike,\n        num_requests: int,\n        output_len: int | None = None,\n        enable_multimodal_chat: bool = False,\n        request_id_prefix: str = \"\",\n        no_oversample: bool = False,\n        **kwargs,\n    ) -> list[SampleRequest]:\n        # If --hf-output-len is not set, use the default output length.\n        output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN\n        sampled_requests: list[SampleRequest] = []\n\n        for ind, item in enumerate(self.data):\n            if len(sampled_requests) >= num_requests:\n                break\n            # Split the question text from options\n            # (keep only the part before \"Options:\").\n            full_q: str = item.get(\"question\", \"\")\n            question_text = full_q.split(\"Options:\", 1)[0].strip()\n\n            # Multimodal image content.\n            mm_content = process_image(item[\"image\"])\n\n            # Compute prompt token length (note: this is plain text length\n            # if enable_multimodal_chat is False).\n            prompt_len = len(tokenizer(question_text).input_ids)\n\n            if enable_multimodal_chat:\n                # If multimodal content should be embedded in the chat message,\n                # convert to [{\"role\":\"user\",\"content\":[...]}]\n                prompt = self.apply_multimodal_chat_transformation(\n                    question_text, mm_content\n                )\n                mm_for_request = None  # Already embedded in chat content.\n            else:\n                # Default: prompt is plain text,\n                # image is in mm_content for the bench to assemble.\n                prompt = question_text\n                mm_for_request = mm_content\n\n            sampled_requests.append(\n                SampleRequest(\n                    prompt=prompt,\n                    prompt_len=prompt_len,\n                    expected_output_len=output_len,\n                    multi_modal_data=mm_for_request,\n                    request_id=request_id_prefix + str(ind),\n                )\n            )\n\n        self.maybe_oversample_requests(\n            sampled_requests, num_requests, request_id_prefix, no_oversample\n        )\n        return sampled_requests",
      "language": "python"
    },
    {
      "code": "class MMStarDataset(HuggingFaceDataset):\n    \"\"\"\n    Lin-Chen/MMStar: https://huggingface.co/datasets/Lin-Chen/MMStar\n    refer to: https://github.com/sgl-project/SpecForge/pull/106\n    \"\"\"\n\n    DEFAULT_OUTPUT_LEN = 128\n    SUPPORTED_DATASET_PATHS = {\"Lin-Chen/MMStar\"}\n    IS_MULTIMODAL = True\n\n    def sample(\n        self,\n        tokenizer: TokenizerLike,\n        num_requests: int,\n        output_len: int | None = None,\n        enable_multimodal_chat: bool = False,\n        request_id_prefix: str = \"\",\n        no_oversample: bool = False,\n        **kwargs,\n    ) -> list[SampleRequest]:\n        # If --hf-output-len is not set, use the default output length.\n        output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN\n        sampled_requests: list[SampleRequest] = []\n\n        for ind, item in enumerate(self.data):\n            if len(sampled_requests) >= num_requests:\n                break\n            # Split the question text from options\n            # (keep only the part before \"Options:\").\n            full_q: str = item.get(\"question\", \"\")\n            question_text = full_q.split(\"Options:\", 1)[0].strip()\n\n            # Multimodal image content.\n            mm_content = process_image(item[\"image\"])\n\n            # Compute prompt token length (note: this is plain text length\n            # if enable_multimodal_chat is False).\n            prompt_len = len(tokenizer(question_text).input_ids)\n\n            if enable_multimodal_chat:\n                # If multimodal content should be embedded in the chat message,\n                # convert to [{\"role\":\"user\",\"content\":[...]}]\n                prompt = self.apply_multimodal_chat_transformation(\n                    question_text, mm_content\n                )\n                mm_for_request = None  # Already embedded in chat content.\n            else:\n                # Default: prompt is plain text,\n                # image is in mm_content for the bench to assemble.\n                prompt = question_text\n                mm_for_request = mm_content\n\n            sampled_requests.append(\n                SampleRequest(\n                    prompt=prompt,\n                    prompt_len=prompt_len,\n                    expected_output_len=output_len,\n                    multi_modal_data=mm_for_request,\n                    request_id=request_id_prefix + str(ind),\n                )\n            )\n\n        self.maybe_oversample_requests(\n            sampled_requests, num_requests, request_id_prefix, no_oversample\n        )\n        return sampled_requests",
      "language": "python"
    },
    {
      "code": "DEFAULT_OUTPUT_LEN = 128",
      "language": "unknown"
    },
    {
      "code": "DEFAULT_OUTPUT_LEN = 128",
      "language": "unknown"
    },
    {
      "code": "IS_MULTIMODAL = True",
      "language": "unknown"
    },
    {
      "code": "IS_MULTIMODAL = True",
      "language": "unknown"
    },
    {
      "code": "SUPPORTED_DATASET_PATHS = {'Lin-Chen/MMStar'}",
      "language": "unknown"
    },
    {
      "code": "SUPPORTED_DATASET_PATHS = {'Lin-Chen/MMStar'}",
      "language": "unknown"
    },
    {
      "code": "sample(\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    output_len: int | None = None,\n    enable_multimodal_chat: bool = False,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list[SampleRequest]",
      "language": "typescript"
    },
    {
      "code": "sample(\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    output_len: int | None = None,\n    enable_multimodal_chat: bool = False,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list[SampleRequest]",
      "language": "typescript"
    },
    {
      "code": "3172\n3173\n3174\n3175\n3176\n3177\n3178\n3179\n3180\n3181\n3182\n3183\n3184\n3185\n3186\n3187\n3188\n3189\n3190\n3191\n3192\n3193\n3194\n3195\n3196\n3197\n3198\n3199\n3200\n3201\n3202\n3203\n3204\n3205\n3206\n3207\n3208\n3209\n3210\n3211\n3212\n3213\n3214\n3215\n3216\n3217\n3218\n3219\n3220\n3221\n3222\n3223\n3224\n3225\n3226\n3227",
      "language": "unknown"
    },
    {
      "code": "def sample(\n    self,\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    output_len: int | None = None,\n    enable_multimodal_chat: bool = False,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list[SampleRequest]:\n    # If --hf-output-len is not set, use the default output length.\n    output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN\n    sampled_requests: list[SampleRequest] = []\n\n    for ind, item in enumerate(self.data):\n        if len(sampled_requests) >= num_requests:\n            break\n        # Split the question text from options\n        # (keep only the part before \"Options:\").\n        full_q: str = item.get(\"question\", \"\")\n        question_text = full_q.split(\"Options:\", 1)[0].strip()\n\n        # Multimodal image content.\n        mm_content = process_image(item[\"image\"])\n\n        # Compute prompt token length (note: this is plain text length\n        # if enable_multimodal_chat is False).\n        prompt_len = len(tokenizer(question_text).input_ids)\n\n        if enable_multimodal_chat:\n            # If multimodal content should be embedded in the chat message,\n            # convert to [{\"role\":\"user\",\"content\":[...]}]\n            prompt = self.apply_multimodal_chat_transformation(\n                question_text, mm_content\n            )\n            mm_for_request = None  # Already embedded in chat content.\n        else:\n            # Default: prompt is plain text,\n            # image is in mm_content for the bench to assemble.\n            prompt = question_text\n            mm_for_request = mm_content\n\n        sampled_requests.append(\n            SampleRequest(\n                prompt=prompt,\n                prompt_len=prompt_len,\n                expected_output_len=output_len,\n                multi_modal_data=mm_for_request,\n                request_id=request_id_prefix + str(ind),\n            )\n        )\n\n    self.maybe_oversample_requests(\n        sampled_requests, num_requests, request_id_prefix, no_oversample\n    )\n    return sampled_requests",
      "language": "python"
    },
    {
      "code": "def sample(\n    self,\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    output_len: int | None = None,\n    enable_multimodal_chat: bool = False,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list[SampleRequest]:\n    # If --hf-output-len is not set, use the default output length.\n    output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN\n    sampled_requests: list[SampleRequest] = []\n\n    for ind, item in enumerate(self.data):\n        if len(sampled_requests) >= num_requests:\n            break\n        # Split the question text from options\n        # (keep only the part before \"Options:\").\n        full_q: str = item.get(\"question\", \"\")\n        question_text = full_q.split(\"Options:\", 1)[0].strip()\n\n        # Multimodal image content.\n        mm_content = process_image(item[\"image\"])\n\n        # Compute prompt token length (note: this is plain text length\n        # if enable_multimodal_chat is False).\n        prompt_len = len(tokenizer(question_text).input_ids)\n\n        if enable_multimodal_chat:\n            # If multimodal content should be embedded in the chat message,\n            # convert to [{\"role\":\"user\",\"content\":[...]}]\n            prompt = self.apply_multimodal_chat_transformation(\n                question_text, mm_content\n            )\n            mm_for_request = None  # Already embedded in chat content.\n        else:\n            # Default: prompt is plain text,\n            # image is in mm_content for the bench to assemble.\n            prompt = question_text\n            mm_for_request = mm_content\n\n        sampled_requests.append(\n            SampleRequest(\n                prompt=prompt,\n                prompt_len=prompt_len,\n                expected_output_len=output_len,\n                multi_modal_data=mm_for_request,\n                request_id=request_id_prefix + str(ind),\n            )\n        )\n\n    self.maybe_oversample_requests(\n        sampled_requests, num_requests, request_id_prefix, no_oversample\n    )\n    return sampled_requests",
      "language": "python"
    },
    {
      "code": "2462\n2463\n2464\n2465\n2466\n2467\n2468\n2469\n2470\n2471\n2472\n2473\n2474\n2475\n2476\n2477\n2478\n2479\n2480\n2481\n2482\n2483\n2484\n2485\n2486\n2487\n2488\n2489\n2490\n2491\n2492\n2493\n2494\n2495\n2496\n2497\n2498\n2499\n2500\n2501\n2502\n2503\n2504\n2505\n2506\n2507\n2508\n2509\n2510\n2511\n2512\n2513",
      "language": "unknown"
    },
    {
      "code": "class MMVUDataset(HuggingFaceDataset):\n    \"\"\"\n    MMVU Dataset.\n    https://huggingface.co/datasets/yale-nlp/MMVU\n    \"\"\"\n\n    DEFAULT_OUTPUT_LEN = 128\n    SUPPORTED_DATASET_PATHS = {\n        \"yale-nlp/MMVU\": lambda x: x[\"question\"]\n        + \" \"\n        + (\" \".join(f\"{k}.{v}\" for k, v in x[\"choices\"].items())),\n    }\n\n    def sample(\n        self,\n        tokenizer: TokenizerLike,\n        num_requests: int,\n        output_len: int | None = None,\n        enable_multimodal_chat: bool = False,\n        request_id_prefix: str = \"\",\n        no_oversample: bool = False,\n        **kwargs,\n    ) -> list:\n        output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN\n        sampled_requests = []\n        for i, item in enumerate(self.data):\n            if len(sampled_requests) >= num_requests:\n                break\n            parser_fn = self.SUPPORTED_DATASET_PATHS.get(self.hf_name)\n            if parser_fn is None:\n                raise ValueError(f\"Unsupported dataset path: {self.hf_name}\")\n            prompt = parser_fn(item)\n            mm_content = process_video(item[\"video\"])\n            prompt_len = len(tokenizer(prompt).input_ids)\n            if enable_multimodal_chat:\n                # Note: when chat is enabled the request prompt_len is no longer\n                # accurate and we will be using request output to count the\n                # actual prompt len\n                prompt = self.apply_multimodal_chat_transformation(prompt, mm_content)\n            sampled_requests.append(\n                SampleRequest(\n                    prompt=prompt,\n                    prompt_len=prompt_len,\n                    expected_output_len=output_len,\n                    multi_modal_data=mm_content,\n                    request_id=request_id_prefix + str(i),\n                )\n            )\n        self.maybe_oversample_requests(\n            sampled_requests, num_requests, request_id_prefix, no_oversample\n        )\n        return sampled_requests",
      "language": "python"
    },
    {
      "code": "class MMVUDataset(HuggingFaceDataset):\n    \"\"\"\n    MMVU Dataset.\n    https://huggingface.co/datasets/yale-nlp/MMVU\n    \"\"\"\n\n    DEFAULT_OUTPUT_LEN = 128\n    SUPPORTED_DATASET_PATHS = {\n        \"yale-nlp/MMVU\": lambda x: x[\"question\"]\n        + \" \"\n        + (\" \".join(f\"{k}.{v}\" for k, v in x[\"choices\"].items())),\n    }\n\n    def sample(\n        self,\n        tokenizer: TokenizerLike,\n        num_requests: int,\n        output_len: int | None = None,\n        enable_multimodal_chat: bool = False,\n        request_id_prefix: str = \"\",\n        no_oversample: bool = False,\n        **kwargs,\n    ) -> list:\n        output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN\n        sampled_requests = []\n        for i, item in enumerate(self.data):\n            if len(sampled_requests) >= num_requests:\n                break\n            parser_fn = self.SUPPORTED_DATASET_PATHS.get(self.hf_name)\n            if parser_fn is None:\n                raise ValueError(f\"Unsupported dataset path: {self.hf_name}\")\n            prompt = parser_fn(item)\n            mm_content = process_video(item[\"video\"])\n            prompt_len = len(tokenizer(prompt).input_ids)\n            if enable_multimodal_chat:\n                # Note: when chat is enabled the request prompt_len is no longer\n                # accurate and we will be using request output to count the\n                # actual prompt len\n                prompt = self.apply_multimodal_chat_transformation(prompt, mm_content)\n            sampled_requests.append(\n                SampleRequest(\n                    prompt=prompt,\n                    prompt_len=prompt_len,\n                    expected_output_len=output_len,\n                    multi_modal_data=mm_content,\n                    request_id=request_id_prefix + str(i),\n                )\n            )\n        self.maybe_oversample_requests(\n            sampled_requests, num_requests, request_id_prefix, no_oversample\n        )\n        return sampled_requests",
      "language": "python"
    },
    {
      "code": "DEFAULT_OUTPUT_LEN = 128",
      "language": "unknown"
    },
    {
      "code": "DEFAULT_OUTPUT_LEN = 128",
      "language": "unknown"
    },
    {
      "code": "SUPPORTED_DATASET_PATHS = {\n    \"yale-nlp/MMVU\": lambda x: x[\"question\"]\n    + \" \"\n    + join(f\"{k}.{v}\" for k, v in (items()))\n}",
      "language": "unknown"
    },
    {
      "code": "SUPPORTED_DATASET_PATHS = {\n    \"yale-nlp/MMVU\": lambda x: x[\"question\"]\n    + \" \"\n    + join(f\"{k}.{v}\" for k, v in (items()))\n}",
      "language": "unknown"
    },
    {
      "code": "sample(\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    output_len: int | None = None,\n    enable_multimodal_chat: bool = False,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list",
      "language": "typescript"
    },
    {
      "code": "sample(\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    output_len: int | None = None,\n    enable_multimodal_chat: bool = False,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list",
      "language": "typescript"
    },
    {
      "code": "2475\n2476\n2477\n2478\n2479\n2480\n2481\n2482\n2483\n2484\n2485\n2486\n2487\n2488\n2489\n2490\n2491\n2492\n2493\n2494\n2495\n2496\n2497\n2498\n2499\n2500\n2501\n2502\n2503\n2504\n2505\n2506\n2507\n2508\n2509\n2510\n2511\n2512\n2513",
      "language": "unknown"
    },
    {
      "code": "def sample(\n    self,\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    output_len: int | None = None,\n    enable_multimodal_chat: bool = False,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list:\n    output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN\n    sampled_requests = []\n    for i, item in enumerate(self.data):\n        if len(sampled_requests) >= num_requests:\n            break\n        parser_fn = self.SUPPORTED_DATASET_PATHS.get(self.hf_name)\n        if parser_fn is None:\n            raise ValueError(f\"Unsupported dataset path: {self.hf_name}\")\n        prompt = parser_fn(item)\n        mm_content = process_video(item[\"video\"])\n        prompt_len = len(tokenizer(prompt).input_ids)\n        if enable_multimodal_chat:\n            # Note: when chat is enabled the request prompt_len is no longer\n            # accurate and we will be using request output to count the\n            # actual prompt len\n            prompt = self.apply_multimodal_chat_transformation(prompt, mm_content)\n        sampled_requests.append(\n            SampleRequest(\n                prompt=prompt,\n                prompt_len=prompt_len,\n                expected_output_len=output_len,\n                multi_modal_data=mm_content,\n                request_id=request_id_prefix + str(i),\n            )\n        )\n    self.maybe_oversample_requests(\n        sampled_requests, num_requests, request_id_prefix, no_oversample\n    )\n    return sampled_requests",
      "language": "python"
    },
    {
      "code": "def sample(\n    self,\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    output_len: int | None = None,\n    enable_multimodal_chat: bool = False,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list:\n    output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN\n    sampled_requests = []\n    for i, item in enumerate(self.data):\n        if len(sampled_requests) >= num_requests:\n            break\n        parser_fn = self.SUPPORTED_DATASET_PATHS.get(self.hf_name)\n        if parser_fn is None:\n            raise ValueError(f\"Unsupported dataset path: {self.hf_name}\")\n        prompt = parser_fn(item)\n        mm_content = process_video(item[\"video\"])\n        prompt_len = len(tokenizer(prompt).input_ids)\n        if enable_multimodal_chat:\n            # Note: when chat is enabled the request prompt_len is no longer\n            # accurate and we will be using request output to count the\n            # actual prompt len\n            prompt = self.apply_multimodal_chat_transformation(prompt, mm_content)\n        sampled_requests.append(\n            SampleRequest(\n                prompt=prompt,\n                prompt_len=prompt_len,\n                expected_output_len=output_len,\n                multi_modal_data=mm_content,\n                request_id=request_id_prefix + str(i),\n            )\n        )\n    self.maybe_oversample_requests(\n        sampled_requests, num_requests, request_id_prefix, no_oversample\n    )\n    return sampled_requests",
      "language": "python"
    },
    {
      "code": "2585\n2586\n2587\n2588\n2589\n2590\n2591\n2592\n2593\n2594\n2595\n2596\n2597\n2598\n2599\n2600\n2601\n2602\n2603\n2604\n2605\n2606\n2607\n2608\n2609\n2610\n2611\n2612\n2613\n2614\n2615\n2616\n2617\n2618\n2619\n2620\n2621\n2622\n2623\n2624\n2625\n2626\n2627\n2628\n2629\n2630\n2631\n2632\n2633\n2634\n2635\n2636\n2637\n2638\n2639",
      "language": "unknown"
    },
    {
      "code": "class MTBenchDataset(HuggingFaceDataset):\n    \"\"\"\n    MT-Bench Dataset.\n    https://huggingface.co/datasets/philschmid/mt-bench\n\n    We create a single turn dataset for MT-Bench.\n    This is similar to Spec decoding benchmark setup in vLLM\n    https://github.com/vllm-project/vllm/blob/9d98ab5ec/examples/offline_inference/eagle.py#L14-L18\n    \"\"\"  # noqa: E501\n\n    DEFAULT_OUTPUT_LEN = 256  # avg len used in SD bench in vLLM\n    SUPPORTED_DATASET_PATHS = {\n        \"philschmid/mt-bench\",\n    }\n\n    def sample(\n        self,\n        tokenizer: TokenizerLike,\n        num_requests: int,\n        output_len: int | None = None,\n        enable_multimodal_chat: bool = False,\n        skip_chat_template: bool = False,\n        request_id_prefix: str = \"\",\n        no_oversample: bool = False,\n        **kwargs,\n    ) -> list:\n        output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN\n        sampled_requests = []\n\n        for i, item in enumerate(self.data):\n            if len(sampled_requests) >= num_requests:\n                break\n            prompt = item[\"turns\"][0]\n\n            # apply template\n            if not skip_chat_template:\n                prompt = tokenizer.apply_chat_template(\n                    [{\"role\": \"user\", \"content\": prompt}],\n                    add_generation_prompt=True,\n                    tokenize=False,\n                )\n\n            prompt_len = len(tokenizer(prompt).input_ids)\n            sampled_requests.append(\n                SampleRequest(\n                    prompt=prompt,\n                    prompt_len=prompt_len,\n                    expected_output_len=output_len,\n                    request_id=request_id_prefix + str(i),\n                )\n            )\n        self.maybe_oversample_requests(\n            sampled_requests, num_requests, request_id_prefix, no_oversample\n        )\n        return sampled_requests",
      "language": "json"
    },
    {
      "code": "class MTBenchDataset(HuggingFaceDataset):\n    \"\"\"\n    MT-Bench Dataset.\n    https://huggingface.co/datasets/philschmid/mt-bench\n\n    We create a single turn dataset for MT-Bench.\n    This is similar to Spec decoding benchmark setup in vLLM\n    https://github.com/vllm-project/vllm/blob/9d98ab5ec/examples/offline_inference/eagle.py#L14-L18\n    \"\"\"  # noqa: E501\n\n    DEFAULT_OUTPUT_LEN = 256  # avg len used in SD bench in vLLM\n    SUPPORTED_DATASET_PATHS = {\n        \"philschmid/mt-bench\",\n    }\n\n    def sample(\n        self,\n        tokenizer: TokenizerLike,\n        num_requests: int,\n        output_len: int | None = None,\n        enable_multimodal_chat: bool = False,\n        skip_chat_template: bool = False,\n        request_id_prefix: str = \"\",\n        no_oversample: bool = False,\n        **kwargs,\n    ) -> list:\n        output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN\n        sampled_requests = []\n\n        for i, item in enumerate(self.data):\n            if len(sampled_requests) >= num_requests:\n                break\n            prompt = item[\"turns\"][0]\n\n            # apply template\n            if not skip_chat_template:\n                prompt = tokenizer.apply_chat_template(\n                    [{\"role\": \"user\", \"content\": prompt}],\n                    add_generation_prompt=True,\n                    tokenize=False,\n                )\n\n            prompt_len = len(tokenizer(prompt).input_ids)\n            sampled_requests.append(\n                SampleRequest(\n                    prompt=prompt,\n                    prompt_len=prompt_len,\n                    expected_output_len=output_len,\n                    request_id=request_id_prefix + str(i),\n                )\n            )\n        self.maybe_oversample_requests(\n            sampled_requests, num_requests, request_id_prefix, no_oversample\n        )\n        return sampled_requests",
      "language": "json"
    },
    {
      "code": "DEFAULT_OUTPUT_LEN = 256",
      "language": "unknown"
    },
    {
      "code": "DEFAULT_OUTPUT_LEN = 256",
      "language": "unknown"
    },
    {
      "code": "SUPPORTED_DATASET_PATHS = {'philschmid/mt-bench'}",
      "language": "unknown"
    },
    {
      "code": "SUPPORTED_DATASET_PATHS = {'philschmid/mt-bench'}",
      "language": "unknown"
    },
    {
      "code": "sample(\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    output_len: int | None = None,\n    enable_multimodal_chat: bool = False,\n    skip_chat_template: bool = False,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list",
      "language": "typescript"
    },
    {
      "code": "sample(\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    output_len: int | None = None,\n    enable_multimodal_chat: bool = False,\n    skip_chat_template: bool = False,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list",
      "language": "typescript"
    },
    {
      "code": "2600\n2601\n2602\n2603\n2604\n2605\n2606\n2607\n2608\n2609\n2610\n2611\n2612\n2613\n2614\n2615\n2616\n2617\n2618\n2619\n2620\n2621\n2622\n2623\n2624\n2625\n2626\n2627\n2628\n2629\n2630\n2631\n2632\n2633\n2634\n2635\n2636\n2637\n2638\n2639",
      "language": "unknown"
    },
    {
      "code": "def sample(\n    self,\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    output_len: int | None = None,\n    enable_multimodal_chat: bool = False,\n    skip_chat_template: bool = False,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list:\n    output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN\n    sampled_requests = []\n\n    for i, item in enumerate(self.data):\n        if len(sampled_requests) >= num_requests:\n            break\n        prompt = item[\"turns\"][0]\n\n        # apply template\n        if not skip_chat_template:\n            prompt = tokenizer.apply_chat_template(\n                [{\"role\": \"user\", \"content\": prompt}],\n                add_generation_prompt=True,\n                tokenize=False,\n            )\n\n        prompt_len = len(tokenizer(prompt).input_ids)\n        sampled_requests.append(\n            SampleRequest(\n                prompt=prompt,\n                prompt_len=prompt_len,\n                expected_output_len=output_len,\n                request_id=request_id_prefix + str(i),\n            )\n        )\n    self.maybe_oversample_requests(\n        sampled_requests, num_requests, request_id_prefix, no_oversample\n    )\n    return sampled_requests",
      "language": "json"
    },
    {
      "code": "def sample(\n    self,\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    output_len: int | None = None,\n    enable_multimodal_chat: bool = False,\n    skip_chat_template: bool = False,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list:\n    output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN\n    sampled_requests = []\n\n    for i, item in enumerate(self.data):\n        if len(sampled_requests) >= num_requests:\n            break\n        prompt = item[\"turns\"][0]\n\n        # apply template\n        if not skip_chat_template:\n            prompt = tokenizer.apply_chat_template(\n                [{\"role\": \"user\", \"content\": prompt}],\n                add_generation_prompt=True,\n                tokenize=False,\n            )\n\n        prompt_len = len(tokenizer(prompt).input_ids)\n        sampled_requests.append(\n            SampleRequest(\n                prompt=prompt,\n                prompt_len=prompt_len,\n                expected_output_len=output_len,\n                request_id=request_id_prefix + str(i),\n            )\n        )\n    self.maybe_oversample_requests(\n        sampled_requests, num_requests, request_id_prefix, no_oversample\n    )\n    return sampled_requests",
      "language": "json"
    },
    {
      "code": "2344\n2345\n2346\n2347\n2348\n2349\n2350\n2351\n2352\n2353\n2354\n2355\n2356\n2357\n2358\n2359\n2360\n2361\n2362\n2363\n2364\n2365\n2366\n2367\n2368\n2369\n2370\n2371\n2372\n2373\n2374\n2375\n2376\n2377\n2378\n2379\n2380\n2381\n2382\n2383\n2384\n2385\n2386\n2387\n2388\n2389\n2390\n2391\n2392\n2393\n2394\n2395\n2396\n2397\n2398\n2399\n2400\n2401",
      "language": "unknown"
    },
    {
      "code": "class MultiModalConversationDataset(HuggingFaceDataset):\n    \"\"\"Dataset for multimodal conversation data.\"\"\"\n\n    SUPPORTED_DATASET_PATHS = {\n        \"lmms-lab/LLaVA-OneVision-Data\",\n    }\n    IS_MULTIMODAL = True\n\n    def sample(\n        self,\n        tokenizer: TokenizerLike,\n        num_requests: int,\n        output_len: int | None = None,\n        enable_multimodal_chat: bool = False,\n        request_id_prefix: str = \"\",\n        no_oversample: bool = False,\n        **kwargs,\n    ) -> list:\n        # Filter examples with at least 2 conversations\n        filtered_data = self.data.filter(lambda x: len(x[\"conversations\"]) >= 2)\n        sampled_requests = []\n        ind = 0\n        dynamic_output = output_len is None\n\n        for item in filtered_data:\n            if len(sampled_requests) >= num_requests:\n                break\n            conv = item[\"conversations\"]\n            prompt, completion = conv[0][\"value\"], conv[1][\"value\"]\n\n            prompt_ids = tokenizer(prompt).input_ids\n            completion_ids = tokenizer(completion).input_ids\n            prompt_len = len(prompt_ids)\n            completion_len = len(completion_ids)\n            output_len = completion_len if dynamic_output else output_len\n            assert isinstance(output_len, int) and output_len > 0\n            if dynamic_output and not is_valid_sequence(prompt_len, completion_len):\n                continue\n            mm_content = process_image(item[\"image\"]) if \"image\" in item else None\n            if enable_multimodal_chat:\n                # Note: when chat is enabled the request prompt_len is no longer\n                # accurate and we will be using request output to count the\n                # actual prompt len and output len\n                prompt = self.apply_multimodal_chat_transformation(prompt, mm_content)\n            sampled_requests.append(\n                SampleRequest(\n                    prompt=prompt,\n                    prompt_len=prompt_len,\n                    expected_output_len=output_len,\n                    multi_modal_data=mm_content,\n                    request_id=request_id_prefix + str(ind),\n                )\n            )\n            ind += 1\n        self.maybe_oversample_requests(\n            sampled_requests, num_requests, request_id_prefix, no_oversample\n        )\n        return sampled_requests",
      "language": "python"
    },
    {
      "code": "class MultiModalConversationDataset(HuggingFaceDataset):\n    \"\"\"Dataset for multimodal conversation data.\"\"\"\n\n    SUPPORTED_DATASET_PATHS = {\n        \"lmms-lab/LLaVA-OneVision-Data\",\n    }\n    IS_MULTIMODAL = True\n\n    def sample(\n        self,\n        tokenizer: TokenizerLike,\n        num_requests: int,\n        output_len: int | None = None,\n        enable_multimodal_chat: bool = False,\n        request_id_prefix: str = \"\",\n        no_oversample: bool = False,\n        **kwargs,\n    ) -> list:\n        # Filter examples with at least 2 conversations\n        filtered_data = self.data.filter(lambda x: len(x[\"conversations\"]) >= 2)\n        sampled_requests = []\n        ind = 0\n        dynamic_output = output_len is None\n\n        for item in filtered_data:\n            if len(sampled_requests) >= num_requests:\n                break\n            conv = item[\"conversations\"]\n            prompt, completion = conv[0][\"value\"], conv[1][\"value\"]\n\n            prompt_ids = tokenizer(prompt).input_ids\n            completion_ids = tokenizer(completion).input_ids\n            prompt_len = len(prompt_ids)\n            completion_len = len(completion_ids)\n            output_len = completion_len if dynamic_output else output_len\n            assert isinstance(output_len, int) and output_len > 0\n            if dynamic_output and not is_valid_sequence(prompt_len, completion_len):\n                continue\n            mm_content = process_image(item[\"image\"]) if \"image\" in item else None\n            if enable_multimodal_chat:\n                # Note: when chat is enabled the request prompt_len is no longer\n                # accurate and we will be using request output to count the\n                # actual prompt len and output len\n                prompt = self.apply_multimodal_chat_transformation(prompt, mm_content)\n            sampled_requests.append(\n                SampleRequest(\n                    prompt=prompt,\n                    prompt_len=prompt_len,\n                    expected_output_len=output_len,\n                    multi_modal_data=mm_content,\n                    request_id=request_id_prefix + str(ind),\n                )\n            )\n            ind += 1\n        self.maybe_oversample_requests(\n            sampled_requests, num_requests, request_id_prefix, no_oversample\n        )\n        return sampled_requests",
      "language": "python"
    },
    {
      "code": "IS_MULTIMODAL = True",
      "language": "unknown"
    },
    {
      "code": "IS_MULTIMODAL = True",
      "language": "unknown"
    },
    {
      "code": "SUPPORTED_DATASET_PATHS = {'lmms-lab/LLaVA-OneVision-Data'}",
      "language": "unknown"
    },
    {
      "code": "SUPPORTED_DATASET_PATHS = {'lmms-lab/LLaVA-OneVision-Data'}",
      "language": "unknown"
    },
    {
      "code": "sample(\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    output_len: int | None = None,\n    enable_multimodal_chat: bool = False,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list",
      "language": "typescript"
    },
    {
      "code": "sample(\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    output_len: int | None = None,\n    enable_multimodal_chat: bool = False,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list",
      "language": "typescript"
    },
    {
      "code": "2352\n2353\n2354\n2355\n2356\n2357\n2358\n2359\n2360\n2361\n2362\n2363\n2364\n2365\n2366\n2367\n2368\n2369\n2370\n2371\n2372\n2373\n2374\n2375\n2376\n2377\n2378\n2379\n2380\n2381\n2382\n2383\n2384\n2385\n2386\n2387\n2388\n2389\n2390\n2391\n2392\n2393\n2394\n2395\n2396\n2397\n2398\n2399\n2400\n2401",
      "language": "unknown"
    },
    {
      "code": "def sample(\n    self,\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    output_len: int | None = None,\n    enable_multimodal_chat: bool = False,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list:\n    # Filter examples with at least 2 conversations\n    filtered_data = self.data.filter(lambda x: len(x[\"conversations\"]) >= 2)\n    sampled_requests = []\n    ind = 0\n    dynamic_output = output_len is None\n\n    for item in filtered_data:\n        if len(sampled_requests) >= num_requests:\n            break\n        conv = item[\"conversations\"]\n        prompt, completion = conv[0][\"value\"], conv[1][\"value\"]\n\n        prompt_ids = tokenizer(prompt).input_ids\n        completion_ids = tokenizer(completion).input_ids\n        prompt_len = len(prompt_ids)\n        completion_len = len(completion_ids)\n        output_len = completion_len if dynamic_output else output_len\n        assert isinstance(output_len, int) and output_len > 0\n        if dynamic_output and not is_valid_sequence(prompt_len, completion_len):\n            continue\n        mm_content = process_image(item[\"image\"]) if \"image\" in item else None\n        if enable_multimodal_chat:\n            # Note: when chat is enabled the request prompt_len is no longer\n            # accurate and we will be using request output to count the\n            # actual prompt len and output len\n            prompt = self.apply_multimodal_chat_transformation(prompt, mm_content)\n        sampled_requests.append(\n            SampleRequest(\n                prompt=prompt,\n                prompt_len=prompt_len,\n                expected_output_len=output_len,\n                multi_modal_data=mm_content,\n                request_id=request_id_prefix + str(ind),\n            )\n        )\n        ind += 1\n    self.maybe_oversample_requests(\n        sampled_requests, num_requests, request_id_prefix, no_oversample\n    )\n    return sampled_requests",
      "language": "python"
    },
    {
      "code": "def sample(\n    self,\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    output_len: int | None = None,\n    enable_multimodal_chat: bool = False,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list:\n    # Filter examples with at least 2 conversations\n    filtered_data = self.data.filter(lambda x: len(x[\"conversations\"]) >= 2)\n    sampled_requests = []\n    ind = 0\n    dynamic_output = output_len is None\n\n    for item in filtered_data:\n        if len(sampled_requests) >= num_requests:\n            break\n        conv = item[\"conversations\"]\n        prompt, completion = conv[0][\"value\"], conv[1][\"value\"]\n\n        prompt_ids = tokenizer(prompt).input_ids\n        completion_ids = tokenizer(completion).input_ids\n        prompt_len = len(prompt_ids)\n        completion_len = len(completion_ids)\n        output_len = completion_len if dynamic_output else output_len\n        assert isinstance(output_len, int) and output_len > 0\n        if dynamic_output and not is_valid_sequence(prompt_len, completion_len):\n            continue\n        mm_content = process_image(item[\"image\"]) if \"image\" in item else None\n        if enable_multimodal_chat:\n            # Note: when chat is enabled the request prompt_len is no longer\n            # accurate and we will be using request output to count the\n            # actual prompt len and output len\n            prompt = self.apply_multimodal_chat_transformation(prompt, mm_content)\n        sampled_requests.append(\n            SampleRequest(\n                prompt=prompt,\n                prompt_len=prompt_len,\n                expected_output_len=output_len,\n                multi_modal_data=mm_content,\n                request_id=request_id_prefix + str(ind),\n            )\n        )\n        ind += 1\n    self.maybe_oversample_requests(\n        sampled_requests, num_requests, request_id_prefix, no_oversample\n    )\n    return sampled_requests",
      "language": "python"
    },
    {
      "code": "2845\n2846\n2847\n2848\n2849\n2850\n2851\n2852\n2853\n2854\n2855\n2856\n2857\n2858\n2859\n2860\n2861\n2862\n2863\n2864\n2865\n2866\n2867\n2868\n2869\n2870\n2871\n2872\n2873\n2874\n2875\n2876\n2877\n2878\n2879\n2880\n2881\n2882\n2883\n2884\n2885\n2886",
      "language": "unknown"
    },
    {
      "code": "class NextEditPredictionDataset(HuggingFaceDataset):\n    \"\"\"\n    Dataset class for processing a Next Edit Prediction dataset.\n    \"\"\"\n\n    SUPPORTED_DATASET_PATHS = {\n        \"zed-industries/zeta\",\n    }\n    MAPPING_PROMPT_FUNCS = {\n        \"zed-industries/zeta\": _format_zeta_prompt,\n    }\n\n    def sample(\n        self,\n        tokenizer: TokenizerLike,\n        num_requests: int,\n        request_id_prefix: str = \"\",\n        no_oversample: bool = False,\n        **kwargs,\n    ):\n        formatting_prompt_func = self.MAPPING_PROMPT_FUNCS.get(self.hf_name)\n        if formatting_prompt_func is None:\n            raise ValueError(f\"Unsupported dataset path: {self.hf_name}\")\n        samples = []\n        for i, sample in enumerate(self.data):\n            sample = formatting_prompt_func(sample)\n            samples.append(\n                SampleRequest(\n                    prompt=sample[\"prompt\"],\n                    prompt_len=len(tokenizer(sample[\"prompt\"]).input_ids),\n                    expected_output_len=len(\n                        tokenizer(sample[\"expected_output\"]).input_ids\n                    ),\n                    request_id=request_id_prefix + str(i),\n                )\n            )\n            if len(samples) >= num_requests:\n                break\n        self.maybe_oversample_requests(\n            samples, num_requests, request_id_prefix, no_oversample\n        )\n        return samples",
      "language": "python"
    },
    {
      "code": "class NextEditPredictionDataset(HuggingFaceDataset):\n    \"\"\"\n    Dataset class for processing a Next Edit Prediction dataset.\n    \"\"\"\n\n    SUPPORTED_DATASET_PATHS = {\n        \"zed-industries/zeta\",\n    }\n    MAPPING_PROMPT_FUNCS = {\n        \"zed-industries/zeta\": _format_zeta_prompt,\n    }\n\n    def sample(\n        self,\n        tokenizer: TokenizerLike,\n        num_requests: int,\n        request_id_prefix: str = \"\",\n        no_oversample: bool = False,\n        **kwargs,\n    ):\n        formatting_prompt_func = self.MAPPING_PROMPT_FUNCS.get(self.hf_name)\n        if formatting_prompt_func is None:\n            raise ValueError(f\"Unsupported dataset path: {self.hf_name}\")\n        samples = []\n        for i, sample in enumerate(self.data):\n            sample = formatting_prompt_func(sample)\n            samples.append(\n                SampleRequest(\n                    prompt=sample[\"prompt\"],\n                    prompt_len=len(tokenizer(sample[\"prompt\"]).input_ids),\n                    expected_output_len=len(\n                        tokenizer(sample[\"expected_output\"]).input_ids\n                    ),\n                    request_id=request_id_prefix + str(i),\n                )\n            )\n            if len(samples) >= num_requests:\n                break\n        self.maybe_oversample_requests(\n            samples, num_requests, request_id_prefix, no_oversample\n        )\n        return samples",
      "language": "python"
    },
    {
      "code": "MAPPING_PROMPT_FUNCS = {\n    \"zed-industries/zeta\": _format_zeta_prompt\n}",
      "language": "unknown"
    },
    {
      "code": "MAPPING_PROMPT_FUNCS = {\n    \"zed-industries/zeta\": _format_zeta_prompt\n}",
      "language": "unknown"
    },
    {
      "code": "SUPPORTED_DATASET_PATHS = {'zed-industries/zeta'}",
      "language": "unknown"
    },
    {
      "code": "SUPPORTED_DATASET_PATHS = {'zed-industries/zeta'}",
      "language": "unknown"
    },
    {
      "code": "sample(\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n)",
      "language": "typescript"
    },
    {
      "code": "sample(\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n)",
      "language": "typescript"
    },
    {
      "code": "2857\n2858\n2859\n2860\n2861\n2862\n2863\n2864\n2865\n2866\n2867\n2868\n2869\n2870\n2871\n2872\n2873\n2874\n2875\n2876\n2877\n2878\n2879\n2880\n2881\n2882\n2883\n2884\n2885\n2886",
      "language": "unknown"
    },
    {
      "code": "def sample(\n    self,\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n):\n    formatting_prompt_func = self.MAPPING_PROMPT_FUNCS.get(self.hf_name)\n    if formatting_prompt_func is None:\n        raise ValueError(f\"Unsupported dataset path: {self.hf_name}\")\n    samples = []\n    for i, sample in enumerate(self.data):\n        sample = formatting_prompt_func(sample)\n        samples.append(\n            SampleRequest(\n                prompt=sample[\"prompt\"],\n                prompt_len=len(tokenizer(sample[\"prompt\"]).input_ids),\n                expected_output_len=len(\n                    tokenizer(sample[\"expected_output\"]).input_ids\n                ),\n                request_id=request_id_prefix + str(i),\n            )\n        )\n        if len(samples) >= num_requests:\n            break\n    self.maybe_oversample_requests(\n        samples, num_requests, request_id_prefix, no_oversample\n    )\n    return samples",
      "language": "python"
    },
    {
      "code": "def sample(\n    self,\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n):\n    formatting_prompt_func = self.MAPPING_PROMPT_FUNCS.get(self.hf_name)\n    if formatting_prompt_func is None:\n        raise ValueError(f\"Unsupported dataset path: {self.hf_name}\")\n    samples = []\n    for i, sample in enumerate(self.data):\n        sample = formatting_prompt_func(sample)\n        samples.append(\n            SampleRequest(\n                prompt=sample[\"prompt\"],\n                prompt_len=len(tokenizer(sample[\"prompt\"]).input_ids),\n                expected_output_len=len(\n                    tokenizer(sample[\"expected_output\"]).input_ids\n                ),\n                request_id=request_id_prefix + str(i),\n            )\n        )\n        if len(samples) >= num_requests:\n            break\n    self.maybe_oversample_requests(\n        samples, num_requests, request_id_prefix, no_oversample\n    )\n    return samples",
      "language": "python"
    },
    {
      "code": "3070\n3071\n3072\n3073\n3074\n3075\n3076\n3077\n3078\n3079\n3080\n3081\n3082\n3083\n3084\n3085\n3086\n3087\n3088\n3089\n3090\n3091\n3092\n3093\n3094\n3095\n3096\n3097\n3098\n3099\n3100\n3101\n3102\n3103\n3104\n3105\n3106\n3107\n3108\n3109\n3110\n3111\n3112\n3113\n3114\n3115\n3116\n3117\n3118\n3119\n3120\n3121\n3122\n3123\n3124\n3125\n3126\n3127\n3128\n3129\n3130\n3131\n3132\n3133\n3134\n3135\n3136\n3137\n3138\n3139\n3140\n3141\n3142\n3143\n3144\n3145\n3146\n3147\n3148\n3149\n3150\n3151\n3152\n3153\n3154",
      "language": "unknown"
    },
    {
      "code": "class PrefixRepetitionRandomDataset(BenchmarkDataset):\n    # Default values copied from benchmark_serving.py for the repeated prefix\n    # dataset.\n    DEFAULT_PREFIX_LEN = 256\n    DEFAULT_SUFFIX_LEN = 256\n    DEFAULT_NUM_PREFIXES = 10\n    DEFAULT_OUTPUT_LEN = 128\n\n    def __init__(\n        self,\n        **kwargs,\n    ) -> None:\n        super().__init__(**kwargs)\n        random.seed(self.random_seed)\n        np.random.seed(self.random_seed)\n\n    def sample(\n        self,\n        tokenizer: TokenizerLike,\n        num_requests: int,\n        prefix_len: int = DEFAULT_PREFIX_LEN,\n        suffix_len: int = DEFAULT_SUFFIX_LEN,\n        num_prefixes: int = DEFAULT_NUM_PREFIXES,\n        output_len: int = DEFAULT_OUTPUT_LEN,\n        request_id_prefix: str = \"\",\n        no_oversample: bool = False,\n        **kwargs,\n    ) -> list[SampleRequest]:\n        vocab_size = tokenizer.vocab_size\n        prompts_per_prefix = num_requests // num_prefixes\n        if prompts_per_prefix == 0:\n            raise ValueError(\n                f\"num_requests ({num_requests}) must be greater than or equal \"\n                f\"to num_prefixes ({num_prefixes})\"\n            )\n\n        def _generate_exact_length_tokens(target_length: int) -> list[int]:\n            \"\"\"Generate tokens that decode and re-encode to exactly\n            target_length.\"\"\"\n            # Generate random tokens\n            tokens = np.random.randint(0, vocab_size, size=target_length).tolist()\n\n            _, adjusted_tokens, token_mismatch = gen_prompt_decode_to_target_len(  # noqa: E501\n                tokenizer=tokenizer,\n                token_sequence=tokens,\n                target_token_len=target_length,\n                add_special_tokens=False,\n            )\n            return adjusted_tokens, token_mismatch\n\n        requests = []\n        token_mismatch_total = 0\n        for _ in range(num_prefixes):\n            prefix_tokens, prefix_mismatch = _generate_exact_length_tokens(prefix_len)\n            token_mismatch_total += prefix_mismatch\n\n            for _ in range(prompts_per_prefix):\n                suffix_tokens, suffix_mismatch = _generate_exact_length_tokens(\n                    suffix_len\n                )\n                token_mismatch_total += suffix_mismatch\n                combined_tokens = prefix_tokens + suffix_tokens\n                prompt = tokenizer.decode(combined_tokens)\n                prompt_len = len(combined_tokens)\n                requests.append(\n                    SampleRequest(\n                        prompt=prompt,\n                        prompt_len=prompt_len,\n                        expected_output_len=output_len,\n                    )\n                )\n\n        if token_mismatch_total != 0:\n            sign = \"more\" if token_mismatch_total > 0 else \"fewer\"\n            logger.warning(\n                \"Across all generated prompts, there were %d %s tokens \"\n                \"than expected after decoding and re-encoding. This is \"\n                \"expected due to the imperfect nature of the sampling \"\n                \"procedure.\",\n                abs(token_mismatch_total),\n                sign,\n            )\n        if not getattr(self, \"disable_shuffle\", False):\n            random.shuffle(requests)\n        return requests",
      "language": "python"
    },
    {
      "code": "class PrefixRepetitionRandomDataset(BenchmarkDataset):\n    # Default values copied from benchmark_serving.py for the repeated prefix\n    # dataset.\n    DEFAULT_PREFIX_LEN = 256\n    DEFAULT_SUFFIX_LEN = 256\n    DEFAULT_NUM_PREFIXES = 10\n    DEFAULT_OUTPUT_LEN = 128\n\n    def __init__(\n        self,\n        **kwargs,\n    ) -> None:\n        super().__init__(**kwargs)\n        random.seed(self.random_seed)\n        np.random.seed(self.random_seed)\n\n    def sample(\n        self,\n        tokenizer: TokenizerLike,\n        num_requests: int,\n        prefix_len: int = DEFAULT_PREFIX_LEN,\n        suffix_len: int = DEFAULT_SUFFIX_LEN,\n        num_prefixes: int = DEFAULT_NUM_PREFIXES,\n        output_len: int = DEFAULT_OUTPUT_LEN,\n        request_id_prefix: str = \"\",\n        no_oversample: bool = False,\n        **kwargs,\n    ) -> list[SampleRequest]:\n        vocab_size = tokenizer.vocab_size\n        prompts_per_prefix = num_requests // num_prefixes\n        if prompts_per_prefix == 0:\n            raise ValueError(\n                f\"num_requests ({num_requests}) must be greater than or equal \"\n                f\"to num_prefixes ({num_prefixes})\"\n            )\n\n        def _generate_exact_length_tokens(target_length: int) -> list[int]:\n            \"\"\"Generate tokens that decode and re-encode to exactly\n            target_length.\"\"\"\n            # Generate random tokens\n            tokens = np.random.randint(0, vocab_size, size=target_length).tolist()\n\n            _, adjusted_tokens, token_mismatch = gen_prompt_decode_to_target_len(  # noqa: E501\n                tokenizer=tokenizer,\n                token_sequence=tokens,\n                target_token_len=target_length,\n                add_special_tokens=False,\n            )\n            return adjusted_tokens, token_mismatch\n\n        requests = []\n        token_mismatch_total = 0\n        for _ in range(num_prefixes):\n            prefix_tokens, prefix_mismatch = _generate_exact_length_tokens(prefix_len)\n            token_mismatch_total += prefix_mismatch\n\n            for _ in range(prompts_per_prefix):\n                suffix_tokens, suffix_mismatch = _generate_exact_length_tokens(\n                    suffix_len\n                )\n                token_mismatch_total += suffix_mismatch\n                combined_tokens = prefix_tokens + suffix_tokens\n                prompt = tokenizer.decode(combined_tokens)\n                prompt_len = len(combined_tokens)\n                requests.append(\n                    SampleRequest(\n                        prompt=prompt,\n                        prompt_len=prompt_len,\n                        expected_output_len=output_len,\n                    )\n                )\n\n        if token_mismatch_total != 0:\n            sign = \"more\" if token_mismatch_total > 0 else \"fewer\"\n            logger.warning(\n                \"Across all generated prompts, there were %d %s tokens \"\n                \"than expected after decoding and re-encoding. This is \"\n                \"expected due to the imperfect nature of the sampling \"\n                \"procedure.\",\n                abs(token_mismatch_total),\n                sign,\n            )\n        if not getattr(self, \"disable_shuffle\", False):\n            random.shuffle(requests)\n        return requests",
      "language": "python"
    },
    {
      "code": "DEFAULT_NUM_PREFIXES = 10",
      "language": "unknown"
    },
    {
      "code": "DEFAULT_NUM_PREFIXES = 10",
      "language": "unknown"
    },
    {
      "code": "DEFAULT_OUTPUT_LEN = 128",
      "language": "unknown"
    },
    {
      "code": "DEFAULT_OUTPUT_LEN = 128",
      "language": "unknown"
    },
    {
      "code": "DEFAULT_PREFIX_LEN = 256",
      "language": "unknown"
    },
    {
      "code": "DEFAULT_PREFIX_LEN = 256",
      "language": "unknown"
    },
    {
      "code": "DEFAULT_SUFFIX_LEN = 256",
      "language": "unknown"
    },
    {
      "code": "DEFAULT_SUFFIX_LEN = 256",
      "language": "unknown"
    },
    {
      "code": "__init__(**kwargs) -> None",
      "language": "python"
    },
    {
      "code": "__init__(**kwargs) -> None",
      "language": "python"
    },
    {
      "code": "3078\n3079\n3080\n3081\n3082\n3083\n3084",
      "language": "unknown"
    },
    {
      "code": "def __init__(\n    self,\n    **kwargs,\n) -> None:\n    super().__init__(**kwargs)\n    random.seed(self.random_seed)\n    np.random.seed(self.random_seed)",
      "language": "python"
    },
    {
      "code": "def __init__(\n    self,\n    **kwargs,\n) -> None:\n    super().__init__(**kwargs)\n    random.seed(self.random_seed)\n    np.random.seed(self.random_seed)",
      "language": "python"
    },
    {
      "code": "sample(\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    prefix_len: int = DEFAULT_PREFIX_LEN,\n    suffix_len: int = DEFAULT_SUFFIX_LEN,\n    num_prefixes: int = DEFAULT_NUM_PREFIXES,\n    output_len: int = DEFAULT_OUTPUT_LEN,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list[SampleRequest]",
      "language": "typescript"
    },
    {
      "code": "sample(\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    prefix_len: int = DEFAULT_PREFIX_LEN,\n    suffix_len: int = DEFAULT_SUFFIX_LEN,\n    num_prefixes: int = DEFAULT_NUM_PREFIXES,\n    output_len: int = DEFAULT_OUTPUT_LEN,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list[SampleRequest]",
      "language": "typescript"
    },
    {
      "code": "3086\n3087\n3088\n3089\n3090\n3091\n3092\n3093\n3094\n3095\n3096\n3097\n3098\n3099\n3100\n3101\n3102\n3103\n3104\n3105\n3106\n3107\n3108\n3109\n3110\n3111\n3112\n3113\n3114\n3115\n3116\n3117\n3118\n3119\n3120\n3121\n3122\n3123\n3124\n3125\n3126\n3127\n3128\n3129\n3130\n3131\n3132\n3133\n3134\n3135\n3136\n3137\n3138\n3139\n3140\n3141\n3142\n3143\n3144\n3145\n3146\n3147\n3148\n3149\n3150\n3151\n3152\n3153\n3154",
      "language": "unknown"
    },
    {
      "code": "def sample(\n    self,\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    prefix_len: int = DEFAULT_PREFIX_LEN,\n    suffix_len: int = DEFAULT_SUFFIX_LEN,\n    num_prefixes: int = DEFAULT_NUM_PREFIXES,\n    output_len: int = DEFAULT_OUTPUT_LEN,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list[SampleRequest]:\n    vocab_size = tokenizer.vocab_size\n    prompts_per_prefix = num_requests // num_prefixes\n    if prompts_per_prefix == 0:\n        raise ValueError(\n            f\"num_requests ({num_requests}) must be greater than or equal \"\n            f\"to num_prefixes ({num_prefixes})\"\n        )\n\n    def _generate_exact_length_tokens(target_length: int) -> list[int]:\n        \"\"\"Generate tokens that decode and re-encode to exactly\n        target_length.\"\"\"\n        # Generate random tokens\n        tokens = np.random.randint(0, vocab_size, size=target_length).tolist()\n\n        _, adjusted_tokens, token_mismatch = gen_prompt_decode_to_target_len(  # noqa: E501\n            tokenizer=tokenizer,\n            token_sequence=tokens,\n            target_token_len=target_length,\n            add_special_tokens=False,\n        )\n        return adjusted_tokens, token_mismatch\n\n    requests = []\n    token_mismatch_total = 0\n    for _ in range(num_prefixes):\n        prefix_tokens, prefix_mismatch = _generate_exact_length_tokens(prefix_len)\n        token_mismatch_total += prefix_mismatch\n\n        for _ in range(prompts_per_prefix):\n            suffix_tokens, suffix_mismatch = _generate_exact_length_tokens(\n                suffix_len\n            )\n            token_mismatch_total += suffix_mismatch\n            combined_tokens = prefix_tokens + suffix_tokens\n            prompt = tokenizer.decode(combined_tokens)\n            prompt_len = len(combined_tokens)\n            requests.append(\n                SampleRequest(\n                    prompt=prompt,\n                    prompt_len=prompt_len,\n                    expected_output_len=output_len,\n                )\n            )\n\n    if token_mismatch_total != 0:\n        sign = \"more\" if token_mismatch_total > 0 else \"fewer\"\n        logger.warning(\n            \"Across all generated prompts, there were %d %s tokens \"\n            \"than expected after decoding and re-encoding. This is \"\n            \"expected due to the imperfect nature of the sampling \"\n            \"procedure.\",\n            abs(token_mismatch_total),\n            sign,\n        )\n    if not getattr(self, \"disable_shuffle\", False):\n        random.shuffle(requests)\n    return requests",
      "language": "python"
    },
    {
      "code": "def sample(\n    self,\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    prefix_len: int = DEFAULT_PREFIX_LEN,\n    suffix_len: int = DEFAULT_SUFFIX_LEN,\n    num_prefixes: int = DEFAULT_NUM_PREFIXES,\n    output_len: int = DEFAULT_OUTPUT_LEN,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list[SampleRequest]:\n    vocab_size = tokenizer.vocab_size\n    prompts_per_prefix = num_requests // num_prefixes\n    if prompts_per_prefix == 0:\n        raise ValueError(\n            f\"num_requests ({num_requests}) must be greater than or equal \"\n            f\"to num_prefixes ({num_prefixes})\"\n        )\n\n    def _generate_exact_length_tokens(target_length: int) -> list[int]:\n        \"\"\"Generate tokens that decode and re-encode to exactly\n        target_length.\"\"\"\n        # Generate random tokens\n        tokens = np.random.randint(0, vocab_size, size=target_length).tolist()\n\n        _, adjusted_tokens, token_mismatch = gen_prompt_decode_to_target_len(  # noqa: E501\n            tokenizer=tokenizer,\n            token_sequence=tokens,\n            target_token_len=target_length,\n            add_special_tokens=False,\n        )\n        return adjusted_tokens, token_mismatch\n\n    requests = []\n    token_mismatch_total = 0\n    for _ in range(num_prefixes):\n        prefix_tokens, prefix_mismatch = _generate_exact_length_tokens(prefix_len)\n        token_mismatch_total += prefix_mismatch\n\n        for _ in range(prompts_per_prefix):\n            suffix_tokens, suffix_mismatch = _generate_exact_length_tokens(\n                suffix_len\n            )\n            token_mismatch_total += suffix_mismatch\n            combined_tokens = prefix_tokens + suffix_tokens\n            prompt = tokenizer.decode(combined_tokens)\n            prompt_len = len(combined_tokens)\n            requests.append(\n                SampleRequest(\n                    prompt=prompt,\n                    prompt_len=prompt_len,\n                    expected_output_len=output_len,\n                )\n            )\n\n    if token_mismatch_total != 0:\n        sign = \"more\" if token_mismatch_total > 0 else \"fewer\"\n        logger.warning(\n            \"Across all generated prompts, there were %d %s tokens \"\n            \"than expected after decoding and re-encoding. This is \"\n            \"expected due to the imperfect nature of the sampling \"\n            \"procedure.\",\n            abs(token_mismatch_total),\n            sign,\n        )\n    if not getattr(self, \"disable_shuffle\", False):\n        random.shuffle(requests)\n    return requests",
      "language": "python"
    },
    {
      "code": "441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n500\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\n600\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668",
      "language": "unknown"
    },
    {
      "code": "class RandomDataset(BenchmarkDataset):\n    \"\"\"\n    Synthetic text-only dataset for serving/throughput benchmarks.\n\n    Strategy:\n    - Sample input/output token lengths per request from integer-uniform ranges\n      around configured means (controlled by range_ratio).\n    - Prepend a fixed random prefix of length prefix_len.\n    - Generate the remaining tokens as a reproducible sequence:\n      (offset + index + arange(input_len)) % vocab_size.\n    - Decode then re-encode/truncate to ensure prompt token counts match.\n    - Uses numpy.default_rng seeded with random_seed for reproducible sampling.\n    \"\"\"\n\n    # Default values copied from benchmark_serving.py for the random dataset.\n    DEFAULT_PREFIX_LEN = 0\n    DEFAULT_RANGE_RATIO = 0.0\n    DEFAULT_INPUT_LEN = 1024\n    DEFAULT_OUTPUT_LEN = 128\n\n    def __init__(self, **kwargs) -> None:\n        super().__init__(**kwargs)\n        # Use numpy's default_rng for deterministic sampling\n        # Do not use random.seed() or np.random.seed() elsewhere in this class.\n        # This ensures that the RNG is isolated from global RNG state.\n        self._rng = np.random.default_rng(self.random_seed)\n\n    def sample(\n        self,\n        tokenizer: TokenizerLike,\n        num_requests: int,\n        request_id_prefix: str = \"\",\n        no_oversample: bool = False,\n        prefix_len: int = DEFAULT_PREFIX_LEN,\n        range_ratio: float = DEFAULT_RANGE_RATIO,\n        input_len: int = DEFAULT_INPUT_LEN,\n        output_len: int = DEFAULT_OUTPUT_LEN,\n        batchsize: int = 1,\n        **kwargs,\n    ) -> list[SampleRequest]:\n        # validate total input tokens (prefix + sampled) is at least 1.\n        num_special = int(tokenizer.num_special_tokens_to_add())\n        real_input_len = max(0, int(input_len) - num_special)\n        min_sampled_input = math.floor(real_input_len * (1.0 - float(range_ratio)))\n        min_total_input = int(prefix_len) + min_sampled_input\n        if min_total_input < 1:\n            raise ValueError(\n                \"--random-input-len is too small: with tokenizer special \"\n                f\"tokens {num_special} and --random-range-ratio {range_ratio}, \"\n                \"the minimum possible total input tokens (prefix + sampled) is \"\n                f\"{min_total_input}. Increase --random-input-len and/or \"\n                \"--random-prefix-len, or decrease --random-range-ratio so that \"\n                \"prefix_len + floor(max(0, random_input_len - num_special)) \"\n                \"* (1 - range_ratio) >= 1.\"\n            )\n\n        input_lens, output_lens, offsets = self.get_sampling_params(\n            num_requests, range_ratio, input_len, output_len, tokenizer\n        )\n\n        vocab_size = tokenizer.vocab_size\n        prohibited_tokens = tokenizer.all_special_ids\n        all_tokens = np.arange(vocab_size)\n        allowed_tokens = np.array(list(set(all_tokens) - set(prohibited_tokens)))\n\n        # Generate prefix once\n        prefix_token_ids = self.get_prefix(allowed_tokens, prefix_len)\n\n        requests = []\n        token_mismatch_total = 0\n        for i in range(num_requests):\n            prompt, total_input_len, token_mismatch = self.generate_token_sequence(  # noqa: E501\n                tokenizer=tokenizer,\n                prefix_token_ids=prefix_token_ids,\n                prefix_len=prefix_len,\n                vocab_size=vocab_size,\n                input_len=int(input_lens[i]),\n                offset=int(offsets[i]),\n                index=i,\n                allowed_tokens=allowed_tokens,\n            )\n            token_mismatch_total += token_mismatch\n            requests.append(\n                SampleRequest(\n                    prompt=prompt,\n                    prompt_len=total_input_len,\n                    expected_output_len=int(output_lens[i]),\n                    request_id=request_id_prefix + str(i),\n                )\n            )\n        # only used for embeddings benchmark.\n        if batchsize > 1:\n            batch_requests = []\n            # Create batched requests\n            for i in range(0, num_requests, batchsize):\n                batch = requests[i : i + batchsize]\n                batch_requests.append(\n                    SampleRequest(\n                        prompt=[req.prompt for req in batch],\n                        prompt_len=sum(req.prompt_len for req in batch),\n                        expected_output_len=0,\n                        request_id=request_id_prefix + str(i // batchsize),\n                    )\n                )\n            requests = batch_requests\n\n        if token_mismatch_total != 0:\n            sign = \"more\" if token_mismatch_total > 0 else \"fewer\"\n            logger.warning(\n                \"Across all generated prompts, there were %d %s tokens \"\n                \"than expected after decoding and re-encoding. This is \"\n                \"expected due to the imperfect nature of the sampling \"\n                \"procedure.\",\n                abs(token_mismatch_total),\n                sign,\n            )\n\n        return requests\n\n    def get_prefix(\n        self,\n        allowed_tokens: np.ndarray,\n        prefix_len: int,\n    ) -> list[int]:\n        \"\"\"\n        Get the prefix for the dataset.\n        \"\"\"\n        return (\n            allowed_tokens[\n                self._rng.integers(0, len(allowed_tokens), size=prefix_len)\n            ].tolist()\n            if prefix_len > 0\n            else []\n        )\n\n    def get_sampling_params(\n        self,\n        num_requests: int,\n        range_ratio: float,\n        input_len: int,\n        output_len: int,\n        tokenizer: TokenizerLike,\n    ) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"\n        Get the sampling parameters for the dataset.\n        \"\"\"\n        # Enforce range_ratio < 1\n        if not (0.0 <= range_ratio < 1.0):\n            raise ValueError(\"range_ratio must be in [0, 1).\")\n        num_special_tokens = int(tokenizer.num_special_tokens_to_add())\n        real_input_len = max(0, int(input_len) - num_special_tokens)\n        # Bounds use floor for low and ceil for high\n        input_low = math.floor(real_input_len * (1 - range_ratio))\n        input_high = math.ceil(real_input_len * (1 + range_ratio))\n        output_low = math.floor(output_len * (1 - range_ratio))\n        output_high = math.ceil(output_len * (1 + range_ratio))\n        # Ensure the lower bound for output length is at least 1 to\n        # prevent sampling 0 tokens.\n        output_low = max(output_low, 1)\n        output_high = max(output_high, 1)\n\n        if input_low > input_high:\n            raise ValueError(\n                f\"Invalid input sampling interval: low={input_low} > high={input_high}\"\n            )\n        if output_low > output_high:\n            raise ValueError(\n                \"Invalid output sampling interval: \"\n                f\"low={output_low} > high={output_high}\"\n            )\n\n        logger.info(\n            \"Sampling input_len from [%s, %s] and output_len from [%s, %s]\",\n            input_low,\n            input_high,\n            output_low,\n            output_high,\n        )\n\n        input_lens = self._rng.integers(input_low, input_high + 1, size=num_requests)\n        output_lens = self._rng.integers(output_low, output_high + 1, size=num_requests)\n        offsets = self._rng.integers(0, tokenizer.vocab_size, size=num_requests)\n        return input_lens, output_lens, offsets\n\n    def generate_token_sequence(\n        self,\n        *,\n        tokenizer: TokenizerLike,\n        prefix_token_ids: list[int],\n        prefix_len: int,\n        vocab_size: int,\n        input_len: int,\n        offset: int,\n        index: int,\n        allowed_tokens: np.ndarray,\n    ) -> tuple[str, int, int]:\n        \"\"\"\n        Returns (prompt, total_input_len).\n\n        NOTE: After decoding the prompt we have to encode and decode it again.\n        This is done because in some cases N consecutive tokens\n        give a string tokenized into != N number of tokens.\n        For example for GPT2Tokenizer:\n        [6880, 6881] -> ['Ġcalls', 'here'] ->\n        [1650, 939, 486] -> ['Ġcall', 'sh', 'ere']\n        To avoid uncontrolled change of the prompt length,\n        the encoded sequence is truncated before being decoded again.\n        \"\"\"\n        # Build the inner sequence by sampling\n        # sequentially from the allowed tokens\n        inner_seq = allowed_tokens[\n            (offset + index + np.arange(input_len)) % len(allowed_tokens)\n        ].tolist()\n        token_sequence = prefix_token_ids + inner_seq\n\n        # Decode, then re-encode and truncate to preserve token count invariants\n        total_input_len = prefix_len + int(input_len)\n        prompt, adjusted_token_sequence, token_mismatch = (\n            gen_prompt_decode_to_target_len(\n                tokenizer=tokenizer,\n                token_sequence=token_sequence,\n                target_token_len=total_input_len,\n                add_special_tokens=False,\n                rng=self._rng,\n            )\n        )\n        total_input_len = len(adjusted_token_sequence)\n        return prompt, total_input_len, token_mismatch",
      "language": "python"
    },
    {
      "code": "class RandomDataset(BenchmarkDataset):\n    \"\"\"\n    Synthetic text-only dataset for serving/throughput benchmarks.\n\n    Strategy:\n    - Sample input/output token lengths per request from integer-uniform ranges\n      around configured means (controlled by range_ratio).\n    - Prepend a fixed random prefix of length prefix_len.\n    - Generate the remaining tokens as a reproducible sequence:\n      (offset + index + arange(input_len)) % vocab_size.\n    - Decode then re-encode/truncate to ensure prompt token counts match.\n    - Uses numpy.default_rng seeded with random_seed for reproducible sampling.\n    \"\"\"\n\n    # Default values copied from benchmark_serving.py for the random dataset.\n    DEFAULT_PREFIX_LEN = 0\n    DEFAULT_RANGE_RATIO = 0.0\n    DEFAULT_INPUT_LEN = 1024\n    DEFAULT_OUTPUT_LEN = 128\n\n    def __init__(self, **kwargs) -> None:\n        super().__init__(**kwargs)\n        # Use numpy's default_rng for deterministic sampling\n        # Do not use random.seed() or np.random.seed() elsewhere in this class.\n        # This ensures that the RNG is isolated from global RNG state.\n        self._rng = np.random.default_rng(self.random_seed)\n\n    def sample(\n        self,\n        tokenizer: TokenizerLike,\n        num_requests: int,\n        request_id_prefix: str = \"\",\n        no_oversample: bool = False,\n        prefix_len: int = DEFAULT_PREFIX_LEN,\n        range_ratio: float = DEFAULT_RANGE_RATIO,\n        input_len: int = DEFAULT_INPUT_LEN,\n        output_len: int = DEFAULT_OUTPUT_LEN,\n        batchsize: int = 1,\n        **kwargs,\n    ) -> list[SampleRequest]:\n        # validate total input tokens (prefix + sampled) is at least 1.\n        num_special = int(tokenizer.num_special_tokens_to_add())\n        real_input_len = max(0, int(input_len) - num_special)\n        min_sampled_input = math.floor(real_input_len * (1.0 - float(range_ratio)))\n        min_total_input = int(prefix_len) + min_sampled_input\n        if min_total_input < 1:\n            raise ValueError(\n                \"--random-input-len is too small: with tokenizer special \"\n                f\"tokens {num_special} and --random-range-ratio {range_ratio}, \"\n                \"the minimum possible total input tokens (prefix + sampled) is \"\n                f\"{min_total_input}. Increase --random-input-len and/or \"\n                \"--random-prefix-len, or decrease --random-range-ratio so that \"\n                \"prefix_len + floor(max(0, random_input_len - num_special)) \"\n                \"* (1 - range_ratio) >= 1.\"\n            )\n\n        input_lens, output_lens, offsets = self.get_sampling_params(\n            num_requests, range_ratio, input_len, output_len, tokenizer\n        )\n\n        vocab_size = tokenizer.vocab_size\n        prohibited_tokens = tokenizer.all_special_ids\n        all_tokens = np.arange(vocab_size)\n        allowed_tokens = np.array(list(set(all_tokens) - set(prohibited_tokens)))\n\n        # Generate prefix once\n        prefix_token_ids = self.get_prefix(allowed_tokens, prefix_len)\n\n        requests = []\n        token_mismatch_total = 0\n        for i in range(num_requests):\n            prompt, total_input_len, token_mismatch = self.generate_token_sequence(  # noqa: E501\n                tokenizer=tokenizer,\n                prefix_token_ids=prefix_token_ids,\n                prefix_len=prefix_len,\n                vocab_size=vocab_size,\n                input_len=int(input_lens[i]),\n                offset=int(offsets[i]),\n                index=i,\n                allowed_tokens=allowed_tokens,\n            )\n            token_mismatch_total += token_mismatch\n            requests.append(\n                SampleRequest(\n                    prompt=prompt,\n                    prompt_len=total_input_len,\n                    expected_output_len=int(output_lens[i]),\n                    request_id=request_id_prefix + str(i),\n                )\n            )\n        # only used for embeddings benchmark.\n        if batchsize > 1:\n            batch_requests = []\n            # Create batched requests\n            for i in range(0, num_requests, batchsize):\n                batch = requests[i : i + batchsize]\n                batch_requests.append(\n                    SampleRequest(\n                        prompt=[req.prompt for req in batch],\n                        prompt_len=sum(req.prompt_len for req in batch),\n                        expected_output_len=0,\n                        request_id=request_id_prefix + str(i // batchsize),\n                    )\n                )\n            requests = batch_requests\n\n        if token_mismatch_total != 0:\n            sign = \"more\" if token_mismatch_total > 0 else \"fewer\"\n            logger.warning(\n                \"Across all generated prompts, there were %d %s tokens \"\n                \"than expected after decoding and re-encoding. This is \"\n                \"expected due to the imperfect nature of the sampling \"\n                \"procedure.\",\n                abs(token_mismatch_total),\n                sign,\n            )\n\n        return requests\n\n    def get_prefix(\n        self,\n        allowed_tokens: np.ndarray,\n        prefix_len: int,\n    ) -> list[int]:\n        \"\"\"\n        Get the prefix for the dataset.\n        \"\"\"\n        return (\n            allowed_tokens[\n                self._rng.integers(0, len(allowed_tokens), size=prefix_len)\n            ].tolist()\n            if prefix_len > 0\n            else []\n        )\n\n    def get_sampling_params(\n        self,\n        num_requests: int,\n        range_ratio: float,\n        input_len: int,\n        output_len: int,\n        tokenizer: TokenizerLike,\n    ) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"\n        Get the sampling parameters for the dataset.\n        \"\"\"\n        # Enforce range_ratio < 1\n        if not (0.0 <= range_ratio < 1.0):\n            raise ValueError(\"range_ratio must be in [0, 1).\")\n        num_special_tokens = int(tokenizer.num_special_tokens_to_add())\n        real_input_len = max(0, int(input_len) - num_special_tokens)\n        # Bounds use floor for low and ceil for high\n        input_low = math.floor(real_input_len * (1 - range_ratio))\n        input_high = math.ceil(real_input_len * (1 + range_ratio))\n        output_low = math.floor(output_len * (1 - range_ratio))\n        output_high = math.ceil(output_len * (1 + range_ratio))\n        # Ensure the lower bound for output length is at least 1 to\n        # prevent sampling 0 tokens.\n        output_low = max(output_low, 1)\n        output_high = max(output_high, 1)\n\n        if input_low > input_high:\n            raise ValueError(\n                f\"Invalid input sampling interval: low={input_low} > high={input_high}\"\n            )\n        if output_low > output_high:\n            raise ValueError(\n                \"Invalid output sampling interval: \"\n                f\"low={output_low} > high={output_high}\"\n            )\n\n        logger.info(\n            \"Sampling input_len from [%s, %s] and output_len from [%s, %s]\",\n            input_low,\n            input_high,\n            output_low,\n            output_high,\n        )\n\n        input_lens = self._rng.integers(input_low, input_high + 1, size=num_requests)\n        output_lens = self._rng.integers(output_low, output_high + 1, size=num_requests)\n        offsets = self._rng.integers(0, tokenizer.vocab_size, size=num_requests)\n        return input_lens, output_lens, offsets\n\n    def generate_token_sequence(\n        self,\n        *,\n        tokenizer: TokenizerLike,\n        prefix_token_ids: list[int],\n        prefix_len: int,\n        vocab_size: int,\n        input_len: int,\n        offset: int,\n        index: int,\n        allowed_tokens: np.ndarray,\n    ) -> tuple[str, int, int]:\n        \"\"\"\n        Returns (prompt, total_input_len).\n\n        NOTE: After decoding the prompt we have to encode and decode it again.\n        This is done because in some cases N consecutive tokens\n        give a string tokenized into != N number of tokens.\n        For example for GPT2Tokenizer:\n        [6880, 6881] -> ['Ġcalls', 'here'] ->\n        [1650, 939, 486] -> ['Ġcall', 'sh', 'ere']\n        To avoid uncontrolled change of the prompt length,\n        the encoded sequence is truncated before being decoded again.\n        \"\"\"\n        # Build the inner sequence by sampling\n        # sequentially from the allowed tokens\n        inner_seq = allowed_tokens[\n            (offset + index + np.arange(input_len)) % len(allowed_tokens)\n        ].tolist()\n        token_sequence = prefix_token_ids + inner_seq\n\n        # Decode, then re-encode and truncate to preserve token count invariants\n        total_input_len = prefix_len + int(input_len)\n        prompt, adjusted_token_sequence, token_mismatch = (\n            gen_prompt_decode_to_target_len(\n                tokenizer=tokenizer,\n                token_sequence=token_sequence,\n                target_token_len=total_input_len,\n                add_special_tokens=False,\n                rng=self._rng,\n            )\n        )\n        total_input_len = len(adjusted_token_sequence)\n        return prompt, total_input_len, token_mismatch",
      "language": "python"
    },
    {
      "code": "DEFAULT_INPUT_LEN = 1024",
      "language": "unknown"
    },
    {
      "code": "DEFAULT_INPUT_LEN = 1024",
      "language": "unknown"
    },
    {
      "code": "DEFAULT_OUTPUT_LEN = 128",
      "language": "unknown"
    },
    {
      "code": "DEFAULT_OUTPUT_LEN = 128",
      "language": "unknown"
    },
    {
      "code": "DEFAULT_PREFIX_LEN = 0",
      "language": "unknown"
    },
    {
      "code": "DEFAULT_PREFIX_LEN = 0",
      "language": "unknown"
    },
    {
      "code": "DEFAULT_RANGE_RATIO = 0.0",
      "language": "unknown"
    },
    {
      "code": "DEFAULT_RANGE_RATIO = 0.0",
      "language": "unknown"
    },
    {
      "code": "_rng = default_rng(random_seed)",
      "language": "unknown"
    },
    {
      "code": "_rng = default_rng(random_seed)",
      "language": "unknown"
    },
    {
      "code": "__init__(**kwargs) -> None",
      "language": "python"
    },
    {
      "code": "__init__(**kwargs) -> None",
      "language": "python"
    },
    {
      "code": "461\n462\n463\n464\n465\n466",
      "language": "unknown"
    },
    {
      "code": "def __init__(self, **kwargs) -> None:\n    super().__init__(**kwargs)\n    # Use numpy's default_rng for deterministic sampling\n    # Do not use random.seed() or np.random.seed() elsewhere in this class.\n    # This ensures that the RNG is isolated from global RNG state.\n    self._rng = np.random.default_rng(self.random_seed)",
      "language": "python"
    },
    {
      "code": "def __init__(self, **kwargs) -> None:\n    super().__init__(**kwargs)\n    # Use numpy's default_rng for deterministic sampling\n    # Do not use random.seed() or np.random.seed() elsewhere in this class.\n    # This ensures that the RNG is isolated from global RNG state.\n    self._rng = np.random.default_rng(self.random_seed)",
      "language": "python"
    },
    {
      "code": "generate_token_sequence(\n    *,\n    tokenizer: TokenizerLike,\n    prefix_token_ids: list[int],\n    prefix_len: int,\n    vocab_size: int,\n    input_len: int,\n    offset: int,\n    index: int,\n    allowed_tokens: ndarray,\n) -> tuple[str, int, int]",
      "language": "php"
    },
    {
      "code": "generate_token_sequence(\n    *,\n    tokenizer: TokenizerLike,\n    prefix_token_ids: list[int],\n    prefix_len: int,\n    vocab_size: int,\n    input_len: int,\n    offset: int,\n    index: int,\n    allowed_tokens: ndarray,\n) -> tuple[str, int, int]",
      "language": "php"
    },
    {
      "code": "625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668",
      "language": "unknown"
    },
    {
      "code": "def generate_token_sequence(\n    self,\n    *,\n    tokenizer: TokenizerLike,\n    prefix_token_ids: list[int],\n    prefix_len: int,\n    vocab_size: int,\n    input_len: int,\n    offset: int,\n    index: int,\n    allowed_tokens: np.ndarray,\n) -> tuple[str, int, int]:\n    \"\"\"\n    Returns (prompt, total_input_len).\n\n    NOTE: After decoding the prompt we have to encode and decode it again.\n    This is done because in some cases N consecutive tokens\n    give a string tokenized into != N number of tokens.\n    For example for GPT2Tokenizer:\n    [6880, 6881] -> ['Ġcalls', 'here'] ->\n    [1650, 939, 486] -> ['Ġcall', 'sh', 'ere']\n    To avoid uncontrolled change of the prompt length,\n    the encoded sequence is truncated before being decoded again.\n    \"\"\"\n    # Build the inner sequence by sampling\n    # sequentially from the allowed tokens\n    inner_seq = allowed_tokens[\n        (offset + index + np.arange(input_len)) % len(allowed_tokens)\n    ].tolist()\n    token_sequence = prefix_token_ids + inner_seq\n\n    # Decode, then re-encode and truncate to preserve token count invariants\n    total_input_len = prefix_len + int(input_len)\n    prompt, adjusted_token_sequence, token_mismatch = (\n        gen_prompt_decode_to_target_len(\n            tokenizer=tokenizer,\n            token_sequence=token_sequence,\n            target_token_len=total_input_len,\n            add_special_tokens=False,\n            rng=self._rng,\n        )\n    )\n    total_input_len = len(adjusted_token_sequence)\n    return prompt, total_input_len, token_mismatch",
      "language": "python"
    },
    {
      "code": "def generate_token_sequence(\n    self,\n    *,\n    tokenizer: TokenizerLike,\n    prefix_token_ids: list[int],\n    prefix_len: int,\n    vocab_size: int,\n    input_len: int,\n    offset: int,\n    index: int,\n    allowed_tokens: np.ndarray,\n) -> tuple[str, int, int]:\n    \"\"\"\n    Returns (prompt, total_input_len).\n\n    NOTE: After decoding the prompt we have to encode and decode it again.\n    This is done because in some cases N consecutive tokens\n    give a string tokenized into != N number of tokens.\n    For example for GPT2Tokenizer:\n    [6880, 6881] -> ['Ġcalls', 'here'] ->\n    [1650, 939, 486] -> ['Ġcall', 'sh', 'ere']\n    To avoid uncontrolled change of the prompt length,\n    the encoded sequence is truncated before being decoded again.\n    \"\"\"\n    # Build the inner sequence by sampling\n    # sequentially from the allowed tokens\n    inner_seq = allowed_tokens[\n        (offset + index + np.arange(input_len)) % len(allowed_tokens)\n    ].tolist()\n    token_sequence = prefix_token_ids + inner_seq\n\n    # Decode, then re-encode and truncate to preserve token count invariants\n    total_input_len = prefix_len + int(input_len)\n    prompt, adjusted_token_sequence, token_mismatch = (\n        gen_prompt_decode_to_target_len(\n            tokenizer=tokenizer,\n            token_sequence=token_sequence,\n            target_token_len=total_input_len,\n            add_special_tokens=False,\n            rng=self._rng,\n        )\n    )\n    total_input_len = len(adjusted_token_sequence)\n    return prompt, total_input_len, token_mismatch",
      "language": "python"
    },
    {
      "code": "get_prefix(\n    allowed_tokens: ndarray, prefix_len: int\n) -> list[int]",
      "language": "php"
    },
    {
      "code": "get_prefix(\n    allowed_tokens: ndarray, prefix_len: int\n) -> list[int]",
      "language": "php"
    },
    {
      "code": "560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574",
      "language": "unknown"
    },
    {
      "code": "def get_prefix(\n    self,\n    allowed_tokens: np.ndarray,\n    prefix_len: int,\n) -> list[int]:\n    \"\"\"\n    Get the prefix for the dataset.\n    \"\"\"\n    return (\n        allowed_tokens[\n            self._rng.integers(0, len(allowed_tokens), size=prefix_len)\n        ].tolist()\n        if prefix_len > 0\n        else []\n    )",
      "language": "python"
    },
    {
      "code": "def get_prefix(\n    self,\n    allowed_tokens: np.ndarray,\n    prefix_len: int,\n) -> list[int]:\n    \"\"\"\n    Get the prefix for the dataset.\n    \"\"\"\n    return (\n        allowed_tokens[\n            self._rng.integers(0, len(allowed_tokens), size=prefix_len)\n        ].tolist()\n        if prefix_len > 0\n        else []\n    )",
      "language": "python"
    },
    {
      "code": "get_sampling_params(\n    num_requests: int,\n    range_ratio: float,\n    input_len: int,\n    output_len: int,\n    tokenizer: TokenizerLike,\n) -> tuple[ndarray, ndarray, ndarray]",
      "language": "php"
    },
    {
      "code": "get_sampling_params(\n    num_requests: int,\n    range_ratio: float,\n    input_len: int,\n    output_len: int,\n    tokenizer: TokenizerLike,\n) -> tuple[ndarray, ndarray, ndarray]",
      "language": "php"
    },
    {
      "code": "576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\n600\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623",
      "language": "unknown"
    },
    {
      "code": "def get_sampling_params(\n    self,\n    num_requests: int,\n    range_ratio: float,\n    input_len: int,\n    output_len: int,\n    tokenizer: TokenizerLike,\n) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Get the sampling parameters for the dataset.\n    \"\"\"\n    # Enforce range_ratio < 1\n    if not (0.0 <= range_ratio < 1.0):\n        raise ValueError(\"range_ratio must be in [0, 1).\")\n    num_special_tokens = int(tokenizer.num_special_tokens_to_add())\n    real_input_len = max(0, int(input_len) - num_special_tokens)\n    # Bounds use floor for low and ceil for high\n    input_low = math.floor(real_input_len * (1 - range_ratio))\n    input_high = math.ceil(real_input_len * (1 + range_ratio))\n    output_low = math.floor(output_len * (1 - range_ratio))\n    output_high = math.ceil(output_len * (1 + range_ratio))\n    # Ensure the lower bound for output length is at least 1 to\n    # prevent sampling 0 tokens.\n    output_low = max(output_low, 1)\n    output_high = max(output_high, 1)\n\n    if input_low > input_high:\n        raise ValueError(\n            f\"Invalid input sampling interval: low={input_low} > high={input_high}\"\n        )\n    if output_low > output_high:\n        raise ValueError(\n            \"Invalid output sampling interval: \"\n            f\"low={output_low} > high={output_high}\"\n        )\n\n    logger.info(\n        \"Sampling input_len from [%s, %s] and output_len from [%s, %s]\",\n        input_low,\n        input_high,\n        output_low,\n        output_high,\n    )\n\n    input_lens = self._rng.integers(input_low, input_high + 1, size=num_requests)\n    output_lens = self._rng.integers(output_low, output_high + 1, size=num_requests)\n    offsets = self._rng.integers(0, tokenizer.vocab_size, size=num_requests)\n    return input_lens, output_lens, offsets",
      "language": "python"
    },
    {
      "code": "def get_sampling_params(\n    self,\n    num_requests: int,\n    range_ratio: float,\n    input_len: int,\n    output_len: int,\n    tokenizer: TokenizerLike,\n) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Get the sampling parameters for the dataset.\n    \"\"\"\n    # Enforce range_ratio < 1\n    if not (0.0 <= range_ratio < 1.0):\n        raise ValueError(\"range_ratio must be in [0, 1).\")\n    num_special_tokens = int(tokenizer.num_special_tokens_to_add())\n    real_input_len = max(0, int(input_len) - num_special_tokens)\n    # Bounds use floor for low and ceil for high\n    input_low = math.floor(real_input_len * (1 - range_ratio))\n    input_high = math.ceil(real_input_len * (1 + range_ratio))\n    output_low = math.floor(output_len * (1 - range_ratio))\n    output_high = math.ceil(output_len * (1 + range_ratio))\n    # Ensure the lower bound for output length is at least 1 to\n    # prevent sampling 0 tokens.\n    output_low = max(output_low, 1)\n    output_high = max(output_high, 1)\n\n    if input_low > input_high:\n        raise ValueError(\n            f\"Invalid input sampling interval: low={input_low} > high={input_high}\"\n        )\n    if output_low > output_high:\n        raise ValueError(\n            \"Invalid output sampling interval: \"\n            f\"low={output_low} > high={output_high}\"\n        )\n\n    logger.info(\n        \"Sampling input_len from [%s, %s] and output_len from [%s, %s]\",\n        input_low,\n        input_high,\n        output_low,\n        output_high,\n    )\n\n    input_lens = self._rng.integers(input_low, input_high + 1, size=num_requests)\n    output_lens = self._rng.integers(output_low, output_high + 1, size=num_requests)\n    offsets = self._rng.integers(0, tokenizer.vocab_size, size=num_requests)\n    return input_lens, output_lens, offsets",
      "language": "python"
    },
    {
      "code": "sample(\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    prefix_len: int = DEFAULT_PREFIX_LEN,\n    range_ratio: float = DEFAULT_RANGE_RATIO,\n    input_len: int = DEFAULT_INPUT_LEN,\n    output_len: int = DEFAULT_OUTPUT_LEN,\n    batchsize: int = 1,\n    **kwargs,\n) -> list[SampleRequest]",
      "language": "typescript"
    },
    {
      "code": "sample(\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    prefix_len: int = DEFAULT_PREFIX_LEN,\n    range_ratio: float = DEFAULT_RANGE_RATIO,\n    input_len: int = DEFAULT_INPUT_LEN,\n    output_len: int = DEFAULT_OUTPUT_LEN,\n    batchsize: int = 1,\n    **kwargs,\n) -> list[SampleRequest]",
      "language": "typescript"
    },
    {
      "code": "468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n500\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558",
      "language": "unknown"
    },
    {
      "code": "def sample(\n    self,\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    prefix_len: int = DEFAULT_PREFIX_LEN,\n    range_ratio: float = DEFAULT_RANGE_RATIO,\n    input_len: int = DEFAULT_INPUT_LEN,\n    output_len: int = DEFAULT_OUTPUT_LEN,\n    batchsize: int = 1,\n    **kwargs,\n) -> list[SampleRequest]:\n    # validate total input tokens (prefix + sampled) is at least 1.\n    num_special = int(tokenizer.num_special_tokens_to_add())\n    real_input_len = max(0, int(input_len) - num_special)\n    min_sampled_input = math.floor(real_input_len * (1.0 - float(range_ratio)))\n    min_total_input = int(prefix_len) + min_sampled_input\n    if min_total_input < 1:\n        raise ValueError(\n            \"--random-input-len is too small: with tokenizer special \"\n            f\"tokens {num_special} and --random-range-ratio {range_ratio}, \"\n            \"the minimum possible total input tokens (prefix + sampled) is \"\n            f\"{min_total_input}. Increase --random-input-len and/or \"\n            \"--random-prefix-len, or decrease --random-range-ratio so that \"\n            \"prefix_len + floor(max(0, random_input_len - num_special)) \"\n            \"* (1 - range_ratio) >= 1.\"\n        )\n\n    input_lens, output_lens, offsets = self.get_sampling_params(\n        num_requests, range_ratio, input_len, output_len, tokenizer\n    )\n\n    vocab_size = tokenizer.vocab_size\n    prohibited_tokens = tokenizer.all_special_ids\n    all_tokens = np.arange(vocab_size)\n    allowed_tokens = np.array(list(set(all_tokens) - set(prohibited_tokens)))\n\n    # Generate prefix once\n    prefix_token_ids = self.get_prefix(allowed_tokens, prefix_len)\n\n    requests = []\n    token_mismatch_total = 0\n    for i in range(num_requests):\n        prompt, total_input_len, token_mismatch = self.generate_token_sequence(  # noqa: E501\n            tokenizer=tokenizer,\n            prefix_token_ids=prefix_token_ids,\n            prefix_len=prefix_len,\n            vocab_size=vocab_size,\n            input_len=int(input_lens[i]),\n            offset=int(offsets[i]),\n            index=i,\n            allowed_tokens=allowed_tokens,\n        )\n        token_mismatch_total += token_mismatch\n        requests.append(\n            SampleRequest(\n                prompt=prompt,\n                prompt_len=total_input_len,\n                expected_output_len=int(output_lens[i]),\n                request_id=request_id_prefix + str(i),\n            )\n        )\n    # only used for embeddings benchmark.\n    if batchsize > 1:\n        batch_requests = []\n        # Create batched requests\n        for i in range(0, num_requests, batchsize):\n            batch = requests[i : i + batchsize]\n            batch_requests.append(\n                SampleRequest(\n                    prompt=[req.prompt for req in batch],\n                    prompt_len=sum(req.prompt_len for req in batch),\n                    expected_output_len=0,\n                    request_id=request_id_prefix + str(i // batchsize),\n                )\n            )\n        requests = batch_requests\n\n    if token_mismatch_total != 0:\n        sign = \"more\" if token_mismatch_total > 0 else \"fewer\"\n        logger.warning(\n            \"Across all generated prompts, there were %d %s tokens \"\n            \"than expected after decoding and re-encoding. This is \"\n            \"expected due to the imperfect nature of the sampling \"\n            \"procedure.\",\n            abs(token_mismatch_total),\n            sign,\n        )\n\n    return requests",
      "language": "python"
    },
    {
      "code": "def sample(\n    self,\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    prefix_len: int = DEFAULT_PREFIX_LEN,\n    range_ratio: float = DEFAULT_RANGE_RATIO,\n    input_len: int = DEFAULT_INPUT_LEN,\n    output_len: int = DEFAULT_OUTPUT_LEN,\n    batchsize: int = 1,\n    **kwargs,\n) -> list[SampleRequest]:\n    # validate total input tokens (prefix + sampled) is at least 1.\n    num_special = int(tokenizer.num_special_tokens_to_add())\n    real_input_len = max(0, int(input_len) - num_special)\n    min_sampled_input = math.floor(real_input_len * (1.0 - float(range_ratio)))\n    min_total_input = int(prefix_len) + min_sampled_input\n    if min_total_input < 1:\n        raise ValueError(\n            \"--random-input-len is too small: with tokenizer special \"\n            f\"tokens {num_special} and --random-range-ratio {range_ratio}, \"\n            \"the minimum possible total input tokens (prefix + sampled) is \"\n            f\"{min_total_input}. Increase --random-input-len and/or \"\n            \"--random-prefix-len, or decrease --random-range-ratio so that \"\n            \"prefix_len + floor(max(0, random_input_len - num_special)) \"\n            \"* (1 - range_ratio) >= 1.\"\n        )\n\n    input_lens, output_lens, offsets = self.get_sampling_params(\n        num_requests, range_ratio, input_len, output_len, tokenizer\n    )\n\n    vocab_size = tokenizer.vocab_size\n    prohibited_tokens = tokenizer.all_special_ids\n    all_tokens = np.arange(vocab_size)\n    allowed_tokens = np.array(list(set(all_tokens) - set(prohibited_tokens)))\n\n    # Generate prefix once\n    prefix_token_ids = self.get_prefix(allowed_tokens, prefix_len)\n\n    requests = []\n    token_mismatch_total = 0\n    for i in range(num_requests):\n        prompt, total_input_len, token_mismatch = self.generate_token_sequence(  # noqa: E501\n            tokenizer=tokenizer,\n            prefix_token_ids=prefix_token_ids,\n            prefix_len=prefix_len,\n            vocab_size=vocab_size,\n            input_len=int(input_lens[i]),\n            offset=int(offsets[i]),\n            index=i,\n            allowed_tokens=allowed_tokens,\n        )\n        token_mismatch_total += token_mismatch\n        requests.append(\n            SampleRequest(\n                prompt=prompt,\n                prompt_len=total_input_len,\n                expected_output_len=int(output_lens[i]),\n                request_id=request_id_prefix + str(i),\n            )\n        )\n    # only used for embeddings benchmark.\n    if batchsize > 1:\n        batch_requests = []\n        # Create batched requests\n        for i in range(0, num_requests, batchsize):\n            batch = requests[i : i + batchsize]\n            batch_requests.append(\n                SampleRequest(\n                    prompt=[req.prompt for req in batch],\n                    prompt_len=sum(req.prompt_len for req in batch),\n                    expected_output_len=0,\n                    request_id=request_id_prefix + str(i // batchsize),\n                )\n            )\n        requests = batch_requests\n\n    if token_mismatch_total != 0:\n        sign = \"more\" if token_mismatch_total > 0 else \"fewer\"\n        logger.warning(\n            \"Across all generated prompts, there were %d %s tokens \"\n            \"than expected after decoding and re-encoding. This is \"\n            \"expected due to the imperfect nature of the sampling \"\n            \"procedure.\",\n            abs(token_mismatch_total),\n            sign,\n        )\n\n    return requests",
      "language": "python"
    },
    {
      "code": "676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\n700\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780",
      "language": "unknown"
    },
    {
      "code": "class RandomDatasetForReranking(RandomDataset):\n    \"\"\"\n    Random dataset specialized for the needs of scoring:\n    - Batches of inputs\n    - Inputs composed of pairs\n    \"\"\"\n\n    def __init__(self, **kwargs) -> None:\n        super().__init__(**kwargs)\n\n    def sample(\n        self,\n        tokenizer: TokenizerLike,\n        num_requests: int,\n        request_id_prefix: str = \"\",\n        range_ratio: float = RandomDataset.DEFAULT_RANGE_RATIO,\n        input_len: int = RandomDataset.DEFAULT_INPUT_LEN,\n        batchsize: int = 1,\n        is_reranker: bool = True,\n        **kwargs,\n    ) -> list[SampleRequest]:\n        n_sep_tokens = int(is_reranker)\n\n        query_len_param = (input_len // 2) - n_sep_tokens if is_reranker else input_len\n\n        query_lens, _, query_offsets = self.get_sampling_params(\n            1, range_ratio, query_len_param, 0, tokenizer\n        )\n\n        query_len = int(query_lens[0])\n\n        if not is_reranker:\n            assert num_requests > 1 and batchsize > 1\n            num_requests -= 1\n            batchsize -= 1\n            doc_len_param = input_len\n        else:\n            doc_len_param = input_len - query_len - n_sep_tokens\n\n        doc_lens, _, doc_offsets = self.get_sampling_params(\n            num_requests, range_ratio, doc_len_param, 0, tokenizer\n        )\n\n        vocab_size = tokenizer.vocab_size\n        prohibited_tokens = tokenizer.all_special_ids\n        all_tokens = np.arange(vocab_size)\n        allowed_tokens = np.array(list(set(all_tokens) - set(prohibited_tokens)))\n\n        query_prompt, query_input_len, token_mismatch_total = (\n            self.generate_token_sequence(\n                tokenizer=tokenizer,\n                prefix_token_ids=[],\n                prefix_len=0,\n                vocab_size=vocab_size,\n                input_len=query_len,\n                offset=int(query_offsets[0]),\n                index=0,\n                allowed_tokens=allowed_tokens,\n            )\n        )\n\n        requests = []\n        for i in range(num_requests):\n            prompt, total_input_len, token_mismatch = self.generate_token_sequence(  # noqa: E501\n                tokenizer=tokenizer,\n                prefix_token_ids=[],\n                prefix_len=0,\n                vocab_size=vocab_size,\n                input_len=int(doc_lens[i]),\n                offset=int(doc_offsets[i]),\n                index=i + 1,\n                allowed_tokens=allowed_tokens,\n            )\n            token_mismatch_total += token_mismatch\n            requests.append((prompt, total_input_len))\n\n        batch_requests = []\n        # Create batched requests\n        for i in range(0, num_requests, batchsize):\n            batch = requests[i : i + batchsize]\n            query_contrib = (\n                (query_input_len + n_sep_tokens) * len(batch)\n                if is_reranker\n                else query_input_len\n            )\n            batch_requests.append(\n                SampleRequest(\n                    prompt=[query_prompt] + [req[0] for req in batch],\n                    prompt_len=query_contrib + sum(req[1] for req in batch),\n                    expected_output_len=0,\n                    request_id=request_id_prefix + str(i // batchsize),\n                )\n            )\n\n        if token_mismatch_total != 0:\n            logger.warning(\n                \"Across all generated prompts, there were %d %s tokens \"\n                \"than expected after decoding and re-encoding. This is \"\n                \"expected due to the imperfect nature of the sampling \"\n                \"procedure.\",\n                abs(token_mismatch_total),\n                \"more\" if token_mismatch_total > 0 else \"fewer\",\n            )\n\n        return batch_requests",
      "language": "python"
    },
    {
      "code": "class RandomDatasetForReranking(RandomDataset):\n    \"\"\"\n    Random dataset specialized for the needs of scoring:\n    - Batches of inputs\n    - Inputs composed of pairs\n    \"\"\"\n\n    def __init__(self, **kwargs) -> None:\n        super().__init__(**kwargs)\n\n    def sample(\n        self,\n        tokenizer: TokenizerLike,\n        num_requests: int,\n        request_id_prefix: str = \"\",\n        range_ratio: float = RandomDataset.DEFAULT_RANGE_RATIO,\n        input_len: int = RandomDataset.DEFAULT_INPUT_LEN,\n        batchsize: int = 1,\n        is_reranker: bool = True,\n        **kwargs,\n    ) -> list[SampleRequest]:\n        n_sep_tokens = int(is_reranker)\n\n        query_len_param = (input_len // 2) - n_sep_tokens if is_reranker else input_len\n\n        query_lens, _, query_offsets = self.get_sampling_params(\n            1, range_ratio, query_len_param, 0, tokenizer\n        )\n\n        query_len = int(query_lens[0])\n\n        if not is_reranker:\n            assert num_requests > 1 and batchsize > 1\n            num_requests -= 1\n            batchsize -= 1\n            doc_len_param = input_len\n        else:\n            doc_len_param = input_len - query_len - n_sep_tokens\n\n        doc_lens, _, doc_offsets = self.get_sampling_params(\n            num_requests, range_ratio, doc_len_param, 0, tokenizer\n        )\n\n        vocab_size = tokenizer.vocab_size\n        prohibited_tokens = tokenizer.all_special_ids\n        all_tokens = np.arange(vocab_size)\n        allowed_tokens = np.array(list(set(all_tokens) - set(prohibited_tokens)))\n\n        query_prompt, query_input_len, token_mismatch_total = (\n            self.generate_token_sequence(\n                tokenizer=tokenizer,\n                prefix_token_ids=[],\n                prefix_len=0,\n                vocab_size=vocab_size,\n                input_len=query_len,\n                offset=int(query_offsets[0]),\n                index=0,\n                allowed_tokens=allowed_tokens,\n            )\n        )\n\n        requests = []\n        for i in range(num_requests):\n            prompt, total_input_len, token_mismatch = self.generate_token_sequence(  # noqa: E501\n                tokenizer=tokenizer,\n                prefix_token_ids=[],\n                prefix_len=0,\n                vocab_size=vocab_size,\n                input_len=int(doc_lens[i]),\n                offset=int(doc_offsets[i]),\n                index=i + 1,\n                allowed_tokens=allowed_tokens,\n            )\n            token_mismatch_total += token_mismatch\n            requests.append((prompt, total_input_len))\n\n        batch_requests = []\n        # Create batched requests\n        for i in range(0, num_requests, batchsize):\n            batch = requests[i : i + batchsize]\n            query_contrib = (\n                (query_input_len + n_sep_tokens) * len(batch)\n                if is_reranker\n                else query_input_len\n            )\n            batch_requests.append(\n                SampleRequest(\n                    prompt=[query_prompt] + [req[0] for req in batch],\n                    prompt_len=query_contrib + sum(req[1] for req in batch),\n                    expected_output_len=0,\n                    request_id=request_id_prefix + str(i // batchsize),\n                )\n            )\n\n        if token_mismatch_total != 0:\n            logger.warning(\n                \"Across all generated prompts, there were %d %s tokens \"\n                \"than expected after decoding and re-encoding. This is \"\n                \"expected due to the imperfect nature of the sampling \"\n                \"procedure.\",\n                abs(token_mismatch_total),\n                \"more\" if token_mismatch_total > 0 else \"fewer\",\n            )\n\n        return batch_requests",
      "language": "python"
    },
    {
      "code": "__init__(**kwargs) -> None",
      "language": "python"
    },
    {
      "code": "__init__(**kwargs) -> None",
      "language": "python"
    },
    {
      "code": "def __init__(self, **kwargs) -> None:\n    super().__init__(**kwargs)",
      "language": "python"
    },
    {
      "code": "def __init__(self, **kwargs) -> None:\n    super().__init__(**kwargs)",
      "language": "python"
    },
    {
      "code": "sample(\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    request_id_prefix: str = \"\",\n    range_ratio: float = DEFAULT_RANGE_RATIO,\n    input_len: int = DEFAULT_INPUT_LEN,\n    batchsize: int = 1,\n    is_reranker: bool = True,\n    **kwargs,\n) -> list[SampleRequest]",
      "language": "typescript"
    },
    {
      "code": "sample(\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    request_id_prefix: str = \"\",\n    range_ratio: float = DEFAULT_RANGE_RATIO,\n    input_len: int = DEFAULT_INPUT_LEN,\n    batchsize: int = 1,\n    is_reranker: bool = True,\n    **kwargs,\n) -> list[SampleRequest]",
      "language": "typescript"
    },
    {
      "code": "686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\n700\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780",
      "language": "unknown"
    },
    {
      "code": "def sample(\n    self,\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    request_id_prefix: str = \"\",\n    range_ratio: float = RandomDataset.DEFAULT_RANGE_RATIO,\n    input_len: int = RandomDataset.DEFAULT_INPUT_LEN,\n    batchsize: int = 1,\n    is_reranker: bool = True,\n    **kwargs,\n) -> list[SampleRequest]:\n    n_sep_tokens = int(is_reranker)\n\n    query_len_param = (input_len // 2) - n_sep_tokens if is_reranker else input_len\n\n    query_lens, _, query_offsets = self.get_sampling_params(\n        1, range_ratio, query_len_param, 0, tokenizer\n    )\n\n    query_len = int(query_lens[0])\n\n    if not is_reranker:\n        assert num_requests > 1 and batchsize > 1\n        num_requests -= 1\n        batchsize -= 1\n        doc_len_param = input_len\n    else:\n        doc_len_param = input_len - query_len - n_sep_tokens\n\n    doc_lens, _, doc_offsets = self.get_sampling_params(\n        num_requests, range_ratio, doc_len_param, 0, tokenizer\n    )\n\n    vocab_size = tokenizer.vocab_size\n    prohibited_tokens = tokenizer.all_special_ids\n    all_tokens = np.arange(vocab_size)\n    allowed_tokens = np.array(list(set(all_tokens) - set(prohibited_tokens)))\n\n    query_prompt, query_input_len, token_mismatch_total = (\n        self.generate_token_sequence(\n            tokenizer=tokenizer,\n            prefix_token_ids=[],\n            prefix_len=0,\n            vocab_size=vocab_size,\n            input_len=query_len,\n            offset=int(query_offsets[0]),\n            index=0,\n            allowed_tokens=allowed_tokens,\n        )\n    )\n\n    requests = []\n    for i in range(num_requests):\n        prompt, total_input_len, token_mismatch = self.generate_token_sequence(  # noqa: E501\n            tokenizer=tokenizer,\n            prefix_token_ids=[],\n            prefix_len=0,\n            vocab_size=vocab_size,\n            input_len=int(doc_lens[i]),\n            offset=int(doc_offsets[i]),\n            index=i + 1,\n            allowed_tokens=allowed_tokens,\n        )\n        token_mismatch_total += token_mismatch\n        requests.append((prompt, total_input_len))\n\n    batch_requests = []\n    # Create batched requests\n    for i in range(0, num_requests, batchsize):\n        batch = requests[i : i + batchsize]\n        query_contrib = (\n            (query_input_len + n_sep_tokens) * len(batch)\n            if is_reranker\n            else query_input_len\n        )\n        batch_requests.append(\n            SampleRequest(\n                prompt=[query_prompt] + [req[0] for req in batch],\n                prompt_len=query_contrib + sum(req[1] for req in batch),\n                expected_output_len=0,\n                request_id=request_id_prefix + str(i // batchsize),\n            )\n        )\n\n    if token_mismatch_total != 0:\n        logger.warning(\n            \"Across all generated prompts, there were %d %s tokens \"\n            \"than expected after decoding and re-encoding. This is \"\n            \"expected due to the imperfect nature of the sampling \"\n            \"procedure.\",\n            abs(token_mismatch_total),\n            \"more\" if token_mismatch_total > 0 else \"fewer\",\n        )\n\n    return batch_requests",
      "language": "python"
    },
    {
      "code": "def sample(\n    self,\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    request_id_prefix: str = \"\",\n    range_ratio: float = RandomDataset.DEFAULT_RANGE_RATIO,\n    input_len: int = RandomDataset.DEFAULT_INPUT_LEN,\n    batchsize: int = 1,\n    is_reranker: bool = True,\n    **kwargs,\n) -> list[SampleRequest]:\n    n_sep_tokens = int(is_reranker)\n\n    query_len_param = (input_len // 2) - n_sep_tokens if is_reranker else input_len\n\n    query_lens, _, query_offsets = self.get_sampling_params(\n        1, range_ratio, query_len_param, 0, tokenizer\n    )\n\n    query_len = int(query_lens[0])\n\n    if not is_reranker:\n        assert num_requests > 1 and batchsize > 1\n        num_requests -= 1\n        batchsize -= 1\n        doc_len_param = input_len\n    else:\n        doc_len_param = input_len - query_len - n_sep_tokens\n\n    doc_lens, _, doc_offsets = self.get_sampling_params(\n        num_requests, range_ratio, doc_len_param, 0, tokenizer\n    )\n\n    vocab_size = tokenizer.vocab_size\n    prohibited_tokens = tokenizer.all_special_ids\n    all_tokens = np.arange(vocab_size)\n    allowed_tokens = np.array(list(set(all_tokens) - set(prohibited_tokens)))\n\n    query_prompt, query_input_len, token_mismatch_total = (\n        self.generate_token_sequence(\n            tokenizer=tokenizer,\n            prefix_token_ids=[],\n            prefix_len=0,\n            vocab_size=vocab_size,\n            input_len=query_len,\n            offset=int(query_offsets[0]),\n            index=0,\n            allowed_tokens=allowed_tokens,\n        )\n    )\n\n    requests = []\n    for i in range(num_requests):\n        prompt, total_input_len, token_mismatch = self.generate_token_sequence(  # noqa: E501\n            tokenizer=tokenizer,\n            prefix_token_ids=[],\n            prefix_len=0,\n            vocab_size=vocab_size,\n            input_len=int(doc_lens[i]),\n            offset=int(doc_offsets[i]),\n            index=i + 1,\n            allowed_tokens=allowed_tokens,\n        )\n        token_mismatch_total += token_mismatch\n        requests.append((prompt, total_input_len))\n\n    batch_requests = []\n    # Create batched requests\n    for i in range(0, num_requests, batchsize):\n        batch = requests[i : i + batchsize]\n        query_contrib = (\n            (query_input_len + n_sep_tokens) * len(batch)\n            if is_reranker\n            else query_input_len\n        )\n        batch_requests.append(\n            SampleRequest(\n                prompt=[query_prompt] + [req[0] for req in batch],\n                prompt_len=query_contrib + sum(req[1] for req in batch),\n                expected_output_len=0,\n                request_id=request_id_prefix + str(i // batchsize),\n            )\n        )\n\n    if token_mismatch_total != 0:\n        logger.warning(\n            \"Across all generated prompts, there were %d %s tokens \"\n            \"than expected after decoding and re-encoding. This is \"\n            \"expected due to the imperfect nature of the sampling \"\n            \"procedure.\",\n            abs(token_mismatch_total),\n            \"more\" if token_mismatch_total > 0 else \"fewer\",\n        )\n\n    return batch_requests",
      "language": "python"
    },
    {
      "code": "788\n 789\n 790\n 791\n 792\n 793\n 794\n 795\n 796\n 797\n 798\n 799\n 800\n 801\n 802\n 803\n 804\n 805\n 806\n 807\n 808\n 809\n 810\n 811\n 812\n 813\n 814\n 815\n 816\n 817\n 818\n 819\n 820\n 821\n 822\n 823\n 824\n 825\n 826\n 827\n 828\n 829\n 830\n 831\n 832\n 833\n 834\n 835\n 836\n 837\n 838\n 839\n 840\n 841\n 842\n 843\n 844\n 845\n 846\n 847\n 848\n 849\n 850\n 851\n 852\n 853\n 854\n 855\n 856\n 857\n 858\n 859\n 860\n 861\n 862\n 863\n 864\n 865\n 866\n 867\n 868\n 869\n 870\n 871\n 872\n 873\n 874\n 875\n 876\n 877\n 878\n 879\n 880\n 881\n 882\n 883\n 884\n 885\n 886\n 887\n 888\n 889\n 890\n 891\n 892\n 893\n 894\n 895\n 896\n 897\n 898\n 899\n 900\n 901\n 902\n 903\n 904\n 905\n 906\n 907\n 908\n 909\n 910\n 911\n 912\n 913\n 914\n 915\n 916\n 917\n 918\n 919\n 920\n 921\n 922\n 923\n 924\n 925\n 926\n 927\n 928\n 929\n 930\n 931\n 932\n 933\n 934\n 935\n 936\n 937\n 938\n 939\n 940\n 941\n 942\n 943\n 944\n 945\n 946\n 947\n 948\n 949\n 950\n 951\n 952\n 953\n 954\n 955\n 956\n 957\n 958\n 959\n 960\n 961\n 962\n 963\n 964\n 965\n 966\n 967\n 968\n 969\n 970\n 971\n 972\n 973\n 974\n 975\n 976\n 977\n 978\n 979\n 980\n 981\n 982\n 983\n 984\n 985\n 986\n 987\n 988\n 989\n 990\n 991\n 992\n 993\n 994\n 995\n 996\n 997\n 998\n 999\n1000\n1001\n1002\n1003\n1004\n1005\n1006\n1007\n1008\n1009\n1010\n1011\n1012\n1013\n1014\n1015\n1016\n1017\n1018\n1019\n1020\n1021\n1022\n1023\n1024\n1025\n1026\n1027\n1028\n1029\n1030\n1031\n1032\n1033\n1034\n1035\n1036\n1037\n1038\n1039\n1040\n1041\n1042\n1043\n1044\n1045\n1046\n1047\n1048\n1049\n1050\n1051\n1052\n1053\n1054\n1055\n1056\n1057\n1058\n1059\n1060\n1061\n1062\n1063\n1064\n1065\n1066\n1067\n1068\n1069\n1070\n1071\n1072\n1073\n1074\n1075\n1076\n1077\n1078\n1079\n1080\n1081\n1082\n1083\n1084\n1085\n1086\n1087\n1088\n1089\n1090\n1091\n1092\n1093\n1094\n1095\n1096\n1097\n1098\n1099\n1100\n1101\n1102\n1103\n1104\n1105\n1106\n1107\n1108\n1109\n1110\n1111\n1112\n1113\n1114\n1115\n1116\n1117\n1118\n1119\n1120\n1121\n1122\n1123\n1124\n1125\n1126\n1127\n1128\n1129\n1130\n1131\n1132\n1133\n1134\n1135\n1136\n1137\n1138\n1139\n1140\n1141\n1142\n1143\n1144\n1145\n1146\n1147\n1148\n1149\n1150\n1151\n1152\n1153\n1154\n1155\n1156\n1157\n1158\n1159\n1160\n1161\n1162\n1163\n1164\n1165\n1166\n1167\n1168\n1169\n1170\n1171\n1172\n1173\n1174\n1175\n1176\n1177\n1178\n1179\n1180\n1181\n1182\n1183\n1184\n1185\n1186\n1187\n1188\n1189\n1190\n1191\n1192\n1193\n1194\n1195\n1196\n1197\n1198\n1199\n1200\n1201\n1202\n1203",
      "language": "unknown"
    },
    {
      "code": "class RandomMultiModalDataset(RandomDataset):\n    \"\"\"\n    Synthetic multimodal dataset (text + images) that extends RandomDataset.\n\n    Status:\n    - Images: supported via synthetic RGB data.\n    - Video: supported via synthetic RGB data.\n    - Audio: not yet supported.\n\n    Sampling overview:\n    1) Number of items per request is sampled uniformly from the integer range\n       [floor(n·(1−r)), ceil(n·(1+r))], where n is the base count and r is\n       `num_mm_items_range_ratio` in [0, 1]. r=0 keeps it fixed; r=1 allows 0.\n       The maximum is further clamped to the sum of per-modality limits.\n    2) Each item’s modality and shape is sampled from `bucket_config`, a dict\n       mapping (height, width, num_frames) → probability. We treat\n       `num_frames`=1 as image and `num_frames` > 1 as video.\n       Entries with zero probability are removed and the rest are renormalized\n       to sum to 1.\n    3) Per-modality hard caps are enforced via `limit_mm_per_prompt`.\n       When a modality reaches its cap, all of its buckets are excluded and the\n       remaining probabilities are renormalized.\n\n    Example bucket configuration:\n    {(256, 256, 1): 0.5, (720, 1280, 1): 0.4, (720, 1280, 16): 0.1}\n      - Two image buckets (`num_frames`=1) and one video bucket\n      (`num_frames`=16).\n    OBS.: Only image sampling is supported for now.\n    \"\"\"\n\n    IS_MULTIMODAL = True\n    DEFAULT_LIMIT_MM_PER_PROMPT = {\"image\": 255, \"video\": 1}\n\n    DEFAULT_BASE_ITEMS_PER_REQUEST = 1\n    DEFAULT_NUM_MM_ITEMS_RANGE_RATIO = 0.0\n    DEFAULT_MM_ITEM_BUCKET_CONFIG = {\n        (256, 256, 1): 0.5,\n        (720, 1280, 1): 0.5,\n        (720, 1280, 16): 0.0,\n    }\n    DEFAULT_ENABLE_MULTIMODAL_CHAT = False\n\n    def __init__(self, **kwargs) -> None:\n        super().__init__(**kwargs)\n\n    def generate_synthetic_image(self, width: int, height: int) -> Image.Image:\n        \"\"\"Generate synthetic PIL image with random RGB values.\n\n        NOTE: iid pixel sampling results in worst-case compression\n        (good for stressing I/O), but very unlike real photos.\n        We could consider a “low-freq” mode (e.g., noise blur)\n        to emulate network realism instead of max stress.\n        \"\"\"\n        random_pixels = self._rng.integers(\n            0,\n            256,\n            (height, width, 3),\n            dtype=np.uint8,\n        )\n        return Image.fromarray(random_pixels)\n\n    def generate_synthetic_video(\n        self, width: int, height: int, num_frames: int\n    ) -> dict:\n        \"\"\"Generate synthetic video with random values.\n\n        Creates a video with random pixel values, encodes it to MP4 format,\n        and returns the content as bytes.\n        \"\"\"\n        import cv2\n\n        random_pixels = self._rng.integers(\n            0,\n            256,\n            (num_frames, height, width, 3),\n            dtype=np.uint8,\n        )\n\n        # Create a temporary video file in memory\n        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n        fps = 30  # frames per second\n\n        with NamedTemporaryFile(suffix=\".mp4\", delete_on_close=False) as temp_file:\n            temp_path = temp_file.name\n\n            # Create video writer\n            video_writer = cv2.VideoWriter(\n                temp_path, fourcc=fourcc, fps=fps, frameSize=(width, height)\n            )\n\n            if not video_writer.isOpened():\n                raise RuntimeError(\"Failed to create video writer\")\n\n            for frame in random_pixels:\n                video_writer.write(frame)\n\n            video_writer.release()\n            temp_file.close()\n\n            # Read the video file content\n            with open(temp_path, \"rb\") as f:\n                video_content = f.read()\n\n            return {\"bytes\": video_content}\n\n    def map_config_to_modality(self, config: tuple[int, int, int]) -> str:\n        \"\"\"Map the configuration to the modality.\"\"\"\n        if config[-1] == 1:\n            return \"image\"\n        elif config[-1] > 1:\n            return \"video\"\n        else:\n            raise ValueError(f\"Invalid multimodal item configuration: {config}\")\n\n    def normalize_bucket_config(\n        self, bucket_config: dict[tuple[int, int, int], float]\n    ) -> dict[tuple[int, int, int], float]:\n        \"\"\"\n        Remove zero probability entries\n        and normalize the bucket config to sum to 1.\n        \"\"\"\n        # Raise error if value is negative\n        if any(v < 0 for v in bucket_config.values()):\n            raise ValueError(\"Bucket config values must be non-negative.\")\n        # Remove zero probability entries\n        bucket_config = {k: v for k, v in bucket_config.items() if v > 0}\n        # if bucket config is empty, raise error\n        if not bucket_config:\n            raise ValueError(\n                \"Got invalid bucket config. Bucket config values must be non-zero.\"\n            )\n        # Normalize the remaining bucket config to sum to 1\n        total = sum(bucket_config.values())\n        return {k: v / total for k, v in bucket_config.items()}\n\n    def generate_mm_item(\n        self,\n        mm_item_config: tuple[int, int, int],\n    ) -> Mapping[str, Any]:\n        \"\"\"\n        Create synthetic images and videos and\n        apply process_image/process_video respectively.\n        This follows the OpenAI API chat completions\n        https://github.com/openai/openai-python\n        \"\"\"\n\n        if self.map_config_to_modality(mm_item_config) == \"image\":\n            return process_image(\n                self.generate_synthetic_image(mm_item_config[1], mm_item_config[0])\n            )\n        elif self.map_config_to_modality(mm_item_config) == \"video\":\n            return process_video(\n                self.generate_synthetic_video(\n                    mm_item_config[1], mm_item_config[0], mm_item_config[2]\n                )\n            )\n        else:\n            raise ValueError(f\"Invalid multimodal item configuration: {mm_item_config}\")\n\n    def get_mm_item_sampling_params(\n        self,\n        base_items_per_request: int,\n        num_mm_items_range_ratio: float,\n        limit_mm_per_prompt: dict[str, int],\n        bucket_config: dict[tuple[int, int, int], float],\n    ) -> tuple[int, int, dict[str, int], dict[tuple[int, int, int], float]]:\n        \"\"\"\n        Get the sampling parameters for the multimodal items.\n        \"\"\"\n        # Enforce num_mm_items_range_ratio <= 1\n        if not (0.0 <= num_mm_items_range_ratio <= 1.0):\n            raise ValueError(\"num_mm_items_range_ratio must be in [0, 1].\")\n\n        # Ensure modalities to sample are in limit_mm_per_prompt\n        for k, v in bucket_config.items():\n            # get modality from bucket config\n            modality = self.map_config_to_modality(k)\n            if modality not in limit_mm_per_prompt:\n                raise ValueError(\n                    f\"Modality {modality} is not in \"\n                    f\"limit_mm_per_prompt: \"\n                    f\"{limit_mm_per_prompt.keys()}\"\n                )\n\n        # Remove zero probability entries\n        # and normalize bucket config to sum to 1\n        bucket_config = self.normalize_bucket_config(bucket_config)\n        logger.info(\n            \"Normalized bucket config: %s\",\n            bucket_config,\n        )\n        # Only consider limit per prompt for modalities in bucket config\n        allowed_modalities = {self.map_config_to_modality(cfg) for cfg in bucket_config}\n        limit_mm_per_prompt = {\n            k: v for k, v in limit_mm_per_prompt.items() if k in allowed_modalities\n        }\n        if not limit_mm_per_prompt:\n            raise ValueError(\"No valid limits for modalities present in bucket_config.\")\n\n        logger.info(\n            \"Updated mm-limit-per-prompt: %s\",\n            limit_mm_per_prompt,\n        )\n\n        # Get max and min num mm items and ensure\n        # it is at most the sum of limit_mm_per_prompt for all modalities\n        max_num_mm_items = min(\n            sum(limit_mm_per_prompt.values()),\n            math.ceil(base_items_per_request * (1 + num_mm_items_range_ratio)),\n        )\n        # Ensure min num mm items is at least 0\n        min_num_mm_items = max(\n            0, math.floor(base_items_per_request * (1 - num_mm_items_range_ratio))\n        )\n        # Raise error if min num mm items is greater than max num mm items\n        if min_num_mm_items > max_num_mm_items:\n            raise ValueError(\n                f\"Min num mm items is greater than max mm items: \"\n                f\"{min_num_mm_items} > {max_num_mm_items}\"\n            )\n\n        logger.info(\n            \"Sampling number of multimodal items from [%s, %s]\",\n            min_num_mm_items,\n            max_num_mm_items,\n        )\n\n        return (\n            min_num_mm_items,\n            max_num_mm_items,\n            limit_mm_per_prompt,\n            bucket_config,\n        )\n\n    def get_mm_item_iterator(\n        self,\n        min_num_mm_items: int,\n        max_num_mm_items: int,\n        bucket_config: dict[tuple[int, int, int], float],\n        limit_mm_per_prompt: dict[str, int],\n    ) -> Iterator[tuple[int, int, int]]:\n        \"\"\"\n        Iterator over the multimodal items for each request\n        whose size is between min_num_mm_items and max_num_mm_items.\n\n        Loop over the bucket config and sample a multimodal item.\n        Loop until the number of multimodal items sampled is equal to\n        request_num_mm_items or limit of multimodal items per prompt\n        for all modalities is reached.\n\n        Note:\n        - This function operates on a per-request shallow copy of\n          `bucket_config` (tuple->float). The original dict passed to\n          `sample` is not mutated. If this ever changes, a test\n          is implemented and will fail.\n        \"\"\"\n        # Get the number of multimodal items to sample\n        request_num_mm_items = int(\n            self._rng.integers(min_num_mm_items, max_num_mm_items + 1)\n        )\n        # If request_num_mm_items is 0, yield an empty iterator\n        if request_num_mm_items == 0:\n            return\n        # Initialize modality counters\n        modality_counter = {self.map_config_to_modality(k): 0 for k in bucket_config}\n        # Copy the bucket config to avoid modifying the original\n        bucket_config_copy = bucket_config.copy()\n        # Loop over the number of multimodal items to sample\n        while sum(modality_counter.values()) < request_num_mm_items:\n            # Sample a multimodal item config\n            mm_item_config = self._rng.choice(\n                list(bucket_config_copy.keys()), p=list(bucket_config_copy.values())\n            )\n            modality = self.map_config_to_modality(mm_item_config)\n            # Check that modality count is less than limit per prompt\n            if modality_counter[modality] < limit_mm_per_prompt[modality]:\n                modality_counter[modality] += 1\n                yield (mm_item_config)\n            else:\n                # If the counter is greater than the limit per prompt\n                # set all multimodal items of this modality to 0\n                for k, v in bucket_config_copy.items():\n                    if self.map_config_to_modality(k) == modality:\n                        bucket_config_copy[k] = 0\n                # If all configs are 0, break the loop\n                # This should not happen as request_num_mm_items is at most\n                # the sum of limit_mm_per_prompt for all modalities\n                if all(v == 0 for v in bucket_config_copy.values()):\n                    logger.warning(\n                        \"Exhausted all multimodal items of modality %s\", modality\n                    )\n                    break\n                # Renormalize the bucket config\n                bucket_config_copy = self.normalize_bucket_config(bucket_config_copy)\n\n    def sample(\n        self,\n        tokenizer: TokenizerLike,\n        num_requests: int,\n        request_id_prefix: str = \"\",\n        no_oversample: bool = False,\n        prefix_len: int = RandomDataset.DEFAULT_PREFIX_LEN,\n        range_ratio: float = RandomDataset.DEFAULT_RANGE_RATIO,\n        input_len: int = RandomDataset.DEFAULT_INPUT_LEN,\n        output_len: int = RandomDataset.DEFAULT_OUTPUT_LEN,\n        limit_mm_per_prompt: dict[str, int] = DEFAULT_LIMIT_MM_PER_PROMPT,\n        base_items_per_request: int = DEFAULT_BASE_ITEMS_PER_REQUEST,\n        num_mm_items_range_ratio: float = DEFAULT_NUM_MM_ITEMS_RANGE_RATIO,\n        bucket_config: dict[\n            tuple[int, int, int], float\n        ] = DEFAULT_MM_ITEM_BUCKET_CONFIG,\n        enable_multimodal_chat: bool = DEFAULT_ENABLE_MULTIMODAL_CHAT,\n        **kwargs,\n    ) -> list[SampleRequest]:\n        # Get the sampling parameters for the dataset\n        input_lens, output_lens, offsets = self.get_sampling_params(\n            num_requests, range_ratio, input_len, output_len, tokenizer\n        )\n\n        (\n            min_num_mm_items,\n            max_num_mm_items,\n            limit_mm_per_prompt,\n            bucket_config,\n        ) = self.get_mm_item_sampling_params(\n            base_items_per_request,\n            num_mm_items_range_ratio,\n            limit_mm_per_prompt,\n            bucket_config,\n        )\n\n        vocab_size = tokenizer.vocab_size\n        # Can't use tokenizer.all_special_ids since\n        # it returns ONLY ids from special_tokens_map.json\n        # We want to exclude placeholder tokens and all\n        # tokens that indicate start/end of image as it\n        # may break prompt replacement logic.\n        prohibited_tokens = list(\n            tok_id\n            for tok_id, token in tokenizer.added_tokens_decoder.items()\n            if token.special\n        )\n        all_tokens = np.arange(vocab_size)\n        allowed_tokens = np.array(list(set(all_tokens) - set(prohibited_tokens)))\n        logger.debug(\n            \"Sampling from %d out of %d (vocab size)\", len(allowed_tokens), vocab_size\n        )\n        # Generate prefix once\n        prefix_token_ids = self.get_prefix(allowed_tokens, prefix_len)\n        # Add synthetic multimodal items to each request\n        mm_requests = []\n        token_mismatch_total = 0\n        for i in range(num_requests):\n            prompt, total_input_len, token_mismatch = self.generate_token_sequence(  # noqa: E501\n                tokenizer=tokenizer,\n                prefix_token_ids=prefix_token_ids,\n                prefix_len=prefix_len,\n                vocab_size=vocab_size,\n                input_len=int(input_lens[i]),\n                offset=int(offsets[i]),\n                index=i,\n                allowed_tokens=allowed_tokens,\n            )\n            token_mismatch_total += token_mismatch\n            # Get multimodal item iterator for a given request\n            mm_item_iterator = self.get_mm_item_iterator(\n                min_num_mm_items,\n                max_num_mm_items,\n                bucket_config,\n                limit_mm_per_prompt,\n            )\n\n            mm_content = cast(\n                list[dict[str, Any]],\n                [\n                    self.generate_mm_item(mm_item_config)\n                    for mm_item_config in mm_item_iterator\n                ],\n            )\n\n            if enable_multimodal_chat:\n                # NOTE: For now this option is only provided for completeness\n                # given that the serve.py benchmark currently does not use it.\n                mm_chat_prompt: Any = prompt\n                mm_chat_prompt = self.apply_multimodal_chat_transformation(\n                    prompt, mm_content\n                )\n                sample_request = SampleRequest(\n                    prompt=mm_chat_prompt,\n                    prompt_len=total_input_len,\n                    expected_output_len=int(output_lens[i]),\n                    multi_modal_data=None,\n                    request_id=request_id_prefix + str(i),\n                )\n            else:\n                sample_request = SampleRequest(\n                    prompt=prompt,\n                    prompt_len=total_input_len,\n                    expected_output_len=int(output_lens[i]),\n                    multi_modal_data=mm_content,\n                    request_id=request_id_prefix + str(i),\n                )\n            mm_requests.append(sample_request)\n\n        if token_mismatch_total != 0:\n            sign = \"more\" if token_mismatch_total > 0 else \"fewer\"\n            logger.warning(\n                \"Across all generated prompts, there were %d %s tokens \"\n                \"than expected after decoding and re-encoding. This is \"\n                \"expected due to the imperfect nature of the sampling \"\n                \"procedure.\",\n                abs(token_mismatch_total),\n                sign,\n            )\n\n        return mm_requests",
      "language": "python"
    },
    {
      "code": "class RandomMultiModalDataset(RandomDataset):\n    \"\"\"\n    Synthetic multimodal dataset (text + images) that extends RandomDataset.\n\n    Status:\n    - Images: supported via synthetic RGB data.\n    - Video: supported via synthetic RGB data.\n    - Audio: not yet supported.\n\n    Sampling overview:\n    1) Number of items per request is sampled uniformly from the integer range\n       [floor(n·(1−r)), ceil(n·(1+r))], where n is the base count and r is\n       `num_mm_items_range_ratio` in [0, 1]. r=0 keeps it fixed; r=1 allows 0.\n       The maximum is further clamped to the sum of per-modality limits.\n    2) Each item’s modality and shape is sampled from `bucket_config`, a dict\n       mapping (height, width, num_frames) → probability. We treat\n       `num_frames`=1 as image and `num_frames` > 1 as video.\n       Entries with zero probability are removed and the rest are renormalized\n       to sum to 1.\n    3) Per-modality hard caps are enforced via `limit_mm_per_prompt`.\n       When a modality reaches its cap, all of its buckets are excluded and the\n       remaining probabilities are renormalized.\n\n    Example bucket configuration:\n    {(256, 256, 1): 0.5, (720, 1280, 1): 0.4, (720, 1280, 16): 0.1}\n      - Two image buckets (`num_frames`=1) and one video bucket\n      (`num_frames`=16).\n    OBS.: Only image sampling is supported for now.\n    \"\"\"\n\n    IS_MULTIMODAL = True\n    DEFAULT_LIMIT_MM_PER_PROMPT = {\"image\": 255, \"video\": 1}\n\n    DEFAULT_BASE_ITEMS_PER_REQUEST = 1\n    DEFAULT_NUM_MM_ITEMS_RANGE_RATIO = 0.0\n    DEFAULT_MM_ITEM_BUCKET_CONFIG = {\n        (256, 256, 1): 0.5,\n        (720, 1280, 1): 0.5,\n        (720, 1280, 16): 0.0,\n    }\n    DEFAULT_ENABLE_MULTIMODAL_CHAT = False\n\n    def __init__(self, **kwargs) -> None:\n        super().__init__(**kwargs)\n\n    def generate_synthetic_image(self, width: int, height: int) -> Image.Image:\n        \"\"\"Generate synthetic PIL image with random RGB values.\n\n        NOTE: iid pixel sampling results in worst-case compression\n        (good for stressing I/O), but very unlike real photos.\n        We could consider a “low-freq” mode (e.g., noise blur)\n        to emulate network realism instead of max stress.\n        \"\"\"\n        random_pixels = self._rng.integers(\n            0,\n            256,\n            (height, width, 3),\n            dtype=np.uint8,\n        )\n        return Image.fromarray(random_pixels)\n\n    def generate_synthetic_video(\n        self, width: int, height: int, num_frames: int\n    ) -> dict:\n        \"\"\"Generate synthetic video with random values.\n\n        Creates a video with random pixel values, encodes it to MP4 format,\n        and returns the content as bytes.\n        \"\"\"\n        import cv2\n\n        random_pixels = self._rng.integers(\n            0,\n            256,\n            (num_frames, height, width, 3),\n            dtype=np.uint8,\n        )\n\n        # Create a temporary video file in memory\n        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n        fps = 30  # frames per second\n\n        with NamedTemporaryFile(suffix=\".mp4\", delete_on_close=False) as temp_file:\n            temp_path = temp_file.name\n\n            # Create video writer\n            video_writer = cv2.VideoWriter(\n                temp_path, fourcc=fourcc, fps=fps, frameSize=(width, height)\n            )\n\n            if not video_writer.isOpened():\n                raise RuntimeError(\"Failed to create video writer\")\n\n            for frame in random_pixels:\n                video_writer.write(frame)\n\n            video_writer.release()\n            temp_file.close()\n\n            # Read the video file content\n            with open(temp_path, \"rb\") as f:\n                video_content = f.read()\n\n            return {\"bytes\": video_content}\n\n    def map_config_to_modality(self, config: tuple[int, int, int]) -> str:\n        \"\"\"Map the configuration to the modality.\"\"\"\n        if config[-1] == 1:\n            return \"image\"\n        elif config[-1] > 1:\n            return \"video\"\n        else:\n            raise ValueError(f\"Invalid multimodal item configuration: {config}\")\n\n    def normalize_bucket_config(\n        self, bucket_config: dict[tuple[int, int, int], float]\n    ) -> dict[tuple[int, int, int], float]:\n        \"\"\"\n        Remove zero probability entries\n        and normalize the bucket config to sum to 1.\n        \"\"\"\n        # Raise error if value is negative\n        if any(v < 0 for v in bucket_config.values()):\n            raise ValueError(\"Bucket config values must be non-negative.\")\n        # Remove zero probability entries\n        bucket_config = {k: v for k, v in bucket_config.items() if v > 0}\n        # if bucket config is empty, raise error\n        if not bucket_config:\n            raise ValueError(\n                \"Got invalid bucket config. Bucket config values must be non-zero.\"\n            )\n        # Normalize the remaining bucket config to sum to 1\n        total = sum(bucket_config.values())\n        return {k: v / total for k, v in bucket_config.items()}\n\n    def generate_mm_item(\n        self,\n        mm_item_config: tuple[int, int, int],\n    ) -> Mapping[str, Any]:\n        \"\"\"\n        Create synthetic images and videos and\n        apply process_image/process_video respectively.\n        This follows the OpenAI API chat completions\n        https://github.com/openai/openai-python\n        \"\"\"\n\n        if self.map_config_to_modality(mm_item_config) == \"image\":\n            return process_image(\n                self.generate_synthetic_image(mm_item_config[1], mm_item_config[0])\n            )\n        elif self.map_config_to_modality(mm_item_config) == \"video\":\n            return process_video(\n                self.generate_synthetic_video(\n                    mm_item_config[1], mm_item_config[0], mm_item_config[2]\n                )\n            )\n        else:\n            raise ValueError(f\"Invalid multimodal item configuration: {mm_item_config}\")\n\n    def get_mm_item_sampling_params(\n        self,\n        base_items_per_request: int,\n        num_mm_items_range_ratio: float,\n        limit_mm_per_prompt: dict[str, int],\n        bucket_config: dict[tuple[int, int, int], float],\n    ) -> tuple[int, int, dict[str, int], dict[tuple[int, int, int], float]]:\n        \"\"\"\n        Get the sampling parameters for the multimodal items.\n        \"\"\"\n        # Enforce num_mm_items_range_ratio <= 1\n        if not (0.0 <= num_mm_items_range_ratio <= 1.0):\n            raise ValueError(\"num_mm_items_range_ratio must be in [0, 1].\")\n\n        # Ensure modalities to sample are in limit_mm_per_prompt\n        for k, v in bucket_config.items():\n            # get modality from bucket config\n            modality = self.map_config_to_modality(k)\n            if modality not in limit_mm_per_prompt:\n                raise ValueError(\n                    f\"Modality {modality} is not in \"\n                    f\"limit_mm_per_prompt: \"\n                    f\"{limit_mm_per_prompt.keys()}\"\n                )\n\n        # Remove zero probability entries\n        # and normalize bucket config to sum to 1\n        bucket_config = self.normalize_bucket_config(bucket_config)\n        logger.info(\n            \"Normalized bucket config: %s\",\n            bucket_config,\n        )\n        # Only consider limit per prompt for modalities in bucket config\n        allowed_modalities = {self.map_config_to_modality(cfg) for cfg in bucket_config}\n        limit_mm_per_prompt = {\n            k: v for k, v in limit_mm_per_prompt.items() if k in allowed_modalities\n        }\n        if not limit_mm_per_prompt:\n            raise ValueError(\"No valid limits for modalities present in bucket_config.\")\n\n        logger.info(\n            \"Updated mm-limit-per-prompt: %s\",\n            limit_mm_per_prompt,\n        )\n\n        # Get max and min num mm items and ensure\n        # it is at most the sum of limit_mm_per_prompt for all modalities\n        max_num_mm_items = min(\n            sum(limit_mm_per_prompt.values()),\n            math.ceil(base_items_per_request * (1 + num_mm_items_range_ratio)),\n        )\n        # Ensure min num mm items is at least 0\n        min_num_mm_items = max(\n            0, math.floor(base_items_per_request * (1 - num_mm_items_range_ratio))\n        )\n        # Raise error if min num mm items is greater than max num mm items\n        if min_num_mm_items > max_num_mm_items:\n            raise ValueError(\n                f\"Min num mm items is greater than max mm items: \"\n                f\"{min_num_mm_items} > {max_num_mm_items}\"\n            )\n\n        logger.info(\n            \"Sampling number of multimodal items from [%s, %s]\",\n            min_num_mm_items,\n            max_num_mm_items,\n        )\n\n        return (\n            min_num_mm_items,\n            max_num_mm_items,\n            limit_mm_per_prompt,\n            bucket_config,\n        )\n\n    def get_mm_item_iterator(\n        self,\n        min_num_mm_items: int,\n        max_num_mm_items: int,\n        bucket_config: dict[tuple[int, int, int], float],\n        limit_mm_per_prompt: dict[str, int],\n    ) -> Iterator[tuple[int, int, int]]:\n        \"\"\"\n        Iterator over the multimodal items for each request\n        whose size is between min_num_mm_items and max_num_mm_items.\n\n        Loop over the bucket config and sample a multimodal item.\n        Loop until the number of multimodal items sampled is equal to\n        request_num_mm_items or limit of multimodal items per prompt\n        for all modalities is reached.\n\n        Note:\n        - This function operates on a per-request shallow copy of\n          `bucket_config` (tuple->float). The original dict passed to\n          `sample` is not mutated. If this ever changes, a test\n          is implemented and will fail.\n        \"\"\"\n        # Get the number of multimodal items to sample\n        request_num_mm_items = int(\n            self._rng.integers(min_num_mm_items, max_num_mm_items + 1)\n        )\n        # If request_num_mm_items is 0, yield an empty iterator\n        if request_num_mm_items == 0:\n            return\n        # Initialize modality counters\n        modality_counter = {self.map_config_to_modality(k): 0 for k in bucket_config}\n        # Copy the bucket config to avoid modifying the original\n        bucket_config_copy = bucket_config.copy()\n        # Loop over the number of multimodal items to sample\n        while sum(modality_counter.values()) < request_num_mm_items:\n            # Sample a multimodal item config\n            mm_item_config = self._rng.choice(\n                list(bucket_config_copy.keys()), p=list(bucket_config_copy.values())\n            )\n            modality = self.map_config_to_modality(mm_item_config)\n            # Check that modality count is less than limit per prompt\n            if modality_counter[modality] < limit_mm_per_prompt[modality]:\n                modality_counter[modality] += 1\n                yield (mm_item_config)\n            else:\n                # If the counter is greater than the limit per prompt\n                # set all multimodal items of this modality to 0\n                for k, v in bucket_config_copy.items():\n                    if self.map_config_to_modality(k) == modality:\n                        bucket_config_copy[k] = 0\n                # If all configs are 0, break the loop\n                # This should not happen as request_num_mm_items is at most\n                # the sum of limit_mm_per_prompt for all modalities\n                if all(v == 0 for v in bucket_config_copy.values()):\n                    logger.warning(\n                        \"Exhausted all multimodal items of modality %s\", modality\n                    )\n                    break\n                # Renormalize the bucket config\n                bucket_config_copy = self.normalize_bucket_config(bucket_config_copy)\n\n    def sample(\n        self,\n        tokenizer: TokenizerLike,\n        num_requests: int,\n        request_id_prefix: str = \"\",\n        no_oversample: bool = False,\n        prefix_len: int = RandomDataset.DEFAULT_PREFIX_LEN,\n        range_ratio: float = RandomDataset.DEFAULT_RANGE_RATIO,\n        input_len: int = RandomDataset.DEFAULT_INPUT_LEN,\n        output_len: int = RandomDataset.DEFAULT_OUTPUT_LEN,\n        limit_mm_per_prompt: dict[str, int] = DEFAULT_LIMIT_MM_PER_PROMPT,\n        base_items_per_request: int = DEFAULT_BASE_ITEMS_PER_REQUEST,\n        num_mm_items_range_ratio: float = DEFAULT_NUM_MM_ITEMS_RANGE_RATIO,\n        bucket_config: dict[\n            tuple[int, int, int], float\n        ] = DEFAULT_MM_ITEM_BUCKET_CONFIG,\n        enable_multimodal_chat: bool = DEFAULT_ENABLE_MULTIMODAL_CHAT,\n        **kwargs,\n    ) -> list[SampleRequest]:\n        # Get the sampling parameters for the dataset\n        input_lens, output_lens, offsets = self.get_sampling_params(\n            num_requests, range_ratio, input_len, output_len, tokenizer\n        )\n\n        (\n            min_num_mm_items,\n            max_num_mm_items,\n            limit_mm_per_prompt,\n            bucket_config,\n        ) = self.get_mm_item_sampling_params(\n            base_items_per_request,\n            num_mm_items_range_ratio,\n            limit_mm_per_prompt,\n            bucket_config,\n        )\n\n        vocab_size = tokenizer.vocab_size\n        # Can't use tokenizer.all_special_ids since\n        # it returns ONLY ids from special_tokens_map.json\n        # We want to exclude placeholder tokens and all\n        # tokens that indicate start/end of image as it\n        # may break prompt replacement logic.\n        prohibited_tokens = list(\n            tok_id\n            for tok_id, token in tokenizer.added_tokens_decoder.items()\n            if token.special\n        )\n        all_tokens = np.arange(vocab_size)\n        allowed_tokens = np.array(list(set(all_tokens) - set(prohibited_tokens)))\n        logger.debug(\n            \"Sampling from %d out of %d (vocab size)\", len(allowed_tokens), vocab_size\n        )\n        # Generate prefix once\n        prefix_token_ids = self.get_prefix(allowed_tokens, prefix_len)\n        # Add synthetic multimodal items to each request\n        mm_requests = []\n        token_mismatch_total = 0\n        for i in range(num_requests):\n            prompt, total_input_len, token_mismatch = self.generate_token_sequence(  # noqa: E501\n                tokenizer=tokenizer,\n                prefix_token_ids=prefix_token_ids,\n                prefix_len=prefix_len,\n                vocab_size=vocab_size,\n                input_len=int(input_lens[i]),\n                offset=int(offsets[i]),\n                index=i,\n                allowed_tokens=allowed_tokens,\n            )\n            token_mismatch_total += token_mismatch\n            # Get multimodal item iterator for a given request\n            mm_item_iterator = self.get_mm_item_iterator(\n                min_num_mm_items,\n                max_num_mm_items,\n                bucket_config,\n                limit_mm_per_prompt,\n            )\n\n            mm_content = cast(\n                list[dict[str, Any]],\n                [\n                    self.generate_mm_item(mm_item_config)\n                    for mm_item_config in mm_item_iterator\n                ],\n            )\n\n            if enable_multimodal_chat:\n                # NOTE: For now this option is only provided for completeness\n                # given that the serve.py benchmark currently does not use it.\n                mm_chat_prompt: Any = prompt\n                mm_chat_prompt = self.apply_multimodal_chat_transformation(\n                    prompt, mm_content\n                )\n                sample_request = SampleRequest(\n                    prompt=mm_chat_prompt,\n                    prompt_len=total_input_len,\n                    expected_output_len=int(output_lens[i]),\n                    multi_modal_data=None,\n                    request_id=request_id_prefix + str(i),\n                )\n            else:\n                sample_request = SampleRequest(\n                    prompt=prompt,\n                    prompt_len=total_input_len,\n                    expected_output_len=int(output_lens[i]),\n                    multi_modal_data=mm_content,\n                    request_id=request_id_prefix + str(i),\n                )\n            mm_requests.append(sample_request)\n\n        if token_mismatch_total != 0:\n            sign = \"more\" if token_mismatch_total > 0 else \"fewer\"\n            logger.warning(\n                \"Across all generated prompts, there were %d %s tokens \"\n                \"than expected after decoding and re-encoding. This is \"\n                \"expected due to the imperfect nature of the sampling \"\n                \"procedure.\",\n                abs(token_mismatch_total),\n                sign,\n            )\n\n        return mm_requests",
      "language": "python"
    },
    {
      "code": "DEFAULT_BASE_ITEMS_PER_REQUEST = 1",
      "language": "unknown"
    },
    {
      "code": "DEFAULT_BASE_ITEMS_PER_REQUEST = 1",
      "language": "unknown"
    },
    {
      "code": "DEFAULT_ENABLE_MULTIMODAL_CHAT = False",
      "language": "unknown"
    },
    {
      "code": "DEFAULT_ENABLE_MULTIMODAL_CHAT = False",
      "language": "unknown"
    },
    {
      "code": "DEFAULT_LIMIT_MM_PER_PROMPT = {'image': 255, 'video': 1}",
      "language": "json"
    },
    {
      "code": "DEFAULT_LIMIT_MM_PER_PROMPT = {'image': 255, 'video': 1}",
      "language": "json"
    },
    {
      "code": "DEFAULT_MM_ITEM_BUCKET_CONFIG = {\n    (256, 256, 1): 0.5,\n    (720, 1280, 1): 0.5,\n    (720, 1280, 16): 0.0,\n}",
      "language": "json"
    },
    {
      "code": "DEFAULT_MM_ITEM_BUCKET_CONFIG = {\n    (256, 256, 1): 0.5,\n    (720, 1280, 1): 0.5,\n    (720, 1280, 16): 0.0,\n}",
      "language": "json"
    },
    {
      "code": "DEFAULT_NUM_MM_ITEMS_RANGE_RATIO = 0.0",
      "language": "unknown"
    },
    {
      "code": "DEFAULT_NUM_MM_ITEMS_RANGE_RATIO = 0.0",
      "language": "unknown"
    },
    {
      "code": "IS_MULTIMODAL = True",
      "language": "unknown"
    },
    {
      "code": "IS_MULTIMODAL = True",
      "language": "unknown"
    },
    {
      "code": "__init__(**kwargs) -> None",
      "language": "python"
    },
    {
      "code": "__init__(**kwargs) -> None",
      "language": "python"
    },
    {
      "code": "def __init__(self, **kwargs) -> None:\n    super().__init__(**kwargs)",
      "language": "python"
    },
    {
      "code": "def __init__(self, **kwargs) -> None:\n    super().__init__(**kwargs)",
      "language": "python"
    },
    {
      "code": "generate_mm_item(\n    mm_item_config: tuple[int, int, int],\n) -> Mapping[str, Any]",
      "language": "php"
    },
    {
      "code": "generate_mm_item(\n    mm_item_config: tuple[int, int, int],\n) -> Mapping[str, Any]",
      "language": "php"
    },
    {
      "code": "923\n924\n925\n926\n927\n928\n929\n930\n931\n932\n933\n934\n935\n936\n937\n938\n939\n940\n941\n942\n943\n944\n945",
      "language": "unknown"
    },
    {
      "code": "def generate_mm_item(\n    self,\n    mm_item_config: tuple[int, int, int],\n) -> Mapping[str, Any]:\n    \"\"\"\n    Create synthetic images and videos and\n    apply process_image/process_video respectively.\n    This follows the OpenAI API chat completions\n    https://github.com/openai/openai-python\n    \"\"\"\n\n    if self.map_config_to_modality(mm_item_config) == \"image\":\n        return process_image(\n            self.generate_synthetic_image(mm_item_config[1], mm_item_config[0])\n        )\n    elif self.map_config_to_modality(mm_item_config) == \"video\":\n        return process_video(\n            self.generate_synthetic_video(\n                mm_item_config[1], mm_item_config[0], mm_item_config[2]\n            )\n        )\n    else:\n        raise ValueError(f\"Invalid multimodal item configuration: {mm_item_config}\")",
      "language": "python"
    },
    {
      "code": "def generate_mm_item(\n    self,\n    mm_item_config: tuple[int, int, int],\n) -> Mapping[str, Any]:\n    \"\"\"\n    Create synthetic images and videos and\n    apply process_image/process_video respectively.\n    This follows the OpenAI API chat completions\n    https://github.com/openai/openai-python\n    \"\"\"\n\n    if self.map_config_to_modality(mm_item_config) == \"image\":\n        return process_image(\n            self.generate_synthetic_image(mm_item_config[1], mm_item_config[0])\n        )\n    elif self.map_config_to_modality(mm_item_config) == \"video\":\n        return process_video(\n            self.generate_synthetic_video(\n                mm_item_config[1], mm_item_config[0], mm_item_config[2]\n            )\n        )\n    else:\n        raise ValueError(f\"Invalid multimodal item configuration: {mm_item_config}\")",
      "language": "python"
    },
    {
      "code": "generate_synthetic_image(width: int, height: int) -> Image",
      "language": "php"
    },
    {
      "code": "generate_synthetic_image(width: int, height: int) -> Image",
      "language": "php"
    },
    {
      "code": "833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847",
      "language": "unknown"
    },
    {
      "code": "def generate_synthetic_image(self, width: int, height: int) -> Image.Image:\n    \"\"\"Generate synthetic PIL image with random RGB values.\n\n    NOTE: iid pixel sampling results in worst-case compression\n    (good for stressing I/O), but very unlike real photos.\n    We could consider a “low-freq” mode (e.g., noise blur)\n    to emulate network realism instead of max stress.\n    \"\"\"\n    random_pixels = self._rng.integers(\n        0,\n        256,\n        (height, width, 3),\n        dtype=np.uint8,\n    )\n    return Image.fromarray(random_pixels)",
      "language": "python"
    },
    {
      "code": "def generate_synthetic_image(self, width: int, height: int) -> Image.Image:\n    \"\"\"Generate synthetic PIL image with random RGB values.\n\n    NOTE: iid pixel sampling results in worst-case compression\n    (good for stressing I/O), but very unlike real photos.\n    We could consider a “low-freq” mode (e.g., noise blur)\n    to emulate network realism instead of max stress.\n    \"\"\"\n    random_pixels = self._rng.integers(\n        0,\n        256,\n        (height, width, 3),\n        dtype=np.uint8,\n    )\n    return Image.fromarray(random_pixels)",
      "language": "python"
    },
    {
      "code": "generate_synthetic_video(\n    width: int, height: int, num_frames: int\n) -> dict",
      "language": "php"
    },
    {
      "code": "generate_synthetic_video(\n    width: int, height: int, num_frames: int\n) -> dict",
      "language": "php"
    },
    {
      "code": "849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891",
      "language": "unknown"
    },
    {
      "code": "def generate_synthetic_video(\n    self, width: int, height: int, num_frames: int\n) -> dict:\n    \"\"\"Generate synthetic video with random values.\n\n    Creates a video with random pixel values, encodes it to MP4 format,\n    and returns the content as bytes.\n    \"\"\"\n    import cv2\n\n    random_pixels = self._rng.integers(\n        0,\n        256,\n        (num_frames, height, width, 3),\n        dtype=np.uint8,\n    )\n\n    # Create a temporary video file in memory\n    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n    fps = 30  # frames per second\n\n    with NamedTemporaryFile(suffix=\".mp4\", delete_on_close=False) as temp_file:\n        temp_path = temp_file.name\n\n        # Create video writer\n        video_writer = cv2.VideoWriter(\n            temp_path, fourcc=fourcc, fps=fps, frameSize=(width, height)\n        )\n\n        if not video_writer.isOpened():\n            raise RuntimeError(\"Failed to create video writer\")\n\n        for frame in random_pixels:\n            video_writer.write(frame)\n\n        video_writer.release()\n        temp_file.close()\n\n        # Read the video file content\n        with open(temp_path, \"rb\") as f:\n            video_content = f.read()\n\n        return {\"bytes\": video_content}",
      "language": "python"
    },
    {
      "code": "def generate_synthetic_video(\n    self, width: int, height: int, num_frames: int\n) -> dict:\n    \"\"\"Generate synthetic video with random values.\n\n    Creates a video with random pixel values, encodes it to MP4 format,\n    and returns the content as bytes.\n    \"\"\"\n    import cv2\n\n    random_pixels = self._rng.integers(\n        0,\n        256,\n        (num_frames, height, width, 3),\n        dtype=np.uint8,\n    )\n\n    # Create a temporary video file in memory\n    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n    fps = 30  # frames per second\n\n    with NamedTemporaryFile(suffix=\".mp4\", delete_on_close=False) as temp_file:\n        temp_path = temp_file.name\n\n        # Create video writer\n        video_writer = cv2.VideoWriter(\n            temp_path, fourcc=fourcc, fps=fps, frameSize=(width, height)\n        )\n\n        if not video_writer.isOpened():\n            raise RuntimeError(\"Failed to create video writer\")\n\n        for frame in random_pixels:\n            video_writer.write(frame)\n\n        video_writer.release()\n        temp_file.close()\n\n        # Read the video file content\n        with open(temp_path, \"rb\") as f:\n            video_content = f.read()\n\n        return {\"bytes\": video_content}",
      "language": "python"
    },
    {
      "code": "get_mm_item_iterator(\n    min_num_mm_items: int,\n    max_num_mm_items: int,\n    bucket_config: dict[tuple[int, int, int], float],\n    limit_mm_per_prompt: dict[str, int],\n) -> Iterator[tuple[int, int, int]]",
      "language": "php"
    },
    {
      "code": "get_mm_item_iterator(\n    min_num_mm_items: int,\n    max_num_mm_items: int,\n    bucket_config: dict[tuple[int, int, int], float],\n    limit_mm_per_prompt: dict[str, int],\n) -> Iterator[tuple[int, int, int]]",
      "language": "php"
    },
    {
      "code": "1022\n1023\n1024\n1025\n1026\n1027\n1028\n1029\n1030\n1031\n1032\n1033\n1034\n1035\n1036\n1037\n1038\n1039\n1040\n1041\n1042\n1043\n1044\n1045\n1046\n1047\n1048\n1049\n1050\n1051\n1052\n1053\n1054\n1055\n1056\n1057\n1058\n1059\n1060\n1061\n1062\n1063\n1064\n1065\n1066\n1067\n1068\n1069\n1070\n1071\n1072\n1073\n1074\n1075\n1076\n1077\n1078\n1079\n1080\n1081",
      "language": "unknown"
    },
    {
      "code": "def get_mm_item_iterator(\n    self,\n    min_num_mm_items: int,\n    max_num_mm_items: int,\n    bucket_config: dict[tuple[int, int, int], float],\n    limit_mm_per_prompt: dict[str, int],\n) -> Iterator[tuple[int, int, int]]:\n    \"\"\"\n    Iterator over the multimodal items for each request\n    whose size is between min_num_mm_items and max_num_mm_items.\n\n    Loop over the bucket config and sample a multimodal item.\n    Loop until the number of multimodal items sampled is equal to\n    request_num_mm_items or limit of multimodal items per prompt\n    for all modalities is reached.\n\n    Note:\n    - This function operates on a per-request shallow copy of\n      `bucket_config` (tuple->float). The original dict passed to\n      `sample` is not mutated. If this ever changes, a test\n      is implemented and will fail.\n    \"\"\"\n    # Get the number of multimodal items to sample\n    request_num_mm_items = int(\n        self._rng.integers(min_num_mm_items, max_num_mm_items + 1)\n    )\n    # If request_num_mm_items is 0, yield an empty iterator\n    if request_num_mm_items == 0:\n        return\n    # Initialize modality counters\n    modality_counter = {self.map_config_to_modality(k): 0 for k in bucket_config}\n    # Copy the bucket config to avoid modifying the original\n    bucket_config_copy = bucket_config.copy()\n    # Loop over the number of multimodal items to sample\n    while sum(modality_counter.values()) < request_num_mm_items:\n        # Sample a multimodal item config\n        mm_item_config = self._rng.choice(\n            list(bucket_config_copy.keys()), p=list(bucket_config_copy.values())\n        )\n        modality = self.map_config_to_modality(mm_item_config)\n        # Check that modality count is less than limit per prompt\n        if modality_counter[modality] < limit_mm_per_prompt[modality]:\n            modality_counter[modality] += 1\n            yield (mm_item_config)\n        else:\n            # If the counter is greater than the limit per prompt\n            # set all multimodal items of this modality to 0\n            for k, v in bucket_config_copy.items():\n                if self.map_config_to_modality(k) == modality:\n                    bucket_config_copy[k] = 0\n            # If all configs are 0, break the loop\n            # This should not happen as request_num_mm_items is at most\n            # the sum of limit_mm_per_prompt for all modalities\n            if all(v == 0 for v in bucket_config_copy.values()):\n                logger.warning(\n                    \"Exhausted all multimodal items of modality %s\", modality\n                )\n                break\n            # Renormalize the bucket config\n            bucket_config_copy = self.normalize_bucket_config(bucket_config_copy)",
      "language": "python"
    },
    {
      "code": "def get_mm_item_iterator(\n    self,\n    min_num_mm_items: int,\n    max_num_mm_items: int,\n    bucket_config: dict[tuple[int, int, int], float],\n    limit_mm_per_prompt: dict[str, int],\n) -> Iterator[tuple[int, int, int]]:\n    \"\"\"\n    Iterator over the multimodal items for each request\n    whose size is between min_num_mm_items and max_num_mm_items.\n\n    Loop over the bucket config and sample a multimodal item.\n    Loop until the number of multimodal items sampled is equal to\n    request_num_mm_items or limit of multimodal items per prompt\n    for all modalities is reached.\n\n    Note:\n    - This function operates on a per-request shallow copy of\n      `bucket_config` (tuple->float). The original dict passed to\n      `sample` is not mutated. If this ever changes, a test\n      is implemented and will fail.\n    \"\"\"\n    # Get the number of multimodal items to sample\n    request_num_mm_items = int(\n        self._rng.integers(min_num_mm_items, max_num_mm_items + 1)\n    )\n    # If request_num_mm_items is 0, yield an empty iterator\n    if request_num_mm_items == 0:\n        return\n    # Initialize modality counters\n    modality_counter = {self.map_config_to_modality(k): 0 for k in bucket_config}\n    # Copy the bucket config to avoid modifying the original\n    bucket_config_copy = bucket_config.copy()\n    # Loop over the number of multimodal items to sample\n    while sum(modality_counter.values()) < request_num_mm_items:\n        # Sample a multimodal item config\n        mm_item_config = self._rng.choice(\n            list(bucket_config_copy.keys()), p=list(bucket_config_copy.values())\n        )\n        modality = self.map_config_to_modality(mm_item_config)\n        # Check that modality count is less than limit per prompt\n        if modality_counter[modality] < limit_mm_per_prompt[modality]:\n            modality_counter[modality] += 1\n            yield (mm_item_config)\n        else:\n            # If the counter is greater than the limit per prompt\n            # set all multimodal items of this modality to 0\n            for k, v in bucket_config_copy.items():\n                if self.map_config_to_modality(k) == modality:\n                    bucket_config_copy[k] = 0\n            # If all configs are 0, break the loop\n            # This should not happen as request_num_mm_items is at most\n            # the sum of limit_mm_per_prompt for all modalities\n            if all(v == 0 for v in bucket_config_copy.values()):\n                logger.warning(\n                    \"Exhausted all multimodal items of modality %s\", modality\n                )\n                break\n            # Renormalize the bucket config\n            bucket_config_copy = self.normalize_bucket_config(bucket_config_copy)",
      "language": "python"
    },
    {
      "code": "get_mm_item_sampling_params(\n    base_items_per_request: int,\n    num_mm_items_range_ratio: float,\n    limit_mm_per_prompt: dict[str, int],\n    bucket_config: dict[tuple[int, int, int], float],\n) -> tuple[\n    int,\n    int,\n    dict[str, int],\n    dict[tuple[int, int, int], float],\n]",
      "language": "php"
    },
    {
      "code": "get_mm_item_sampling_params(\n    base_items_per_request: int,\n    num_mm_items_range_ratio: float,\n    limit_mm_per_prompt: dict[str, int],\n    bucket_config: dict[tuple[int, int, int], float],\n) -> tuple[\n    int,\n    int,\n    dict[str, int],\n    dict[tuple[int, int, int], float],\n]",
      "language": "php"
    },
    {
      "code": "947\n 948\n 949\n 950\n 951\n 952\n 953\n 954\n 955\n 956\n 957\n 958\n 959\n 960\n 961\n 962\n 963\n 964\n 965\n 966\n 967\n 968\n 969\n 970\n 971\n 972\n 973\n 974\n 975\n 976\n 977\n 978\n 979\n 980\n 981\n 982\n 983\n 984\n 985\n 986\n 987\n 988\n 989\n 990\n 991\n 992\n 993\n 994\n 995\n 996\n 997\n 998\n 999\n1000\n1001\n1002\n1003\n1004\n1005\n1006\n1007\n1008\n1009\n1010\n1011\n1012\n1013\n1014\n1015\n1016\n1017\n1018\n1019\n1020",
      "language": "unknown"
    },
    {
      "code": "def get_mm_item_sampling_params(\n    self,\n    base_items_per_request: int,\n    num_mm_items_range_ratio: float,\n    limit_mm_per_prompt: dict[str, int],\n    bucket_config: dict[tuple[int, int, int], float],\n) -> tuple[int, int, dict[str, int], dict[tuple[int, int, int], float]]:\n    \"\"\"\n    Get the sampling parameters for the multimodal items.\n    \"\"\"\n    # Enforce num_mm_items_range_ratio <= 1\n    if not (0.0 <= num_mm_items_range_ratio <= 1.0):\n        raise ValueError(\"num_mm_items_range_ratio must be in [0, 1].\")\n\n    # Ensure modalities to sample are in limit_mm_per_prompt\n    for k, v in bucket_config.items():\n        # get modality from bucket config\n        modality = self.map_config_to_modality(k)\n        if modality not in limit_mm_per_prompt:\n            raise ValueError(\n                f\"Modality {modality} is not in \"\n                f\"limit_mm_per_prompt: \"\n                f\"{limit_mm_per_prompt.keys()}\"\n            )\n\n    # Remove zero probability entries\n    # and normalize bucket config to sum to 1\n    bucket_config = self.normalize_bucket_config(bucket_config)\n    logger.info(\n        \"Normalized bucket config: %s\",\n        bucket_config,\n    )\n    # Only consider limit per prompt for modalities in bucket config\n    allowed_modalities = {self.map_config_to_modality(cfg) for cfg in bucket_config}\n    limit_mm_per_prompt = {\n        k: v for k, v in limit_mm_per_prompt.items() if k in allowed_modalities\n    }\n    if not limit_mm_per_prompt:\n        raise ValueError(\"No valid limits for modalities present in bucket_config.\")\n\n    logger.info(\n        \"Updated mm-limit-per-prompt: %s\",\n        limit_mm_per_prompt,\n    )\n\n    # Get max and min num mm items and ensure\n    # it is at most the sum of limit_mm_per_prompt for all modalities\n    max_num_mm_items = min(\n        sum(limit_mm_per_prompt.values()),\n        math.ceil(base_items_per_request * (1 + num_mm_items_range_ratio)),\n    )\n    # Ensure min num mm items is at least 0\n    min_num_mm_items = max(\n        0, math.floor(base_items_per_request * (1 - num_mm_items_range_ratio))\n    )\n    # Raise error if min num mm items is greater than max num mm items\n    if min_num_mm_items > max_num_mm_items:\n        raise ValueError(\n            f\"Min num mm items is greater than max mm items: \"\n            f\"{min_num_mm_items} > {max_num_mm_items}\"\n        )\n\n    logger.info(\n        \"Sampling number of multimodal items from [%s, %s]\",\n        min_num_mm_items,\n        max_num_mm_items,\n    )\n\n    return (\n        min_num_mm_items,\n        max_num_mm_items,\n        limit_mm_per_prompt,\n        bucket_config,\n    )",
      "language": "python"
    },
    {
      "code": "def get_mm_item_sampling_params(\n    self,\n    base_items_per_request: int,\n    num_mm_items_range_ratio: float,\n    limit_mm_per_prompt: dict[str, int],\n    bucket_config: dict[tuple[int, int, int], float],\n) -> tuple[int, int, dict[str, int], dict[tuple[int, int, int], float]]:\n    \"\"\"\n    Get the sampling parameters for the multimodal items.\n    \"\"\"\n    # Enforce num_mm_items_range_ratio <= 1\n    if not (0.0 <= num_mm_items_range_ratio <= 1.0):\n        raise ValueError(\"num_mm_items_range_ratio must be in [0, 1].\")\n\n    # Ensure modalities to sample are in limit_mm_per_prompt\n    for k, v in bucket_config.items():\n        # get modality from bucket config\n        modality = self.map_config_to_modality(k)\n        if modality not in limit_mm_per_prompt:\n            raise ValueError(\n                f\"Modality {modality} is not in \"\n                f\"limit_mm_per_prompt: \"\n                f\"{limit_mm_per_prompt.keys()}\"\n            )\n\n    # Remove zero probability entries\n    # and normalize bucket config to sum to 1\n    bucket_config = self.normalize_bucket_config(bucket_config)\n    logger.info(\n        \"Normalized bucket config: %s\",\n        bucket_config,\n    )\n    # Only consider limit per prompt for modalities in bucket config\n    allowed_modalities = {self.map_config_to_modality(cfg) for cfg in bucket_config}\n    limit_mm_per_prompt = {\n        k: v for k, v in limit_mm_per_prompt.items() if k in allowed_modalities\n    }\n    if not limit_mm_per_prompt:\n        raise ValueError(\"No valid limits for modalities present in bucket_config.\")\n\n    logger.info(\n        \"Updated mm-limit-per-prompt: %s\",\n        limit_mm_per_prompt,\n    )\n\n    # Get max and min num mm items and ensure\n    # it is at most the sum of limit_mm_per_prompt for all modalities\n    max_num_mm_items = min(\n        sum(limit_mm_per_prompt.values()),\n        math.ceil(base_items_per_request * (1 + num_mm_items_range_ratio)),\n    )\n    # Ensure min num mm items is at least 0\n    min_num_mm_items = max(\n        0, math.floor(base_items_per_request * (1 - num_mm_items_range_ratio))\n    )\n    # Raise error if min num mm items is greater than max num mm items\n    if min_num_mm_items > max_num_mm_items:\n        raise ValueError(\n            f\"Min num mm items is greater than max mm items: \"\n            f\"{min_num_mm_items} > {max_num_mm_items}\"\n        )\n\n    logger.info(\n        \"Sampling number of multimodal items from [%s, %s]\",\n        min_num_mm_items,\n        max_num_mm_items,\n    )\n\n    return (\n        min_num_mm_items,\n        max_num_mm_items,\n        limit_mm_per_prompt,\n        bucket_config,\n    )",
      "language": "python"
    },
    {
      "code": "map_config_to_modality(config: tuple[int, int, int]) -> str",
      "language": "php"
    },
    {
      "code": "map_config_to_modality(config: tuple[int, int, int]) -> str",
      "language": "php"
    },
    {
      "code": "893\n894\n895\n896\n897\n898\n899\n900",
      "language": "unknown"
    },
    {
      "code": "def map_config_to_modality(self, config: tuple[int, int, int]) -> str:\n    \"\"\"Map the configuration to the modality.\"\"\"\n    if config[-1] == 1:\n        return \"image\"\n    elif config[-1] > 1:\n        return \"video\"\n    else:\n        raise ValueError(f\"Invalid multimodal item configuration: {config}\")",
      "language": "python"
    },
    {
      "code": "def map_config_to_modality(self, config: tuple[int, int, int]) -> str:\n    \"\"\"Map the configuration to the modality.\"\"\"\n    if config[-1] == 1:\n        return \"image\"\n    elif config[-1] > 1:\n        return \"video\"\n    else:\n        raise ValueError(f\"Invalid multimodal item configuration: {config}\")",
      "language": "python"
    },
    {
      "code": "normalize_bucket_config(\n    bucket_config: dict[tuple[int, int, int], float],\n) -> dict[tuple[int, int, int], float]",
      "language": "php"
    },
    {
      "code": "normalize_bucket_config(\n    bucket_config: dict[tuple[int, int, int], float],\n) -> dict[tuple[int, int, int], float]",
      "language": "php"
    },
    {
      "code": "902\n903\n904\n905\n906\n907\n908\n909\n910\n911\n912\n913\n914\n915\n916\n917\n918\n919\n920\n921",
      "language": "unknown"
    },
    {
      "code": "def normalize_bucket_config(\n    self, bucket_config: dict[tuple[int, int, int], float]\n) -> dict[tuple[int, int, int], float]:\n    \"\"\"\n    Remove zero probability entries\n    and normalize the bucket config to sum to 1.\n    \"\"\"\n    # Raise error if value is negative\n    if any(v < 0 for v in bucket_config.values()):\n        raise ValueError(\"Bucket config values must be non-negative.\")\n    # Remove zero probability entries\n    bucket_config = {k: v for k, v in bucket_config.items() if v > 0}\n    # if bucket config is empty, raise error\n    if not bucket_config:\n        raise ValueError(\n            \"Got invalid bucket config. Bucket config values must be non-zero.\"\n        )\n    # Normalize the remaining bucket config to sum to 1\n    total = sum(bucket_config.values())\n    return {k: v / total for k, v in bucket_config.items()}",
      "language": "python"
    },
    {
      "code": "def normalize_bucket_config(\n    self, bucket_config: dict[tuple[int, int, int], float]\n) -> dict[tuple[int, int, int], float]:\n    \"\"\"\n    Remove zero probability entries\n    and normalize the bucket config to sum to 1.\n    \"\"\"\n    # Raise error if value is negative\n    if any(v < 0 for v in bucket_config.values()):\n        raise ValueError(\"Bucket config values must be non-negative.\")\n    # Remove zero probability entries\n    bucket_config = {k: v for k, v in bucket_config.items() if v > 0}\n    # if bucket config is empty, raise error\n    if not bucket_config:\n        raise ValueError(\n            \"Got invalid bucket config. Bucket config values must be non-zero.\"\n        )\n    # Normalize the remaining bucket config to sum to 1\n    total = sum(bucket_config.values())\n    return {k: v / total for k, v in bucket_config.items()}",
      "language": "python"
    },
    {
      "code": "sample(\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    prefix_len: int = DEFAULT_PREFIX_LEN,\n    range_ratio: float = DEFAULT_RANGE_RATIO,\n    input_len: int = DEFAULT_INPUT_LEN,\n    output_len: int = DEFAULT_OUTPUT_LEN,\n    limit_mm_per_prompt: dict[\n        str, int\n    ] = DEFAULT_LIMIT_MM_PER_PROMPT,\n    base_items_per_request: int = DEFAULT_BASE_ITEMS_PER_REQUEST,\n    num_mm_items_range_ratio: float = DEFAULT_NUM_MM_ITEMS_RANGE_RATIO,\n    bucket_config: dict[\n        tuple[int, int, int], float\n    ] = DEFAULT_MM_ITEM_BUCKET_CONFIG,\n    enable_multimodal_chat: bool = DEFAULT_ENABLE_MULTIMODAL_CHAT,\n    **kwargs,\n) -> list[SampleRequest]",
      "language": "typescript"
    },
    {
      "code": "sample(\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    prefix_len: int = DEFAULT_PREFIX_LEN,\n    range_ratio: float = DEFAULT_RANGE_RATIO,\n    input_len: int = DEFAULT_INPUT_LEN,\n    output_len: int = DEFAULT_OUTPUT_LEN,\n    limit_mm_per_prompt: dict[\n        str, int\n    ] = DEFAULT_LIMIT_MM_PER_PROMPT,\n    base_items_per_request: int = DEFAULT_BASE_ITEMS_PER_REQUEST,\n    num_mm_items_range_ratio: float = DEFAULT_NUM_MM_ITEMS_RANGE_RATIO,\n    bucket_config: dict[\n        tuple[int, int, int], float\n    ] = DEFAULT_MM_ITEM_BUCKET_CONFIG,\n    enable_multimodal_chat: bool = DEFAULT_ENABLE_MULTIMODAL_CHAT,\n    **kwargs,\n) -> list[SampleRequest]",
      "language": "typescript"
    },
    {
      "code": "1083\n1084\n1085\n1086\n1087\n1088\n1089\n1090\n1091\n1092\n1093\n1094\n1095\n1096\n1097\n1098\n1099\n1100\n1101\n1102\n1103\n1104\n1105\n1106\n1107\n1108\n1109\n1110\n1111\n1112\n1113\n1114\n1115\n1116\n1117\n1118\n1119\n1120\n1121\n1122\n1123\n1124\n1125\n1126\n1127\n1128\n1129\n1130\n1131\n1132\n1133\n1134\n1135\n1136\n1137\n1138\n1139\n1140\n1141\n1142\n1143\n1144\n1145\n1146\n1147\n1148\n1149\n1150\n1151\n1152\n1153\n1154\n1155\n1156\n1157\n1158\n1159\n1160\n1161\n1162\n1163\n1164\n1165\n1166\n1167\n1168\n1169\n1170\n1171\n1172\n1173\n1174\n1175\n1176\n1177\n1178\n1179\n1180\n1181\n1182\n1183\n1184\n1185\n1186\n1187\n1188\n1189\n1190\n1191\n1192\n1193\n1194\n1195\n1196\n1197\n1198\n1199\n1200\n1201\n1202\n1203",
      "language": "unknown"
    },
    {
      "code": "def sample(\n    self,\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    prefix_len: int = RandomDataset.DEFAULT_PREFIX_LEN,\n    range_ratio: float = RandomDataset.DEFAULT_RANGE_RATIO,\n    input_len: int = RandomDataset.DEFAULT_INPUT_LEN,\n    output_len: int = RandomDataset.DEFAULT_OUTPUT_LEN,\n    limit_mm_per_prompt: dict[str, int] = DEFAULT_LIMIT_MM_PER_PROMPT,\n    base_items_per_request: int = DEFAULT_BASE_ITEMS_PER_REQUEST,\n    num_mm_items_range_ratio: float = DEFAULT_NUM_MM_ITEMS_RANGE_RATIO,\n    bucket_config: dict[\n        tuple[int, int, int], float\n    ] = DEFAULT_MM_ITEM_BUCKET_CONFIG,\n    enable_multimodal_chat: bool = DEFAULT_ENABLE_MULTIMODAL_CHAT,\n    **kwargs,\n) -> list[SampleRequest]:\n    # Get the sampling parameters for the dataset\n    input_lens, output_lens, offsets = self.get_sampling_params(\n        num_requests, range_ratio, input_len, output_len, tokenizer\n    )\n\n    (\n        min_num_mm_items,\n        max_num_mm_items,\n        limit_mm_per_prompt,\n        bucket_config,\n    ) = self.get_mm_item_sampling_params(\n        base_items_per_request,\n        num_mm_items_range_ratio,\n        limit_mm_per_prompt,\n        bucket_config,\n    )\n\n    vocab_size = tokenizer.vocab_size\n    # Can't use tokenizer.all_special_ids since\n    # it returns ONLY ids from special_tokens_map.json\n    # We want to exclude placeholder tokens and all\n    # tokens that indicate start/end of image as it\n    # may break prompt replacement logic.\n    prohibited_tokens = list(\n        tok_id\n        for tok_id, token in tokenizer.added_tokens_decoder.items()\n        if token.special\n    )\n    all_tokens = np.arange(vocab_size)\n    allowed_tokens = np.array(list(set(all_tokens) - set(prohibited_tokens)))\n    logger.debug(\n        \"Sampling from %d out of %d (vocab size)\", len(allowed_tokens), vocab_size\n    )\n    # Generate prefix once\n    prefix_token_ids = self.get_prefix(allowed_tokens, prefix_len)\n    # Add synthetic multimodal items to each request\n    mm_requests = []\n    token_mismatch_total = 0\n    for i in range(num_requests):\n        prompt, total_input_len, token_mismatch = self.generate_token_sequence(  # noqa: E501\n            tokenizer=tokenizer,\n            prefix_token_ids=prefix_token_ids,\n            prefix_len=prefix_len,\n            vocab_size=vocab_size,\n            input_len=int(input_lens[i]),\n            offset=int(offsets[i]),\n            index=i,\n            allowed_tokens=allowed_tokens,\n        )\n        token_mismatch_total += token_mismatch\n        # Get multimodal item iterator for a given request\n        mm_item_iterator = self.get_mm_item_iterator(\n            min_num_mm_items,\n            max_num_mm_items,\n            bucket_config,\n            limit_mm_per_prompt,\n        )\n\n        mm_content = cast(\n            list[dict[str, Any]],\n            [\n                self.generate_mm_item(mm_item_config)\n                for mm_item_config in mm_item_iterator\n            ],\n        )\n\n        if enable_multimodal_chat:\n            # NOTE: For now this option is only provided for completeness\n            # given that the serve.py benchmark currently does not use it.\n            mm_chat_prompt: Any = prompt\n            mm_chat_prompt = self.apply_multimodal_chat_transformation(\n                prompt, mm_content\n            )\n            sample_request = SampleRequest(\n                prompt=mm_chat_prompt,\n                prompt_len=total_input_len,\n                expected_output_len=int(output_lens[i]),\n                multi_modal_data=None,\n                request_id=request_id_prefix + str(i),\n            )\n        else:\n            sample_request = SampleRequest(\n                prompt=prompt,\n                prompt_len=total_input_len,\n                expected_output_len=int(output_lens[i]),\n                multi_modal_data=mm_content,\n                request_id=request_id_prefix + str(i),\n            )\n        mm_requests.append(sample_request)\n\n    if token_mismatch_total != 0:\n        sign = \"more\" if token_mismatch_total > 0 else \"fewer\"\n        logger.warning(\n            \"Across all generated prompts, there were %d %s tokens \"\n            \"than expected after decoding and re-encoding. This is \"\n            \"expected due to the imperfect nature of the sampling \"\n            \"procedure.\",\n            abs(token_mismatch_total),\n            sign,\n        )\n\n    return mm_requests",
      "language": "python"
    },
    {
      "code": "def sample(\n    self,\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    prefix_len: int = RandomDataset.DEFAULT_PREFIX_LEN,\n    range_ratio: float = RandomDataset.DEFAULT_RANGE_RATIO,\n    input_len: int = RandomDataset.DEFAULT_INPUT_LEN,\n    output_len: int = RandomDataset.DEFAULT_OUTPUT_LEN,\n    limit_mm_per_prompt: dict[str, int] = DEFAULT_LIMIT_MM_PER_PROMPT,\n    base_items_per_request: int = DEFAULT_BASE_ITEMS_PER_REQUEST,\n    num_mm_items_range_ratio: float = DEFAULT_NUM_MM_ITEMS_RANGE_RATIO,\n    bucket_config: dict[\n        tuple[int, int, int], float\n    ] = DEFAULT_MM_ITEM_BUCKET_CONFIG,\n    enable_multimodal_chat: bool = DEFAULT_ENABLE_MULTIMODAL_CHAT,\n    **kwargs,\n) -> list[SampleRequest]:\n    # Get the sampling parameters for the dataset\n    input_lens, output_lens, offsets = self.get_sampling_params(\n        num_requests, range_ratio, input_len, output_len, tokenizer\n    )\n\n    (\n        min_num_mm_items,\n        max_num_mm_items,\n        limit_mm_per_prompt,\n        bucket_config,\n    ) = self.get_mm_item_sampling_params(\n        base_items_per_request,\n        num_mm_items_range_ratio,\n        limit_mm_per_prompt,\n        bucket_config,\n    )\n\n    vocab_size = tokenizer.vocab_size\n    # Can't use tokenizer.all_special_ids since\n    # it returns ONLY ids from special_tokens_map.json\n    # We want to exclude placeholder tokens and all\n    # tokens that indicate start/end of image as it\n    # may break prompt replacement logic.\n    prohibited_tokens = list(\n        tok_id\n        for tok_id, token in tokenizer.added_tokens_decoder.items()\n        if token.special\n    )\n    all_tokens = np.arange(vocab_size)\n    allowed_tokens = np.array(list(set(all_tokens) - set(prohibited_tokens)))\n    logger.debug(\n        \"Sampling from %d out of %d (vocab size)\", len(allowed_tokens), vocab_size\n    )\n    # Generate prefix once\n    prefix_token_ids = self.get_prefix(allowed_tokens, prefix_len)\n    # Add synthetic multimodal items to each request\n    mm_requests = []\n    token_mismatch_total = 0\n    for i in range(num_requests):\n        prompt, total_input_len, token_mismatch = self.generate_token_sequence(  # noqa: E501\n            tokenizer=tokenizer,\n            prefix_token_ids=prefix_token_ids,\n            prefix_len=prefix_len,\n            vocab_size=vocab_size,\n            input_len=int(input_lens[i]),\n            offset=int(offsets[i]),\n            index=i,\n            allowed_tokens=allowed_tokens,\n        )\n        token_mismatch_total += token_mismatch\n        # Get multimodal item iterator for a given request\n        mm_item_iterator = self.get_mm_item_iterator(\n            min_num_mm_items,\n            max_num_mm_items,\n            bucket_config,\n            limit_mm_per_prompt,\n        )\n\n        mm_content = cast(\n            list[dict[str, Any]],\n            [\n                self.generate_mm_item(mm_item_config)\n                for mm_item_config in mm_item_iterator\n            ],\n        )\n\n        if enable_multimodal_chat:\n            # NOTE: For now this option is only provided for completeness\n            # given that the serve.py benchmark currently does not use it.\n            mm_chat_prompt: Any = prompt\n            mm_chat_prompt = self.apply_multimodal_chat_transformation(\n                prompt, mm_content\n            )\n            sample_request = SampleRequest(\n                prompt=mm_chat_prompt,\n                prompt_len=total_input_len,\n                expected_output_len=int(output_lens[i]),\n                multi_modal_data=None,\n                request_id=request_id_prefix + str(i),\n            )\n        else:\n            sample_request = SampleRequest(\n                prompt=prompt,\n                prompt_len=total_input_len,\n                expected_output_len=int(output_lens[i]),\n                multi_modal_data=mm_content,\n                request_id=request_id_prefix + str(i),\n            )\n        mm_requests.append(sample_request)\n\n    if token_mismatch_total != 0:\n        sign = \"more\" if token_mismatch_total > 0 else \"fewer\"\n        logger.warning(\n            \"Across all generated prompts, there were %d %s tokens \"\n            \"than expected after decoding and re-encoding. This is \"\n            \"expected due to the imperfect nature of the sampling \"\n            \"procedure.\",\n            abs(token_mismatch_total),\n            sign,\n        )\n\n    return mm_requests",
      "language": "python"
    },
    {
      "code": "72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83",
      "language": "unknown"
    },
    {
      "code": "@dataclass\nclass SampleRequest:\n    \"\"\"\n    Represents a single inference request for benchmarking.\n    \"\"\"\n\n    prompt: str | list[str]\n    prompt_len: int\n    expected_output_len: int\n    multi_modal_data: MultiModalDataDict | dict | list[dict] | None = None\n    lora_request: LoRARequest | None = None\n    request_id: str | None = None",
      "language": "python"
    },
    {
      "code": "@dataclass\nclass SampleRequest:\n    \"\"\"\n    Represents a single inference request for benchmarking.\n    \"\"\"\n\n    prompt: str | list[str]\n    prompt_len: int\n    expected_output_len: int\n    multi_modal_data: MultiModalDataDict | dict | list[dict] | None = None\n    lora_request: LoRARequest | None = None\n    request_id: str | None = None",
      "language": "python"
    },
    {
      "code": "expected_output_len: int",
      "language": "yaml"
    },
    {
      "code": "expected_output_len: int",
      "language": "yaml"
    },
    {
      "code": "lora_request: LoRARequest | None = None",
      "language": "yaml"
    },
    {
      "code": "lora_request: LoRARequest | None = None",
      "language": "yaml"
    },
    {
      "code": "multi_modal_data: (\n    MultiModalDataDict | dict | list[dict] | None\n) = None",
      "language": "yaml"
    },
    {
      "code": "multi_modal_data: (\n    MultiModalDataDict | dict | list[dict] | None\n) = None",
      "language": "yaml"
    },
    {
      "code": "prompt: str | list[str]",
      "language": "yaml"
    },
    {
      "code": "prompt: str | list[str]",
      "language": "yaml"
    },
    {
      "code": "prompt_len: int",
      "language": "yaml"
    },
    {
      "code": "prompt_len: int",
      "language": "yaml"
    },
    {
      "code": "request_id: str | None = None",
      "language": "yaml"
    },
    {
      "code": "request_id: str | None = None",
      "language": "yaml"
    },
    {
      "code": "__init__(\n    prompt: str | list[str],\n    prompt_len: int,\n    expected_output_len: int,\n    multi_modal_data: MultiModalDataDict\n    | dict\n    | list[dict]\n    | None = None,\n    lora_request: LoRARequest | None = None,\n    request_id: str | None = None,\n) -> None",
      "language": "python"
    },
    {
      "code": "__init__(\n    prompt: str | list[str],\n    prompt_len: int,\n    expected_output_len: int,\n    multi_modal_data: MultiModalDataDict\n    | dict\n    | list[dict]\n    | None = None,\n    lora_request: LoRARequest | None = None,\n    request_id: str | None = None,\n) -> None",
      "language": "python"
    },
    {
      "code": "1211\n1212\n1213\n1214\n1215\n1216\n1217\n1218\n1219\n1220\n1221\n1222\n1223\n1224\n1225\n1226\n1227\n1228\n1229\n1230\n1231\n1232\n1233\n1234\n1235\n1236\n1237\n1238\n1239\n1240\n1241\n1242\n1243\n1244\n1245\n1246\n1247\n1248\n1249\n1250\n1251\n1252\n1253\n1254\n1255\n1256\n1257\n1258\n1259\n1260\n1261\n1262\n1263\n1264\n1265\n1266\n1267\n1268\n1269\n1270\n1271\n1272\n1273\n1274\n1275\n1276\n1277\n1278\n1279\n1280\n1281\n1282\n1283\n1284\n1285\n1286\n1287\n1288\n1289\n1290\n1291\n1292\n1293\n1294",
      "language": "unknown"
    },
    {
      "code": "class ShareGPTDataset(BenchmarkDataset):\n    \"\"\"\n    Implements the ShareGPT dataset.  Loads data from a JSON file and generates\n    sample requests based on conversation turns.\n    \"\"\"\n\n    def __init__(self, **kwargs) -> None:\n        super().__init__(**kwargs)\n        self.load_data()\n\n    def load_data(self) -> None:\n        if self.dataset_path is None:\n            raise ValueError(\"dataset_path must be provided for loading data.\")\n\n        with open(self.dataset_path, encoding=\"utf-8\") as f:\n            self.data = json.load(f)\n        # Filter entries with at least two conversation turns.\n        self.data = [\n            entry\n            for entry in self.data\n            if \"conversations\" in entry and len(entry[\"conversations\"]) >= 2\n        ]\n        random.seed(self.random_seed)\n        if not getattr(self, \"disable_shuffle\", False):\n            random.shuffle(self.data)\n\n    def sample(\n        self,\n        tokenizer: TokenizerLike,\n        num_requests: int,\n        lora_path: str | None = None,\n        max_loras: int | None = None,\n        output_len: int | None = None,\n        enable_multimodal_chat: bool = False,\n        request_id_prefix: str = \"\",\n        no_oversample: bool = False,\n        **kwargs,\n    ) -> list:\n        samples: list = []\n        ind = 0\n        for entry in self.data:\n            if len(samples) >= num_requests:\n                break\n            prompt, completion = (\n                entry[\"conversations\"][0][\"value\"],\n                entry[\"conversations\"][1][\"value\"],\n            )\n\n            lora_request = self.get_random_lora_request(\n                max_loras=max_loras, lora_path=lora_path\n            )\n            prompt_ids = tokenizer(prompt).input_ids\n            completion_ids = tokenizer(completion).input_ids\n            prompt_len = len(prompt_ids)\n            new_output_len = len(completion_ids) if output_len is None else output_len\n            if not is_valid_sequence(\n                prompt_len,\n                new_output_len,\n                skip_min_output_len_check=output_len is not None,\n            ):\n                continue\n            if image_path := entry.get(\"image\"):\n                mm_content = process_image(image_path)\n            elif video_path := entry.get(\"video\"):\n                mm_content = process_video(video_path)\n            else:\n                mm_content = None\n            if enable_multimodal_chat:\n                prompt = self.apply_multimodal_chat_transformation(prompt, mm_content)\n            samples.append(\n                SampleRequest(\n                    prompt=prompt,\n                    prompt_len=prompt_len,\n                    expected_output_len=new_output_len,\n                    lora_request=lora_request,\n                    multi_modal_data=mm_content,\n                    request_id=request_id_prefix + str(ind),\n                )\n            )\n            ind += 1\n        self.maybe_oversample_requests(\n            samples, num_requests, request_id_prefix, no_oversample\n        )\n        return samples",
      "language": "python"
    },
    {
      "code": "class ShareGPTDataset(BenchmarkDataset):\n    \"\"\"\n    Implements the ShareGPT dataset.  Loads data from a JSON file and generates\n    sample requests based on conversation turns.\n    \"\"\"\n\n    def __init__(self, **kwargs) -> None:\n        super().__init__(**kwargs)\n        self.load_data()\n\n    def load_data(self) -> None:\n        if self.dataset_path is None:\n            raise ValueError(\"dataset_path must be provided for loading data.\")\n\n        with open(self.dataset_path, encoding=\"utf-8\") as f:\n            self.data = json.load(f)\n        # Filter entries with at least two conversation turns.\n        self.data = [\n            entry\n            for entry in self.data\n            if \"conversations\" in entry and len(entry[\"conversations\"]) >= 2\n        ]\n        random.seed(self.random_seed)\n        if not getattr(self, \"disable_shuffle\", False):\n            random.shuffle(self.data)\n\n    def sample(\n        self,\n        tokenizer: TokenizerLike,\n        num_requests: int,\n        lora_path: str | None = None,\n        max_loras: int | None = None,\n        output_len: int | None = None,\n        enable_multimodal_chat: bool = False,\n        request_id_prefix: str = \"\",\n        no_oversample: bool = False,\n        **kwargs,\n    ) -> list:\n        samples: list = []\n        ind = 0\n        for entry in self.data:\n            if len(samples) >= num_requests:\n                break\n            prompt, completion = (\n                entry[\"conversations\"][0][\"value\"],\n                entry[\"conversations\"][1][\"value\"],\n            )\n\n            lora_request = self.get_random_lora_request(\n                max_loras=max_loras, lora_path=lora_path\n            )\n            prompt_ids = tokenizer(prompt).input_ids\n            completion_ids = tokenizer(completion).input_ids\n            prompt_len = len(prompt_ids)\n            new_output_len = len(completion_ids) if output_len is None else output_len\n            if not is_valid_sequence(\n                prompt_len,\n                new_output_len,\n                skip_min_output_len_check=output_len is not None,\n            ):\n                continue\n            if image_path := entry.get(\"image\"):\n                mm_content = process_image(image_path)\n            elif video_path := entry.get(\"video\"):\n                mm_content = process_video(video_path)\n            else:\n                mm_content = None\n            if enable_multimodal_chat:\n                prompt = self.apply_multimodal_chat_transformation(prompt, mm_content)\n            samples.append(\n                SampleRequest(\n                    prompt=prompt,\n                    prompt_len=prompt_len,\n                    expected_output_len=new_output_len,\n                    lora_request=lora_request,\n                    multi_modal_data=mm_content,\n                    request_id=request_id_prefix + str(ind),\n                )\n            )\n            ind += 1\n        self.maybe_oversample_requests(\n            samples, num_requests, request_id_prefix, no_oversample\n        )\n        return samples",
      "language": "python"
    },
    {
      "code": "__init__(**kwargs) -> None",
      "language": "python"
    },
    {
      "code": "__init__(**kwargs) -> None",
      "language": "python"
    },
    {
      "code": "1217\n1218\n1219",
      "language": "unknown"
    },
    {
      "code": "def __init__(self, **kwargs) -> None:\n    super().__init__(**kwargs)\n    self.load_data()",
      "language": "python"
    },
    {
      "code": "def __init__(self, **kwargs) -> None:\n    super().__init__(**kwargs)\n    self.load_data()",
      "language": "python"
    },
    {
      "code": "load_data() -> None",
      "language": "rust"
    },
    {
      "code": "load_data() -> None",
      "language": "rust"
    },
    {
      "code": "1221\n1222\n1223\n1224\n1225\n1226\n1227\n1228\n1229\n1230\n1231\n1232\n1233\n1234\n1235",
      "language": "unknown"
    },
    {
      "code": "def load_data(self) -> None:\n    if self.dataset_path is None:\n        raise ValueError(\"dataset_path must be provided for loading data.\")\n\n    with open(self.dataset_path, encoding=\"utf-8\") as f:\n        self.data = json.load(f)\n    # Filter entries with at least two conversation turns.\n    self.data = [\n        entry\n        for entry in self.data\n        if \"conversations\" in entry and len(entry[\"conversations\"]) >= 2\n    ]\n    random.seed(self.random_seed)\n    if not getattr(self, \"disable_shuffle\", False):\n        random.shuffle(self.data)",
      "language": "python"
    },
    {
      "code": "def load_data(self) -> None:\n    if self.dataset_path is None:\n        raise ValueError(\"dataset_path must be provided for loading data.\")\n\n    with open(self.dataset_path, encoding=\"utf-8\") as f:\n        self.data = json.load(f)\n    # Filter entries with at least two conversation turns.\n    self.data = [\n        entry\n        for entry in self.data\n        if \"conversations\" in entry and len(entry[\"conversations\"]) >= 2\n    ]\n    random.seed(self.random_seed)\n    if not getattr(self, \"disable_shuffle\", False):\n        random.shuffle(self.data)",
      "language": "python"
    },
    {
      "code": "sample(\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    lora_path: str | None = None,\n    max_loras: int | None = None,\n    output_len: int | None = None,\n    enable_multimodal_chat: bool = False,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list",
      "language": "typescript"
    },
    {
      "code": "sample(\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    lora_path: str | None = None,\n    max_loras: int | None = None,\n    output_len: int | None = None,\n    enable_multimodal_chat: bool = False,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list",
      "language": "typescript"
    },
    {
      "code": "1237\n1238\n1239\n1240\n1241\n1242\n1243\n1244\n1245\n1246\n1247\n1248\n1249\n1250\n1251\n1252\n1253\n1254\n1255\n1256\n1257\n1258\n1259\n1260\n1261\n1262\n1263\n1264\n1265\n1266\n1267\n1268\n1269\n1270\n1271\n1272\n1273\n1274\n1275\n1276\n1277\n1278\n1279\n1280\n1281\n1282\n1283\n1284\n1285\n1286\n1287\n1288\n1289\n1290\n1291\n1292\n1293\n1294",
      "language": "unknown"
    },
    {
      "code": "def sample(\n    self,\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    lora_path: str | None = None,\n    max_loras: int | None = None,\n    output_len: int | None = None,\n    enable_multimodal_chat: bool = False,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list:\n    samples: list = []\n    ind = 0\n    for entry in self.data:\n        if len(samples) >= num_requests:\n            break\n        prompt, completion = (\n            entry[\"conversations\"][0][\"value\"],\n            entry[\"conversations\"][1][\"value\"],\n        )\n\n        lora_request = self.get_random_lora_request(\n            max_loras=max_loras, lora_path=lora_path\n        )\n        prompt_ids = tokenizer(prompt).input_ids\n        completion_ids = tokenizer(completion).input_ids\n        prompt_len = len(prompt_ids)\n        new_output_len = len(completion_ids) if output_len is None else output_len\n        if not is_valid_sequence(\n            prompt_len,\n            new_output_len,\n            skip_min_output_len_check=output_len is not None,\n        ):\n            continue\n        if image_path := entry.get(\"image\"):\n            mm_content = process_image(image_path)\n        elif video_path := entry.get(\"video\"):\n            mm_content = process_video(video_path)\n        else:\n            mm_content = None\n        if enable_multimodal_chat:\n            prompt = self.apply_multimodal_chat_transformation(prompt, mm_content)\n        samples.append(\n            SampleRequest(\n                prompt=prompt,\n                prompt_len=prompt_len,\n                expected_output_len=new_output_len,\n                lora_request=lora_request,\n                multi_modal_data=mm_content,\n                request_id=request_id_prefix + str(ind),\n            )\n        )\n        ind += 1\n    self.maybe_oversample_requests(\n        samples, num_requests, request_id_prefix, no_oversample\n    )\n    return samples",
      "language": "python"
    },
    {
      "code": "def sample(\n    self,\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    lora_path: str | None = None,\n    max_loras: int | None = None,\n    output_len: int | None = None,\n    enable_multimodal_chat: bool = False,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list:\n    samples: list = []\n    ind = 0\n    for entry in self.data:\n        if len(samples) >= num_requests:\n            break\n        prompt, completion = (\n            entry[\"conversations\"][0][\"value\"],\n            entry[\"conversations\"][1][\"value\"],\n        )\n\n        lora_request = self.get_random_lora_request(\n            max_loras=max_loras, lora_path=lora_path\n        )\n        prompt_ids = tokenizer(prompt).input_ids\n        completion_ids = tokenizer(completion).input_ids\n        prompt_len = len(prompt_ids)\n        new_output_len = len(completion_ids) if output_len is None else output_len\n        if not is_valid_sequence(\n            prompt_len,\n            new_output_len,\n            skip_min_output_len_check=output_len is not None,\n        ):\n            continue\n        if image_path := entry.get(\"image\"):\n            mm_content = process_image(image_path)\n        elif video_path := entry.get(\"video\"):\n            mm_content = process_video(video_path)\n        else:\n            mm_content = None\n        if enable_multimodal_chat:\n            prompt = self.apply_multimodal_chat_transformation(prompt, mm_content)\n        samples.append(\n            SampleRequest(\n                prompt=prompt,\n                prompt_len=prompt_len,\n                expected_output_len=new_output_len,\n                lora_request=lora_request,\n                multi_modal_data=mm_content,\n                request_id=request_id_prefix + str(ind),\n            )\n        )\n        ind += 1\n    self.maybe_oversample_requests(\n        samples, num_requests, request_id_prefix, no_oversample\n    )\n    return samples",
      "language": "python"
    },
    {
      "code": "2079\n2080\n2081\n2082\n2083\n2084\n2085\n2086\n2087\n2088\n2089\n2090\n2091\n2092\n2093\n2094\n2095\n2096\n2097\n2098\n2099\n2100\n2101\n2102\n2103\n2104\n2105\n2106\n2107\n2108\n2109\n2110\n2111\n2112\n2113\n2114\n2115\n2116\n2117\n2118\n2119\n2120\n2121\n2122\n2123\n2124\n2125\n2126\n2127\n2128\n2129\n2130\n2131\n2132\n2133\n2134\n2135\n2136\n2137\n2138\n2139\n2140\n2141\n2142\n2143\n2144\n2145\n2146\n2147\n2148\n2149\n2150\n2151\n2152\n2153\n2154\n2155\n2156\n2157\n2158\n2159\n2160\n2161\n2162",
      "language": "unknown"
    },
    {
      "code": "@deprecated(\n    \"SonnetDataset is deprecated and will be removed in a future version.\",\n)\nclass SonnetDataset(BenchmarkDataset):\n    \"\"\"\n    Simplified implementation of the Sonnet dataset.  Loads poem lines from a\n    text file and generates sample requests.  Default values here copied from\n    `benchmark_serving.py` for the sonnet dataset.\n    \"\"\"\n\n    DEFAULT_PREFIX_LEN = 200\n    DEFAULT_INPUT_LEN = 550\n    DEFAULT_OUTPUT_LEN = 150\n\n    def __init__(\n        self,\n        **kwargs,\n    ) -> None:\n        super().__init__(**kwargs)\n        self.load_data()\n\n    def load_data(self) -> None:\n        if not self.dataset_path:\n            raise ValueError(\"dataset_path must be provided.\")\n        with open(self.dataset_path, encoding=\"utf-8\") as f:\n            self.data = f.readlines()\n\n    def sample(\n        self,\n        tokenizer: TokenizerLike,\n        num_requests: int,\n        prefix_len: int = DEFAULT_PREFIX_LEN,\n        input_len: int = DEFAULT_INPUT_LEN,\n        output_len: int = DEFAULT_OUTPUT_LEN,\n        return_prompt_formatted: bool = False,\n        request_id_prefix: str = \"\",\n        no_oversample: bool = False,\n        **kwargs,\n    ) -> list:\n        # Calculate average token length for a poem line.\n        tokenized_lines = [tokenizer(line).input_ids for line in self.data]\n        avg_len = sum(len(tokens) for tokens in tokenized_lines) / len(tokenized_lines)\n\n        # Build the base prompt.\n        base_prompt = \"Pick as many lines as you can from these poem lines:\\n\"\n        base_msg = [{\"role\": \"user\", \"content\": base_prompt}]\n        base_fmt = tokenizer.apply_chat_template(\n            base_msg, add_generation_prompt=True, tokenize=False\n        )\n        base_offset = len(tokenizer(base_fmt).input_ids)\n        if input_len <= base_offset:\n            raise ValueError(\n                f\"'input_len' must be higher than the base prompt length \"\n                f\"({base_offset}).\"\n            )\n\n        # Determine how many poem lines to use.\n        num_input_lines = round((input_len - base_offset) / avg_len)\n        num_prefix_lines = max(round((prefix_len - base_offset) / avg_len), 0)\n        prefix_lines = self.data[:num_prefix_lines]\n\n        samples = []\n        ind = 0\n        while len(samples) < num_requests:\n            extra_lines = random.choices(\n                self.data, k=num_input_lines - num_prefix_lines\n            )\n            prompt = f\"{base_prompt}{''.join(prefix_lines + extra_lines)}\"\n            msg = [{\"role\": \"user\", \"content\": prompt}]\n            prompt_formatted = tokenizer.apply_chat_template(\n                msg, add_generation_prompt=True, tokenize=False\n            )\n            prompt_len = len(tokenizer(prompt_formatted).input_ids)\n            if prompt_len <= input_len:\n                samples.append(\n                    SampleRequest(\n                        prompt=prompt_formatted if return_prompt_formatted else prompt,\n                        prompt_len=prompt_len,\n                        expected_output_len=output_len,\n                        request_id=request_id_prefix + str(ind),\n                    )\n                )\n                ind += 1\n        return samples",
      "language": "python"
    },
    {
      "code": "@deprecated(\n    \"SonnetDataset is deprecated and will be removed in a future version.\",\n)\nclass SonnetDataset(BenchmarkDataset):\n    \"\"\"\n    Simplified implementation of the Sonnet dataset.  Loads poem lines from a\n    text file and generates sample requests.  Default values here copied from\n    `benchmark_serving.py` for the sonnet dataset.\n    \"\"\"\n\n    DEFAULT_PREFIX_LEN = 200\n    DEFAULT_INPUT_LEN = 550\n    DEFAULT_OUTPUT_LEN = 150\n\n    def __init__(\n        self,\n        **kwargs,\n    ) -> None:\n        super().__init__(**kwargs)\n        self.load_data()\n\n    def load_data(self) -> None:\n        if not self.dataset_path:\n            raise ValueError(\"dataset_path must be provided.\")\n        with open(self.dataset_path, encoding=\"utf-8\") as f:\n            self.data = f.readlines()\n\n    def sample(\n        self,\n        tokenizer: TokenizerLike,\n        num_requests: int,\n        prefix_len: int = DEFAULT_PREFIX_LEN,\n        input_len: int = DEFAULT_INPUT_LEN,\n        output_len: int = DEFAULT_OUTPUT_LEN,\n        return_prompt_formatted: bool = False,\n        request_id_prefix: str = \"\",\n        no_oversample: bool = False,\n        **kwargs,\n    ) -> list:\n        # Calculate average token length for a poem line.\n        tokenized_lines = [tokenizer(line).input_ids for line in self.data]\n        avg_len = sum(len(tokens) for tokens in tokenized_lines) / len(tokenized_lines)\n\n        # Build the base prompt.\n        base_prompt = \"Pick as many lines as you can from these poem lines:\\n\"\n        base_msg = [{\"role\": \"user\", \"content\": base_prompt}]\n        base_fmt = tokenizer.apply_chat_template(\n            base_msg, add_generation_prompt=True, tokenize=False\n        )\n        base_offset = len(tokenizer(base_fmt).input_ids)\n        if input_len <= base_offset:\n            raise ValueError(\n                f\"'input_len' must be higher than the base prompt length \"\n                f\"({base_offset}).\"\n            )\n\n        # Determine how many poem lines to use.\n        num_input_lines = round((input_len - base_offset) / avg_len)\n        num_prefix_lines = max(round((prefix_len - base_offset) / avg_len), 0)\n        prefix_lines = self.data[:num_prefix_lines]\n\n        samples = []\n        ind = 0\n        while len(samples) < num_requests:\n            extra_lines = random.choices(\n                self.data, k=num_input_lines - num_prefix_lines\n            )\n            prompt = f\"{base_prompt}{''.join(prefix_lines + extra_lines)}\"\n            msg = [{\"role\": \"user\", \"content\": prompt}]\n            prompt_formatted = tokenizer.apply_chat_template(\n                msg, add_generation_prompt=True, tokenize=False\n            )\n            prompt_len = len(tokenizer(prompt_formatted).input_ids)\n            if prompt_len <= input_len:\n                samples.append(\n                    SampleRequest(\n                        prompt=prompt_formatted if return_prompt_formatted else prompt,\n                        prompt_len=prompt_len,\n                        expected_output_len=output_len,\n                        request_id=request_id_prefix + str(ind),\n                    )\n                )\n                ind += 1\n        return samples",
      "language": "python"
    },
    {
      "code": "DEFAULT_INPUT_LEN = 550",
      "language": "unknown"
    },
    {
      "code": "DEFAULT_INPUT_LEN = 550",
      "language": "unknown"
    },
    {
      "code": "DEFAULT_OUTPUT_LEN = 150",
      "language": "unknown"
    },
    {
      "code": "DEFAULT_OUTPUT_LEN = 150",
      "language": "unknown"
    },
    {
      "code": "DEFAULT_PREFIX_LEN = 200",
      "language": "unknown"
    },
    {
      "code": "DEFAULT_PREFIX_LEN = 200",
      "language": "unknown"
    },
    {
      "code": "__init__(**kwargs) -> None",
      "language": "python"
    },
    {
      "code": "__init__(**kwargs) -> None",
      "language": "python"
    },
    {
      "code": "2093\n2094\n2095\n2096\n2097\n2098",
      "language": "unknown"
    },
    {
      "code": "def __init__(\n    self,\n    **kwargs,\n) -> None:\n    super().__init__(**kwargs)\n    self.load_data()",
      "language": "python"
    },
    {
      "code": "def __init__(\n    self,\n    **kwargs,\n) -> None:\n    super().__init__(**kwargs)\n    self.load_data()",
      "language": "python"
    },
    {
      "code": "load_data() -> None",
      "language": "rust"
    },
    {
      "code": "load_data() -> None",
      "language": "rust"
    },
    {
      "code": "2100\n2101\n2102\n2103\n2104",
      "language": "unknown"
    },
    {
      "code": "def load_data(self) -> None:\n    if not self.dataset_path:\n        raise ValueError(\"dataset_path must be provided.\")\n    with open(self.dataset_path, encoding=\"utf-8\") as f:\n        self.data = f.readlines()",
      "language": "python"
    },
    {
      "code": "def load_data(self) -> None:\n    if not self.dataset_path:\n        raise ValueError(\"dataset_path must be provided.\")\n    with open(self.dataset_path, encoding=\"utf-8\") as f:\n        self.data = f.readlines()",
      "language": "python"
    },
    {
      "code": "sample(\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    prefix_len: int = DEFAULT_PREFIX_LEN,\n    input_len: int = DEFAULT_INPUT_LEN,\n    output_len: int = DEFAULT_OUTPUT_LEN,\n    return_prompt_formatted: bool = False,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list",
      "language": "typescript"
    },
    {
      "code": "sample(\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    prefix_len: int = DEFAULT_PREFIX_LEN,\n    input_len: int = DEFAULT_INPUT_LEN,\n    output_len: int = DEFAULT_OUTPUT_LEN,\n    return_prompt_formatted: bool = False,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list",
      "language": "typescript"
    },
    {
      "code": "2106\n2107\n2108\n2109\n2110\n2111\n2112\n2113\n2114\n2115\n2116\n2117\n2118\n2119\n2120\n2121\n2122\n2123\n2124\n2125\n2126\n2127\n2128\n2129\n2130\n2131\n2132\n2133\n2134\n2135\n2136\n2137\n2138\n2139\n2140\n2141\n2142\n2143\n2144\n2145\n2146\n2147\n2148\n2149\n2150\n2151\n2152\n2153\n2154\n2155\n2156\n2157\n2158\n2159\n2160\n2161\n2162",
      "language": "unknown"
    },
    {
      "code": "def sample(\n    self,\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    prefix_len: int = DEFAULT_PREFIX_LEN,\n    input_len: int = DEFAULT_INPUT_LEN,\n    output_len: int = DEFAULT_OUTPUT_LEN,\n    return_prompt_formatted: bool = False,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list:\n    # Calculate average token length for a poem line.\n    tokenized_lines = [tokenizer(line).input_ids for line in self.data]\n    avg_len = sum(len(tokens) for tokens in tokenized_lines) / len(tokenized_lines)\n\n    # Build the base prompt.\n    base_prompt = \"Pick as many lines as you can from these poem lines:\\n\"\n    base_msg = [{\"role\": \"user\", \"content\": base_prompt}]\n    base_fmt = tokenizer.apply_chat_template(\n        base_msg, add_generation_prompt=True, tokenize=False\n    )\n    base_offset = len(tokenizer(base_fmt).input_ids)\n    if input_len <= base_offset:\n        raise ValueError(\n            f\"'input_len' must be higher than the base prompt length \"\n            f\"({base_offset}).\"\n        )\n\n    # Determine how many poem lines to use.\n    num_input_lines = round((input_len - base_offset) / avg_len)\n    num_prefix_lines = max(round((prefix_len - base_offset) / avg_len), 0)\n    prefix_lines = self.data[:num_prefix_lines]\n\n    samples = []\n    ind = 0\n    while len(samples) < num_requests:\n        extra_lines = random.choices(\n            self.data, k=num_input_lines - num_prefix_lines\n        )\n        prompt = f\"{base_prompt}{''.join(prefix_lines + extra_lines)}\"\n        msg = [{\"role\": \"user\", \"content\": prompt}]\n        prompt_formatted = tokenizer.apply_chat_template(\n            msg, add_generation_prompt=True, tokenize=False\n        )\n        prompt_len = len(tokenizer(prompt_formatted).input_ids)\n        if prompt_len <= input_len:\n            samples.append(\n                SampleRequest(\n                    prompt=prompt_formatted if return_prompt_formatted else prompt,\n                    prompt_len=prompt_len,\n                    expected_output_len=output_len,\n                    request_id=request_id_prefix + str(ind),\n                )\n            )\n            ind += 1\n    return samples",
      "language": "python"
    },
    {
      "code": "def sample(\n    self,\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    prefix_len: int = DEFAULT_PREFIX_LEN,\n    input_len: int = DEFAULT_INPUT_LEN,\n    output_len: int = DEFAULT_OUTPUT_LEN,\n    return_prompt_formatted: bool = False,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list:\n    # Calculate average token length for a poem line.\n    tokenized_lines = [tokenizer(line).input_ids for line in self.data]\n    avg_len = sum(len(tokens) for tokens in tokenized_lines) / len(tokenized_lines)\n\n    # Build the base prompt.\n    base_prompt = \"Pick as many lines as you can from these poem lines:\\n\"\n    base_msg = [{\"role\": \"user\", \"content\": base_prompt}]\n    base_fmt = tokenizer.apply_chat_template(\n        base_msg, add_generation_prompt=True, tokenize=False\n    )\n    base_offset = len(tokenizer(base_fmt).input_ids)\n    if input_len <= base_offset:\n        raise ValueError(\n            f\"'input_len' must be higher than the base prompt length \"\n            f\"({base_offset}).\"\n        )\n\n    # Determine how many poem lines to use.\n    num_input_lines = round((input_len - base_offset) / avg_len)\n    num_prefix_lines = max(round((prefix_len - base_offset) / avg_len), 0)\n    prefix_lines = self.data[:num_prefix_lines]\n\n    samples = []\n    ind = 0\n    while len(samples) < num_requests:\n        extra_lines = random.choices(\n            self.data, k=num_input_lines - num_prefix_lines\n        )\n        prompt = f\"{base_prompt}{''.join(prefix_lines + extra_lines)}\"\n        msg = [{\"role\": \"user\", \"content\": prompt}]\n        prompt_formatted = tokenizer.apply_chat_template(\n            msg, add_generation_prompt=True, tokenize=False\n        )\n        prompt_len = len(tokenizer(prompt_formatted).input_ids)\n        if prompt_len <= input_len:\n            samples.append(\n                SampleRequest(\n                    prompt=prompt_formatted if return_prompt_formatted else prompt,\n                    prompt_len=prompt_len,\n                    expected_output_len=output_len,\n                    request_id=request_id_prefix + str(ind),\n                )\n            )\n            ind += 1\n    return samples",
      "language": "python"
    },
    {
      "code": "2034\n2035\n2036\n2037\n2038\n2039\n2040\n2041\n2042\n2043\n2044\n2045\n2046\n2047\n2048\n2049\n2050\n2051\n2052\n2053\n2054\n2055\n2056\n2057\n2058\n2059\n2060\n2061\n2062\n2063\n2064\n2065\n2066\n2067\n2068\n2069\n2070\n2071",
      "language": "unknown"
    },
    {
      "code": "class SpecBench(CustomDataset):\n    \"\"\"\n    Implements the SpecBench dataset: https://github.com/hemingkx/Spec-Bench\n    Download the dataset using:\n    wget https://raw.githubusercontent.com/hemingkx/Spec-Bench/refs/heads/main/data/spec_bench/question.jsonl\n    \"\"\"  # noqa: E501\n\n    def __init__(self, **kwargs) -> None:\n        self.category = kwargs.pop(\"category\", None)\n        super().__init__(**kwargs)\n        self.load_data()\n\n    def load_data(self) -> None:\n        if self.dataset_path is None:\n            raise ValueError(\"dataset_path must be provided for loading data.\")\n\n        self.data = []\n\n        # Load the JSONL file\n        jsonl_data = pd.read_json(path_or_buf=self.dataset_path, lines=True)\n\n        # check if the JSONL file has a 'turns' column\n        if \"turns\" not in jsonl_data.columns:\n            raise ValueError(\"JSONL file must contain a 'turns' column.\")\n\n        for _, row in jsonl_data.iterrows():\n            # sample only from a specific category if specified\n            if (not self.category) or (self.category == row[\"category\"]):\n                prompt = row[\"turns\"][0]\n                self.data.append({\"prompt\": prompt})\n\n        random.seed(self.random_seed)\n        if not getattr(self, \"disable_shuffle\", False):\n            random.shuffle(self.data)\n\n    def sample(self, **kwargs) -> list:\n        # leverage CustomDataset sample\n        return super().sample(**kwargs)",
      "language": "python"
    },
    {
      "code": "class SpecBench(CustomDataset):\n    \"\"\"\n    Implements the SpecBench dataset: https://github.com/hemingkx/Spec-Bench\n    Download the dataset using:\n    wget https://raw.githubusercontent.com/hemingkx/Spec-Bench/refs/heads/main/data/spec_bench/question.jsonl\n    \"\"\"  # noqa: E501\n\n    def __init__(self, **kwargs) -> None:\n        self.category = kwargs.pop(\"category\", None)\n        super().__init__(**kwargs)\n        self.load_data()\n\n    def load_data(self) -> None:\n        if self.dataset_path is None:\n            raise ValueError(\"dataset_path must be provided for loading data.\")\n\n        self.data = []\n\n        # Load the JSONL file\n        jsonl_data = pd.read_json(path_or_buf=self.dataset_path, lines=True)\n\n        # check if the JSONL file has a 'turns' column\n        if \"turns\" not in jsonl_data.columns:\n            raise ValueError(\"JSONL file must contain a 'turns' column.\")\n\n        for _, row in jsonl_data.iterrows():\n            # sample only from a specific category if specified\n            if (not self.category) or (self.category == row[\"category\"]):\n                prompt = row[\"turns\"][0]\n                self.data.append({\"prompt\": prompt})\n\n        random.seed(self.random_seed)\n        if not getattr(self, \"disable_shuffle\", False):\n            random.shuffle(self.data)\n\n    def sample(self, **kwargs) -> list:\n        # leverage CustomDataset sample\n        return super().sample(**kwargs)",
      "language": "python"
    },
    {
      "code": "category = pop('category', None)",
      "language": "rust"
    },
    {
      "code": "category = pop('category', None)",
      "language": "rust"
    },
    {
      "code": "__init__(**kwargs) -> None",
      "language": "python"
    },
    {
      "code": "__init__(**kwargs) -> None",
      "language": "python"
    },
    {
      "code": "2041\n2042\n2043\n2044",
      "language": "unknown"
    },
    {
      "code": "def __init__(self, **kwargs) -> None:\n    self.category = kwargs.pop(\"category\", None)\n    super().__init__(**kwargs)\n    self.load_data()",
      "language": "python"
    },
    {
      "code": "def __init__(self, **kwargs) -> None:\n    self.category = kwargs.pop(\"category\", None)\n    super().__init__(**kwargs)\n    self.load_data()",
      "language": "python"
    },
    {
      "code": "load_data() -> None",
      "language": "rust"
    },
    {
      "code": "load_data() -> None",
      "language": "rust"
    },
    {
      "code": "2046\n2047\n2048\n2049\n2050\n2051\n2052\n2053\n2054\n2055\n2056\n2057\n2058\n2059\n2060\n2061\n2062\n2063\n2064\n2065\n2066\n2067",
      "language": "unknown"
    },
    {
      "code": "def load_data(self) -> None:\n    if self.dataset_path is None:\n        raise ValueError(\"dataset_path must be provided for loading data.\")\n\n    self.data = []\n\n    # Load the JSONL file\n    jsonl_data = pd.read_json(path_or_buf=self.dataset_path, lines=True)\n\n    # check if the JSONL file has a 'turns' column\n    if \"turns\" not in jsonl_data.columns:\n        raise ValueError(\"JSONL file must contain a 'turns' column.\")\n\n    for _, row in jsonl_data.iterrows():\n        # sample only from a specific category if specified\n        if (not self.category) or (self.category == row[\"category\"]):\n            prompt = row[\"turns\"][0]\n            self.data.append({\"prompt\": prompt})\n\n    random.seed(self.random_seed)\n    if not getattr(self, \"disable_shuffle\", False):\n        random.shuffle(self.data)",
      "language": "python"
    },
    {
      "code": "def load_data(self) -> None:\n    if self.dataset_path is None:\n        raise ValueError(\"dataset_path must be provided for loading data.\")\n\n    self.data = []\n\n    # Load the JSONL file\n    jsonl_data = pd.read_json(path_or_buf=self.dataset_path, lines=True)\n\n    # check if the JSONL file has a 'turns' column\n    if \"turns\" not in jsonl_data.columns:\n        raise ValueError(\"JSONL file must contain a 'turns' column.\")\n\n    for _, row in jsonl_data.iterrows():\n        # sample only from a specific category if specified\n        if (not self.category) or (self.category == row[\"category\"]):\n            prompt = row[\"turns\"][0]\n            self.data.append({\"prompt\": prompt})\n\n    random.seed(self.random_seed)\n    if not getattr(self, \"disable_shuffle\", False):\n        random.shuffle(self.data)",
      "language": "python"
    },
    {
      "code": "sample(**kwargs) -> list",
      "language": "php"
    },
    {
      "code": "sample(**kwargs) -> list",
      "language": "php"
    },
    {
      "code": "2069\n2070\n2071",
      "language": "unknown"
    },
    {
      "code": "def sample(self, **kwargs) -> list:\n    # leverage CustomDataset sample\n    return super().sample(**kwargs)",
      "language": "python"
    },
    {
      "code": "def sample(self, **kwargs) -> list:\n    # leverage CustomDataset sample\n    return super().sample(**kwargs)",
      "language": "python"
    },
    {
      "code": "2409\n2410\n2411\n2412\n2413\n2414\n2415\n2416\n2417\n2418\n2419\n2420\n2421\n2422\n2423\n2424\n2425\n2426\n2427\n2428\n2429\n2430\n2431\n2432\n2433\n2434\n2435\n2436\n2437\n2438\n2439\n2440\n2441\n2442\n2443\n2444\n2445\n2446\n2447\n2448\n2449\n2450\n2451\n2452\n2453\n2454\n2455\n2456\n2457\n2458\n2459",
      "language": "unknown"
    },
    {
      "code": "class VisionArenaDataset(HuggingFaceDataset):\n    \"\"\"\n    Vision Arena Dataset.\n    \"\"\"\n\n    DEFAULT_OUTPUT_LEN = 128\n    SUPPORTED_DATASET_PATHS = {\n        \"lmarena-ai/VisionArena-Chat\": lambda x: x[\"conversation\"][0][0][\"content\"],\n        \"lmarena-ai/vision-arena-bench-v0.1\": lambda x: x[\"turns\"][0][0][\"content\"],\n    }\n    IS_MULTIMODAL = True\n\n    def sample(\n        self,\n        tokenizer: TokenizerLike,\n        num_requests: int,\n        output_len: int | None = None,\n        enable_multimodal_chat: bool = False,\n        request_id_prefix: str = \"\",\n        no_oversample: bool = False,\n        **kwargs,\n    ) -> list:\n        output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN\n        sampled_requests = []\n        for i, item in enumerate(self.data):\n            if len(sampled_requests) >= num_requests:\n                break\n            parser_fn = self.SUPPORTED_DATASET_PATHS.get(self.hf_name)\n            if parser_fn is None:\n                raise ValueError(f\"Unsupported dataset path: {self.hf_name}\")\n            prompt = parser_fn(item)\n            mm_content = process_image(item[\"images\"][0])\n            prompt_len = len(tokenizer(prompt).input_ids)\n            if enable_multimodal_chat:\n                # Note: when chat is enabled the request prompt_len is no longer\n                # accurate and we will be using request output to count the\n                # actual prompt len\n                prompt = self.apply_multimodal_chat_transformation(prompt, mm_content)\n            sampled_requests.append(\n                SampleRequest(\n                    prompt=prompt,\n                    prompt_len=prompt_len,\n                    expected_output_len=output_len,\n                    multi_modal_data=mm_content,\n                    request_id=request_id_prefix + str(i),\n                )\n            )\n        self.maybe_oversample_requests(\n            sampled_requests, num_requests, request_id_prefix, no_oversample\n        )\n        return sampled_requests",
      "language": "python"
    },
    {
      "code": "class VisionArenaDataset(HuggingFaceDataset):\n    \"\"\"\n    Vision Arena Dataset.\n    \"\"\"\n\n    DEFAULT_OUTPUT_LEN = 128\n    SUPPORTED_DATASET_PATHS = {\n        \"lmarena-ai/VisionArena-Chat\": lambda x: x[\"conversation\"][0][0][\"content\"],\n        \"lmarena-ai/vision-arena-bench-v0.1\": lambda x: x[\"turns\"][0][0][\"content\"],\n    }\n    IS_MULTIMODAL = True\n\n    def sample(\n        self,\n        tokenizer: TokenizerLike,\n        num_requests: int,\n        output_len: int | None = None,\n        enable_multimodal_chat: bool = False,\n        request_id_prefix: str = \"\",\n        no_oversample: bool = False,\n        **kwargs,\n    ) -> list:\n        output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN\n        sampled_requests = []\n        for i, item in enumerate(self.data):\n            if len(sampled_requests) >= num_requests:\n                break\n            parser_fn = self.SUPPORTED_DATASET_PATHS.get(self.hf_name)\n            if parser_fn is None:\n                raise ValueError(f\"Unsupported dataset path: {self.hf_name}\")\n            prompt = parser_fn(item)\n            mm_content = process_image(item[\"images\"][0])\n            prompt_len = len(tokenizer(prompt).input_ids)\n            if enable_multimodal_chat:\n                # Note: when chat is enabled the request prompt_len is no longer\n                # accurate and we will be using request output to count the\n                # actual prompt len\n                prompt = self.apply_multimodal_chat_transformation(prompt, mm_content)\n            sampled_requests.append(\n                SampleRequest(\n                    prompt=prompt,\n                    prompt_len=prompt_len,\n                    expected_output_len=output_len,\n                    multi_modal_data=mm_content,\n                    request_id=request_id_prefix + str(i),\n                )\n            )\n        self.maybe_oversample_requests(\n            sampled_requests, num_requests, request_id_prefix, no_oversample\n        )\n        return sampled_requests",
      "language": "python"
    },
    {
      "code": "DEFAULT_OUTPUT_LEN = 128",
      "language": "unknown"
    },
    {
      "code": "DEFAULT_OUTPUT_LEN = 128",
      "language": "unknown"
    },
    {
      "code": "IS_MULTIMODAL = True",
      "language": "unknown"
    },
    {
      "code": "IS_MULTIMODAL = True",
      "language": "unknown"
    },
    {
      "code": "SUPPORTED_DATASET_PATHS = {\n    \"lmarena-ai/VisionArena-Chat\": lambda x: x[\n        \"conversation\"\n    ][0][0][\"content\"],\n    \"lmarena-ai/vision-arena-bench-v0.1\": lambda x: x[\n        \"turns\"\n    ][0][0][\"content\"],\n}",
      "language": "unknown"
    },
    {
      "code": "SUPPORTED_DATASET_PATHS = {\n    \"lmarena-ai/VisionArena-Chat\": lambda x: x[\n        \"conversation\"\n    ][0][0][\"content\"],\n    \"lmarena-ai/vision-arena-bench-v0.1\": lambda x: x[\n        \"turns\"\n    ][0][0][\"content\"],\n}",
      "language": "unknown"
    },
    {
      "code": "sample(\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    output_len: int | None = None,\n    enable_multimodal_chat: bool = False,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list",
      "language": "typescript"
    },
    {
      "code": "sample(\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    output_len: int | None = None,\n    enable_multimodal_chat: bool = False,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list",
      "language": "typescript"
    },
    {
      "code": "2421\n2422\n2423\n2424\n2425\n2426\n2427\n2428\n2429\n2430\n2431\n2432\n2433\n2434\n2435\n2436\n2437\n2438\n2439\n2440\n2441\n2442\n2443\n2444\n2445\n2446\n2447\n2448\n2449\n2450\n2451\n2452\n2453\n2454\n2455\n2456\n2457\n2458\n2459",
      "language": "unknown"
    },
    {
      "code": "def sample(\n    self,\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    output_len: int | None = None,\n    enable_multimodal_chat: bool = False,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list:\n    output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN\n    sampled_requests = []\n    for i, item in enumerate(self.data):\n        if len(sampled_requests) >= num_requests:\n            break\n        parser_fn = self.SUPPORTED_DATASET_PATHS.get(self.hf_name)\n        if parser_fn is None:\n            raise ValueError(f\"Unsupported dataset path: {self.hf_name}\")\n        prompt = parser_fn(item)\n        mm_content = process_image(item[\"images\"][0])\n        prompt_len = len(tokenizer(prompt).input_ids)\n        if enable_multimodal_chat:\n            # Note: when chat is enabled the request prompt_len is no longer\n            # accurate and we will be using request output to count the\n            # actual prompt len\n            prompt = self.apply_multimodal_chat_transformation(prompt, mm_content)\n        sampled_requests.append(\n            SampleRequest(\n                prompt=prompt,\n                prompt_len=prompt_len,\n                expected_output_len=output_len,\n                multi_modal_data=mm_content,\n                request_id=request_id_prefix + str(i),\n            )\n        )\n    self.maybe_oversample_requests(\n        sampled_requests, num_requests, request_id_prefix, no_oversample\n    )\n    return sampled_requests",
      "language": "python"
    },
    {
      "code": "def sample(\n    self,\n    tokenizer: TokenizerLike,\n    num_requests: int,\n    output_len: int | None = None,\n    enable_multimodal_chat: bool = False,\n    request_id_prefix: str = \"\",\n    no_oversample: bool = False,\n    **kwargs,\n) -> list:\n    output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN\n    sampled_requests = []\n    for i, item in enumerate(self.data):\n        if len(sampled_requests) >= num_requests:\n            break\n        parser_fn = self.SUPPORTED_DATASET_PATHS.get(self.hf_name)\n        if parser_fn is None:\n            raise ValueError(f\"Unsupported dataset path: {self.hf_name}\")\n        prompt = parser_fn(item)\n        mm_content = process_image(item[\"images\"][0])\n        prompt_len = len(tokenizer(prompt).input_ids)\n        if enable_multimodal_chat:\n            # Note: when chat is enabled the request prompt_len is no longer\n            # accurate and we will be using request output to count the\n            # actual prompt len\n            prompt = self.apply_multimodal_chat_transformation(prompt, mm_content)\n        sampled_requests.append(\n            SampleRequest(\n                prompt=prompt,\n                prompt_len=prompt_len,\n                expected_output_len=output_len,\n                multi_modal_data=mm_content,\n                request_id=request_id_prefix + str(i),\n            )\n        )\n    self.maybe_oversample_requests(\n        sampled_requests, num_requests, request_id_prefix, no_oversample\n    )\n    return sampled_requests",
      "language": "python"
    },
    {
      "code": "1297\n1298\n1299\n1300\n1301\n1302\n1303\n1304\n1305\n1306\n1307\n1308\n1309\n1310\n1311\n1312\n1313\n1314",
      "language": "unknown"
    },
    {
      "code": "class _ValidateDatasetArgs(argparse.Action):\n    \"\"\"Argparse action to validate dataset name and path compatibility.\"\"\"\n\n    def __call__(self, parser, namespace, values, option_string=None):\n        setattr(namespace, self.dest, values)\n\n        # Get current values of both dataset_name and dataset_path\n        dataset_name = getattr(namespace, \"dataset_name\", \"random\")\n        dataset_path = getattr(namespace, \"dataset_path\", None)\n\n        # Validate the combination\n        if dataset_name == \"random\" and dataset_path is not None:\n            parser.error(\n                \"Cannot use 'random' dataset with --dataset-path. \"\n                \"Please specify the appropriate --dataset-name (e.g., \"\n                \"'sharegpt', 'custom', 'sonnet') for your dataset file: \"\n                f\"{dataset_path}\"\n            )",
      "language": "python"
    },
    {
      "code": "class _ValidateDatasetArgs(argparse.Action):\n    \"\"\"Argparse action to validate dataset name and path compatibility.\"\"\"\n\n    def __call__(self, parser, namespace, values, option_string=None):\n        setattr(namespace, self.dest, values)\n\n        # Get current values of both dataset_name and dataset_path\n        dataset_name = getattr(namespace, \"dataset_name\", \"random\")\n        dataset_path = getattr(namespace, \"dataset_path\", None)\n\n        # Validate the combination\n        if dataset_name == \"random\" and dataset_path is not None:\n            parser.error(\n                \"Cannot use 'random' dataset with --dataset-path. \"\n                \"Please specify the appropriate --dataset-name (e.g., \"\n                \"'sharegpt', 'custom', 'sonnet') for your dataset file: \"\n                f\"{dataset_path}\"\n            )",
      "language": "python"
    },
    {
      "code": "__call__(parser, namespace, values, option_string=None)",
      "language": "rust"
    },
    {
      "code": "__call__(parser, namespace, values, option_string=None)",
      "language": "rust"
    },
    {
      "code": "1300\n1301\n1302\n1303\n1304\n1305\n1306\n1307\n1308\n1309\n1310\n1311\n1312\n1313\n1314",
      "language": "unknown"
    },
    {
      "code": "def __call__(self, parser, namespace, values, option_string=None):\n    setattr(namespace, self.dest, values)\n\n    # Get current values of both dataset_name and dataset_path\n    dataset_name = getattr(namespace, \"dataset_name\", \"random\")\n    dataset_path = getattr(namespace, \"dataset_path\", None)\n\n    # Validate the combination\n    if dataset_name == \"random\" and dataset_path is not None:\n        parser.error(\n            \"Cannot use 'random' dataset with --dataset-path. \"\n            \"Please specify the appropriate --dataset-name (e.g., \"\n            \"'sharegpt', 'custom', 'sonnet') for your dataset file: \"\n            f\"{dataset_path}\"\n        )",
      "language": "python"
    },
    {
      "code": "def __call__(self, parser, namespace, values, option_string=None):\n    setattr(namespace, self.dest, values)\n\n    # Get current values of both dataset_name and dataset_path\n    dataset_name = getattr(namespace, \"dataset_name\", \"random\")\n    dataset_path = getattr(namespace, \"dataset_path\", None)\n\n    # Validate the combination\n    if dataset_name == \"random\" and dataset_path is not None:\n        parser.error(\n            \"Cannot use 'random' dataset with --dataset-path. \"\n            \"Please specify the appropriate --dataset-name (e.g., \"\n            \"'sharegpt', 'custom', 'sonnet') for your dataset file: \"\n            f\"{dataset_path}\"\n        )",
      "language": "python"
    },
    {
      "code": "_format_zeta_prompt(\n    sample: dict,\n    original_start_marker: str = \"<|editable_region_start|>\",\n) -> dict",
      "language": "typescript"
    },
    {
      "code": "_format_zeta_prompt(\n    sample: dict,\n    original_start_marker: str = \"<|editable_region_start|>\",\n) -> dict",
      "language": "typescript"
    },
    {
      "code": "2812\n2813\n2814\n2815\n2816\n2817\n2818\n2819\n2820\n2821\n2822\n2823\n2824\n2825\n2826\n2827\n2828\n2829\n2830\n2831\n2832\n2833\n2834\n2835\n2836\n2837\n2838\n2839\n2840\n2841\n2842",
      "language": "unknown"
    },
    {
      "code": "def _format_zeta_prompt(\n    sample: dict, original_start_marker: str = \"<|editable_region_start|>\"\n) -> dict:\n    \"\"\"Format the zeta prompt for the Next Edit Prediction (NEP) dataset.\n\n    This function formats examples from the NEP dataset\n    into prompts and expected outputs. It could be\n    further extended to support more NEP datasets.\n\n    Args:\n        sample: The dataset sample containing events,\n            inputs, and outputs.\n        original_start_marker: The marker indicating the\n            start of the editable region. Defaults to\n            \"<|editable_region_start|>\".\n\n    Returns:\n        A dictionary with the formatted prompts and expected outputs.\n    \"\"\"\n    events = sample[\"events\"]\n    input = sample[\"input\"]\n    output = sample[\"output\"]\n    prompt = zeta_prompt.format(events, input)\n\n    # following the original implementation, extract the focused region\n    # from the raw output\n    output_start_index = output.find(original_start_marker)\n    output_focused_region = output[output_start_index:]\n    expected_output = output_focused_region\n\n    return {\"prompt\": prompt, \"expected_output\": expected_output}",
      "language": "json"
    },
    {
      "code": "def _format_zeta_prompt(\n    sample: dict, original_start_marker: str = \"<|editable_region_start|>\"\n) -> dict:\n    \"\"\"Format the zeta prompt for the Next Edit Prediction (NEP) dataset.\n\n    This function formats examples from the NEP dataset\n    into prompts and expected outputs. It could be\n    further extended to support more NEP datasets.\n\n    Args:\n        sample: The dataset sample containing events,\n            inputs, and outputs.\n        original_start_marker: The marker indicating the\n            start of the editable region. Defaults to\n            \"<|editable_region_start|>\".\n\n    Returns:\n        A dictionary with the formatted prompts and expected outputs.\n    \"\"\"\n    events = sample[\"events\"]\n    input = sample[\"input\"]\n    output = sample[\"output\"]\n    prompt = zeta_prompt.format(events, input)\n\n    # following the original implementation, extract the focused region\n    # from the raw output\n    output_start_index = output.find(original_start_marker)\n    output_focused_region = output[output_start_index:]\n    expected_output = output_focused_region\n\n    return {\"prompt\": prompt, \"expected_output\": expected_output}",
      "language": "json"
    },
    {
      "code": "add_dataset_parser(parser: ArgumentParser)",
      "language": "unknown"
    },
    {
      "code": "add_dataset_parser(parser: ArgumentParser)",
      "language": "unknown"
    },
    {
      "code": "1317\n1318\n1319\n1320\n1321\n1322\n1323\n1324\n1325\n1326\n1327\n1328\n1329\n1330\n1331\n1332\n1333\n1334\n1335\n1336\n1337\n1338\n1339\n1340\n1341\n1342\n1343\n1344\n1345\n1346\n1347\n1348\n1349\n1350\n1351\n1352\n1353\n1354\n1355\n1356\n1357\n1358\n1359\n1360\n1361\n1362\n1363\n1364\n1365\n1366\n1367\n1368\n1369\n1370\n1371\n1372\n1373\n1374\n1375\n1376\n1377\n1378\n1379\n1380\n1381\n1382\n1383\n1384\n1385\n1386\n1387\n1388\n1389\n1390\n1391\n1392\n1393\n1394\n1395\n1396\n1397\n1398\n1399\n1400\n1401\n1402\n1403\n1404\n1405\n1406\n1407\n1408\n1409\n1410\n1411\n1412\n1413\n1414\n1415\n1416\n1417\n1418\n1419\n1420\n1421\n1422\n1423\n1424\n1425\n1426\n1427\n1428\n1429\n1430\n1431\n1432\n1433\n1434\n1435\n1436\n1437\n1438\n1439\n1440\n1441\n1442\n1443\n1444\n1445\n1446\n1447\n1448\n1449\n1450\n1451\n1452\n1453\n1454\n1455\n1456\n1457\n1458\n1459\n1460\n1461\n1462\n1463\n1464\n1465\n1466\n1467\n1468\n1469\n1470\n1471\n1472\n1473\n1474\n1475\n1476\n1477\n1478\n1479\n1480\n1481\n1482\n1483\n1484\n1485\n1486\n1487\n1488\n1489\n1490\n1491\n1492\n1493\n1494\n1495\n1496\n1497\n1498\n1499\n1500\n1501\n1502\n1503\n1504\n1505\n1506\n1507\n1508\n1509\n1510\n1511\n1512\n1513\n1514\n1515\n1516\n1517\n1518\n1519\n1520\n1521\n1522\n1523\n1524\n1525\n1526\n1527\n1528\n1529\n1530\n1531\n1532\n1533\n1534\n1535\n1536\n1537\n1538\n1539\n1540\n1541\n1542\n1543\n1544\n1545\n1546\n1547\n1548\n1549\n1550\n1551\n1552\n1553\n1554\n1555\n1556\n1557\n1558\n1559\n1560\n1561\n1562\n1563\n1564\n1565\n1566\n1567\n1568\n1569\n1570\n1571\n1572\n1573\n1574\n1575\n1576\n1577\n1578\n1579\n1580\n1581\n1582\n1583\n1584\n1585\n1586\n1587\n1588\n1589\n1590\n1591\n1592\n1593\n1594\n1595\n1596\n1597\n1598\n1599\n1600\n1601\n1602\n1603\n1604\n1605\n1606\n1607\n1608\n1609\n1610\n1611\n1612\n1613\n1614\n1615\n1616\n1617\n1618\n1619\n1620\n1621\n1622\n1623\n1624\n1625\n1626\n1627\n1628\n1629\n1630\n1631\n1632\n1633\n1634\n1635\n1636\n1637\n1638",
      "language": "unknown"
    },
    {
      "code": "def add_dataset_parser(parser: FlexibleArgumentParser):\n    parser.add_argument(\"--seed\", type=int, default=0)\n    parser.add_argument(\n        \"--num-prompts\",\n        type=int,\n        default=1000,\n        help=\"Number of prompts to process.\",\n    )\n    parser.add_argument(\n        \"--dataset-name\",\n        type=str,\n        default=\"random\",\n        action=_ValidateDatasetArgs,\n        choices=[\n            \"sharegpt\",\n            \"burstgpt\",\n            \"sonnet\",\n            \"random\",\n            \"random-mm\",\n            \"random-rerank\",\n            \"hf\",\n            \"custom\",\n            \"prefix_repetition\",\n            \"spec_bench\",\n        ],\n        help=\"Name of the dataset to benchmark on.\",\n    )\n    parser.add_argument(\n        \"--no-stream\",\n        action=\"store_true\",\n        help=\"Do not load the dataset in streaming mode.\",\n    )\n    parser.add_argument(\n        \"--dataset-path\",\n        type=str,\n        default=None,\n        action=_ValidateDatasetArgs,\n        help=\"Path to the sharegpt/sonnet dataset. \"\n        \"Or the huggingface dataset ID if using HF dataset.\",\n    )\n    parser.add_argument(\n        \"--no-oversample\",\n        action=\"store_true\",\n        help=\"Do not oversample if the dataset has fewer samples than num-prompts.\",\n    )\n    parser.add_argument(\n        \"--skip-chat-template\",\n        action=\"store_true\",\n        help=\"Skip applying chat template to prompt for datasets that support it.\",\n    )\n    parser.add_argument(\n        \"--disable-shuffle\",\n        action=\"store_true\",\n        help=\"Disable shuffling of dataset samples for deterministic ordering.\",\n    )\n\n    # group for dataset specific arguments\n    custom_group = parser.add_argument_group(\"custom dataset options\")\n    custom_group.add_argument(\n        \"--custom-output-len\",\n        type=int,\n        default=256,\n        help=\"Number of output tokens per request, used only for custom dataset.\",\n    )\n\n    spec_bench_group = parser.add_argument_group(\"spec bench dataset options\")\n    spec_bench_group.add_argument(\n        \"--spec-bench-output-len\",\n        type=int,\n        default=256,\n        help=\"Num of output tokens per request, used only for spec bench dataset.\",\n    )\n    spec_bench_group.add_argument(\n        \"--spec-bench-category\",\n        type=str,\n        default=None,\n        help=\"Category for spec bench dataset. If None, use all categories.\",\n    )\n\n    sonnet_group = parser.add_argument_group(\"sonnet dataset options\")\n    sonnet_group.add_argument(\n        \"--sonnet-input-len\",\n        type=int,\n        default=550,\n        help=\"Number of input tokens per request, used only for sonnet dataset.\",\n    )\n    sonnet_group.add_argument(\n        \"--sonnet-output-len\",\n        type=int,\n        default=150,\n        help=\"Number of output tokens per request, used only for sonnet dataset.\",\n    )\n    sonnet_group.add_argument(\n        \"--sonnet-prefix-len\",\n        type=int,\n        default=200,\n        help=\"Number of prefix tokens per request, used only for sonnet dataset.\",\n    )\n\n    sharegpt_group = parser.add_argument_group(\"sharegpt dataset options\")\n    sharegpt_group.add_argument(\n        \"--sharegpt-output-len\",\n        type=int,\n        default=None,\n        help=\"Output length for each request. Overrides the output length \"\n        \"from the ShareGPT dataset.\",\n    )\n\n    blazedit_group = parser.add_argument_group(\"blazedit dataset options\")\n    blazedit_group.add_argument(\n        \"--blazedit-min-distance\",\n        type=float,\n        default=0.0,\n        help=\"Minimum distance for blazedit dataset. Min: 0, Max: 1.0\",\n    )\n    blazedit_group.add_argument(\n        \"--blazedit-max-distance\",\n        type=float,\n        default=1.0,\n        help=\"Maximum distance for blazedit dataset. Min: 0, Max: 1.0\",\n    )\n\n    random_group = parser.add_argument_group(\"random dataset options\")\n    random_group.add_argument(\n        \"--random-input-len\",\n        type=int,\n        default=1024,\n        help=\"Number of input tokens per request, used only for random sampling.\",\n    )\n    random_group.add_argument(\n        \"--random-output-len\",\n        type=int,\n        default=128,\n        help=\"Number of output tokens per request, used only for random sampling.\",\n    )\n    random_group.add_argument(\n        \"--random-range-ratio\",\n        type=float,\n        default=0.0,\n        help=\"Range ratio for sampling input/output length, \"\n        \"used only for random sampling. Must be in the range [0, 1) to define \"\n        \"a symmetric sampling range\"\n        \"[length * (1 - range_ratio), length * (1 + range_ratio)].\",\n    )\n    random_group.add_argument(\n        \"--random-prefix-len\",\n        type=int,\n        default=0,\n        help=(\n            \"Number of fixed prefix tokens before the random context \"\n            \"in a request. \"\n            \"The total input length is the sum of `random-prefix-len` and \"\n            \"a random \"\n            \"context length sampled from [input_len * (1 - range_ratio), \"\n            \"input_len * (1 + range_ratio)].\"\n        ),\n    )\n    random_group.add_argument(\n        \"--random-batch-size\",\n        type=int,\n        default=1,\n        help=(\"Batch size for random sampling. Only used for embeddings benchmark.\"),\n    )\n    random_group.add_argument(\n        \"--no-reranker\",\n        action=\"store_true\",\n        help=(\n            \"Whether the model supports reranking natively.\"\n            \" Only used for reranker benchmark.\"\n        ),\n    )\n\n    # random multimodal dataset options\n    random_mm_group = parser.add_argument_group(\n        \"random multimodal dataset options extended from random dataset\"\n    )\n    random_mm_group.add_argument(\n        \"--random-mm-base-items-per-request\",\n        type=int,\n        default=RandomMultiModalDataset.DEFAULT_BASE_ITEMS_PER_REQUEST,\n        help=(\n            \"Base number of multimodal items per request for random-mm. \"\n            \"Actual per-request count is sampled around this base using \"\n            \"--random-mm-num-mm-items-range-ratio.\"\n        ),\n    )\n    random_mm_group.add_argument(\n        \"--random-mm-num-mm-items-range-ratio\",\n        type=float,\n        default=RandomMultiModalDataset.DEFAULT_NUM_MM_ITEMS_RANGE_RATIO,\n        help=(\n            \"Range ratio r in [0, 1] for sampling items per request. \"\n            \"We sample uniformly from the closed integer range \"\n            \"[floor(n*(1-r)), ceil(n*(1+r))] \"\n            \"where n is the base items per request. \"\n            \"r=0 keeps it fixed; r=1 allows 0 items. The maximum is clamped \"\n            \"to the sum of per-modality limits from \"\n            \"--random-mm-limit-mm-per-prompt. \"\n            \"An error is raised if the computed min exceeds the max.\"\n        ),\n    )\n    random_mm_group.add_argument(\n        \"--random-mm-limit-mm-per-prompt\",\n        type=json.loads,\n        default=RandomMultiModalDataset.DEFAULT_LIMIT_MM_PER_PROMPT,\n        help=(\n            \"Per-modality hard caps for items attached per request, e.g. \"\n            '\\'{\"image\": 3, \"video\": 0}\\'. The sampled per-request item '\n            \"count is clamped to the sum of these limits. When a modality \"\n            \"reaches its cap, its buckets are excluded and probabilities are \"\n            \"renormalized.\"\n            \"OBS.: Only image sampling is supported for now.\"\n        ),\n    )\n\n    def _parse_mm_bucket_config(v: object) -> dict[tuple[int, int, int], float]:\n        # If already a dict (e.g., programmatic call), normalize keys\n        def normalize(d: dict) -> dict[tuple[int, int, int], float]:\n            out: dict[tuple[int, int, int], float] = {}\n            for k, val in d.items():\n                key = k\n                if isinstance(key, str):\n                    with suppress(Exception):\n                        key = ast.literal_eval(key)\n                if not (\n                    isinstance(key, tuple)\n                    and len(key) == 3\n                    and all(isinstance(x, int) for x in key)\n                ):\n                    raise ValueError(\n                        f\"Invalid bucket key {k!r}. Expected tuple (H, W, T).\"\n                    )\n                out[(int(key[0]), int(key[1]), int(key[2]))] = float(val)\n            return out\n\n        if isinstance(v, dict):\n            return normalize(v)\n        if isinstance(v, str):\n            # Python literal (supports tuple keys)\n            parsed = ast.literal_eval(v)\n            if not isinstance(parsed, dict):\n                raise ValueError(\"Bucket config must parse to a dict.\")\n            return normalize(parsed)\n        raise ValueError(\"Unsupported value for --random-mm-bucket-config.\")\n\n    random_mm_group.add_argument(\n        \"--random-mm-bucket-config\",\n        type=_parse_mm_bucket_config,\n        default=RandomMultiModalDataset.DEFAULT_MM_ITEM_BUCKET_CONFIG,\n        help=(\n            \"The bucket config is a dictionary mapping a multimodal item\"\n            \"sampling configuration to a probability.\"\n            \"Currently allows for 2 modalities: images and videos. \"\n            \"An bucket key is a tuple of (height, width, num_frames)\"\n            \"The value is the probability of sampling that specific item. \"\n            \"Example: \"\n            \"--random-mm-bucket-config \"\n            \"{(256, 256, 1): 0.5, (720, 1280, 1): 0.4, (720, 1280, 16): 0.10} \"\n            \"First item: images with resolution 256x256 w.p. 0.5\"\n            \"Second item: images with resolution 720x1280 w.p. 0.4 \"\n            \"Third item: videos with resolution 720x1280 and 16 frames w.p. 0.1\"\n            \"OBS.: If the probabilities do not sum to 1, they are normalized.\"\n            \"OBS bis.: Only image sampling is supported for now.\"\n        ),\n    )\n\n    hf_group = parser.add_argument_group(\"hf dataset options\")\n    hf_group.add_argument(\n        \"--hf-subset\", type=str, default=None, help=\"Subset of the HF dataset.\"\n    )\n    hf_group.add_argument(\n        \"--hf-split\", type=str, default=None, help=\"Split of the HF dataset.\"\n    )\n    hf_group.add_argument(\n        \"--hf-name\",\n        type=str,\n        default=None,\n        help=(\n            \"Name of the dataset on HuggingFace \"\n            \"(e.g., 'lmarena-ai/VisionArena-Chat'). \"\n            \"Specify this if your dataset-path is a local path.\"\n        ),\n    )\n    hf_group.add_argument(\n        \"--hf-output-len\",\n        type=int,\n        default=None,\n        help=\"Output length for each request. Overrides the output lengths \"\n        \"from the sampled HF dataset.\",\n    )\n\n    prefix_repetition_group = parser.add_argument_group(\n        \"prefix repetition dataset options\"\n    )\n    prefix_repetition_group.add_argument(\n        \"--prefix-repetition-prefix-len\",\n        type=int,\n        default=256,\n        help=\"Number of prefix tokens per request, used only for prefix \"\n        \"repetition dataset.\",\n    )\n    prefix_repetition_group.add_argument(\n        \"--prefix-repetition-suffix-len\",\n        type=int,\n        default=256,\n        help=\"Number of suffix tokens per request, used only for prefix \"\n        \"repetition dataset. Total input length is prefix_len + suffix_len.\",\n    )\n    prefix_repetition_group.add_argument(\n        \"--prefix-repetition-num-prefixes\",\n        type=int,\n        default=10,\n        help=\"Number of prefixes to generate, used only for prefix repetition \"\n        \"dataset. Prompts per prefix is num_requests // num_prefixes.\",\n    )\n    prefix_repetition_group.add_argument(\n        \"--prefix-repetition-output-len\",\n        type=int,\n        default=128,\n        help=\"Number of output tokens per request, used only for prefix \"\n        \"repetition dataset.\",\n    )",
      "language": "sql"
    },
    {
      "code": "def add_dataset_parser(parser: FlexibleArgumentParser):\n    parser.add_argument(\"--seed\", type=int, default=0)\n    parser.add_argument(\n        \"--num-prompts\",\n        type=int,\n        default=1000,\n        help=\"Number of prompts to process.\",\n    )\n    parser.add_argument(\n        \"--dataset-name\",\n        type=str,\n        default=\"random\",\n        action=_ValidateDatasetArgs,\n        choices=[\n            \"sharegpt\",\n            \"burstgpt\",\n            \"sonnet\",\n            \"random\",\n            \"random-mm\",\n            \"random-rerank\",\n            \"hf\",\n            \"custom\",\n            \"prefix_repetition\",\n            \"spec_bench\",\n        ],\n        help=\"Name of the dataset to benchmark on.\",\n    )\n    parser.add_argument(\n        \"--no-stream\",\n        action=\"store_true\",\n        help=\"Do not load the dataset in streaming mode.\",\n    )\n    parser.add_argument(\n        \"--dataset-path\",\n        type=str,\n        default=None,\n        action=_ValidateDatasetArgs,\n        help=\"Path to the sharegpt/sonnet dataset. \"\n        \"Or the huggingface dataset ID if using HF dataset.\",\n    )\n    parser.add_argument(\n        \"--no-oversample\",\n        action=\"store_true\",\n        help=\"Do not oversample if the dataset has fewer samples than num-prompts.\",\n    )\n    parser.add_argument(\n        \"--skip-chat-template\",\n        action=\"store_true\",\n        help=\"Skip applying chat template to prompt for datasets that support it.\",\n    )\n    parser.add_argument(\n        \"--disable-shuffle\",\n        action=\"store_true\",\n        help=\"Disable shuffling of dataset samples for deterministic ordering.\",\n    )\n\n    # group for dataset specific arguments\n    custom_group = parser.add_argument_group(\"custom dataset options\")\n    custom_group.add_argument(\n        \"--custom-output-len\",\n        type=int,\n        default=256,\n        help=\"Number of output tokens per request, used only for custom dataset.\",\n    )\n\n    spec_bench_group = parser.add_argument_group(\"spec bench dataset options\")\n    spec_bench_group.add_argument(\n        \"--spec-bench-output-len\",\n        type=int,\n        default=256,\n        help=\"Num of output tokens per request, used only for spec bench dataset.\",\n    )\n    spec_bench_group.add_argument(\n        \"--spec-bench-category\",\n        type=str,\n        default=None,\n        help=\"Category for spec bench dataset. If None, use all categories.\",\n    )\n\n    sonnet_group = parser.add_argument_group(\"sonnet dataset options\")\n    sonnet_group.add_argument(\n        \"--sonnet-input-len\",\n        type=int,\n        default=550,\n        help=\"Number of input tokens per request, used only for sonnet dataset.\",\n    )\n    sonnet_group.add_argument(\n        \"--sonnet-output-len\",\n        type=int,\n        default=150,\n        help=\"Number of output tokens per request, used only for sonnet dataset.\",\n    )\n    sonnet_group.add_argument(\n        \"--sonnet-prefix-len\",\n        type=int,\n        default=200,\n        help=\"Number of prefix tokens per request, used only for sonnet dataset.\",\n    )\n\n    sharegpt_group = parser.add_argument_group(\"sharegpt dataset options\")\n    sharegpt_group.add_argument(\n        \"--sharegpt-output-len\",\n        type=int,\n        default=None,\n        help=\"Output length for each request. Overrides the output length \"\n        \"from the ShareGPT dataset.\",\n    )\n\n    blazedit_group = parser.add_argument_group(\"blazedit dataset options\")\n    blazedit_group.add_argument(\n        \"--blazedit-min-distance\",\n        type=float,\n        default=0.0,\n        help=\"Minimum distance for blazedit dataset. Min: 0, Max: 1.0\",\n    )\n    blazedit_group.add_argument(\n        \"--blazedit-max-distance\",\n        type=float,\n        default=1.0,\n        help=\"Maximum distance for blazedit dataset. Min: 0, Max: 1.0\",\n    )\n\n    random_group = parser.add_argument_group(\"random dataset options\")\n    random_group.add_argument(\n        \"--random-input-len\",\n        type=int,\n        default=1024,\n        help=\"Number of input tokens per request, used only for random sampling.\",\n    )\n    random_group.add_argument(\n        \"--random-output-len\",\n        type=int,\n        default=128,\n        help=\"Number of output tokens per request, used only for random sampling.\",\n    )\n    random_group.add_argument(\n        \"--random-range-ratio\",\n        type=float,\n        default=0.0,\n        help=\"Range ratio for sampling input/output length, \"\n        \"used only for random sampling. Must be in the range [0, 1) to define \"\n        \"a symmetric sampling range\"\n        \"[length * (1 - range_ratio), length * (1 + range_ratio)].\",\n    )\n    random_group.add_argument(\n        \"--random-prefix-len\",\n        type=int,\n        default=0,\n        help=(\n            \"Number of fixed prefix tokens before the random context \"\n            \"in a request. \"\n            \"The total input length is the sum of `random-prefix-len` and \"\n            \"a random \"\n            \"context length sampled from [input_len * (1 - range_ratio), \"\n            \"input_len * (1 + range_ratio)].\"\n        ),\n    )\n    random_group.add_argument(\n        \"--random-batch-size\",\n        type=int,\n        default=1,\n        help=(\"Batch size for random sampling. Only used for embeddings benchmark.\"),\n    )\n    random_group.add_argument(\n        \"--no-reranker\",\n        action=\"store_true\",\n        help=(\n            \"Whether the model supports reranking natively.\"\n            \" Only used for reranker benchmark.\"\n        ),\n    )\n\n    # random multimodal dataset options\n    random_mm_group = parser.add_argument_group(\n        \"random multimodal dataset options extended from random dataset\"\n    )\n    random_mm_group.add_argument(\n        \"--random-mm-base-items-per-request\",\n        type=int,\n        default=RandomMultiModalDataset.DEFAULT_BASE_ITEMS_PER_REQUEST,\n        help=(\n            \"Base number of multimodal items per request for random-mm. \"\n            \"Actual per-request count is sampled around this base using \"\n            \"--random-mm-num-mm-items-range-ratio.\"\n        ),\n    )\n    random_mm_group.add_argument(\n        \"--random-mm-num-mm-items-range-ratio\",\n        type=float,\n        default=RandomMultiModalDataset.DEFAULT_NUM_MM_ITEMS_RANGE_RATIO,\n        help=(\n            \"Range ratio r in [0, 1] for sampling items per request. \"\n            \"We sample uniformly from the closed integer range \"\n            \"[floor(n*(1-r)), ceil(n*(1+r))] \"\n            \"where n is the base items per request. \"\n            \"r=0 keeps it fixed; r=1 allows 0 items. The maximum is clamped \"\n            \"to the sum of per-modality limits from \"\n            \"--random-mm-limit-mm-per-prompt. \"\n            \"An error is raised if the computed min exceeds the max.\"\n        ),\n    )\n    random_mm_group.add_argument(\n        \"--random-mm-limit-mm-per-prompt\",\n        type=json.loads,\n        default=RandomMultiModalDataset.DEFAULT_LIMIT_MM_PER_PROMPT,\n        help=(\n            \"Per-modality hard caps for items attached per request, e.g. \"\n            '\\'{\"image\": 3, \"video\": 0}\\'. The sampled per-request item '\n            \"count is clamped to the sum of these limits. When a modality \"\n            \"reaches its cap, its buckets are excluded and probabilities are \"\n            \"renormalized.\"\n            \"OBS.: Only image sampling is supported for now.\"\n        ),\n    )\n\n    def _parse_mm_bucket_config(v: object) -> dict[tuple[int, int, int], float]:\n        # If already a dict (e.g., programmatic call), normalize keys\n        def normalize(d: dict) -> dict[tuple[int, int, int], float]:\n            out: dict[tuple[int, int, int], float] = {}\n            for k, val in d.items():\n                key = k\n                if isinstance(key, str):\n                    with suppress(Exception):\n                        key = ast.literal_eval(key)\n                if not (\n                    isinstance(key, tuple)\n                    and len(key) == 3\n                    and all(isinstance(x, int) for x in key)\n                ):\n                    raise ValueError(\n                        f\"Invalid bucket key {k!r}. Expected tuple (H, W, T).\"\n                    )\n                out[(int(key[0]), int(key[1]), int(key[2]))] = float(val)\n            return out\n\n        if isinstance(v, dict):\n            return normalize(v)\n        if isinstance(v, str):\n            # Python literal (supports tuple keys)\n            parsed = ast.literal_eval(v)\n            if not isinstance(parsed, dict):\n                raise ValueError(\"Bucket config must parse to a dict.\")\n            return normalize(parsed)\n        raise ValueError(\"Unsupported value for --random-mm-bucket-config.\")\n\n    random_mm_group.add_argument(\n        \"--random-mm-bucket-config\",\n        type=_parse_mm_bucket_config,\n        default=RandomMultiModalDataset.DEFAULT_MM_ITEM_BUCKET_CONFIG,\n        help=(\n            \"The bucket config is a dictionary mapping a multimodal item\"\n            \"sampling configuration to a probability.\"\n            \"Currently allows for 2 modalities: images and videos. \"\n            \"An bucket key is a tuple of (height, width, num_frames)\"\n            \"The value is the probability of sampling that specific item. \"\n            \"Example: \"\n            \"--random-mm-bucket-config \"\n            \"{(256, 256, 1): 0.5, (720, 1280, 1): 0.4, (720, 1280, 16): 0.10} \"\n            \"First item: images with resolution 256x256 w.p. 0.5\"\n            \"Second item: images with resolution 720x1280 w.p. 0.4 \"\n            \"Third item: videos with resolution 720x1280 and 16 frames w.p. 0.1\"\n            \"OBS.: If the probabilities do not sum to 1, they are normalized.\"\n            \"OBS bis.: Only image sampling is supported for now.\"\n        ),\n    )\n\n    hf_group = parser.add_argument_group(\"hf dataset options\")\n    hf_group.add_argument(\n        \"--hf-subset\", type=str, default=None, help=\"Subset of the HF dataset.\"\n    )\n    hf_group.add_argument(\n        \"--hf-split\", type=str, default=None, help=\"Split of the HF dataset.\"\n    )\n    hf_group.add_argument(\n        \"--hf-name\",\n        type=str,\n        default=None,\n        help=(\n            \"Name of the dataset on HuggingFace \"\n            \"(e.g., 'lmarena-ai/VisionArena-Chat'). \"\n            \"Specify this if your dataset-path is a local path.\"\n        ),\n    )\n    hf_group.add_argument(\n        \"--hf-output-len\",\n        type=int,\n        default=None,\n        help=\"Output length for each request. Overrides the output lengths \"\n        \"from the sampled HF dataset.\",\n    )\n\n    prefix_repetition_group = parser.add_argument_group(\n        \"prefix repetition dataset options\"\n    )\n    prefix_repetition_group.add_argument(\n        \"--prefix-repetition-prefix-len\",\n        type=int,\n        default=256,\n        help=\"Number of prefix tokens per request, used only for prefix \"\n        \"repetition dataset.\",\n    )\n    prefix_repetition_group.add_argument(\n        \"--prefix-repetition-suffix-len\",\n        type=int,\n        default=256,\n        help=\"Number of suffix tokens per request, used only for prefix \"\n        \"repetition dataset. Total input length is prefix_len + suffix_len.\",\n    )\n    prefix_repetition_group.add_argument(\n        \"--prefix-repetition-num-prefixes\",\n        type=int,\n        default=10,\n        help=\"Number of prefixes to generate, used only for prefix repetition \"\n        \"dataset. Prompts per prefix is num_requests // num_prefixes.\",\n    )\n    prefix_repetition_group.add_argument(\n        \"--prefix-repetition-output-len\",\n        type=int,\n        default=128,\n        help=\"Number of output tokens per request, used only for prefix \"\n        \"repetition dataset.\",\n    )",
      "language": "sql"
    },
    {
      "code": "gen_prompt_decode_to_target_len(\n    tokenizer: TokenizerLike,\n    token_sequence: list[int],\n    target_token_len: int,\n    max_retry: int = 10,\n    add_special_tokens: bool = False,\n    rng: Generator | None = None,\n) -> tuple[str, list[int]]",
      "language": "typescript"
    },
    {
      "code": "gen_prompt_decode_to_target_len(\n    tokenizer: TokenizerLike,\n    token_sequence: list[int],\n    target_token_len: int,\n    max_retry: int = 10,\n    add_special_tokens: bool = False,\n    rng: Generator | None = None,\n) -> tuple[str, list[int]]",
      "language": "typescript"
    },
    {
      "code": "381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433",
      "language": "unknown"
    },
    {
      "code": "def gen_prompt_decode_to_target_len(\n    tokenizer: TokenizerLike,\n    token_sequence: list[int],\n    target_token_len: int,\n    max_retry: int = 10,\n    add_special_tokens: bool = False,\n    rng: np.random.Generator | None = None,\n) -> tuple[str, list[int]]:\n    \"\"\"\n    Ensure decoded-then-encoded prompt length matches the target token length.\n\n    This function decodes an initial token sequence to text and re-encodes it\n    , iteratively adjusting the token sequence length to match a target.\n    This is necessary because some tokenizers do not guarantee a 1:1 mapping\n    between consecutive tokens and the decoded-then-encoded sequence length.\n    For example, for GPT2Tokenizer:\n    [6880, 6881] -> ['Ġcalls', 'here'] ->\n    [1650, 939, 486] -> ['Ġcall', 'sh', 'ere']\n\n    Returns a tuple of the final prompt string and the adjusted token sequence.\n    \"\"\"\n    remain_num_try = max_retry\n    token_mismatch = 0\n    while True:\n        prompt = tokenizer.decode(token_sequence)\n        token_sequence = tokenizer.encode(prompt, add_special_tokens=add_special_tokens)\n        if remain_num_try <= 0:\n            if len(token_sequence) != target_token_len:\n                token_mismatch = len(token_sequence) - target_token_len\n            break\n\n        if len(token_sequence) == target_token_len:\n            break\n        elif len(token_sequence) < target_token_len:\n            if rng is not None:\n                extra_tokens = rng.integers(\n                    0,\n                    tokenizer.vocab_size,\n                    size=target_token_len - len(token_sequence),\n                ).tolist()\n            else:\n                extra_tokens = np.random.randint(\n                    0,\n                    tokenizer.vocab_size,\n                    size=target_token_len - len(token_sequence),\n                ).tolist()\n            token_sequence.extend(extra_tokens)\n        elif len(token_sequence) > target_token_len:\n            token_sequence = token_sequence[:target_token_len]\n\n        remain_num_try -= 1\n\n    return prompt, token_sequence, token_mismatch",
      "language": "json"
    },
    {
      "code": "def gen_prompt_decode_to_target_len(\n    tokenizer: TokenizerLike,\n    token_sequence: list[int],\n    target_token_len: int,\n    max_retry: int = 10,\n    add_special_tokens: bool = False,\n    rng: np.random.Generator | None = None,\n) -> tuple[str, list[int]]:\n    \"\"\"\n    Ensure decoded-then-encoded prompt length matches the target token length.\n\n    This function decodes an initial token sequence to text and re-encodes it\n    , iteratively adjusting the token sequence length to match a target.\n    This is necessary because some tokenizers do not guarantee a 1:1 mapping\n    between consecutive tokens and the decoded-then-encoded sequence length.\n    For example, for GPT2Tokenizer:\n    [6880, 6881] -> ['Ġcalls', 'here'] ->\n    [1650, 939, 486] -> ['Ġcall', 'sh', 'ere']\n\n    Returns a tuple of the final prompt string and the adjusted token sequence.\n    \"\"\"\n    remain_num_try = max_retry\n    token_mismatch = 0\n    while True:\n        prompt = tokenizer.decode(token_sequence)\n        token_sequence = tokenizer.encode(prompt, add_special_tokens=add_special_tokens)\n        if remain_num_try <= 0:\n            if len(token_sequence) != target_token_len:\n                token_mismatch = len(token_sequence) - target_token_len\n            break\n\n        if len(token_sequence) == target_token_len:\n            break\n        elif len(token_sequence) < target_token_len:\n            if rng is not None:\n                extra_tokens = rng.integers(\n                    0,\n                    tokenizer.vocab_size,\n                    size=target_token_len - len(token_sequence),\n                ).tolist()\n            else:\n                extra_tokens = np.random.randint(\n                    0,\n                    tokenizer.vocab_size,\n                    size=target_token_len - len(token_sequence),\n                ).tolist()\n            token_sequence.extend(extra_tokens)\n        elif len(token_sequence) > target_token_len:\n            token_sequence = token_sequence[:target_token_len]\n\n        remain_num_try -= 1\n\n    return prompt, token_sequence, token_mismatch",
      "language": "json"
    },
    {
      "code": "get_samples(\n    args, tokenizer: TokenizerLike\n) -> list[SampleRequest]",
      "language": "php"
    },
    {
      "code": "get_samples(\n    args, tokenizer: TokenizerLike\n) -> list[SampleRequest]",
      "language": "php"
    },
    {
      "code": "1641\n1642\n1643\n1644\n1645\n1646\n1647\n1648\n1649\n1650\n1651\n1652\n1653\n1654\n1655\n1656\n1657\n1658\n1659\n1660\n1661\n1662\n1663\n1664\n1665\n1666\n1667\n1668\n1669\n1670\n1671\n1672\n1673\n1674\n1675\n1676\n1677\n1678\n1679\n1680\n1681\n1682\n1683\n1684\n1685\n1686\n1687\n1688\n1689\n1690\n1691\n1692\n1693\n1694\n1695\n1696\n1697\n1698\n1699\n1700\n1701\n1702\n1703\n1704\n1705\n1706\n1707\n1708\n1709\n1710\n1711\n1712\n1713\n1714\n1715\n1716\n1717\n1718\n1719\n1720\n1721\n1722\n1723\n1724\n1725\n1726\n1727\n1728\n1729\n1730\n1731\n1732\n1733\n1734\n1735\n1736\n1737\n1738\n1739\n1740\n1741\n1742\n1743\n1744\n1745\n1746\n1747\n1748\n1749\n1750\n1751\n1752\n1753\n1754\n1755\n1756\n1757\n1758\n1759\n1760\n1761\n1762\n1763\n1764\n1765\n1766\n1767\n1768\n1769\n1770\n1771\n1772\n1773\n1774\n1775\n1776\n1777\n1778\n1779\n1780\n1781\n1782\n1783\n1784\n1785\n1786\n1787\n1788\n1789\n1790\n1791\n1792\n1793\n1794\n1795\n1796\n1797\n1798\n1799\n1800\n1801\n1802\n1803\n1804\n1805\n1806\n1807\n1808\n1809\n1810\n1811\n1812\n1813\n1814\n1815\n1816\n1817\n1818\n1819\n1820\n1821\n1822\n1823\n1824\n1825\n1826\n1827\n1828\n1829\n1830\n1831\n1832\n1833\n1834\n1835\n1836\n1837\n1838\n1839\n1840\n1841\n1842\n1843\n1844\n1845\n1846\n1847\n1848\n1849\n1850\n1851\n1852\n1853\n1854\n1855\n1856\n1857\n1858\n1859\n1860\n1861\n1862\n1863\n1864\n1865\n1866\n1867\n1868\n1869\n1870\n1871\n1872\n1873\n1874\n1875\n1876\n1877\n1878\n1879\n1880\n1881\n1882\n1883\n1884\n1885\n1886\n1887\n1888\n1889\n1890\n1891\n1892\n1893\n1894\n1895\n1896\n1897\n1898\n1899\n1900\n1901\n1902\n1903\n1904\n1905\n1906\n1907\n1908\n1909\n1910\n1911\n1912\n1913\n1914\n1915\n1916\n1917\n1918\n1919",
      "language": "unknown"
    },
    {
      "code": "def get_samples(args, tokenizer: TokenizerLike) -> list[SampleRequest]:\n    if not hasattr(args, \"request_id_prefix\"):\n        args.request_id_prefix = \"\"\n\n    if args.dataset_name == \"custom\":\n        dataset = CustomDataset(\n            dataset_path=args.dataset_path, disable_shuffle=args.disable_shuffle\n        )\n        input_requests = dataset.sample(\n            num_requests=args.num_prompts,\n            tokenizer=tokenizer,\n            output_len=args.custom_output_len,\n            skip_chat_template=args.skip_chat_template,\n            request_id_prefix=args.request_id_prefix,\n            no_oversample=args.no_oversample,\n        )\n\n    elif args.dataset_name == \"sonnet\":\n        dataset = SonnetDataset(\n            dataset_path=args.dataset_path, disable_shuffle=args.disable_shuffle\n        )\n        # For the \"sonnet\" dataset, formatting depends on the backend.\n        if args.backend == \"openai-chat\":\n            input_requests = dataset.sample(\n                num_requests=args.num_prompts,\n                input_len=args.sonnet_input_len,\n                output_len=args.sonnet_output_len,\n                prefix_len=args.sonnet_prefix_len,\n                tokenizer=tokenizer,\n                return_prompt_formatted=False,\n                request_id_prefix=args.request_id_prefix,\n                no_oversample=args.no_oversample,\n            )\n        else:\n            assert tokenizer.chat_template or tokenizer.default_chat_template, (\n                \"Tokenizer/model must have chat template for sonnet dataset.\"\n            )\n            input_requests = dataset.sample(\n                num_requests=args.num_prompts,\n                input_len=args.sonnet_input_len,\n                output_len=args.sonnet_output_len,\n                prefix_len=args.sonnet_prefix_len,\n                tokenizer=tokenizer,\n                return_prompt_formatted=True,\n                request_id_prefix=args.request_id_prefix,\n                no_oversample=args.no_oversample,\n            )\n\n    elif args.dataset_name == \"hf\":\n        # all following datasets are implemented from the\n        # HuggingFaceDataset base class\n        hf_kwargs = {}\n        if (\n            args.dataset_path in VisionArenaDataset.SUPPORTED_DATASET_PATHS\n            or args.hf_name in VisionArenaDataset.SUPPORTED_DATASET_PATHS\n        ):\n            dataset_class = VisionArenaDataset\n            args.hf_split = \"train\"\n            args.hf_subset = None\n        elif (\n            args.dataset_path in MMVUDataset.SUPPORTED_DATASET_PATHS\n            or args.hf_name in MMVUDataset.SUPPORTED_DATASET_PATHS\n        ):\n            dataset_class = MMVUDataset\n            args.hf_split = \"validation\"\n            args.hf_subset = None\n        elif (\n            args.dataset_path in InstructCoderDataset.SUPPORTED_DATASET_PATHS\n            or args.hf_name in InstructCoderDataset.SUPPORTED_DATASET_PATHS\n        ):\n            dataset_class = InstructCoderDataset\n            args.hf_split = \"train\"\n        elif (\n            args.dataset_path in MTBenchDataset.SUPPORTED_DATASET_PATHS\n            or args.hf_name in MTBenchDataset.SUPPORTED_DATASET_PATHS\n        ):\n            dataset_class = MTBenchDataset\n            args.hf_split = \"train\"\n        elif (\n            args.dataset_path in MultiModalConversationDataset.SUPPORTED_DATASET_PATHS\n            or args.hf_name in MultiModalConversationDataset.SUPPORTED_DATASET_PATHS\n        ):\n            dataset_class = MultiModalConversationDataset\n        elif (\n            args.dataset_path in ConversationDataset.SUPPORTED_DATASET_PATHS\n            or args.hf_name in ConversationDataset.SUPPORTED_DATASET_PATHS\n        ):\n            dataset_class = ConversationDataset\n        elif (\n            args.dataset_path in AIMODataset.SUPPORTED_DATASET_PATHS\n            or args.hf_name in AIMODataset.SUPPORTED_DATASET_PATHS\n        ):\n            dataset_class = AIMODataset\n            args.hf_split = \"train\"\n        elif (\n            args.dataset_path in NextEditPredictionDataset.SUPPORTED_DATASET_PATHS  # noqa: E501\n            or args.hf_name in NextEditPredictionDataset.SUPPORTED_DATASET_PATHS\n        ):\n            dataset_class = NextEditPredictionDataset\n            args.hf_split = \"train\"\n        elif (\n            args.dataset_path in ASRDataset.SUPPORTED_DATASET_PATHS\n            or args.hf_name in ASRDataset.SUPPORTED_DATASET_PATHS\n        ):\n            dataset_class = ASRDataset\n            args.hf_split = \"train\"\n        elif args.dataset_path in BlazeditDataset.SUPPORTED_DATASET_PATHS:\n            dataset_class = BlazeditDataset\n            args.hf_split = \"train\"\n            hf_kwargs = {\n                \"min_distance\": args.blazedit_min_distance,\n                \"max_distance\": args.blazedit_max_distance,\n            }\n        elif (\n            args.dataset_path in MLPerfDataset.SUPPORTED_DATASET_PATHS\n            or args.hf_name in MLPerfDataset.SUPPORTED_DATASET_PATHS\n        ):\n            dataset_class = MLPerfDataset\n            args.hf_split = \"train\"\n        elif (\n            args.dataset_path in MMStarDataset.SUPPORTED_DATASET_PATHS\n            or args.hf_name in MMStarDataset.SUPPORTED_DATASET_PATHS\n        ):\n            dataset_class = MMStarDataset\n            args.hf_split = \"val\"\n            args.hf_subset = None\n        else:\n            supported_datasets = set(\n                [\n                    dataset_name\n                    for cls in HuggingFaceDataset.__subclasses__()\n                    for dataset_name in cls.SUPPORTED_DATASET_PATHS\n                ]\n            )\n            raise ValueError(\n                f\"Unsupported dataset path: {args.dataset_path}. \"\n                \"Huggingface dataset only supports dataset_path\"\n                f\" from one of following: {supported_datasets}. \"\n                \"Please consider contributing if you would \"\n                \"like to add support for additional dataset formats.\"\n            )\n\n        if dataset_class.IS_MULTIMODAL and not (\n            args.backend in (\"openai-chat\", \"openai-audio\")\n            or \"embeddings-\" in args.backend\n        ):\n            # multi-modal benchmark is only available on OpenAI Chat\n            # endpoint-type.\n            raise ValueError(\n                \"Multi-modal content is only supported on 'openai-chat' and \"\n                \"'openai-audio' backends.\"\n            )\n        input_requests = dataset_class(\n            dataset_path=args.dataset_path,\n            dataset_subset=args.hf_subset,\n            dataset_split=args.hf_split,\n            random_seed=args.seed,\n            no_stream=args.no_stream,\n            hf_name=args.hf_name,\n            disable_shuffle=args.disable_shuffle,\n        ).sample(\n            num_requests=args.num_prompts,\n            tokenizer=tokenizer,\n            output_len=args.hf_output_len,\n            request_id_prefix=args.request_id_prefix,\n            no_oversample=args.no_oversample,\n            skip_chat_template=args.skip_chat_template,\n            **hf_kwargs,\n        )\n\n    else:\n        # For datasets that follow a similar structure, use a mapping.\n        dataset_mapping = {\n            \"spec_bench\": lambda: SpecBench(\n                dataset_path=args.dataset_path,\n                category=args.spec_bench_category,\n                disable_shuffle=args.disable_shuffle,\n            ).sample(\n                num_requests=args.num_prompts,\n                tokenizer=tokenizer,\n                output_len=args.spec_bench_output_len,\n                request_id_prefix=args.request_id_prefix,\n                no_oversample=args.no_oversample,\n            ),\n            \"sharegpt\": lambda: ShareGPTDataset(\n                random_seed=args.seed,\n                dataset_path=args.dataset_path,\n                disable_shuffle=args.disable_shuffle,\n            ).sample(\n                tokenizer=tokenizer,\n                num_requests=args.num_prompts,\n                output_len=args.sharegpt_output_len,\n                request_id_prefix=args.request_id_prefix,\n                no_oversample=args.no_oversample,\n            ),\n            \"burstgpt\": lambda: BurstGPTDataset(\n                random_seed=args.seed,\n                dataset_path=args.dataset_path,\n                disable_shuffle=args.disable_shuffle,\n            ).sample(\n                tokenizer=tokenizer,\n                num_requests=args.num_prompts,\n                request_id_prefix=args.request_id_prefix,\n                no_oversample=args.no_oversample,\n            ),\n            \"random\": lambda: RandomDataset(\n                random_seed=args.seed,\n                dataset_path=args.dataset_path,\n                disable_shuffle=args.disable_shuffle,\n            ).sample(\n                tokenizer=tokenizer,\n                num_requests=args.num_prompts,\n                prefix_len=args.random_prefix_len,\n                input_len=args.random_input_len,\n                output_len=args.random_output_len,\n                range_ratio=args.random_range_ratio,\n                request_id_prefix=args.request_id_prefix,\n                batchsize=args.random_batch_size,\n                no_oversample=args.no_oversample,\n            ),\n            \"random-mm\": lambda: RandomMultiModalDataset(\n                random_seed=args.seed,\n                dataset_path=args.dataset_path,\n                disable_shuffle=args.disable_shuffle,\n            ).sample(\n                tokenizer=tokenizer,\n                num_requests=args.num_prompts,\n                prefix_len=args.random_prefix_len,\n                range_ratio=args.random_range_ratio,\n                input_len=args.random_input_len,\n                output_len=args.random_output_len,\n                base_items_per_request=args.random_mm_base_items_per_request,\n                limit_mm_per_prompt=args.random_mm_limit_mm_per_prompt,\n                num_mm_items_range_ratio=args.random_mm_num_mm_items_range_ratio,\n                bucket_config=args.random_mm_bucket_config,\n                request_id_prefix=args.request_id_prefix,\n                no_oversample=args.no_oversample,\n            ),\n            \"random-rerank\": lambda: RandomDatasetForReranking(\n                random_seed=args.seed,\n                dataset_path=args.dataset_path,\n                disable_shuffle=args.disable_shuffle,\n            ).sample(\n                tokenizer=tokenizer,\n                num_requests=args.num_prompts,\n                input_len=args.random_input_len,\n                range_ratio=args.random_range_ratio,\n                request_id_prefix=args.request_id_prefix,\n                batchsize=args.random_batch_size,\n                is_reranker=not args.no_reranker,\n            ),\n            \"prefix_repetition\": lambda: PrefixRepetitionRandomDataset(\n                random_seed=args.seed,\n                dataset_path=args.dataset_path,\n                disable_shuffle=args.disable_shuffle,\n            ).sample(\n                tokenizer=tokenizer,\n                num_requests=args.num_prompts,\n                prefix_len=args.prefix_repetition_prefix_len,\n                suffix_len=args.prefix_repetition_suffix_len,\n                num_prefixes=args.prefix_repetition_num_prefixes,\n                output_len=args.prefix_repetition_output_len,\n                request_id_prefix=args.request_id_prefix,\n                no_oversample=args.no_oversample,\n            ),\n        }\n\n        try:\n            # Enforce endpoint compatibility for multimodal datasets.\n            if args.dataset_name == \"random-mm\" and args.backend not in [\"openai-chat\"]:\n                raise ValueError(\n                    \"Multi-modal content (images) is only supported on \"\n                    \"'openai-chat' backend.\"\n                )\n            input_requests = dataset_mapping[args.dataset_name]()\n        except KeyError as err:\n            raise ValueError(f\"Unknown dataset: {args.dataset_name}\") from err\n\n    return input_requests",
      "language": "json"
    },
    {
      "code": "def get_samples(args, tokenizer: TokenizerLike) -> list[SampleRequest]:\n    if not hasattr(args, \"request_id_prefix\"):\n        args.request_id_prefix = \"\"\n\n    if args.dataset_name == \"custom\":\n        dataset = CustomDataset(\n            dataset_path=args.dataset_path, disable_shuffle=args.disable_shuffle\n        )\n        input_requests = dataset.sample(\n            num_requests=args.num_prompts,\n            tokenizer=tokenizer,\n            output_len=args.custom_output_len,\n            skip_chat_template=args.skip_chat_template,\n            request_id_prefix=args.request_id_prefix,\n            no_oversample=args.no_oversample,\n        )\n\n    elif args.dataset_name == \"sonnet\":\n        dataset = SonnetDataset(\n            dataset_path=args.dataset_path, disable_shuffle=args.disable_shuffle\n        )\n        # For the \"sonnet\" dataset, formatting depends on the backend.\n        if args.backend == \"openai-chat\":\n            input_requests = dataset.sample(\n                num_requests=args.num_prompts,\n                input_len=args.sonnet_input_len,\n                output_len=args.sonnet_output_len,\n                prefix_len=args.sonnet_prefix_len,\n                tokenizer=tokenizer,\n                return_prompt_formatted=False,\n                request_id_prefix=args.request_id_prefix,\n                no_oversample=args.no_oversample,\n            )\n        else:\n            assert tokenizer.chat_template or tokenizer.default_chat_template, (\n                \"Tokenizer/model must have chat template for sonnet dataset.\"\n            )\n            input_requests = dataset.sample(\n                num_requests=args.num_prompts,\n                input_len=args.sonnet_input_len,\n                output_len=args.sonnet_output_len,\n                prefix_len=args.sonnet_prefix_len,\n                tokenizer=tokenizer,\n                return_prompt_formatted=True,\n                request_id_prefix=args.request_id_prefix,\n                no_oversample=args.no_oversample,\n            )\n\n    elif args.dataset_name == \"hf\":\n        # all following datasets are implemented from the\n        # HuggingFaceDataset base class\n        hf_kwargs = {}\n        if (\n            args.dataset_path in VisionArenaDataset.SUPPORTED_DATASET_PATHS\n            or args.hf_name in VisionArenaDataset.SUPPORTED_DATASET_PATHS\n        ):\n            dataset_class = VisionArenaDataset\n            args.hf_split = \"train\"\n            args.hf_subset = None\n        elif (\n            args.dataset_path in MMVUDataset.SUPPORTED_DATASET_PATHS\n            or args.hf_name in MMVUDataset.SUPPORTED_DATASET_PATHS\n        ):\n            dataset_class = MMVUDataset\n            args.hf_split = \"validation\"\n            args.hf_subset = None\n        elif (\n            args.dataset_path in InstructCoderDataset.SUPPORTED_DATASET_PATHS\n            or args.hf_name in InstructCoderDataset.SUPPORTED_DATASET_PATHS\n        ):\n            dataset_class = InstructCoderDataset\n            args.hf_split = \"train\"\n        elif (\n            args.dataset_path in MTBenchDataset.SUPPORTED_DATASET_PATHS\n            or args.hf_name in MTBenchDataset.SUPPORTED_DATASET_PATHS\n        ):\n            dataset_class = MTBenchDataset\n            args.hf_split = \"train\"\n        elif (\n            args.dataset_path in MultiModalConversationDataset.SUPPORTED_DATASET_PATHS\n            or args.hf_name in MultiModalConversationDataset.SUPPORTED_DATASET_PATHS\n        ):\n            dataset_class = MultiModalConversationDataset\n        elif (\n            args.dataset_path in ConversationDataset.SUPPORTED_DATASET_PATHS\n            or args.hf_name in ConversationDataset.SUPPORTED_DATASET_PATHS\n        ):\n            dataset_class = ConversationDataset\n        elif (\n            args.dataset_path in AIMODataset.SUPPORTED_DATASET_PATHS\n            or args.hf_name in AIMODataset.SUPPORTED_DATASET_PATHS\n        ):\n            dataset_class = AIMODataset\n            args.hf_split = \"train\"\n        elif (\n            args.dataset_path in NextEditPredictionDataset.SUPPORTED_DATASET_PATHS  # noqa: E501\n            or args.hf_name in NextEditPredictionDataset.SUPPORTED_DATASET_PATHS\n        ):\n            dataset_class = NextEditPredictionDataset\n            args.hf_split = \"train\"\n        elif (\n            args.dataset_path in ASRDataset.SUPPORTED_DATASET_PATHS\n            or args.hf_name in ASRDataset.SUPPORTED_DATASET_PATHS\n        ):\n            dataset_class = ASRDataset\n            args.hf_split = \"train\"\n        elif args.dataset_path in BlazeditDataset.SUPPORTED_DATASET_PATHS:\n            dataset_class = BlazeditDataset\n            args.hf_split = \"train\"\n            hf_kwargs = {\n                \"min_distance\": args.blazedit_min_distance,\n                \"max_distance\": args.blazedit_max_distance,\n            }\n        elif (\n            args.dataset_path in MLPerfDataset.SUPPORTED_DATASET_PATHS\n            or args.hf_name in MLPerfDataset.SUPPORTED_DATASET_PATHS\n        ):\n            dataset_class = MLPerfDataset\n            args.hf_split = \"train\"\n        elif (\n            args.dataset_path in MMStarDataset.SUPPORTED_DATASET_PATHS\n            or args.hf_name in MMStarDataset.SUPPORTED_DATASET_PATHS\n        ):\n            dataset_class = MMStarDataset\n            args.hf_split = \"val\"\n            args.hf_subset = None\n        else:\n            supported_datasets = set(\n                [\n                    dataset_name\n                    for cls in HuggingFaceDataset.__subclasses__()\n                    for dataset_name in cls.SUPPORTED_DATASET_PATHS\n                ]\n            )\n            raise ValueError(\n                f\"Unsupported dataset path: {args.dataset_path}. \"\n                \"Huggingface dataset only supports dataset_path\"\n                f\" from one of following: {supported_datasets}. \"\n                \"Please consider contributing if you would \"\n                \"like to add support for additional dataset formats.\"\n            )\n\n        if dataset_class.IS_MULTIMODAL and not (\n            args.backend in (\"openai-chat\", \"openai-audio\")\n            or \"embeddings-\" in args.backend\n        ):\n            # multi-modal benchmark is only available on OpenAI Chat\n            # endpoint-type.\n            raise ValueError(\n                \"Multi-modal content is only supported on 'openai-chat' and \"\n                \"'openai-audio' backends.\"\n            )\n        input_requests = dataset_class(\n            dataset_path=args.dataset_path,\n            dataset_subset=args.hf_subset,\n            dataset_split=args.hf_split,\n            random_seed=args.seed,\n            no_stream=args.no_stream,\n            hf_name=args.hf_name,\n            disable_shuffle=args.disable_shuffle,\n        ).sample(\n            num_requests=args.num_prompts,\n            tokenizer=tokenizer,\n            output_len=args.hf_output_len,\n            request_id_prefix=args.request_id_prefix,\n            no_oversample=args.no_oversample,\n            skip_chat_template=args.skip_chat_template,\n            **hf_kwargs,\n        )\n\n    else:\n        # For datasets that follow a similar structure, use a mapping.\n        dataset_mapping = {\n            \"spec_bench\": lambda: SpecBench(\n                dataset_path=args.dataset_path,\n                category=args.spec_bench_category,\n                disable_shuffle=args.disable_shuffle,\n            ).sample(\n                num_requests=args.num_prompts,\n                tokenizer=tokenizer,\n                output_len=args.spec_bench_output_len,\n                request_id_prefix=args.request_id_prefix,\n                no_oversample=args.no_oversample,\n            ),\n            \"sharegpt\": lambda: ShareGPTDataset(\n                random_seed=args.seed,\n                dataset_path=args.dataset_path,\n                disable_shuffle=args.disable_shuffle,\n            ).sample(\n                tokenizer=tokenizer,\n                num_requests=args.num_prompts,\n                output_len=args.sharegpt_output_len,\n                request_id_prefix=args.request_id_prefix,\n                no_oversample=args.no_oversample,\n            ),\n            \"burstgpt\": lambda: BurstGPTDataset(\n                random_seed=args.seed,\n                dataset_path=args.dataset_path,\n                disable_shuffle=args.disable_shuffle,\n            ).sample(\n                tokenizer=tokenizer,\n                num_requests=args.num_prompts,\n                request_id_prefix=args.request_id_prefix,\n                no_oversample=args.no_oversample,\n            ),\n            \"random\": lambda: RandomDataset(\n                random_seed=args.seed,\n                dataset_path=args.dataset_path,\n                disable_shuffle=args.disable_shuffle,\n            ).sample(\n                tokenizer=tokenizer,\n                num_requests=args.num_prompts,\n                prefix_len=args.random_prefix_len,\n                input_len=args.random_input_len,\n                output_len=args.random_output_len,\n                range_ratio=args.random_range_ratio,\n                request_id_prefix=args.request_id_prefix,\n                batchsize=args.random_batch_size,\n                no_oversample=args.no_oversample,\n            ),\n            \"random-mm\": lambda: RandomMultiModalDataset(\n                random_seed=args.seed,\n                dataset_path=args.dataset_path,\n                disable_shuffle=args.disable_shuffle,\n            ).sample(\n                tokenizer=tokenizer,\n                num_requests=args.num_prompts,\n                prefix_len=args.random_prefix_len,\n                range_ratio=args.random_range_ratio,\n                input_len=args.random_input_len,\n                output_len=args.random_output_len,\n                base_items_per_request=args.random_mm_base_items_per_request,\n                limit_mm_per_prompt=args.random_mm_limit_mm_per_prompt,\n                num_mm_items_range_ratio=args.random_mm_num_mm_items_range_ratio,\n                bucket_config=args.random_mm_bucket_config,\n                request_id_prefix=args.request_id_prefix,\n                no_oversample=args.no_oversample,\n            ),\n            \"random-rerank\": lambda: RandomDatasetForReranking(\n                random_seed=args.seed,\n                dataset_path=args.dataset_path,\n                disable_shuffle=args.disable_shuffle,\n            ).sample(\n                tokenizer=tokenizer,\n                num_requests=args.num_prompts,\n                input_len=args.random_input_len,\n                range_ratio=args.random_range_ratio,\n                request_id_prefix=args.request_id_prefix,\n                batchsize=args.random_batch_size,\n                is_reranker=not args.no_reranker,\n            ),\n            \"prefix_repetition\": lambda: PrefixRepetitionRandomDataset(\n                random_seed=args.seed,\n                dataset_path=args.dataset_path,\n                disable_shuffle=args.disable_shuffle,\n            ).sample(\n                tokenizer=tokenizer,\n                num_requests=args.num_prompts,\n                prefix_len=args.prefix_repetition_prefix_len,\n                suffix_len=args.prefix_repetition_suffix_len,\n                num_prefixes=args.prefix_repetition_num_prefixes,\n                output_len=args.prefix_repetition_output_len,\n                request_id_prefix=args.request_id_prefix,\n                no_oversample=args.no_oversample,\n            ),\n        }\n\n        try:\n            # Enforce endpoint compatibility for multimodal datasets.\n            if args.dataset_name == \"random-mm\" and args.backend not in [\"openai-chat\"]:\n                raise ValueError(\n                    \"Multi-modal content (images) is only supported on \"\n                    \"'openai-chat' backend.\"\n                )\n            input_requests = dataset_mapping[args.dataset_name]()\n        except KeyError as err:\n            raise ValueError(f\"Unknown dataset: {args.dataset_name}\") from err\n\n    return input_requests",
      "language": "json"
    },
    {
      "code": "is_valid_sequence(\n    prompt_len: int,\n    output_len: int,\n    min_len: int = 4,\n    max_prompt_len: int = 1024,\n    max_total_len: int = 2048,\n    skip_min_output_len_check: bool = False,\n) -> bool",
      "language": "typescript"
    },
    {
      "code": "is_valid_sequence(\n    prompt_len: int,\n    output_len: int,\n    min_len: int = 4,\n    max_prompt_len: int = 1024,\n    max_total_len: int = 2048,\n    skip_min_output_len_check: bool = False,\n) -> bool",
      "language": "typescript"
    },
    {
      "code": "262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286",
      "language": "unknown"
    },
    {
      "code": "def is_valid_sequence(\n    prompt_len: int,\n    output_len: int,\n    min_len: int = 4,\n    max_prompt_len: int = 1024,\n    max_total_len: int = 2048,\n    skip_min_output_len_check: bool = False,\n) -> bool:\n    \"\"\"\n    Validate a sequence based on prompt and output lengths.\n\n    Default pruning criteria are copied from the original `sample_hf_requests`\n    and `sample_sharegpt_requests` functions in benchmark_serving.py, as well as\n    from `sample_requests` in benchmark_throughput.py.\n    \"\"\"\n    # Check for invalid conditions\n    prompt_too_short = prompt_len < min_len\n    output_too_short = (not skip_min_output_len_check) and (output_len < min_len)\n    prompt_too_long = prompt_len > max_prompt_len\n    combined_too_long = (prompt_len + output_len) > max_total_len\n\n    # Return True if none of the invalid conditions are met\n    return not (\n        prompt_too_short or output_too_short or prompt_too_long or combined_too_long\n    )",
      "language": "typescript"
    },
    {
      "code": "def is_valid_sequence(\n    prompt_len: int,\n    output_len: int,\n    min_len: int = 4,\n    max_prompt_len: int = 1024,\n    max_total_len: int = 2048,\n    skip_min_output_len_check: bool = False,\n) -> bool:\n    \"\"\"\n    Validate a sequence based on prompt and output lengths.\n\n    Default pruning criteria are copied from the original `sample_hf_requests`\n    and `sample_sharegpt_requests` functions in benchmark_serving.py, as well as\n    from `sample_requests` in benchmark_throughput.py.\n    \"\"\"\n    # Check for invalid conditions\n    prompt_too_short = prompt_len < min_len\n    output_too_short = (not skip_min_output_len_check) and (output_len < min_len)\n    prompt_too_long = prompt_len > max_prompt_len\n    combined_too_long = (prompt_len + output_len) > max_total_len\n\n    # Return True if none of the invalid conditions are met\n    return not (\n        prompt_too_short or output_too_short or prompt_too_long or combined_too_long\n    )",
      "language": "typescript"
    },
    {
      "code": "lora_path_on_disk(lora_path: str) -> str",
      "language": "php"
    },
    {
      "code": "lora_path_on_disk(lora_path: str) -> str",
      "language": "php"
    },
    {
      "code": "289\n290\n291",
      "language": "unknown"
    },
    {
      "code": "@cache\ndef lora_path_on_disk(lora_path: str) -> str:\n    return get_adapter_absolute_path(lora_path)",
      "language": "python"
    },
    {
      "code": "@cache\ndef lora_path_on_disk(lora_path: str) -> str:\n    return get_adapter_absolute_path(lora_path)",
      "language": "python"
    },
    {
      "code": "process_image(image: Any) -> Mapping[str, Any]",
      "language": "php"
    },
    {
      "code": "process_image(image: Any) -> Mapping[str, Any]",
      "language": "php"
    },
    {
      "code": "298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341",
      "language": "unknown"
    },
    {
      "code": "def process_image(image: Any) -> Mapping[str, Any]:\n    \"\"\"\n    Process a single image input and return a multimedia content dictionary.\n\n    Supports the following input types:\n\n    1. Dictionary with raw image bytes: - Expects a dict with a 'bytes' key\n       containing raw image data.  - Loads the bytes as a PIL.Image.Image.\n\n    2. PIL.Image.Image input: - Converts the image to RGB.  - Saves the image as\n       a JPEG in memory.  - Encodes the JPEG data as a base64 string.  - Returns\n       a dictionary with the image as a base64 data URL.\n\n    3. String input: - Treats the string as a URL or local file path.  -\n       Prepends \"file://\" if the string doesn't start with \"http://\" or\n       \"file://\".  - Returns a dictionary with the image URL.\n\n    Raises:\n        ValueError: If the input is not a supported type.\n    \"\"\"\n    if isinstance(image, dict) and \"bytes\" in image:\n        image = Image.open(BytesIO(image[\"bytes\"]))\n    if isinstance(image, Image.Image):\n        image = convert_image_mode(image, \"RGB\")\n        with io.BytesIO() as image_data:\n            image.save(image_data, format=\"JPEG\")\n            image_base64 = base64.b64encode(image_data.getvalue()).decode(\"utf-8\")\n        return {\n            \"type\": \"image_url\",\n            \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_base64}\"},\n        }\n\n    if isinstance(image, str):\n        image_url = (\n            image\n            if image.startswith((\"http://\", \"https://\", \"file://\"))\n            else f\"file://{image}\"\n        )\n        return {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}}\n\n    raise ValueError(\n        f\"Invalid image input {image}. Must be a PIL.Image.Image\"\n        \" or str or dictionary with raw image bytes.\"\n    )",
      "language": "typescript"
    },
    {
      "code": "def process_image(image: Any) -> Mapping[str, Any]:\n    \"\"\"\n    Process a single image input and return a multimedia content dictionary.\n\n    Supports the following input types:\n\n    1. Dictionary with raw image bytes: - Expects a dict with a 'bytes' key\n       containing raw image data.  - Loads the bytes as a PIL.Image.Image.\n\n    2. PIL.Image.Image input: - Converts the image to RGB.  - Saves the image as\n       a JPEG in memory.  - Encodes the JPEG data as a base64 string.  - Returns\n       a dictionary with the image as a base64 data URL.\n\n    3. String input: - Treats the string as a URL or local file path.  -\n       Prepends \"file://\" if the string doesn't start with \"http://\" or\n       \"file://\".  - Returns a dictionary with the image URL.\n\n    Raises:\n        ValueError: If the input is not a supported type.\n    \"\"\"\n    if isinstance(image, dict) and \"bytes\" in image:\n        image = Image.open(BytesIO(image[\"bytes\"]))\n    if isinstance(image, Image.Image):\n        image = convert_image_mode(image, \"RGB\")\n        with io.BytesIO() as image_data:\n            image.save(image_data, format=\"JPEG\")\n            image_base64 = base64.b64encode(image_data.getvalue()).decode(\"utf-8\")\n        return {\n            \"type\": \"image_url\",\n            \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_base64}\"},\n        }\n\n    if isinstance(image, str):\n        image_url = (\n            image\n            if image.startswith((\"http://\", \"https://\", \"file://\"))\n            else f\"file://{image}\"\n        )\n        return {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}}\n\n    raise ValueError(\n        f\"Invalid image input {image}. Must be a PIL.Image.Image\"\n        \" or str or dictionary with raw image bytes.\"\n    )",
      "language": "typescript"
    },
    {
      "code": "process_video(video: Any) -> Mapping[str, Any]",
      "language": "php"
    },
    {
      "code": "process_video(video: Any) -> Mapping[str, Any]",
      "language": "php"
    },
    {
      "code": "344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378",
      "language": "unknown"
    },
    {
      "code": "def process_video(video: Any) -> Mapping[str, Any]:\n    \"\"\"\n    Process a single video input and return a multimedia content dictionary.\n\n    Supports the following input types:\n\n    1. Dictionary with raw video bytes: - Expects a dict with a 'bytes' key\n       containing raw video data.\n\n    2. String input: - Treats the string as a URL or local file path.  -\n       Prepends \"file://\" if the string doesn't start with \"http://\" or\n       \"file://\".  - Returns a dictionary with the image URL.\n\n    Raises:\n        ValueError: If the input is not a supported type.\n    \"\"\"\n    if isinstance(video, dict) and \"bytes\" in video:\n        video_bytes = video[\"bytes\"]\n        video_base64 = base64.b64encode(video_bytes).decode(\"utf-8\")\n        return {\n            \"type\": \"video_url\",\n            \"video_url\": {\"url\": f\"data:video/mp4;base64,{video_base64}\"},\n        }\n\n    if isinstance(video, str):\n        video_url = (\n            video\n            if video.startswith((\"http://\", \"https://\", \"file://\"))\n            else f\"file://{video}\"\n        )\n        return {\"type\": \"video_url\", \"video_url\": {\"url\": video_url}}\n\n    raise ValueError(\n        f\"Invalid video input {video}. Must be a string of local path/remote url, or a dictionary with raw video bytes in the form of `{{'bytes': raw_video_bytes}}`.\"  # noqa: E501\n    )",
      "language": "typescript"
    },
    {
      "code": "def process_video(video: Any) -> Mapping[str, Any]:\n    \"\"\"\n    Process a single video input and return a multimedia content dictionary.\n\n    Supports the following input types:\n\n    1. Dictionary with raw video bytes: - Expects a dict with a 'bytes' key\n       containing raw video data.\n\n    2. String input: - Treats the string as a URL or local file path.  -\n       Prepends \"file://\" if the string doesn't start with \"http://\" or\n       \"file://\".  - Returns a dictionary with the image URL.\n\n    Raises:\n        ValueError: If the input is not a supported type.\n    \"\"\"\n    if isinstance(video, dict) and \"bytes\" in video:\n        video_bytes = video[\"bytes\"]\n        video_base64 = base64.b64encode(video_bytes).decode(\"utf-8\")\n        return {\n            \"type\": \"video_url\",\n            \"video_url\": {\"url\": f\"data:video/mp4;base64,{video_base64}\"},\n        }\n\n    if isinstance(video, str):\n        video_url = (\n            video\n            if video.startswith((\"http://\", \"https://\", \"file://\"))\n            else f\"file://{video}\"\n        )\n        return {\"type\": \"video_url\", \"video_url\": {\"url\": video_url}}\n\n    raise ValueError(\n        f\"Invalid video input {video}. Must be a string of local path/remote url, or a dictionary with raw video bytes in the form of `{{'bytes': raw_video_bytes}}`.\"  # noqa: E501\n    )",
      "language": "typescript"
    }
  ],
  "patterns": [
    {
      "description": "vLLM GitHub Home User Guide User Guide Getting Started Getting Started Quickstart Installation Installation GPU CPU TPU Examples Examples Offline inference Offline inference Async LLM Streaming Audio Language Automatic Prefix Caching Basic Batch LLM Inference Chat With Tools Context Extension Data Parallel Disaggregated Prefill V1 Disaggregated Prefill Encoder Decoder Multimodal KV Load Failure Recovery Test LLM Engine Example LLM Engine Reset Kv Load Sharded State Logits Processor LoRA With Quantization Inference Metrics Mistral-Small MLPSpeculator MultiLoRA Inference Offline Inference with the OpenAI Batch file format Prefix Caching Prompt Embed Inference Qwen2.5-Omni Offline Inference Examples Qwen3 Omni Qwen 1M Reproducibility RLHF RLHF Colocate RLHF Online Quant RLHF Utils Save Sharded State Simple Profiling Skip Loading Weights In Engine Init Spec Decode Structured Outputs Torchrun Dp Example Torchrun Example Vision Language Vision Language Multi Image Online serving Online serving API Client Helm Charts Monitoring Dashboards Disaggregated Encoder Disaggregated Prefill Disaggregated Serving Disaggregated Serving P2P Nccl Xpyd Elastic Ep Gradio OpenAI Chatbot Webserver Gradio Webserver Kv Events Subscriber Multi-Node-Serving Multi Instance Data Parallel OpenAI Chat Completion Client OpenAI Chat Completion Client For Multimodal OpenAI Chat Completion Client With Tools OpenAI Chat Completion Client With Tools Required OpenAI Chat Completion Client With Tools Xlam OpenAI Chat Completion Client With Tools Xlam Streaming OpenAI Chat Completion Tool Calls With Reasoning OpenAI Chat Completion With Reasoning OpenAI Chat Completion With Reasoning Streaming OpenAI Completion Client OpenAI Responses Client OpenAI Responses Client With Mcp Tools OpenAI Responses Client With Tools OpenAI Transcription Client OpenAI Translation Client Setup OpenTelemetry POC Prometheus and Grafana Prompt Embed Inference With OpenAI Client Ray Serve Deepseek Retrieval Augmented Generation With Langchain Retrieval Augmented Generation With Llamaindex Run Cluster Sagemaker-Entrypoint Streamlit OpenAI Chatbot Webserver Structured Outputs Token Generation Client Utils Others Others LMCache Examples Logging Configuration Tensorize vLLM Model Pooling Pooling Classify Embed Plugin Pooling Score Token Classify Token Embed General General vLLM V1 Frequently Asked Questions Production Metrics Reproducibility Security Troubleshooting Usage Stats Collection Inference and Serving Inference and Serving Offline Inference OpenAI-Compatible Server Context Parallel Deployment Data Parallel Deployment Troubleshooting distributed deployments Expert Parallel Deployment Parallelism and Scaling Integrations Integrations LangChain LlamaIndex Deployment Deployment Using Docker Using Kubernetes Using Nginx Frameworks Frameworks Anyscale AnythingLLM AutoGen BentoML Cerebrium Chatbox Dify dstack Haystack Helm Hugging Face Inference Endpoints LiteLLM Lobe Chat LWS Modal Open WebUI Retrieval-Augmented Generation SkyPilot Streamlit NVIDIA Triton Integrations Integrations KAITO KServe Kthena KubeAI KubeRay Llama Stack llm-d llmaz Production stack Training Training Reinforcement Learning from Human Feedback Transformers Reinforcement Learning Configuration Configuration Conserving Memory Engine Arguments Environment Variables Model Resolution Optimization and Tuning Server Arguments TPU Models Models Supported Models Generative Models Pooling Models Extensions Extensions Loading Model weights with fastsafetensors Loading models with Run:ai Model Streamer Loading models with CoreWeave's Tensorizer Hardware Supported Models Hardware Supported Models CPU - Intel® Xeon® XPU - Intel® GPUs TPU Features Features Automatic Prefix Caching Batch Invariance Custom Arguments Custom Logits Processors Disaggregated Encoder Disaggregated Prefilling (experimental) Interleaved Thinking LoRA Adapters MooncakeConnector Usage Guide Multimodal Inputs NixlConnector Usage Guide Prompt Embedding Inputs Reasoning Outputs Sleep Mode Speculative Decoding Structured Outputs Tool Calling Quantization Quantization AutoAWQ AutoRound BitBLAS BitsAndBytes FP8 W8A8 GGUF GPTQModel FP8 INC INT4 W4A16 INT8 W8A8 NVIDIA Model Optimizer Quantized KV Cache AMD Quark TorchAO Developer Guide Developer Guide General General Deprecation Policy Dockerfile Incremental Compilation Workflow Profiling vLLM Vulnerability Management Model Implementation Model Implementation Basic Model Registering a Model Unit Testing Multi-Modal Support Speech-to-Text (Transcription/Translation) Support CI CI CI Failures Nightly Builds of vLLM Wheels Update PyTorch version on vLLM OSS CI/CD Design Documents Design Documents Plugins Plugins IO Processor Plugins LoRA Resolver Plugins Plugin System Architecture Overview CUDA Graphs Dual Batch Overlap How to debug the vLLM-torch.compile integration Fused MoE Modular Kernel Integration with Hugging Face Hybrid KV Cache Manager Logits Processors Metrics Multi-Modal Data Processing Fused MoE Kernel Features Python Multiprocessing Optimization levels P2P NCCL Connector Paged Attention Automatic Prefix Caching torch.compile integration Benchmarking Benchmarking Benchmark CLI Parameter Sweeps Performance Dashboard API Reference API Reference vllm vllm beam_search collect_env connections env_override envs forward_context logger logits_process logprobs outputs pooling_params sampling_params scalar_type scripts sequence tasks tracing version assets assets audio base image video attention attention layer selector backends backends abstract registry utils layers layers chunked_local_attention cross_attention encoder_only_attention mm_encoder_attention ops ops chunked_prefill_paged_decode common flashmla merge_attn_states paged_attn pallas_kv_cache_update prefix_prefill rocm_aiter_mla_sparse triton_decode_attention triton_merge_attn_states triton_reshape_and_cache_flash triton_unified_attention vit_attn_wrappers utils utils fa_utils kv_sharing_utils kv_transfer_utils benchmarks benchmarks datasets datasets Table of contents datasets logger lora_tokenizer_cache zeta_prompt AIMODataset SUPPORTED_DATASET_PATHS sample ASRDataset DEFAULT_OUTPUT_LEN IS_MULTIMODAL SUPPORTED_DATASET_PATHS TRANSCRIPTION_PREAMBLE skip_long_audios sample BenchmarkDataset DEFAULT_SEED IS_MULTIMODAL data dataset_path disable_shuffle random_seed __init__ apply_multimodal_chat_transformation get_random_lora_request load_data maybe_oversample_requests sample BlazeditDataset DEFAULT_OUTPUT_LEN SUPPORTED_DATASET_PATHS sample BurstGPTDataset __init__ _sample_loaded_data load_data sample ConversationDataset IS_MULTIMODAL SUPPORTED_DATASET_PATHS sample CustomDataset __init__ load_data sample HuggingFaceDataset SUPPORTED_DATASET_PATHS dataset_split dataset_subset hf_name load_stream __init__ load_data InstructCoderDataset DEFAULT_OUTPUT_LEN SUPPORTED_DATASET_PATHS sample MLPerfDataset SUPPORTED_DATASET_PATHS sample MMStarDataset DEFAULT_OUTPUT_LEN IS_MULTIMODAL SUPPORTED_DATASET_PATHS sample MMVUDataset DEFAULT_OUTPUT_LEN SUPPORTED_DATASET_PATHS sample MTBenchDataset DEFAULT_OUTPUT_LEN SUPPORTED_DATASET_PATHS sample MultiModalConversationDataset IS_MULTIMODAL SUPPORTED_DATASET_PATHS sample NextEditPredictionDataset MAPPING_PROMPT_FUNCS SUPPORTED_DATASET_PATHS sample PrefixRepetitionRandomDataset DEFAULT_NUM_PREFIXES DEFAULT_OUTPUT_LEN DEFAULT_PREFIX_LEN DEFAULT_SUFFIX_LEN __init__ sample RandomDataset DEFAULT_INPUT_LEN DEFAULT_OUTPUT_LEN DEFAULT_PREFIX_LEN DEFAULT_RANGE_RATIO _rng __init__ generate_token_sequence get_prefix get_sampling_params sample RandomDatasetForReranking __init__ sample RandomMultiModalDataset DEFAULT_BASE_ITEMS_PER_REQUEST DEFAULT_ENABLE_MULTIMODAL_CHAT DEFAULT_LIMIT_MM_PER_PROMPT DEFAULT_MM_ITEM_BUCKET_CONFIG DEFAULT_NUM_MM_ITEMS_RANGE_RATIO IS_MULTIMODAL __init__ generate_mm_item generate_synthetic_image generate_synthetic_video get_mm_item_iterator get_mm_item_sampling_params map_config_to_modality normalize_bucket_config sample SampleRequest expected_output_len lora_request multi_modal_data prompt prompt_len request_id __init__ ShareGPTDataset __init__ load_data sample SonnetDataset DEFAULT_INPUT_LEN DEFAULT_OUTPUT_LEN DEFAULT_PREFIX_LEN __init__ load_data sample SpecBench category __init__ load_data sample VisionArenaDataset DEFAULT_OUTPUT_LEN IS_MULTIMODAL SUPPORTED_DATASET_PATHS sample _ValidateDatasetArgs __call__ _format_zeta_prompt add_dataset_parser gen_prompt_decode_to_target_len get_samples is_valid_sequence lora_path_on_disk process_image process_video latency serve startup throughput lib lib endpoint_request_func ready_checker utils sweep sweep cli param_sweep plot plot_pareto serve serve_sla server sla_sweep utils compilation compilation activation_quant_fusion backends base_static_graph caching collective_fusion compiler_interface counter cuda_graph decorators fix_functionalization fusion fusion_attn fx_utils inductor_pass matcher_utils monitor noop_elimination partition_rules pass_manager piecewise_backend post_cleanup qk_norm_rope_fusion rocm_aiter_fusion sequence_parallelism torch25_custom_graph_pass vllm_inductor_pass wrapper config config attention cache compilation device ec_transfer kv_events kv_transfer load lora model multimodal observability parallel pooler profiler scheduler speculative speech_to_text structured_outputs utils vllm device_allocator device_allocator cumem distributed distributed communication_op kv_events parallel_state tpu_distributed_utils utils device_communicators device_communicators all2all all_reduce_utils base_device_communicator cpu_communicator cuda_communicator cuda_wrapper custom_all_reduce mnnvl_compat pynccl pynccl_allocator pynccl_wrapper quick_all_reduce ray_communicator shm_broadcast shm_object_storage symm_mem tpu_communicator xpu_communicator ec_transfer ec_transfer ec_transfer_state ec_connector ec_connector base example_connector factory eplb eplb async_worker eplb_state rebalance_execute policy policy abstract default kv_transfer kv_transfer kv_transfer_state kv_connector kv_connector base factory utils v1 v1 base decode_bench_connector example_connector lmcache_connector lmcache_mp_connector metrics mooncake_connector multi_connector nixl_connector offloading_connector lmcache_integration lmcache_integration multi_process_adapter utils vllm_v1_adapter p2p p2p p2p_nccl_connector p2p_nccl_engine tensor_memory_pool engine engine arg_utils async_llm_engine llm_engine protocol entrypoints entrypoints api_server chat_utils constants context launcher llm logger renderer responses_utils score_utils ssl tool tool_server utils anthropic anthropic protocol serving_messages cli cli collect_env main openai run_batch serve types benchmark benchmark base latency main serve startup sweep throughput openai openai api_server cli_args orca_metrics protocol run_batch serving_chat serving_chat_stream_harmony serving_completion serving_engine serving_models serving_responses serving_transcription speech_to_text utils parser parser harmony_utils responses_parser pooling pooling classify classify api_router protocol serving embed embed api_router conftest protocol serving pooling pooling api_router protocol serving score score api_router protocol serving sagemaker sagemaker routes serve serve cache cache api_router disagg disagg api_router protocol serving elastic_ep elastic_ep api_router middleware instrumentator instrumentator health metrics server_info lora lora api_router profile profile api_router rlhf rlhf api_router rpc rpc api_router sleep sleep api_router tokenize tokenize api_router serving inputs inputs data parse preprocess logging_utils logging_utils dump_input formatter lazy log_time lora lora lora_model lora_weights model_manager peft_helper request resolver utils worker_manager layers layers base base_linear column_parallel_linear fused_moe logits_processor replicated_linear row_parallel_linear utils vocal_parallel_embedding ops ops ipex_ops ipex_ops lora_ops torch_ops torch_ops lora_ops triton_ops triton_ops fused_moe_lora_op kernel_utils lora_expand_op lora_kernel_metadata lora_shrink_op utils xla_ops xla_ops lora_ops punica_wrapper punica_wrapper punica_base punica_cpu punica_gpu punica_selector punica_tpu punica_xpu utils model_executor model_executor custom_op parameter utils layers layers activation attention_layer_base batch_invariant conv kda layernorm lightning_attn linear logits_processor mla pooler resampler utils vocab_parallel_embedding fla fla ops ops chunk chunk_delta_h chunk_o chunk_scaled_dot_kkt cumsum fused_recurrent index kda l2norm layernorm_guard op solve_tril utils wy_fast fused_moe fused_moe all2all_utils batched_deep_gemm_moe config cpu_fused_moe cutlass_moe deep_gemm_moe deep_gemm_utils deepep_ht_prepare_finalize deepep_ll_prepare_finalize flashinfer_cutedsl_moe flashinfer_cutlass_moe flashinfer_cutlass_prepare_finalize flashinfer_trtllm_moe fused_batched_moe fused_marlin_moe fused_moe fused_moe_method_base fused_moe_modular_method gpt_oss_triton_kernels_moe layer modular_kernel moe_align_block_size moe_pallas moe_permute_unpermute moe_torch_iterative pplx_prepare_finalize prepare_finalize rocm_aiter_fused_moe routing_simulator shared_fused_moe topk_weight_and_reduce triton_deep_gemm_moe trtllm_moe unquantized_fused_moe_method utils zero_expert_fused_moe mamba mamba abstract linear_attn mamba_mixer mamba_mixer2 mamba_utils short_conv ops ops causal_conv1d layernorm_gated mamba_ssm ssd_bmm ssd_chunk_scan ssd_chunk_state ssd_combined ssd_state_passing quantization quantization auto_round awq awq_marlin awq_triton base_config bitblas bitsandbytes cpu_wna16 deepspeedfp experts_int8 fbgemm_fp8 fp8 fp_quant gguf gptq gptq_bitblas gptq_marlin gptq_marlin_24 hqq_marlin inc input_quant_fp8 ipex_quant kv_cache modelopt moe_wna16 mxfp4 petit ptpc_fp8 qutlass_utils rtn schema torchao tpu_int8 compressed_tensors compressed_tensors compressed_tensors compressed_tensors_moe triton_scaled_mm utils schemes schemes compressed_tensors_24 compressed_tensors_scheme compressed_tensors_w4a4_nvfp4 compressed_tensors_w4a8_fp8 compressed_tensors_w4a8_int compressed_tensors_w4a16_24 compressed_tensors_w4a16_nvfp4 compressed_tensors_w8a8_fp8 compressed_tensors_w8a8_int8 compressed_tensors_w8a16_fp8 compressed_tensors_wNa16 transform transform linear module utils schemes schemes linear_qutlass_nvfp4 kernels kernels mixed_precision mixed_precision allspark bitblas conch cutlass dynamic_4bit exllama MPLinearKernel machete marlin xpu scaled_mm scaled_mm aiter cpu cutlass ScaledMMLinearKernel triton xla quark quark quark quark_moe utils schemes schemes quark_ocp_mx quark_scheme quark_w8a8_fp8 quark_w8a8_int8 utils utils allspark_utils bitblas_utils flashinfer_fp4_moe flashinfer_utils fp8_utils gptq_utils int8_utils layer_utils machete_utils marlin_utils marlin_utils_fp4 marlin_utils_fp8 marlin_utils_test marlin_utils_test_24 mxfp4_utils mxfp6_utils mxfp8_utils nvfp4_emulation_utils nvfp4_moe_support ocp_mx_utils petit_utils quant_utils w8a8_utils rotary_embedding rotary_embedding base common deepseek_scaling_rope dual_chunk_rope dynamic_ntk_alpha_rope dynamic_ntk_scaling_rope ernie45_vl_rope linear_scaling_rope llama3_rope llama4_vision_rope mrope ntk_scaling_rope phi3_long_rope_scaled_rope xdrope yarn_scaling_rope model_loader model_loader base_loader bitsandbytes_loader default_loader dummy_loader gguf_loader online_quantization runai_streamer_loader sharded_state_loader tensorizer tensorizer_loader tpu utils weight_utils models models adapters afmoe aimv2 apertus arcee arctic aria audioflamingo3 aya_vision bagel baichuan bailing_moe bamba bee bert bert_with_rope blip blip2 bloom chameleon chatglm clip cohere2_vision commandr config dbrx deepencoder deepseek_eagle deepseek_mtp deepseek_ocr deepseek_v2 deepseek_vl2 dots1 dots_ocr ernie45 ernie45_moe ernie45_vl ernie45_vl_moe ernie_mtp exaone exaone4 fairseq2_llama falcon falcon_h1 flex_olmo fuyu gemma gemma2 gemma3 gemma3_mm gemma3n gemma3n_mm glm glm4 glm4_1v glm4_moe glm4_moe_mtp glm4v gpt2 gpt_bigcode gpt_j gpt_neox gpt_oss granite granite_speech granitemoe granitemoehybrid granitemoeshared gritlm grok1 h2ovl hunyuan_v1 hunyuan_vision hyperclovax_vision idefics2_vision_model idefics3 interfaces interfaces_base intern_vit internlm2 internlm2_ve interns1 interns1_vit internvl jais jais2 jamba jina_vl keye keye_vl1_5 kimi_linear kimi_vl lfm2 lfm2_moe lightonocr llama llama4 llama4_eagle llama_eagle llama_eagle3 llava llava_next llava_next_video llava_onevision longcat_flash longcat_flash_mtp mamba mamba2 medusa midashenglm mimo mimo_mtp mimo_v2_flash minicpm minicpm3 minicpm_eagle minicpmo minicpmv minimax_m2 minimax_text_01 minimax_vl_01 mistral3 mistral_large_3 mistral_large_3_eagle mixtral mllama4 mlp_speculator modernbert module_mapping molmo moonvit mpt nano_nemotron_vl nemotron nemotron_h nemotron_nas nemotron_vl nvlm_d olmo olmo2 olmoe opencua openpangu openpangu_mtp opt orion ouro ovis ovis2_5 paddleocr_vl paligemma persimmon phi phi3 phi3v phi4mm phi4mm_audio phi4mm_utils phimoe pixtral plamo2 plamo3 qwen qwen2 qwen2_5_omni_thinker qwen2_5_vl qwen2_audio qwen2_moe qwen2_rm qwen2_vl qwen3 qwen3_moe qwen3_next qwen3_next_mtp qwen3_omni_moe_thinker qwen3_vl qwen3_vl_moe qwen_vl radio registry roberta rvl seed_oss siglip siglip2navit skyworkr1v smolvlm solar stablelm starcoder2 step3_text step3_vl swin tarsier telechat2 teleflm terratorch ultravox utils vision voxtral voxtral_streaming whisper whisper_utils zamba2 transformers transformers base causal legacy moe multimodal pooling utils warmup warmup deep_gemm_warmup kernel_warmup multimodal multimodal audio base cache evs hasher image inputs parse processing profiling registry utils video platforms platforms cpu cuda interface rocm tpu xpu plugins plugins io_processors io_processors interface lora_resolvers lora_resolvers filesystem_resolver profiler profiler layerwise_profile utils wrapper ray ray lazy_utils ray_env reasoning reasoning abs_reasoning_parsers basic_parsers deepseek_r1_reasoning_parser deepseek_v3_reasoning_parser ernie45_reasoning_parser glm4_moe_reasoning_parser gptoss_reasoning_parser granite_reasoning_parser holo2_reasoning_parser hunyuan_a13b_reasoning_parser identity_reasoning_parser minimax_m2_reasoning_parser mistral_reasoning_parser olmo3_reasoning_parser qwen3_reasoning_parser seedoss_reasoning_parser step3_reasoning_parser tokenizers tokenizers deepseek_v32 deepseek_v32_encoding detokenizer_utils hf mistral protocol registry tool_parsers tool_parsers abstract_tool_parser deepseekv3_tool_parser deepseekv31_tool_parser deepseekv32_tool_parser ernie45_tool_parser functiongemma_tool_parser gigachat3_tool_parser glm4_moe_tool_parser glm47_moe_tool_parser granite_20b_fc_tool_parser granite_tool_parser hermes_tool_parser hunyuan_a13b_tool_parser internlm2_tool_parser jamba_tool_parser kimi_k2_tool_parser llama4_pythonic_tool_parser llama_tool_parser longcat_tool_parser minimax_m2_tool_parser minimax_tool_parser mistral_tool_parser olmo3_tool_parser openai_tool_parser phi4mini_tool_parser pythonic_tool_parser qwen3coder_tool_parser qwen3xml_tool_parser seed_oss_tool_parser step3_tool_parser utils xlam_tool_parser transformers_utils transformers_utils config config_parser_base dynamic_module gguf_utils processor repo_utils runai_utils s3_utils tokenizer utils chat_templates chat_templates registry configs configs afmoe arctic bagel chatglm deepseek_vl2 dotsocr eagle falcon flex_olmo hunyuan_vl jais kimi_linear kimi_vl lfm2_moe medusa midashenglm mistral mlp_speculator moonvit nemotron nemotron_h olmo3 ovis qwen3_next radio step3_vl tarsier2 ultravox speculators speculators algos base processors processors bagel deepseek_ocr deepseek_vl2 hunyuan_vl hunyuan_vl_image ovis ovis2_5 triton_utils triton_utils importing usage usage usage_lib utils utils argparse_utils async_utils cache collection_utils counter deep_gemm flashinfer func_utils gc_utils hashing import_utils jsontree math_utils mem_constants mem_utils nccl network_utils nvtx_pytorch_hooks platform_utils profiling registry serial_utils system_utils tensor_schema torch_utils v1 v1 cudagraph_dispatcher kv_cache_interface outputs request serial_utils utils attention attention backends backends cpu_attn flash_attn flashinfer flex_attention gdn_attn linear_attn mamba1_attn mamba2_attn mamba_attn pallas rocm_aiter_fa rocm_aiter_unified_attn rocm_attn short_conv_attn tree_attn triton_attn utils mla mla aiter_triton_mla common cutlass_mla flashattn_mla flashinfer_mla flashmla flashmla_sparse indexer rocm_aiter_mla rocm_aiter_mla_sparse triton_mla core core block_pool encoder_cache_manager kv_cache_coordinator kv_cache_manager kv_cache_metrics kv_cache_utils single_type_kv_cache_manager sched sched async_scheduler interface output request_queue scheduler utils engine engine async_llm coordinator core core_client detokenizer exceptions input_processor llm_engine logprobs output_processor parallel_sampling utils executor executor abstract multiproc_executor ray_distributed_executor ray_executor ray_utils uniproc_executor kv_offload kv_offload abstract arc_manager backend cpu factory lru_manager mediums spec backends backends cpu worker worker cpu_gpu worker metrics metrics loggers perf prometheus ray_wrappers reader stats pool pool metadata sample sample metadata rejection_sampler sampler logits_processor logits_processor builtin interface state ops ops bad_words logprobs penalties topk_topp_sampler tpu tpu metadata sampler spec_decode spec_decode eagle medusa metadata metrics ngram_proposer suffix_decoding utils structured_output structured_output backend_guidance backend_lm_format_enforcer backend_outlines backend_types backend_xgrammar request utils worker worker block_table cp_utils cpu_model_runner cpu_worker dp_utils ec_connector_model_runner_mixin gpu_input_batch gpu_model_runner gpu_ubatch_wrapper gpu_worker kv_connector_model_runner_mixin lora_model_runner_mixin tpu_input_batch tpu_model_runner tpu_worker ubatch_utils ubatching utils worker_base workspace xpu_model_runner xpu_worker gpu gpu async_utils attn_utils block_table cudagraph_utils dp_utils input_batch model_runner states structured_outputs metrics metrics logits sample sample gumbel logprob metadata min_p output penalties sampler spec_decode spec_decode eagle eagle_cudagraph rejection_sample CLI Reference CLI Reference vllm serve vllm chat vllm complete vllm run-batch vllm bench vllm bench vllm bench latency vllm bench serve vllm bench sweep plot vllm bench sweep plot_pareto vllm bench sweep serve vllm bench sweep serve_sla vllm bench throughput Community Community Contact Us Meetups Sponsors Governance Governance Collaboration Policy Committers Governance Process Blog Forum Slack Table of contents datasets logger lora_tokenizer_cache zeta_prompt AIMODataset SUPPORTED_DATASET_PATHS sample ASRDataset DEFAULT_OUTPUT_LEN IS_MULTIMODAL SUPPORTED_DATASET_PATHS TRANSCRIPTION_PREAMBLE skip_long_audios sample BenchmarkDataset DEFAULT_SEED IS_MULTIMODAL data dataset_path disable_shuffle random_seed __init__ apply_multimodal_chat_transformation get_random_lora_request load_data maybe_oversample_requests sample BlazeditDataset DEFAULT_OUTPUT_LEN SUPPORTED_DATASET_PATHS sample BurstGPTDataset __init__ _sample_loaded_data load_data sample ConversationDataset IS_MULTIMODAL SUPPORTED_DATASET_PATHS sample CustomDataset __init__ load_data sample HuggingFaceDataset SUPPORTED_DATASET_PATHS dataset_split dataset_subset hf_name load_stream __init__ load_data InstructCoderDataset DEFAULT_OUTPUT_LEN SUPPORTED_DATASET_PATHS sample MLPerfDataset SUPPORTED_DATASET_PATHS sample MMStarDataset DEFAULT_OUTPUT_LEN IS_MULTIMODAL SUPPORTED_DATASET_PATHS sample MMVUDataset DEFAULT_OUTPUT_LEN SUPPORTED_DATASET_PATHS sample MTBenchDataset DEFAULT_OUTPUT_LEN SUPPORTED_DATASET_PATHS sample MultiModalConversationDataset IS_MULTIMODAL SUPPORTED_DATASET_PATHS sample NextEditPredictionDataset MAPPING_PROMPT_FUNCS SUPPORTED_DATASET_PATHS sample PrefixRepetitionRandomDataset DEFAULT_NUM_PREFIXES DEFAULT_OUTPUT_LEN DEFAULT_PREFIX_LEN DEFAULT_SUFFIX_LEN __init__ sample RandomDataset DEFAULT_INPUT_LEN DEFAULT_OUTPUT_LEN DEFAULT_PREFIX_LEN DEFAULT_RANGE_RATIO _rng __init__ generate_token_sequence get_prefix get_sampling_params sample RandomDatasetForReranking __init__ sample RandomMultiModalDataset DEFAULT_BASE_ITEMS_PER_REQUEST DEFAULT_ENABLE_MULTIMODAL_CHAT DEFAULT_LIMIT_MM_PER_PROMPT DEFAULT_MM_ITEM_BUCKET_CONFIG DEFAULT_NUM_MM_ITEMS_RANGE_RATIO IS_MULTIMODAL __init__ generate_mm_item generate_synthetic_image generate_synthetic_video get_mm_item_iterator get_mm_item_sampling_params map_config_to_modality normalize_bucket_config sample SampleRequest expected_output_len lora_request multi_modal_data prompt prompt_len request_id __init__ ShareGPTDataset __init__ load_data sample SonnetDataset DEFAULT_INPUT_LEN DEFAULT_OUTPUT_LEN DEFAULT_PREFIX_LEN __init__ load_data sample SpecBench category __init__ load_data sample VisionArenaDataset DEFAULT_OUTPUT_LEN IS_MULTIMODAL SUPPORTED_DATASET_PATHS sample _ValidateDatasetArgs __call__ _format_zeta_prompt add_dataset_parser gen_prompt_decode_to_target_len get_samples is_valid_sequence lora_path_on_disk process_image process_video vllm.benchmarks.datasets ¶ This module defines a framework for sampling benchmark requests from various datasets. Each dataset subclass of BenchmarkDataset must implement sample generation. Supported dataset types include: - ShareGPT - Random (synthetic) - Sonnet - BurstGPT - HuggingFace - VisionArena datasets module-attribute ¶ datasets = PlaceholderModule('datasets') logger module-attribute ¶ logger = getLogger(__name__) lora_tokenizer_cache module-attribute ¶ lora_tokenizer_cache: dict[int, TokenizerLike] = {} zeta_prompt module-attribute ¶ zeta_prompt = \"### Instruction:\\nYou are a code completion assistant and your task is to analyze user edits and then rewrite an excerpt that the user provides, suggesting the appropriate edits within the excerpt, taking into account the cursor location.\\n\\n### User Edits:\\n\\n{}\\n\\n### User Excerpt:\\n\\n{}\\n\\n### Response:\\n\\n\" AIMODataset ¶ Bases: HuggingFaceDataset Dataset class for processing a AIMO dataset with reasoning questions. Source code in vllm/benchmarks/datasets.py 2736 2737 2738 2739 2740 2741 2742 2743 2744 2745 2746 2747 2748 2749 2750 2751 2752 2753 2754 2755 2756 2757 2758 2759 2760 2761 2762 2763 2764 2765 2766 2767 2768 2769 2770 2771 2772 2773 2774 2775 2776 2777 2778 2779 2780 2781 2782 2783 2784 2785 2786 2787 2788class AIMODataset(HuggingFaceDataset): \"\"\" Dataset class for processing a AIMO dataset with reasoning questions. \"\"\" SUPPORTED_DATASET_PATHS = { \"AI-MO/aimo-validation-aime\", \"AI-MO/NuminaMath-1.5\", \"AI-MO/NuminaMath-CoT\", } def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: sampled_requests = [] ind = 0 dynamic_output = output_len is None for item in self.data: if len(sampled_requests) >= num_requests: break prompt, completion = item[\"problem\"], item[\"solution\"] prompt_ids = tokenizer(prompt).input_ids completion_ids = tokenizer(completion).input_ids prompt_len = len(prompt_ids) completion_len = len(completion_ids) output_len = completion_len if dynamic_output else output_len assert isinstance(output_len, int) and output_len > 0 if dynamic_output and not is_valid_sequence( prompt_len, completion_len, max_prompt_len=2048, max_total_len=32000 ): continue sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=None, request_id=request_id_prefix + str(ind), ) ) ind += 1 self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = { \"AI-MO/aimo-validation-aime\", \"AI-MO/NuminaMath-1.5\", \"AI-MO/NuminaMath-CoT\", } sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 2747 2748 2749 2750 2751 2752 2753 2754 2755 2756 2757 2758 2759 2760 2761 2762 2763 2764 2765 2766 2767 2768 2769 2770 2771 2772 2773 2774 2775 2776 2777 2778 2779 2780 2781 2782 2783 2784 2785 2786 2787 2788def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: sampled_requests = [] ind = 0 dynamic_output = output_len is None for item in self.data: if len(sampled_requests) >= num_requests: break prompt, completion = item[\"problem\"], item[\"solution\"] prompt_ids = tokenizer(prompt).input_ids completion_ids = tokenizer(completion).input_ids prompt_len = len(prompt_ids) completion_len = len(completion_ids) output_len = completion_len if dynamic_output else output_len assert isinstance(output_len, int) and output_len > 0 if dynamic_output and not is_valid_sequence( prompt_len, completion_len, max_prompt_len=2048, max_total_len=32000 ): continue sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=None, request_id=request_id_prefix + str(ind), ) ) ind += 1 self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests ASRDataset ¶ Bases: HuggingFaceDataset Dataset class for processing a ASR dataset for transcription. Tested on the following set: +----------------+----------------------------------------+--------------------------+-----------------------------+ | Dataset | Domain | Speaking Style | hf-subset | +----------------+----------------------------------------+--------------------------+-----------------------------+ | TED-LIUM | TED talks | Oratory | release1, release2, release3| | | | | release3-speaker-adaptation | | VoxPopuli | European Parliament | Oratory | en, de, it, fr, ... | | LibriSpeech | Audiobook | Narrated | \"LIUM/tedlium\" | | GigaSpeech | Audiobook, podcast, YouTube | Narrated, spontaneous | xs, s, m, l, xl, dev, test | | SPGISpeech | Financial meetings | Oratory, spontaneous | S, M, L, dev, test | | AMI | Meetings | Spontaneous | ihm, sdm | +----------------+----------------------------------------+--------------------------+-----------------------------+ Source code in vllm/benchmarks/datasets.py 2894 2895 2896 2897 2898 2899 2900 2901 2902 2903 2904 2905 2906 2907 2908 2909 2910 2911 2912 2913 2914 2915 2916 2917 2918 2919 2920 2921 2922 2923 2924 2925 2926 2927 2928 2929 2930 2931 2932 2933 2934 2935 2936 2937 2938 2939 2940 2941 2942 2943 2944 2945 2946 2947 2948 2949 2950 2951 2952 2953 2954 2955 2956 2957 2958 2959 2960 2961 2962 2963 2964 2965 2966 2967 2968 2969 2970 2971 2972 2973 2974 2975 2976class ASRDataset(HuggingFaceDataset): \"\"\" Dataset class for processing a ASR dataset for transcription. Tested on the following set: +----------------+----------------------------------------+--------------------------+-----------------------------+ | Dataset | Domain | Speaking Style | hf-subset | +----------------+----------------------------------------+--------------------------+-----------------------------+ | TED-LIUM | TED talks | Oratory | release1, release2, release3| | | | | release3-speaker-adaptation | | VoxPopuli | European Parliament | Oratory | en, de, it, fr, ... | | LibriSpeech | Audiobook | Narrated | \"LIUM/tedlium\" | | GigaSpeech | Audiobook, podcast, YouTube | Narrated, spontaneous | xs, s, m, l, xl, dev, test | | SPGISpeech | Financial meetings | Oratory, spontaneous | S, M, L, dev, test | | AMI | Meetings | Spontaneous | ihm, sdm | +----------------+----------------------------------------+--------------------------+-----------------------------+ \"\"\" # noqa: E501 SUPPORTED_DATASET_PATHS = { \"openslr/librispeech_asr\", \"facebook/voxpopuli\", \"LIUM/tedlium\", \"edinburghcstr/ami\", \"speechcolab/gigaspeech\", \"kensho/spgispeech\", } DEFAULT_OUTPUT_LEN = 128 IS_MULTIMODAL = True # TODO Whisper-specific. Abstract interface when more models are supported. TRANSCRIPTION_PREAMBLE = \"<|startoftranscript|><|en|><|transcribe|><|notimestamps|>\" skip_long_audios: bool = True def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN prompt = ASRDataset.TRANSCRIPTION_PREAMBLE prompt_len = len(tokenizer(prompt).input_ids) sampled_requests = [] ind = 0 skipped = 0 for item in self.data: if len(sampled_requests) >= num_requests: break audio = item[\"audio\"] y, sr = audio[\"array\"], audio[\"sampling_rate\"] duration_s = librosa.get_duration(y=y, sr=sr) # Whisper max supported duration if self.skip_long_audios and duration_s > 30: skipped += 1 continue mm_content = {\"audio\": (y, sr)} sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_content, request_id=request_id_prefix + str(ind), ) ) ind += 1 if skipped: logger.warning( \"%d samples discarded from dataset due to\" \" their length being greater than\" \" what Whisper supports.\", skipped, ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶ DEFAULT_OUTPUT_LEN = 128 IS_MULTIMODAL class-attribute instance-attribute ¶ IS_MULTIMODAL = True SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = { \"openslr/librispeech_asr\", \"facebook/voxpopuli\", \"LIUM/tedlium\", \"edinburghcstr/ami\", \"speechcolab/gigaspeech\", \"kensho/spgispeech\", } TRANSCRIPTION_PREAMBLE class-attribute instance-attribute ¶ TRANSCRIPTION_PREAMBLE = \"<|startoftranscript|><|en|><|transcribe|><|notimestamps|>\" skip_long_audios class-attribute instance-attribute ¶ skip_long_audios: bool = True sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 2929 2930 2931 2932 2933 2934 2935 2936 2937 2938 2939 2940 2941 2942 2943 2944 2945 2946 2947 2948 2949 2950 2951 2952 2953 2954 2955 2956 2957 2958 2959 2960 2961 2962 2963 2964 2965 2966 2967 2968 2969 2970 2971 2972 2973 2974 2975 2976def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN prompt = ASRDataset.TRANSCRIPTION_PREAMBLE prompt_len = len(tokenizer(prompt).input_ids) sampled_requests = [] ind = 0 skipped = 0 for item in self.data: if len(sampled_requests) >= num_requests: break audio = item[\"audio\"] y, sr = audio[\"array\"], audio[\"sampling_rate\"] duration_s = librosa.get_duration(y=y, sr=sr) # Whisper max supported duration if self.skip_long_audios and duration_s > 30: skipped += 1 continue mm_content = {\"audio\": (y, sr)} sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_content, request_id=request_id_prefix + str(ind), ) ) ind += 1 if skipped: logger.warning( \"%d samples discarded from dataset due to\" \" their length being greater than\" \" what Whisper supports.\", skipped, ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests BenchmarkDataset ¶ Bases: ABC Source code in vllm/benchmarks/datasets.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254class BenchmarkDataset(ABC): DEFAULT_SEED = 0 IS_MULTIMODAL = False def __init__( self, dataset_path: str | None = None, random_seed: int = DEFAULT_SEED, disable_shuffle: bool = False, **kwargs, ) -> None: \"\"\" Initialize the BenchmarkDataset with an optional dataset path and random seed. Args: dataset_path (Optional[str]): Path to the dataset. If None, it indicates that a default or random dataset might be used. random_seed (int): Seed value for reproducible shuffling or sampling. Defaults to DEFAULT_SEED. \"\"\" self.dataset_path = dataset_path # Set the random seed, ensuring that a None value is replaced with the # default seed. self.random_seed = random_seed if random_seed is not None else self.DEFAULT_SEED self.disable_shuffle = disable_shuffle self.data = None def apply_multimodal_chat_transformation( self, prompt: str, mm_content: MultiModalDataDict | dict | list[dict] | None = None, ) -> list[dict]: \"\"\" Transform a prompt and optional multimodal content into a chat format. This method is used for chat models that expect a specific conversation format. \"\"\" content = [{\"text\": prompt, \"type\": \"text\"}] if mm_content is not None: if isinstance(mm_content, list): content.extend(cast(list[dict[str, Any]], mm_content)) elif isinstance(mm_content, dict): content.append(mm_content) else: raise TypeError( \"Could not process multimodal content of type: \" + f\"{type(mm_content)}\" ) return [{\"role\": \"user\", \"content\": content}] def load_data(self) -> None: \"\"\" Load data from the dataset path into self.data. This method must be overridden by subclasses since the method to load data will vary depending on the dataset format and source. Raises: NotImplementedError: If a subclass does not implement this method. \"\"\" # TODO (jenniferzhao): add support for downloading data raise NotImplementedError(\"load_data must be implemented in subclasses.\") def get_random_lora_request( self, max_loras: int | None = None, lora_path: str | None = None, ) -> LoRARequest | None: \"\"\" Optionally select a random LoRA request. This method is used when LoRA parameters are provided. It randomly selects a LoRA based on max_loras. Args: max_loras (Optional[int]): The maximum number of LoRAs available. If `None`, LoRA is not used. lora_path (Optional[str]): Path to the LoRA parameters on disk. If `None`, LoRA is not used. Returns: A new [`LoRARequest`][vllm.lora.request.LoRARequest] (or `None` if not applicable). \"\"\" if max_loras is None or lora_path is None: return None # Generate a random LoRA ID in the range [1, max_loras]. lora_id = random.randint(1, max_loras) lora_request = LoRARequest( lora_name=str(lora_id), lora_int_id=lora_id, lora_path=lora_path_on_disk(lora_path), ) return lora_request @abstractmethod def sample( self, tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, ) -> list[SampleRequest]: \"\"\" Abstract method to generate sample requests from the dataset. Subclasses must override this method to implement dataset-specific logic for generating a list of SampleRequest objects. Args: tokenizer (TokenizerLike): The tokenizer to be used for processing the dataset's text. num_requests (int): The number of sample requests to generate. request_id_prefix (str): The prefix of request_id. Returns: list[SampleRequest]: A list of sample requests generated from the dataset. \"\"\" raise NotImplementedError(\"sample must be implemented in subclasses.\") def maybe_oversample_requests( self, requests: list[SampleRequest], num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, ) -> None: \"\"\" Oversamples the list of requests if its size is less than the desired number. Args: requests (List[SampleRequest]): The current list of sampled requests. num_requests (int): The target number of requests. request_id_prefix (str): The prefix applied to generated request identifiers. \"\"\" if no_oversample: logger.info(\"Skipping oversampling. Total samples: %d.\", len(requests)) return if len(requests) < num_requests: random.seed(self.random_seed) needed = num_requests - len(requests) additional = [] for i in range(needed): req = deepcopy(random.choice(requests)) req.request_id = request_id_prefix + str(len(requests) + i) additional.append(req) requests.extend(additional) logger.info(\"Oversampled requests to reach %d total samples.\", num_requests) ids = [req.request_id for req in requests] if len(ids) != len(set(ids)): raise ValueError( \"Duplicate request_id found in the sampled \" \"requests. Please ensure that each request_id \" \"is unique.\" ) DEFAULT_SEED class-attribute instance-attribute ¶ DEFAULT_SEED = 0 IS_MULTIMODAL class-attribute instance-attribute ¶ IS_MULTIMODAL = False data instance-attribute ¶ data = None dataset_path instance-attribute ¶ dataset_path = dataset_path disable_shuffle instance-attribute ¶ disable_shuffle = disable_shuffle random_seed instance-attribute ¶ random_seed = ( random_seed if random_seed is not None else DEFAULT_SEED ) __init__ ¶ __init__( dataset_path: str | None = None, random_seed: int = DEFAULT_SEED, disable_shuffle: bool = False, **kwargs, ) -> None Initialize the BenchmarkDataset with an optional dataset path and random seed. Parameters: Name Type Description Default dataset_path Optional[str] Path to the dataset. If None, it indicates that a default or random dataset might be used. None random_seed int Seed value for reproducible shuffling or sampling. Defaults to DEFAULT_SEED. DEFAULT_SEED Source code in vllm/benchmarks/datasets.py 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117def __init__( self, dataset_path: str | None = None, random_seed: int = DEFAULT_SEED, disable_shuffle: bool = False, **kwargs, ) -> None: \"\"\" Initialize the BenchmarkDataset with an optional dataset path and random seed. Args: dataset_path (Optional[str]): Path to the dataset. If None, it indicates that a default or random dataset might be used. random_seed (int): Seed value for reproducible shuffling or sampling. Defaults to DEFAULT_SEED. \"\"\" self.dataset_path = dataset_path # Set the random seed, ensuring that a None value is replaced with the # default seed. self.random_seed = random_seed if random_seed is not None else self.DEFAULT_SEED self.disable_shuffle = disable_shuffle self.data = None apply_multimodal_chat_transformation ¶ apply_multimodal_chat_transformation( prompt: str, mm_content: MultiModalDataDict | dict | list[dict] | None = None, ) -> list[dict] Transform a prompt and optional multimodal content into a chat format. This method is used for chat models that expect a specific conversation format. Source code in vllm/benchmarks/datasets.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140def apply_multimodal_chat_transformation( self, prompt: str, mm_content: MultiModalDataDict | dict | list[dict] | None = None, ) -> list[dict]: \"\"\" Transform a prompt and optional multimodal content into a chat format. This method is used for chat models that expect a specific conversation format. \"\"\" content = [{\"text\": prompt, \"type\": \"text\"}] if mm_content is not None: if isinstance(mm_content, list): content.extend(cast(list[dict[str, Any]], mm_content)) elif isinstance(mm_content, dict): content.append(mm_content) else: raise TypeError( \"Could not process multimodal content of type: \" + f\"{type(mm_content)}\" ) return [{\"role\": \"user\", \"content\": content}] get_random_lora_request ¶ get_random_lora_request( max_loras: int | None = None, lora_path: str | None = None, ) -> LoRARequest | None Optionally select a random LoRA request. This method is used when LoRA parameters are provided. It randomly selects a LoRA based on max_loras. Parameters: Name Type Description Default max_loras Optional[int] The maximum number of LoRAs available. If None, LoRA is not used. None lora_path Optional[str] Path to the LoRA parameters on disk. If None, LoRA is not used. None Returns: Type Description LoRARequest | None A new LoRARequest LoRARequest | None (or None if not applicable). Source code in vllm/benchmarks/datasets.py 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186def get_random_lora_request( self, max_loras: int | None = None, lora_path: str | None = None, ) -> LoRARequest | None: \"\"\" Optionally select a random LoRA request. This method is used when LoRA parameters are provided. It randomly selects a LoRA based on max_loras. Args: max_loras (Optional[int]): The maximum number of LoRAs available. If `None`, LoRA is not used. lora_path (Optional[str]): Path to the LoRA parameters on disk. If `None`, LoRA is not used. Returns: A new [`LoRARequest`][vllm.lora.request.LoRARequest] (or `None` if not applicable). \"\"\" if max_loras is None or lora_path is None: return None # Generate a random LoRA ID in the range [1, max_loras]. lora_id = random.randint(1, max_loras) lora_request = LoRARequest( lora_name=str(lora_id), lora_int_id=lora_id, lora_path=lora_path_on_disk(lora_path), ) return lora_request load_data ¶ load_data() -> None Load data from the dataset path into self.data. This method must be overridden by subclasses since the method to load data will vary depending on the dataset format and source. Raises: Type Description NotImplementedError If a subclass does not implement this method. Source code in vllm/benchmarks/datasets.py 142 143 144 145 146 147 148 149 150 151 152 153def load_data(self) -> None: \"\"\" Load data from the dataset path into self.data. This method must be overridden by subclasses since the method to load data will vary depending on the dataset format and source. Raises: NotImplementedError: If a subclass does not implement this method. \"\"\" # TODO (jenniferzhao): add support for downloading data raise NotImplementedError(\"load_data must be implemented in subclasses.\") maybe_oversample_requests ¶ maybe_oversample_requests( requests: list[SampleRequest], num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, ) -> None Oversamples the list of requests if its size is less than the desired number. Parameters: Name Type Description Default requests List[SampleRequest] The current list of sampled requests. required num_requests int The target number of requests. required request_id_prefix str The prefix applied to generated request identifiers. '' Source code in vllm/benchmarks/datasets.py 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254def maybe_oversample_requests( self, requests: list[SampleRequest], num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, ) -> None: \"\"\" Oversamples the list of requests if its size is less than the desired number. Args: requests (List[SampleRequest]): The current list of sampled requests. num_requests (int): The target number of requests. request_id_prefix (str): The prefix applied to generated request identifiers. \"\"\" if no_oversample: logger.info(\"Skipping oversampling. Total samples: %d.\", len(requests)) return if len(requests) < num_requests: random.seed(self.random_seed) needed = num_requests - len(requests) additional = [] for i in range(needed): req = deepcopy(random.choice(requests)) req.request_id = request_id_prefix + str(len(requests) + i) additional.append(req) requests.extend(additional) logger.info(\"Oversampled requests to reach %d total samples.\", num_requests) ids = [req.request_id for req in requests] if len(ids) != len(set(ids)): raise ValueError( \"Duplicate request_id found in the sampled \" \"requests. Please ensure that each request_id \" \"is unique.\" ) sample abstractmethod ¶ sample( tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, ) -> list[SampleRequest] Abstract method to generate sample requests from the dataset. Subclasses must override this method to implement dataset-specific logic for generating a list of SampleRequest objects. Parameters: Name Type Description Default tokenizer TokenizerLike The tokenizer to be used for processing the dataset's text. required num_requests int The number of sample requests to generate. required request_id_prefix str The prefix of request_id. '' Returns: Type Description list[SampleRequest] list[SampleRequest]: A list of sample requests generated from the list[SampleRequest] dataset. Source code in vllm/benchmarks/datasets.py 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212@abstractmethod def sample( self, tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, ) -> list[SampleRequest]: \"\"\" Abstract method to generate sample requests from the dataset. Subclasses must override this method to implement dataset-specific logic for generating a list of SampleRequest objects. Args: tokenizer (TokenizerLike): The tokenizer to be used for processing the dataset's text. num_requests (int): The number of sample requests to generate. request_id_prefix (str): The prefix of request_id. Returns: list[SampleRequest]: A list of sample requests generated from the dataset. \"\"\" raise NotImplementedError(\"sample must be implemented in subclasses.\") BlazeditDataset ¶ Bases: HuggingFaceDataset Blazedit Dataset. https://github.com/ise-uiuc/blazedit 5k char version: vdaita/edit_5k_char 10k char version: vdaita/edit_10k_char Source code in vllm/benchmarks/datasets.py 2647 2648 2649 2650 2651 2652 2653 2654 2655 2656 2657 2658 2659 2660 2661 2662 2663 2664 2665 2666 2667 2668 2669 2670 2671 2672 2673 2674 2675 2676 2677 2678 2679 2680 2681 2682 2683 2684 2685 2686 2687 2688 2689 2690 2691 2692 2693 2694 2695 2696 2697 2698 2699 2700 2701 2702 2703 2704 2705 2706 2707 2708 2709 2710 2711 2712 2713 2714 2715 2716 2717 2718 2719 2720 2721 2722 2723 2724 2725 2726 2727 2728class BlazeditDataset(HuggingFaceDataset): \"\"\" Blazedit Dataset. https://github.com/ise-uiuc/blazedit 5k char version: vdaita/edit_5k_char 10k char version: vdaita/edit_10k_char \"\"\" # noqa: E501 # 5k char version will have output as ~5k chars # 10k char version will have output as ~10k chars # Assuming 3 char per token, 10k chars will be 3333 tokens # We set default to 4000 to be safe DEFAULT_OUTPUT_LEN = 4000 SUPPORTED_DATASET_PATHS = { \"vdaita/edit_5k_char\", \"vdaita/edit_10k_char\", } def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, min_distance: float = 0.0, max_distance: float = 1.0, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break code = item[\"code\"] change_request = item[\"change_request\"] norm_distance = item[\"norm_distance\"] # compare the levenshtein distance normalized by code length if norm_distance < min_distance or norm_distance > max_distance: continue # template copied from # https://github.com/ise-uiuc/blazedit/blob/7765137e656fd62de877422d2e4cf8de51228054/dataset/create_refined_dataset.py#L94-L105 # noqa: E501 prompt = f\"\"\"Given a code file, please apply the change requests and generate the new file. Original file: ```python {code} ``` Change request: {change_request} Please generate the new code file in the \"New file\" section below.\"\"\" # noqa: E501 # apply template if not skip_chat_template: prompt = tokenizer.apply_chat_template( [{\"role\": \"user\", \"content\": prompt}], add_generation_prompt=True, tokenize=False, ) prompt_len = len(tokenizer(prompt).input_ids) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶ DEFAULT_OUTPUT_LEN = 4000 SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = { \"vdaita/edit_5k_char\", \"vdaita/edit_10k_char\", } sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, min_distance: float = 0.0, max_distance: float = 1.0, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 2666 2667 2668 2669 2670 2671 2672 2673 2674 2675 2676 2677 2678 2679 2680 2681 2682 2683 2684 2685 2686 2687 2688 2689 2690 2691 2692 2693 2694 2695 2696 2697 2698 2699 2700 2701 2702 2703 2704 2705 2706 2707 2708 2709 2710 2711 2712 2713 2714 2715 2716 2717 2718 2719 2720 2721 2722 2723 2724 2725 2726 2727 2728 def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, min_distance: float = 0.0, max_distance: float = 1.0, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break code = item[\"code\"] change_request = item[\"change_request\"] norm_distance = item[\"norm_distance\"] # compare the levenshtein distance normalized by code length if norm_distance < min_distance or norm_distance > max_distance: continue # template copied from # https://github.com/ise-uiuc/blazedit/blob/7765137e656fd62de877422d2e4cf8de51228054/dataset/create_refined_dataset.py#L94-L105 # noqa: E501 prompt = f\"\"\"Given a code file, please apply the change requests and generate the new file. Original file: ```python {code} ``` Change request: {change_request} Please generate the new code file in the \"New file\" section below.\"\"\" # noqa: E501 # apply template if not skip_chat_template: prompt = tokenizer.apply_chat_template( [{\"role\": \"user\", \"content\": prompt}], add_generation_prompt=True, tokenize=False, ) prompt_len = len(tokenizer(prompt).input_ids) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests BurstGPTDataset ¶ Bases: BenchmarkDataset Implements the BurstGPT dataset. Loads data from a CSV file and generates sample requests based on synthetic prompt generation. Only rows with Model \"GPT-4\" and positive response tokens are used. Source code in vllm/benchmarks/datasets.py 2170 2171 2172 2173 2174 2175 2176 2177 2178 2179 2180 2181 2182 2183 2184 2185 2186 2187 2188 2189 2190 2191 2192 2193 2194 2195 2196 2197 2198 2199 2200 2201 2202 2203 2204 2205 2206 2207 2208 2209 2210 2211 2212 2213 2214 2215 2216 2217 2218 2219 2220 2221 2222 2223 2224 2225 2226 2227 2228 2229 2230 2231 2232 2233 2234 2235 2236 2237 2238 2239class BurstGPTDataset(BenchmarkDataset): \"\"\" Implements the BurstGPT dataset. Loads data from a CSV file and generates sample requests based on synthetic prompt generation. Only rows with Model \"GPT-4\" and positive response tokens are used. \"\"\" def __init__(self, **kwargs) -> None: super().__init__(**kwargs) self.load_data() def load_data( self, ): if self.dataset_path is None: raise ValueError(\"dataset_path must be provided for loading data.\") df = pd.read_csv(self.dataset_path) # Filter to keep only GPT-4 rows. gpt4_df = df[df[\"Model\"] == \"GPT-4\"] # Remove failed requests (where Response tokens is 0 or less). gpt4_df = gpt4_df[gpt4_df[\"Response tokens\"] > 0] # Sample the desired number of rows. self.data = gpt4_df def _sample_loaded_data(self, num_requests: int) -> list: if num_requests <= len(self.data): data = self.data.sample(n=num_requests, random_state=self.random_seed) else: data = self.data.sample( n=num_requests, random_state=self.random_seed, replace=True, ) # Convert the dataframe to a list of lists. return data.values.tolist() def sample( self, tokenizer: TokenizerLike, num_requests: int, max_loras: int | None = None, lora_path: str | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest]: samples = [] data = self._sample_loaded_data(num_requests=num_requests) for i in range(num_requests): input_len = int(data[i][2]) output_len = int(data[i][3]) lora_req = self.get_random_lora_request( max_loras=max_loras, lora_path=lora_path ) vocab_size = tokenizer.vocab_size # Generate a synthetic prompt: a list of token IDs computed as (i + # j) modulo vocab_size. token_ids = [(i + j) % vocab_size for j in range(input_len)] prompt = tokenizer.decode(token_ids) samples.append( SampleRequest( prompt=prompt, prompt_len=input_len, expected_output_len=output_len, lora_request=lora_req, request_id=request_id_prefix + str(i), ) ) return samples __init__ ¶ __init__(**kwargs) -> None Source code in vllm/benchmarks/datasets.py 2177 2178 2179def __init__(self, **kwargs) -> None: super().__init__(**kwargs) self.load_data() _sample_loaded_data ¶ _sample_loaded_data(num_requests: int) -> list Source code in vllm/benchmarks/datasets.py 2195 2196 2197 2198 2199 2200 2201 2202 2203 2204 2205def _sample_loaded_data(self, num_requests: int) -> list: if num_requests <= len(self.data): data = self.data.sample(n=num_requests, random_state=self.random_seed) else: data = self.data.sample( n=num_requests, random_state=self.random_seed, replace=True, ) # Convert the dataframe to a list of lists. return data.values.tolist() load_data ¶ load_data() Source code in vllm/benchmarks/datasets.py 2181 2182 2183 2184 2185 2186 2187 2188 2189 2190 2191 2192 2193def load_data( self, ): if self.dataset_path is None: raise ValueError(\"dataset_path must be provided for loading data.\") df = pd.read_csv(self.dataset_path) # Filter to keep only GPT-4 rows. gpt4_df = df[df[\"Model\"] == \"GPT-4\"] # Remove failed requests (where Response tokens is 0 or less). gpt4_df = gpt4_df[gpt4_df[\"Response tokens\"] > 0] # Sample the desired number of rows. self.data = gpt4_df sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, max_loras: int | None = None, lora_path: str | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest] Source code in vllm/benchmarks/datasets.py 2207 2208 2209 2210 2211 2212 2213 2214 2215 2216 2217 2218 2219 2220 2221 2222 2223 2224 2225 2226 2227 2228 2229 2230 2231 2232 2233 2234 2235 2236 2237 2238 2239def sample( self, tokenizer: TokenizerLike, num_requests: int, max_loras: int | None = None, lora_path: str | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest]: samples = [] data = self._sample_loaded_data(num_requests=num_requests) for i in range(num_requests): input_len = int(data[i][2]) output_len = int(data[i][3]) lora_req = self.get_random_lora_request( max_loras=max_loras, lora_path=lora_path ) vocab_size = tokenizer.vocab_size # Generate a synthetic prompt: a list of token IDs computed as (i + # j) modulo vocab_size. token_ids = [(i + j) % vocab_size for j in range(input_len)] prompt = tokenizer.decode(token_ids) samples.append( SampleRequest( prompt=prompt, prompt_len=input_len, expected_output_len=output_len, lora_request=lora_req, request_id=request_id_prefix + str(i), ) ) return samples ConversationDataset ¶ Bases: HuggingFaceDataset Dataset for text-only conversation data. Source code in vllm/benchmarks/datasets.py 2284 2285 2286 2287 2288 2289 2290 2291 2292 2293 2294 2295 2296 2297 2298 2299 2300 2301 2302 2303 2304 2305 2306 2307 2308 2309 2310 2311 2312 2313 2314 2315 2316 2317 2318 2319 2320 2321 2322 2323 2324 2325 2326 2327 2328 2329 2330 2331 2332 2333 2334 2335 2336 2337 2338 2339 2340 2341class ConversationDataset(HuggingFaceDataset): \"\"\"Dataset for text-only conversation data.\"\"\" SUPPORTED_DATASET_PATHS = { \"Aeala/ShareGPT_Vicuna_unfiltered\", } IS_MULTIMODAL = False def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: # Filter examples with at least 2 conversations filtered_data = self.data.filter(lambda x: len(x[\"conversations\"]) >= 2) sampled_requests = [] ind = 0 dynamic_output = output_len is None for item in filtered_data: if len(sampled_requests) >= num_requests: break conv = item[\"conversations\"] prompt, completion = conv[0][\"value\"], conv[1][\"value\"] prompt_ids = tokenizer(prompt).input_ids completion_ids = tokenizer(completion).input_ids prompt_len = len(prompt_ids) completion_len = len(completion_ids) output_len = completion_len if dynamic_output else output_len assert isinstance(output_len, int) and output_len > 0 if dynamic_output and not is_valid_sequence(prompt_len, completion_len): continue mm_content = process_image(item[\"image\"]) if \"image\" in item else None if enable_multimodal_chat: # Note: when chat is enabled the request prompt_len is no longer # accurate and we will be using request output to count the # actual prompt len and output len prompt = self.apply_multimodal_chat_transformation(prompt, mm_content) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_content, request_id=request_id_prefix + str(ind), ) ) ind += 1 self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests IS_MULTIMODAL class-attribute instance-attribute ¶ IS_MULTIMODAL = False SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = { \"Aeala/ShareGPT_Vicuna_unfiltered\" } sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 2292 2293 2294 2295 2296 2297 2298 2299 2300 2301 2302 2303 2304 2305 2306 2307 2308 2309 2310 2311 2312 2313 2314 2315 2316 2317 2318 2319 2320 2321 2322 2323 2324 2325 2326 2327 2328 2329 2330 2331 2332 2333 2334 2335 2336 2337 2338 2339 2340 2341def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: # Filter examples with at least 2 conversations filtered_data = self.data.filter(lambda x: len(x[\"conversations\"]) >= 2) sampled_requests = [] ind = 0 dynamic_output = output_len is None for item in filtered_data: if len(sampled_requests) >= num_requests: break conv = item[\"conversations\"] prompt, completion = conv[0][\"value\"], conv[1][\"value\"] prompt_ids = tokenizer(prompt).input_ids completion_ids = tokenizer(completion).input_ids prompt_len = len(prompt_ids) completion_len = len(completion_ids) output_len = completion_len if dynamic_output else output_len assert isinstance(output_len, int) and output_len > 0 if dynamic_output and not is_valid_sequence(prompt_len, completion_len): continue mm_content = process_image(item[\"image\"]) if \"image\" in item else None if enable_multimodal_chat: # Note: when chat is enabled the request prompt_len is no longer # accurate and we will be using request output to count the # actual prompt len and output len prompt = self.apply_multimodal_chat_transformation(prompt, mm_content) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_content, request_id=request_id_prefix + str(ind), ) ) ind += 1 self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests CustomDataset ¶ Bases: BenchmarkDataset Implements the Custom dataset. Loads data from a JSONL file and generates sample requests based on conversation turns. E.g., {\"prompt\": \"What is the capital of India?\"} {\"prompt\": \"What is the capital of Iran?\"} {\"prompt\": \"What is the capital of China?\"} Source code in vllm/benchmarks/datasets.py 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 1937 1938 1939 1940 1941 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025 2026class CustomDataset(BenchmarkDataset): \"\"\" Implements the Custom dataset. Loads data from a JSONL file and generates sample requests based on conversation turns. E.g., ``` {\"prompt\": \"What is the capital of India?\"} {\"prompt\": \"What is the capital of Iran?\"} {\"prompt\": \"What is the capital of China?\"} ``` \"\"\" def __init__(self, **kwargs) -> None: super().__init__(**kwargs) self.load_data() def load_data(self) -> None: if self.dataset_path is None: raise ValueError(\"dataset_path must be provided for loading data.\") # self.data will be a list of dictionaries # e.g., [{\"prompt\": \"What is the capital of India?\"}, ...] # This will be the standardized format which load_data() # has to convert into depending on the filetype of dataset_path. # sample() will assume this standardized format of self.data self.data = [] # Load the JSONL file if self.dataset_path.endswith(\".jsonl\"): jsonl_data = pd.read_json(path_or_buf=self.dataset_path, lines=True) # check if the JSONL file has a 'prompt' column if \"prompt\" not in jsonl_data.columns: raise ValueError(\"JSONL file must contain a 'prompt' column.\") # Convert each row to a dictionary and append to self.data # This will convert the DataFrame to a list of dictionaries # where each dictionary corresponds to a row in the DataFrame. # This is the standardized format we want for self.data for _, row in jsonl_data.iterrows(): self.data.append(row.to_dict()) else: raise NotImplementedError( \"Only JSONL format is supported for CustomDataset.\" ) random.seed(self.random_seed) if not getattr(self, \"disable_shuffle\", False): random.shuffle(self.data) def sample( self, tokenizer: TokenizerLike, num_requests: int, lora_path: str | None = None, max_loras: int | None = None, output_len: int | None = None, enable_multimodal_chat: bool = False, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: # load all data if needed self.num_available_samples = len(self.data) if num_requests <= 0: num_requests = self.num_available_samples logger.info( \"num_requests is set to 0 or negative, \" \"so using all available samples: %d\", num_requests, ) sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break prompt = item[\"prompt\"] # apply template if not skip_chat_template: prompt = tokenizer.apply_chat_template( [{\"role\": \"user\", \"content\": prompt}], add_generation_prompt=True, tokenize=False, ) prompt_len = len(tokenizer(prompt).input_ids) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests __init__ ¶ __init__(**kwargs) -> None Source code in vllm/benchmarks/datasets.py 1938 1939 1940def __init__(self, **kwargs) -> None: super().__init__(**kwargs) self.load_data() load_data ¶ load_data() -> None Source code in vllm/benchmarks/datasets.py 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974def load_data(self) -> None: if self.dataset_path is None: raise ValueError(\"dataset_path must be provided for loading data.\") # self.data will be a list of dictionaries # e.g., [{\"prompt\": \"What is the capital of India?\"}, ...] # This will be the standardized format which load_data() # has to convert into depending on the filetype of dataset_path. # sample() will assume this standardized format of self.data self.data = [] # Load the JSONL file if self.dataset_path.endswith(\".jsonl\"): jsonl_data = pd.read_json(path_or_buf=self.dataset_path, lines=True) # check if the JSONL file has a 'prompt' column if \"prompt\" not in jsonl_data.columns: raise ValueError(\"JSONL file must contain a 'prompt' column.\") # Convert each row to a dictionary and append to self.data # This will convert the DataFrame to a list of dictionaries # where each dictionary corresponds to a row in the DataFrame. # This is the standardized format we want for self.data for _, row in jsonl_data.iterrows(): self.data.append(row.to_dict()) else: raise NotImplementedError( \"Only JSONL format is supported for CustomDataset.\" ) random.seed(self.random_seed) if not getattr(self, \"disable_shuffle\", False): random.shuffle(self.data) sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, lora_path: str | None = None, max_loras: int | None = None, output_len: int | None = None, enable_multimodal_chat: bool = False, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 1976 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025 2026def sample( self, tokenizer: TokenizerLike, num_requests: int, lora_path: str | None = None, max_loras: int | None = None, output_len: int | None = None, enable_multimodal_chat: bool = False, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: # load all data if needed self.num_available_samples = len(self.data) if num_requests <= 0: num_requests = self.num_available_samples logger.info( \"num_requests is set to 0 or negative, \" \"so using all available samples: %d\", num_requests, ) sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break prompt = item[\"prompt\"] # apply template if not skip_chat_template: prompt = tokenizer.apply_chat_template( [{\"role\": \"user\", \"content\": prompt}], add_generation_prompt=True, tokenize=False, ) prompt_len = len(tokenizer(prompt).input_ids) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests HuggingFaceDataset ¶ Bases: BenchmarkDataset Base class for datasets hosted on HuggingFace. Source code in vllm/benchmarks/datasets.py 2245 2246 2247 2248 2249 2250 2251 2252 2253 2254 2255 2256 2257 2258 2259 2260 2261 2262 2263 2264 2265 2266 2267 2268 2269 2270 2271 2272 2273 2274 2275 2276class HuggingFaceDataset(BenchmarkDataset): \"\"\"Base class for datasets hosted on HuggingFace.\"\"\" SUPPORTED_DATASET_PATHS: set[str] | dict[str, Callable] = set() def __init__( self, dataset_path: str, dataset_split: str, no_stream: bool = False, dataset_subset: str | None = None, hf_name: str | None = None, **kwargs, ) -> None: super().__init__(dataset_path=dataset_path, **kwargs) self.dataset_split = dataset_split self.dataset_subset = dataset_subset self.load_stream = not no_stream self.hf_name = hf_name or dataset_path self.load_data() def load_data(self) -> None: \"\"\"Load data from HuggingFace datasets.\"\"\" self.data = load_dataset( self.dataset_path, name=self.dataset_subset, split=self.dataset_split, streaming=self.load_stream, ) if not getattr(self, \"disable_shuffle\", False): self.data = self.data.shuffle(seed=self.random_seed) SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS: set[str] | dict[str, Callable] = ( set() ) dataset_split instance-attribute ¶ dataset_split = dataset_split dataset_subset instance-attribute ¶ dataset_subset = dataset_subset hf_name instance-attribute ¶ hf_name = hf_name or dataset_path load_stream instance-attribute ¶ load_stream = not no_stream __init__ ¶ __init__( dataset_path: str, dataset_split: str, no_stream: bool = False, dataset_subset: str | None = None, hf_name: str | None = None, **kwargs, ) -> None Source code in vllm/benchmarks/datasets.py 2250 2251 2252 2253 2254 2255 2256 2257 2258 2259 2260 2261 2262 2263 2264 2265def __init__( self, dataset_path: str, dataset_split: str, no_stream: bool = False, dataset_subset: str | None = None, hf_name: str | None = None, **kwargs, ) -> None: super().__init__(dataset_path=dataset_path, **kwargs) self.dataset_split = dataset_split self.dataset_subset = dataset_subset self.load_stream = not no_stream self.hf_name = hf_name or dataset_path self.load_data() load_data ¶ load_data() -> None Load data from HuggingFace datasets. Source code in vllm/benchmarks/datasets.py 2267 2268 2269 2270 2271 2272 2273 2274 2275 2276def load_data(self) -> None: \"\"\"Load data from HuggingFace datasets.\"\"\" self.data = load_dataset( self.dataset_path, name=self.dataset_subset, split=self.dataset_split, streaming=self.load_stream, ) if not getattr(self, \"disable_shuffle\", False): self.data = self.data.shuffle(seed=self.random_seed) InstructCoderDataset ¶ Bases: HuggingFaceDataset InstructCoder Dataset. https://huggingface.co/datasets/likaixin/InstructCoder InstructCoder is the dataset designed for general code editing. It consists of 114,239 instruction-input-output triplets, and covers multiple distinct code editing scenario. Source code in vllm/benchmarks/datasets.py 2521 2522 2523 2524 2525 2526 2527 2528 2529 2530 2531 2532 2533 2534 2535 2536 2537 2538 2539 2540 2541 2542 2543 2544 2545 2546 2547 2548 2549 2550 2551 2552 2553 2554 2555 2556 2557 2558 2559 2560 2561 2562 2563 2564 2565 2566 2567 2568 2569 2570 2571 2572 2573 2574 2575 2576 2577class InstructCoderDataset(HuggingFaceDataset): \"\"\" InstructCoder Dataset. https://huggingface.co/datasets/likaixin/InstructCoder InstructCoder is the dataset designed for general code editing. It consists of 114,239 instruction-input-output triplets, and covers multiple distinct code editing scenario. \"\"\" DEFAULT_OUTPUT_LEN = 200 # this is the average default output length SUPPORTED_DATASET_PATHS = { \"likaixin/InstructCoder\", } def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break prompt = ( f\"{item['input']}\\n\\n{item['instruction']} Just output \" \"the code, do not include any explanation.\" ) # apply template if not skip_chat_template: prompt = tokenizer.apply_chat_template( [{\"role\": \"user\", \"content\": prompt}], add_generation_prompt=True, tokenize=False, ) prompt_len = len(tokenizer(prompt).input_ids) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶ DEFAULT_OUTPUT_LEN = 200 SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = {'likaixin/InstructCoder'} sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 2536 2537 2538 2539 2540 2541 2542 2543 2544 2545 2546 2547 2548 2549 2550 2551 2552 2553 2554 2555 2556 2557 2558 2559 2560 2561 2562 2563 2564 2565 2566 2567 2568 2569 2570 2571 2572 2573 2574 2575 2576 2577def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break prompt = ( f\"{item['input']}\\n\\n{item['instruction']} Just output \" \"the code, do not include any explanation.\" ) # apply template if not skip_chat_template: prompt = tokenizer.apply_chat_template( [{\"role\": \"user\", \"content\": prompt}], add_generation_prompt=True, tokenize=False, ) prompt_len = len(tokenizer(prompt).input_ids) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests MLPerfDataset ¶ Bases: HuggingFaceDataset MLPerf Inference Dataset. Dataset on HF: https://huggingface.co/datasets/mgoin/mlperf-inference-llama2-data https://huggingface.co/datasets/mgoin/mlperf-inference-llama3.1-data Each record contains \"system_prompt\": system role instruction. \"question\": user question. \"output\": reference answer. We combine the system prompt and question into a chat-formatted prompt (using the tokenizer's chat template) and set the expected output length to the tokenized length of the provided reference answer. Source code in vllm/benchmarks/datasets.py 2984 2985 2986 2987 2988 2989 2990 2991 2992 2993 2994 2995 2996 2997 2998 2999 3000 3001 3002 3003 3004 3005 3006 3007 3008 3009 3010 3011 3012 3013 3014 3015 3016 3017 3018 3019 3020 3021 3022 3023 3024 3025 3026 3027 3028 3029 3030 3031 3032 3033 3034 3035 3036 3037 3038 3039 3040 3041 3042 3043 3044 3045 3046 3047 3048 3049 3050 3051 3052 3053 3054 3055 3056 3057 3058 3059 3060 3061 3062class MLPerfDataset(HuggingFaceDataset): \"\"\" MLPerf Inference Dataset. Dataset on HF: https://huggingface.co/datasets/mgoin/mlperf-inference-llama2-data https://huggingface.co/datasets/mgoin/mlperf-inference-llama3.1-data Each record contains: - \"system_prompt\": system role instruction. - \"question\": user question. - \"output\": reference answer. We combine the system prompt and question into a chat-formatted prompt (using the tokenizer's chat template) and set the expected output length to the tokenized length of the provided reference answer. \"\"\" SUPPORTED_DATASET_PATHS = { \"mgoin/mlperf-inference-llama2-data\", \"mgoin/mlperf-inference-llama3.1-data\", } def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest]: # Force dynamic output length based on reference completion. dynamic_output = output_len is None sampled_requests: list[SampleRequest] = [] ind = 0 for item in self.data: if len(sampled_requests) >= num_requests: break system_prompt = item[\"system_prompt\"] question = item[\"question\"] reference_answer = item[\"output\"] # Build chat-style prompt using tokenizer template, if available. messages = [ {\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": question}, ] prompt_formatted = tokenizer.apply_chat_template( messages, add_generation_prompt=True, tokenize=False ) prompt_len = len(tokenizer(prompt_formatted).input_ids) # Determine output length from reference answer tokens. ref_out_len = len( tokenizer(reference_answer, add_special_tokens=False).input_ids ) expected_output_len = ref_out_len if dynamic_output else output_len # Validate sequence lengths. if not is_valid_sequence(prompt_len, expected_output_len): continue sampled_requests.append( SampleRequest( prompt=prompt_formatted, prompt_len=prompt_len, expected_output_len=expected_output_len, request_id=request_id_prefix + str(ind), ) ) ind += 1 self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = { \"mgoin/mlperf-inference-llama2-data\", \"mgoin/mlperf-inference-llama3.1-data\", } sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest] Source code in vllm/benchmarks/datasets.py 3007 3008 3009 3010 3011 3012 3013 3014 3015 3016 3017 3018 3019 3020 3021 3022 3023 3024 3025 3026 3027 3028 3029 3030 3031 3032 3033 3034 3035 3036 3037 3038 3039 3040 3041 3042 3043 3044 3045 3046 3047 3048 3049 3050 3051 3052 3053 3054 3055 3056 3057 3058 3059 3060 3061 3062def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest]: # Force dynamic output length based on reference completion. dynamic_output = output_len is None sampled_requests: list[SampleRequest] = [] ind = 0 for item in self.data: if len(sampled_requests) >= num_requests: break system_prompt = item[\"system_prompt\"] question = item[\"question\"] reference_answer = item[\"output\"] # Build chat-style prompt using tokenizer template, if available. messages = [ {\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": question}, ] prompt_formatted = tokenizer.apply_chat_template( messages, add_generation_prompt=True, tokenize=False ) prompt_len = len(tokenizer(prompt_formatted).input_ids) # Determine output length from reference answer tokens. ref_out_len = len( tokenizer(reference_answer, add_special_tokens=False).input_ids ) expected_output_len = ref_out_len if dynamic_output else output_len # Validate sequence lengths. if not is_valid_sequence(prompt_len, expected_output_len): continue sampled_requests.append( SampleRequest( prompt=prompt_formatted, prompt_len=prompt_len, expected_output_len=expected_output_len, request_id=request_id_prefix + str(ind), ) ) ind += 1 self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests MMStarDataset ¶ Bases: HuggingFaceDataset Lin-Chen/MMStar: https://huggingface.co/datasets/Lin-Chen/MMStar refer to: https://github.com/sgl-project/SpecForge/pull/106 Source code in vllm/benchmarks/datasets.py 3162 3163 3164 3165 3166 3167 3168 3169 3170 3171 3172 3173 3174 3175 3176 3177 3178 3179 3180 3181 3182 3183 3184 3185 3186 3187 3188 3189 3190 3191 3192 3193 3194 3195 3196 3197 3198 3199 3200 3201 3202 3203 3204 3205 3206 3207 3208 3209 3210 3211 3212 3213 3214 3215 3216 3217 3218 3219 3220 3221 3222 3223 3224 3225 3226 3227class MMStarDataset(HuggingFaceDataset): \"\"\" Lin-Chen/MMStar: https://huggingface.co/datasets/Lin-Chen/MMStar refer to: https://github.com/sgl-project/SpecForge/pull/106 \"\"\" DEFAULT_OUTPUT_LEN = 128 SUPPORTED_DATASET_PATHS = {\"Lin-Chen/MMStar\"} IS_MULTIMODAL = True def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest]: # If --hf-output-len is not set, use the default output length. output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests: list[SampleRequest] = [] for ind, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break # Split the question text from options # (keep only the part before \"Options:\"). full_q: str = item.get(\"question\", \"\") question_text = full_q.split(\"Options:\", 1)[0].strip() # Multimodal image content. mm_content = process_image(item[\"image\"]) # Compute prompt token length (note: this is plain text length # if enable_multimodal_chat is False). prompt_len = len(tokenizer(question_text).input_ids) if enable_multimodal_chat: # If multimodal content should be embedded in the chat message, # convert to [{\"role\":\"user\",\"content\":[...]}] prompt = self.apply_multimodal_chat_transformation( question_text, mm_content ) mm_for_request = None # Already embedded in chat content. else: # Default: prompt is plain text, # image is in mm_content for the bench to assemble. prompt = question_text mm_for_request = mm_content sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_for_request, request_id=request_id_prefix + str(ind), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶ DEFAULT_OUTPUT_LEN = 128 IS_MULTIMODAL class-attribute instance-attribute ¶ IS_MULTIMODAL = True SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = {'Lin-Chen/MMStar'} sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest] Source code in vllm/benchmarks/datasets.py 3172 3173 3174 3175 3176 3177 3178 3179 3180 3181 3182 3183 3184 3185 3186 3187 3188 3189 3190 3191 3192 3193 3194 3195 3196 3197 3198 3199 3200 3201 3202 3203 3204 3205 3206 3207 3208 3209 3210 3211 3212 3213 3214 3215 3216 3217 3218 3219 3220 3221 3222 3223 3224 3225 3226 3227def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest]: # If --hf-output-len is not set, use the default output length. output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests: list[SampleRequest] = [] for ind, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break # Split the question text from options # (keep only the part before \"Options:\"). full_q: str = item.get(\"question\", \"\") question_text = full_q.split(\"Options:\", 1)[0].strip() # Multimodal image content. mm_content = process_image(item[\"image\"]) # Compute prompt token length (note: this is plain text length # if enable_multimodal_chat is False). prompt_len = len(tokenizer(question_text).input_ids) if enable_multimodal_chat: # If multimodal content should be embedded in the chat message, # convert to [{\"role\":\"user\",\"content\":[...]}] prompt = self.apply_multimodal_chat_transformation( question_text, mm_content ) mm_for_request = None # Already embedded in chat content. else: # Default: prompt is plain text, # image is in mm_content for the bench to assemble. prompt = question_text mm_for_request = mm_content sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_for_request, request_id=request_id_prefix + str(ind), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests MMVUDataset ¶ Bases: HuggingFaceDataset MMVU Dataset. https://huggingface.co/datasets/yale-nlp/MMVU Source code in vllm/benchmarks/datasets.py 2462 2463 2464 2465 2466 2467 2468 2469 2470 2471 2472 2473 2474 2475 2476 2477 2478 2479 2480 2481 2482 2483 2484 2485 2486 2487 2488 2489 2490 2491 2492 2493 2494 2495 2496 2497 2498 2499 2500 2501 2502 2503 2504 2505 2506 2507 2508 2509 2510 2511 2512 2513class MMVUDataset(HuggingFaceDataset): \"\"\" MMVU Dataset. https://huggingface.co/datasets/yale-nlp/MMVU \"\"\" DEFAULT_OUTPUT_LEN = 128 SUPPORTED_DATASET_PATHS = { \"yale-nlp/MMVU\": lambda x: x[\"question\"] + \" \" + (\" \".join(f\"{k}.{v}\" for k, v in x[\"choices\"].items())), } def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break parser_fn = self.SUPPORTED_DATASET_PATHS.get(self.hf_name) if parser_fn is None: raise ValueError(f\"Unsupported dataset path: {self.hf_name}\") prompt = parser_fn(item) mm_content = process_video(item[\"video\"]) prompt_len = len(tokenizer(prompt).input_ids) if enable_multimodal_chat: # Note: when chat is enabled the request prompt_len is no longer # accurate and we will be using request output to count the # actual prompt len prompt = self.apply_multimodal_chat_transformation(prompt, mm_content) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_content, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶ DEFAULT_OUTPUT_LEN = 128 SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = { \"yale-nlp/MMVU\": lambda x: x[\"question\"] + \" \" + join(f\"{k}.{v}\" for k, v in (items())) } sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 2475 2476 2477 2478 2479 2480 2481 2482 2483 2484 2485 2486 2487 2488 2489 2490 2491 2492 2493 2494 2495 2496 2497 2498 2499 2500 2501 2502 2503 2504 2505 2506 2507 2508 2509 2510 2511 2512 2513def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break parser_fn = self.SUPPORTED_DATASET_PATHS.get(self.hf_name) if parser_fn is None: raise ValueError(f\"Unsupported dataset path: {self.hf_name}\") prompt = parser_fn(item) mm_content = process_video(item[\"video\"]) prompt_len = len(tokenizer(prompt).input_ids) if enable_multimodal_chat: # Note: when chat is enabled the request prompt_len is no longer # accurate and we will be using request output to count the # actual prompt len prompt = self.apply_multimodal_chat_transformation(prompt, mm_content) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_content, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests MTBenchDataset ¶ Bases: HuggingFaceDataset MT-Bench Dataset. https://huggingface.co/datasets/philschmid/mt-bench We create a single turn dataset for MT-Bench. This is similar to Spec decoding benchmark setup in vLLM https://github.com/vllm-project/vllm/blob/9d98ab5ec/examples/offline_inference/eagle.py#L14-L18 Source code in vllm/benchmarks/datasets.py 2585 2586 2587 2588 2589 2590 2591 2592 2593 2594 2595 2596 2597 2598 2599 2600 2601 2602 2603 2604 2605 2606 2607 2608 2609 2610 2611 2612 2613 2614 2615 2616 2617 2618 2619 2620 2621 2622 2623 2624 2625 2626 2627 2628 2629 2630 2631 2632 2633 2634 2635 2636 2637 2638 2639class MTBenchDataset(HuggingFaceDataset): \"\"\" MT-Bench Dataset. https://huggingface.co/datasets/philschmid/mt-bench We create a single turn dataset for MT-Bench. This is similar to Spec decoding benchmark setup in vLLM https://github.com/vllm-project/vllm/blob/9d98ab5ec/examples/offline_inference/eagle.py#L14-L18 \"\"\" # noqa: E501 DEFAULT_OUTPUT_LEN = 256 # avg len used in SD bench in vLLM SUPPORTED_DATASET_PATHS = { \"philschmid/mt-bench\", } def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break prompt = item[\"turns\"][0] # apply template if not skip_chat_template: prompt = tokenizer.apply_chat_template( [{\"role\": \"user\", \"content\": prompt}], add_generation_prompt=True, tokenize=False, ) prompt_len = len(tokenizer(prompt).input_ids) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶ DEFAULT_OUTPUT_LEN = 256 SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = {'philschmid/mt-bench'} sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 2600 2601 2602 2603 2604 2605 2606 2607 2608 2609 2610 2611 2612 2613 2614 2615 2616 2617 2618 2619 2620 2621 2622 2623 2624 2625 2626 2627 2628 2629 2630 2631 2632 2633 2634 2635 2636 2637 2638 2639def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break prompt = item[\"turns\"][0] # apply template if not skip_chat_template: prompt = tokenizer.apply_chat_template( [{\"role\": \"user\", \"content\": prompt}], add_generation_prompt=True, tokenize=False, ) prompt_len = len(tokenizer(prompt).input_ids) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests MultiModalConversationDataset ¶ Bases: HuggingFaceDataset Dataset for multimodal conversation data. Source code in vllm/benchmarks/datasets.py 2344 2345 2346 2347 2348 2349 2350 2351 2352 2353 2354 2355 2356 2357 2358 2359 2360 2361 2362 2363 2364 2365 2366 2367 2368 2369 2370 2371 2372 2373 2374 2375 2376 2377 2378 2379 2380 2381 2382 2383 2384 2385 2386 2387 2388 2389 2390 2391 2392 2393 2394 2395 2396 2397 2398 2399 2400 2401class MultiModalConversationDataset(HuggingFaceDataset): \"\"\"Dataset for multimodal conversation data.\"\"\" SUPPORTED_DATASET_PATHS = { \"lmms-lab/LLaVA-OneVision-Data\", } IS_MULTIMODAL = True def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: # Filter examples with at least 2 conversations filtered_data = self.data.filter(lambda x: len(x[\"conversations\"]) >= 2) sampled_requests = [] ind = 0 dynamic_output = output_len is None for item in filtered_data: if len(sampled_requests) >= num_requests: break conv = item[\"conversations\"] prompt, completion = conv[0][\"value\"], conv[1][\"value\"] prompt_ids = tokenizer(prompt).input_ids completion_ids = tokenizer(completion).input_ids prompt_len = len(prompt_ids) completion_len = len(completion_ids) output_len = completion_len if dynamic_output else output_len assert isinstance(output_len, int) and output_len > 0 if dynamic_output and not is_valid_sequence(prompt_len, completion_len): continue mm_content = process_image(item[\"image\"]) if \"image\" in item else None if enable_multimodal_chat: # Note: when chat is enabled the request prompt_len is no longer # accurate and we will be using request output to count the # actual prompt len and output len prompt = self.apply_multimodal_chat_transformation(prompt, mm_content) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_content, request_id=request_id_prefix + str(ind), ) ) ind += 1 self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests IS_MULTIMODAL class-attribute instance-attribute ¶ IS_MULTIMODAL = True SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = {'lmms-lab/LLaVA-OneVision-Data'} sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 2352 2353 2354 2355 2356 2357 2358 2359 2360 2361 2362 2363 2364 2365 2366 2367 2368 2369 2370 2371 2372 2373 2374 2375 2376 2377 2378 2379 2380 2381 2382 2383 2384 2385 2386 2387 2388 2389 2390 2391 2392 2393 2394 2395 2396 2397 2398 2399 2400 2401def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: # Filter examples with at least 2 conversations filtered_data = self.data.filter(lambda x: len(x[\"conversations\"]) >= 2) sampled_requests = [] ind = 0 dynamic_output = output_len is None for item in filtered_data: if len(sampled_requests) >= num_requests: break conv = item[\"conversations\"] prompt, completion = conv[0][\"value\"], conv[1][\"value\"] prompt_ids = tokenizer(prompt).input_ids completion_ids = tokenizer(completion).input_ids prompt_len = len(prompt_ids) completion_len = len(completion_ids) output_len = completion_len if dynamic_output else output_len assert isinstance(output_len, int) and output_len > 0 if dynamic_output and not is_valid_sequence(prompt_len, completion_len): continue mm_content = process_image(item[\"image\"]) if \"image\" in item else None if enable_multimodal_chat: # Note: when chat is enabled the request prompt_len is no longer # accurate and we will be using request output to count the # actual prompt len and output len prompt = self.apply_multimodal_chat_transformation(prompt, mm_content) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_content, request_id=request_id_prefix + str(ind), ) ) ind += 1 self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests NextEditPredictionDataset ¶ Bases: HuggingFaceDataset Dataset class for processing a Next Edit Prediction dataset. Source code in vllm/benchmarks/datasets.py 2845 2846 2847 2848 2849 2850 2851 2852 2853 2854 2855 2856 2857 2858 2859 2860 2861 2862 2863 2864 2865 2866 2867 2868 2869 2870 2871 2872 2873 2874 2875 2876 2877 2878 2879 2880 2881 2882 2883 2884 2885 2886class NextEditPredictionDataset(HuggingFaceDataset): \"\"\" Dataset class for processing a Next Edit Prediction dataset. \"\"\" SUPPORTED_DATASET_PATHS = { \"zed-industries/zeta\", } MAPPING_PROMPT_FUNCS = { \"zed-industries/zeta\": _format_zeta_prompt, } def sample( self, tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ): formatting_prompt_func = self.MAPPING_PROMPT_FUNCS.get(self.hf_name) if formatting_prompt_func is None: raise ValueError(f\"Unsupported dataset path: {self.hf_name}\") samples = [] for i, sample in enumerate(self.data): sample = formatting_prompt_func(sample) samples.append( SampleRequest( prompt=sample[\"prompt\"], prompt_len=len(tokenizer(sample[\"prompt\"]).input_ids), expected_output_len=len( tokenizer(sample[\"expected_output\"]).input_ids ), request_id=request_id_prefix + str(i), ) ) if len(samples) >= num_requests: break self.maybe_oversample_requests( samples, num_requests, request_id_prefix, no_oversample ) return samples MAPPING_PROMPT_FUNCS class-attribute instance-attribute ¶ MAPPING_PROMPT_FUNCS = { \"zed-industries/zeta\": _format_zeta_prompt } SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = {'zed-industries/zeta'} sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) Source code in vllm/benchmarks/datasets.py 2857 2858 2859 2860 2861 2862 2863 2864 2865 2866 2867 2868 2869 2870 2871 2872 2873 2874 2875 2876 2877 2878 2879 2880 2881 2882 2883 2884 2885 2886def sample( self, tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ): formatting_prompt_func = self.MAPPING_PROMPT_FUNCS.get(self.hf_name) if formatting_prompt_func is None: raise ValueError(f\"Unsupported dataset path: {self.hf_name}\") samples = [] for i, sample in enumerate(self.data): sample = formatting_prompt_func(sample) samples.append( SampleRequest( prompt=sample[\"prompt\"], prompt_len=len(tokenizer(sample[\"prompt\"]).input_ids), expected_output_len=len( tokenizer(sample[\"expected_output\"]).input_ids ), request_id=request_id_prefix + str(i), ) ) if len(samples) >= num_requests: break self.maybe_oversample_requests( samples, num_requests, request_id_prefix, no_oversample ) return samples PrefixRepetitionRandomDataset ¶ Bases: BenchmarkDataset Source code in vllm/benchmarks/datasets.py 3070 3071 3072 3073 3074 3075 3076 3077 3078 3079 3080 3081 3082 3083 3084 3085 3086 3087 3088 3089 3090 3091 3092 3093 3094 3095 3096 3097 3098 3099 3100 3101 3102 3103 3104 3105 3106 3107 3108 3109 3110 3111 3112 3113 3114 3115 3116 3117 3118 3119 3120 3121 3122 3123 3124 3125 3126 3127 3128 3129 3130 3131 3132 3133 3134 3135 3136 3137 3138 3139 3140 3141 3142 3143 3144 3145 3146 3147 3148 3149 3150 3151 3152 3153 3154class PrefixRepetitionRandomDataset(BenchmarkDataset): # Default values copied from benchmark_serving.py for the repeated prefix # dataset. DEFAULT_PREFIX_LEN = 256 DEFAULT_SUFFIX_LEN = 256 DEFAULT_NUM_PREFIXES = 10 DEFAULT_OUTPUT_LEN = 128 def __init__( self, **kwargs, ) -> None: super().__init__(**kwargs) random.seed(self.random_seed) np.random.seed(self.random_seed) def sample( self, tokenizer: TokenizerLike, num_requests: int, prefix_len: int = DEFAULT_PREFIX_LEN, suffix_len: int = DEFAULT_SUFFIX_LEN, num_prefixes: int = DEFAULT_NUM_PREFIXES, output_len: int = DEFAULT_OUTPUT_LEN, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest]: vocab_size = tokenizer.vocab_size prompts_per_prefix = num_requests // num_prefixes if prompts_per_prefix == 0: raise ValueError( f\"num_requests ({num_requests}) must be greater than or equal \" f\"to num_prefixes ({num_prefixes})\" ) def _generate_exact_length_tokens(target_length: int) -> list[int]: \"\"\"Generate tokens that decode and re-encode to exactly target_length.\"\"\" # Generate random tokens tokens = np.random.randint(0, vocab_size, size=target_length).tolist() _, adjusted_tokens, token_mismatch = gen_prompt_decode_to_target_len( # noqa: E501 tokenizer=tokenizer, token_sequence=tokens, target_token_len=target_length, add_special_tokens=False, ) return adjusted_tokens, token_mismatch requests = [] token_mismatch_total = 0 for _ in range(num_prefixes): prefix_tokens, prefix_mismatch = _generate_exact_length_tokens(prefix_len) token_mismatch_total += prefix_mismatch for _ in range(prompts_per_prefix): suffix_tokens, suffix_mismatch = _generate_exact_length_tokens( suffix_len ) token_mismatch_total += suffix_mismatch combined_tokens = prefix_tokens + suffix_tokens prompt = tokenizer.decode(combined_tokens) prompt_len = len(combined_tokens) requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, ) ) if token_mismatch_total != 0: sign = \"more\" if token_mismatch_total > 0 else \"fewer\" logger.warning( \"Across all generated prompts, there were %d %s tokens \" \"than expected after decoding and re-encoding. This is \" \"expected due to the imperfect nature of the sampling \" \"procedure.\", abs(token_mismatch_total), sign, ) if not getattr(self, \"disable_shuffle\", False): random.shuffle(requests) return requests DEFAULT_NUM_PREFIXES class-attribute instance-attribute ¶ DEFAULT_NUM_PREFIXES = 10 DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶ DEFAULT_OUTPUT_LEN = 128 DEFAULT_PREFIX_LEN class-attribute instance-attribute ¶ DEFAULT_PREFIX_LEN = 256 DEFAULT_SUFFIX_LEN class-attribute instance-attribute ¶ DEFAULT_SUFFIX_LEN = 256 __init__ ¶ __init__(**kwargs) -> None Source code in vllm/benchmarks/datasets.py 3078 3079 3080 3081 3082 3083 3084def __init__( self, **kwargs, ) -> None: super().__init__(**kwargs) random.seed(self.random_seed) np.random.seed(self.random_seed) sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, prefix_len: int = DEFAULT_PREFIX_LEN, suffix_len: int = DEFAULT_SUFFIX_LEN, num_prefixes: int = DEFAULT_NUM_PREFIXES, output_len: int = DEFAULT_OUTPUT_LEN, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest] Source code in vllm/benchmarks/datasets.py 3086 3087 3088 3089 3090 3091 3092 3093 3094 3095 3096 3097 3098 3099 3100 3101 3102 3103 3104 3105 3106 3107 3108 3109 3110 3111 3112 3113 3114 3115 3116 3117 3118 3119 3120 3121 3122 3123 3124 3125 3126 3127 3128 3129 3130 3131 3132 3133 3134 3135 3136 3137 3138 3139 3140 3141 3142 3143 3144 3145 3146 3147 3148 3149 3150 3151 3152 3153 3154def sample( self, tokenizer: TokenizerLike, num_requests: int, prefix_len: int = DEFAULT_PREFIX_LEN, suffix_len: int = DEFAULT_SUFFIX_LEN, num_prefixes: int = DEFAULT_NUM_PREFIXES, output_len: int = DEFAULT_OUTPUT_LEN, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest]: vocab_size = tokenizer.vocab_size prompts_per_prefix = num_requests // num_prefixes if prompts_per_prefix == 0: raise ValueError( f\"num_requests ({num_requests}) must be greater than or equal \" f\"to num_prefixes ({num_prefixes})\" ) def _generate_exact_length_tokens(target_length: int) -> list[int]: \"\"\"Generate tokens that decode and re-encode to exactly target_length.\"\"\" # Generate random tokens tokens = np.random.randint(0, vocab_size, size=target_length).tolist() _, adjusted_tokens, token_mismatch = gen_prompt_decode_to_target_len( # noqa: E501 tokenizer=tokenizer, token_sequence=tokens, target_token_len=target_length, add_special_tokens=False, ) return adjusted_tokens, token_mismatch requests = [] token_mismatch_total = 0 for _ in range(num_prefixes): prefix_tokens, prefix_mismatch = _generate_exact_length_tokens(prefix_len) token_mismatch_total += prefix_mismatch for _ in range(prompts_per_prefix): suffix_tokens, suffix_mismatch = _generate_exact_length_tokens( suffix_len ) token_mismatch_total += suffix_mismatch combined_tokens = prefix_tokens + suffix_tokens prompt = tokenizer.decode(combined_tokens) prompt_len = len(combined_tokens) requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, ) ) if token_mismatch_total != 0: sign = \"more\" if token_mismatch_total > 0 else \"fewer\" logger.warning( \"Across all generated prompts, there were %d %s tokens \" \"than expected after decoding and re-encoding. This is \" \"expected due to the imperfect nature of the sampling \" \"procedure.\", abs(token_mismatch_total), sign, ) if not getattr(self, \"disable_shuffle\", False): random.shuffle(requests) return requests RandomDataset ¶ Bases: BenchmarkDataset Synthetic text-only dataset for serving/throughput benchmarks. Strategy: - Sample input/output token lengths per request from integer-uniform ranges around configured means (controlled by range_ratio). - Prepend a fixed random prefix of length prefix_len. - Generate the remaining tokens as a reproducible sequence: (offset + index + arange(input_len)) % vocab_size. - Decode then re-encode/truncate to ensure prompt token counts match. - Uses numpy.default_rng seeded with random_seed for reproducible sampling. Source code in vllm/benchmarks/datasets.py 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668class RandomDataset(BenchmarkDataset): \"\"\" Synthetic text-only dataset for serving/throughput benchmarks. Strategy: - Sample input/output token lengths per request from integer-uniform ranges around configured means (controlled by range_ratio). - Prepend a fixed random prefix of length prefix_len. - Generate the remaining tokens as a reproducible sequence: (offset + index + arange(input_len)) % vocab_size. - Decode then re-encode/truncate to ensure prompt token counts match. - Uses numpy.default_rng seeded with random_seed for reproducible sampling. \"\"\" # Default values copied from benchmark_serving.py for the random dataset. DEFAULT_PREFIX_LEN = 0 DEFAULT_RANGE_RATIO = 0.0 DEFAULT_INPUT_LEN = 1024 DEFAULT_OUTPUT_LEN = 128 def __init__(self, **kwargs) -> None: super().__init__(**kwargs) # Use numpy's default_rng for deterministic sampling # Do not use random.seed() or np.random.seed() elsewhere in this class. # This ensures that the RNG is isolated from global RNG state. self._rng = np.random.default_rng(self.random_seed) def sample( self, tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, prefix_len: int = DEFAULT_PREFIX_LEN, range_ratio: float = DEFAULT_RANGE_RATIO, input_len: int = DEFAULT_INPUT_LEN, output_len: int = DEFAULT_OUTPUT_LEN, batchsize: int = 1, **kwargs, ) -> list[SampleRequest]: # validate total input tokens (prefix + sampled) is at least 1. num_special = int(tokenizer.num_special_tokens_to_add()) real_input_len = max(0, int(input_len) - num_special) min_sampled_input = math.floor(real_input_len * (1.0 - float(range_ratio))) min_total_input = int(prefix_len) + min_sampled_input if min_total_input < 1: raise ValueError( \"--random-input-len is too small: with tokenizer special \" f\"tokens {num_special} and --random-range-ratio {range_ratio}, \" \"the minimum possible total input tokens (prefix + sampled) is \" f\"{min_total_input}. Increase --random-input-len and/or \" \"--random-prefix-len, or decrease --random-range-ratio so that \" \"prefix_len + floor(max(0, random_input_len - num_special)) \" \"* (1 - range_ratio) >= 1.\" ) input_lens, output_lens, offsets = self.get_sampling_params( num_requests, range_ratio, input_len, output_len, tokenizer ) vocab_size = tokenizer.vocab_size prohibited_tokens = tokenizer.all_special_ids all_tokens = np.arange(vocab_size) allowed_tokens = np.array(list(set(all_tokens) - set(prohibited_tokens))) # Generate prefix once prefix_token_ids = self.get_prefix(allowed_tokens, prefix_len) requests = [] token_mismatch_total = 0 for i in range(num_requests): prompt, total_input_len, token_mismatch = self.generate_token_sequence( # noqa: E501 tokenizer=tokenizer, prefix_token_ids=prefix_token_ids, prefix_len=prefix_len, vocab_size=vocab_size, input_len=int(input_lens[i]), offset=int(offsets[i]), index=i, allowed_tokens=allowed_tokens, ) token_mismatch_total += token_mismatch requests.append( SampleRequest( prompt=prompt, prompt_len=total_input_len, expected_output_len=int(output_lens[i]), request_id=request_id_prefix + str(i), ) ) # only used for embeddings benchmark. if batchsize > 1: batch_requests = [] # Create batched requests for i in range(0, num_requests, batchsize): batch = requests[i : i + batchsize] batch_requests.append( SampleRequest( prompt=[req.prompt for req in batch], prompt_len=sum(req.prompt_len for req in batch), expected_output_len=0, request_id=request_id_prefix + str(i // batchsize), ) ) requests = batch_requests if token_mismatch_total != 0: sign = \"more\" if token_mismatch_total > 0 else \"fewer\" logger.warning( \"Across all generated prompts, there were %d %s tokens \" \"than expected after decoding and re-encoding. This is \" \"expected due to the imperfect nature of the sampling \" \"procedure.\", abs(token_mismatch_total), sign, ) return requests def get_prefix( self, allowed_tokens: np.ndarray, prefix_len: int, ) -> list[int]: \"\"\" Get the prefix for the dataset. \"\"\" return ( allowed_tokens[ self._rng.integers(0, len(allowed_tokens), size=prefix_len) ].tolist() if prefix_len > 0 else [] ) def get_sampling_params( self, num_requests: int, range_ratio: float, input_len: int, output_len: int, tokenizer: TokenizerLike, ) -> tuple[np.ndarray, np.ndarray, np.ndarray]: \"\"\" Get the sampling parameters for the dataset. \"\"\" # Enforce range_ratio < 1 if not (0.0 <= range_ratio < 1.0): raise ValueError(\"range_ratio must be in [0, 1).\") num_special_tokens = int(tokenizer.num_special_tokens_to_add()) real_input_len = max(0, int(input_len) - num_special_tokens) # Bounds use floor for low and ceil for high input_low = math.floor(real_input_len * (1 - range_ratio)) input_high = math.ceil(real_input_len * (1 + range_ratio)) output_low = math.floor(output_len * (1 - range_ratio)) output_high = math.ceil(output_len * (1 + range_ratio)) # Ensure the lower bound for output length is at least 1 to # prevent sampling 0 tokens. output_low = max(output_low, 1) output_high = max(output_high, 1) if input_low > input_high: raise ValueError( f\"Invalid input sampling interval: low={input_low} > high={input_high}\" ) if output_low > output_high: raise ValueError( \"Invalid output sampling interval: \" f\"low={output_low} > high={output_high}\" ) logger.info( \"Sampling input_len from [%s, %s] and output_len from [%s, %s]\", input_low, input_high, output_low, output_high, ) input_lens = self._rng.integers(input_low, input_high + 1, size=num_requests) output_lens = self._rng.integers(output_low, output_high + 1, size=num_requests) offsets = self._rng.integers(0, tokenizer.vocab_size, size=num_requests) return input_lens, output_lens, offsets def generate_token_sequence( self, *, tokenizer: TokenizerLike, prefix_token_ids: list[int], prefix_len: int, vocab_size: int, input_len: int, offset: int, index: int, allowed_tokens: np.ndarray, ) -> tuple[str, int, int]: \"\"\" Returns (prompt, total_input_len). NOTE: After decoding the prompt we have to encode and decode it again. This is done because in some cases N consecutive tokens give a string tokenized into != N number of tokens. For example for GPT2Tokenizer: [6880, 6881] -> ['Ġcalls', 'here'] -> [1650, 939, 486] -> ['Ġcall', 'sh', 'ere'] To avoid uncontrolled change of the prompt length, the encoded sequence is truncated before being decoded again. \"\"\" # Build the inner sequence by sampling # sequentially from the allowed tokens inner_seq = allowed_tokens[ (offset + index + np.arange(input_len)) % len(allowed_tokens) ].tolist() token_sequence = prefix_token_ids + inner_seq # Decode, then re-encode and truncate to preserve token count invariants total_input_len = prefix_len + int(input_len) prompt, adjusted_token_sequence, token_mismatch = ( gen_prompt_decode_to_target_len( tokenizer=tokenizer, token_sequence=token_sequence, target_token_len=total_input_len, add_special_tokens=False, rng=self._rng, ) ) total_input_len = len(adjusted_token_sequence) return prompt, total_input_len, token_mismatch DEFAULT_INPUT_LEN class-attribute instance-attribute ¶ DEFAULT_INPUT_LEN = 1024 DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶ DEFAULT_OUTPUT_LEN = 128 DEFAULT_PREFIX_LEN class-attribute instance-attribute ¶ DEFAULT_PREFIX_LEN = 0 DEFAULT_RANGE_RATIO class-attribute instance-attribute ¶ DEFAULT_RANGE_RATIO = 0.0 _rng instance-attribute ¶ _rng = default_rng(random_seed) __init__ ¶ __init__(**kwargs) -> None Source code in vllm/benchmarks/datasets.py 461 462 463 464 465 466def __init__(self, **kwargs) -> None: super().__init__(**kwargs) # Use numpy's default_rng for deterministic sampling # Do not use random.seed() or np.random.seed() elsewhere in this class. # This ensures that the RNG is isolated from global RNG state. self._rng = np.random.default_rng(self.random_seed) generate_token_sequence ¶ generate_token_sequence( *, tokenizer: TokenizerLike, prefix_token_ids: list[int], prefix_len: int, vocab_size: int, input_len: int, offset: int, index: int, allowed_tokens: ndarray, ) -> tuple[str, int, int] Returns (prompt, total_input_len). NOTE: After decoding the prompt we have to encode and decode it again. This is done because in some cases N consecutive tokens give a string tokenized into != N number of tokens. For example for GPT2Tokenizer: [6880, 6881] -> ['Ġcalls', 'here'] -> [1650, 939, 486] -> ['Ġcall', 'sh', 'ere'] To avoid uncontrolled change of the prompt length, the encoded sequence is truncated before being decoded again. Source code in vllm/benchmarks/datasets.py 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668def generate_token_sequence( self, *, tokenizer: TokenizerLike, prefix_token_ids: list[int], prefix_len: int, vocab_size: int, input_len: int, offset: int, index: int, allowed_tokens: np.ndarray, ) -> tuple[str, int, int]: \"\"\" Returns (prompt, total_input_len). NOTE: After decoding the prompt we have to encode and decode it again. This is done because in some cases N consecutive tokens give a string tokenized into != N number of tokens. For example for GPT2Tokenizer: [6880, 6881] -> ['Ġcalls', 'here'] -> [1650, 939, 486] -> ['Ġcall', 'sh', 'ere'] To avoid uncontrolled change of the prompt length, the encoded sequence is truncated before being decoded again. \"\"\" # Build the inner sequence by sampling # sequentially from the allowed tokens inner_seq = allowed_tokens[ (offset + index + np.arange(input_len)) % len(allowed_tokens) ].tolist() token_sequence = prefix_token_ids + inner_seq # Decode, then re-encode and truncate to preserve token count invariants total_input_len = prefix_len + int(input_len) prompt, adjusted_token_sequence, token_mismatch = ( gen_prompt_decode_to_target_len( tokenizer=tokenizer, token_sequence=token_sequence, target_token_len=total_input_len, add_special_tokens=False, rng=self._rng, ) ) total_input_len = len(adjusted_token_sequence) return prompt, total_input_len, token_mismatch get_prefix ¶ get_prefix( allowed_tokens: ndarray, prefix_len: int ) -> list[int] Get the prefix for the dataset. Source code in vllm/benchmarks/datasets.py 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574def get_prefix( self, allowed_tokens: np.ndarray, prefix_len: int, ) -> list[int]: \"\"\" Get the prefix for the dataset. \"\"\" return ( allowed_tokens[ self._rng.integers(0, len(allowed_tokens), size=prefix_len) ].tolist() if prefix_len > 0 else [] ) get_sampling_params ¶ get_sampling_params( num_requests: int, range_ratio: float, input_len: int, output_len: int, tokenizer: TokenizerLike, ) -> tuple[ndarray, ndarray, ndarray] Get the sampling parameters for the dataset. Source code in vllm/benchmarks/datasets.py 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623def get_sampling_params( self, num_requests: int, range_ratio: float, input_len: int, output_len: int, tokenizer: TokenizerLike, ) -> tuple[np.ndarray, np.ndarray, np.ndarray]: \"\"\" Get the sampling parameters for the dataset. \"\"\" # Enforce range_ratio < 1 if not (0.0 <= range_ratio < 1.0): raise ValueError(\"range_ratio must be in [0, 1).\") num_special_tokens = int(tokenizer.num_special_tokens_to_add()) real_input_len = max(0, int(input_len) - num_special_tokens) # Bounds use floor for low and ceil for high input_low = math.floor(real_input_len * (1 - range_ratio)) input_high = math.ceil(real_input_len * (1 + range_ratio)) output_low = math.floor(output_len * (1 - range_ratio)) output_high = math.ceil(output_len * (1 + range_ratio)) # Ensure the lower bound for output length is at least 1 to # prevent sampling 0 tokens. output_low = max(output_low, 1) output_high = max(output_high, 1) if input_low > input_high: raise ValueError( f\"Invalid input sampling interval: low={input_low} > high={input_high}\" ) if output_low > output_high: raise ValueError( \"Invalid output sampling interval: \" f\"low={output_low} > high={output_high}\" ) logger.info( \"Sampling input_len from [%s, %s] and output_len from [%s, %s]\", input_low, input_high, output_low, output_high, ) input_lens = self._rng.integers(input_low, input_high + 1, size=num_requests) output_lens = self._rng.integers(output_low, output_high + 1, size=num_requests) offsets = self._rng.integers(0, tokenizer.vocab_size, size=num_requests) return input_lens, output_lens, offsets sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, prefix_len: int = DEFAULT_PREFIX_LEN, range_ratio: float = DEFAULT_RANGE_RATIO, input_len: int = DEFAULT_INPUT_LEN, output_len: int = DEFAULT_OUTPUT_LEN, batchsize: int = 1, **kwargs, ) -> list[SampleRequest] Source code in vllm/benchmarks/datasets.py 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558def sample( self, tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, prefix_len: int = DEFAULT_PREFIX_LEN, range_ratio: float = DEFAULT_RANGE_RATIO, input_len: int = DEFAULT_INPUT_LEN, output_len: int = DEFAULT_OUTPUT_LEN, batchsize: int = 1, **kwargs, ) -> list[SampleRequest]: # validate total input tokens (prefix + sampled) is at least 1. num_special = int(tokenizer.num_special_tokens_to_add()) real_input_len = max(0, int(input_len) - num_special) min_sampled_input = math.floor(real_input_len * (1.0 - float(range_ratio))) min_total_input = int(prefix_len) + min_sampled_input if min_total_input < 1: raise ValueError( \"--random-input-len is too small: with tokenizer special \" f\"tokens {num_special} and --random-range-ratio {range_ratio}, \" \"the minimum possible total input tokens (prefix + sampled) is \" f\"{min_total_input}. Increase --random-input-len and/or \" \"--random-prefix-len, or decrease --random-range-ratio so that \" \"prefix_len + floor(max(0, random_input_len - num_special)) \" \"* (1 - range_ratio) >= 1.\" ) input_lens, output_lens, offsets = self.get_sampling_params( num_requests, range_ratio, input_len, output_len, tokenizer ) vocab_size = tokenizer.vocab_size prohibited_tokens = tokenizer.all_special_ids all_tokens = np.arange(vocab_size) allowed_tokens = np.array(list(set(all_tokens) - set(prohibited_tokens))) # Generate prefix once prefix_token_ids = self.get_prefix(allowed_tokens, prefix_len) requests = [] token_mismatch_total = 0 for i in range(num_requests): prompt, total_input_len, token_mismatch = self.generate_token_sequence( # noqa: E501 tokenizer=tokenizer, prefix_token_ids=prefix_token_ids, prefix_len=prefix_len, vocab_size=vocab_size, input_len=int(input_lens[i]), offset=int(offsets[i]), index=i, allowed_tokens=allowed_tokens, ) token_mismatch_total += token_mismatch requests.append( SampleRequest( prompt=prompt, prompt_len=total_input_len, expected_output_len=int(output_lens[i]), request_id=request_id_prefix + str(i), ) ) # only used for embeddings benchmark. if batchsize > 1: batch_requests = [] # Create batched requests for i in range(0, num_requests, batchsize): batch = requests[i : i + batchsize] batch_requests.append( SampleRequest( prompt=[req.prompt for req in batch], prompt_len=sum(req.prompt_len for req in batch), expected_output_len=0, request_id=request_id_prefix + str(i // batchsize), ) ) requests = batch_requests if token_mismatch_total != 0: sign = \"more\" if token_mismatch_total > 0 else \"fewer\" logger.warning( \"Across all generated prompts, there were %d %s tokens \" \"than expected after decoding and re-encoding. This is \" \"expected due to the imperfect nature of the sampling \" \"procedure.\", abs(token_mismatch_total), sign, ) return requests RandomDatasetForReranking ¶ Bases: RandomDataset Random dataset specialized for the needs of scoring: - Batches of inputs - Inputs composed of pairs Source code in vllm/benchmarks/datasets.py 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780class RandomDatasetForReranking(RandomDataset): \"\"\" Random dataset specialized for the needs of scoring: - Batches of inputs - Inputs composed of pairs \"\"\" def __init__(self, **kwargs) -> None: super().__init__(**kwargs) def sample( self, tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", range_ratio: float = RandomDataset.DEFAULT_RANGE_RATIO, input_len: int = RandomDataset.DEFAULT_INPUT_LEN, batchsize: int = 1, is_reranker: bool = True, **kwargs, ) -> list[SampleRequest]: n_sep_tokens = int(is_reranker) query_len_param = (input_len // 2) - n_sep_tokens if is_reranker else input_len query_lens, _, query_offsets = self.get_sampling_params( 1, range_ratio, query_len_param, 0, tokenizer ) query_len = int(query_lens[0]) if not is_reranker: assert num_requests > 1 and batchsize > 1 num_requests -= 1 batchsize -= 1 doc_len_param = input_len else: doc_len_param = input_len - query_len - n_sep_tokens doc_lens, _, doc_offsets = self.get_sampling_params( num_requests, range_ratio, doc_len_param, 0, tokenizer ) vocab_size = tokenizer.vocab_size prohibited_tokens = tokenizer.all_special_ids all_tokens = np.arange(vocab_size) allowed_tokens = np.array(list(set(all_tokens) - set(prohibited_tokens))) query_prompt, query_input_len, token_mismatch_total = ( self.generate_token_sequence( tokenizer=tokenizer, prefix_token_ids=[], prefix_len=0, vocab_size=vocab_size, input_len=query_len, offset=int(query_offsets[0]), index=0, allowed_tokens=allowed_tokens, ) ) requests = [] for i in range(num_requests): prompt, total_input_len, token_mismatch = self.generate_token_sequence( # noqa: E501 tokenizer=tokenizer, prefix_token_ids=[], prefix_len=0, vocab_size=vocab_size, input_len=int(doc_lens[i]), offset=int(doc_offsets[i]), index=i + 1, allowed_tokens=allowed_tokens, ) token_mismatch_total += token_mismatch requests.append((prompt, total_input_len)) batch_requests = [] # Create batched requests for i in range(0, num_requests, batchsize): batch = requests[i : i + batchsize] query_contrib = ( (query_input_len + n_sep_tokens) * len(batch) if is_reranker else query_input_len ) batch_requests.append( SampleRequest( prompt=[query_prompt] + [req[0] for req in batch], prompt_len=query_contrib + sum(req[1] for req in batch), expected_output_len=0, request_id=request_id_prefix + str(i // batchsize), ) ) if token_mismatch_total != 0: logger.warning( \"Across all generated prompts, there were %d %s tokens \" \"than expected after decoding and re-encoding. This is \" \"expected due to the imperfect nature of the sampling \" \"procedure.\", abs(token_mismatch_total), \"more\" if token_mismatch_total > 0 else \"fewer\", ) return batch_requests __init__ ¶ __init__(**kwargs) -> None Source code in vllm/benchmarks/datasets.py 683 684def __init__(self, **kwargs) -> None: super().__init__(**kwargs) sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", range_ratio: float = DEFAULT_RANGE_RATIO, input_len: int = DEFAULT_INPUT_LEN, batchsize: int = 1, is_reranker: bool = True, **kwargs, ) -> list[SampleRequest] Source code in vllm/benchmarks/datasets.py 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780def sample( self, tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", range_ratio: float = RandomDataset.DEFAULT_RANGE_RATIO, input_len: int = RandomDataset.DEFAULT_INPUT_LEN, batchsize: int = 1, is_reranker: bool = True, **kwargs, ) -> list[SampleRequest]: n_sep_tokens = int(is_reranker) query_len_param = (input_len // 2) - n_sep_tokens if is_reranker else input_len query_lens, _, query_offsets = self.get_sampling_params( 1, range_ratio, query_len_param, 0, tokenizer ) query_len = int(query_lens[0]) if not is_reranker: assert num_requests > 1 and batchsize > 1 num_requests -= 1 batchsize -= 1 doc_len_param = input_len else: doc_len_param = input_len - query_len - n_sep_tokens doc_lens, _, doc_offsets = self.get_sampling_params( num_requests, range_ratio, doc_len_param, 0, tokenizer ) vocab_size = tokenizer.vocab_size prohibited_tokens = tokenizer.all_special_ids all_tokens = np.arange(vocab_size) allowed_tokens = np.array(list(set(all_tokens) - set(prohibited_tokens))) query_prompt, query_input_len, token_mismatch_total = ( self.generate_token_sequence( tokenizer=tokenizer, prefix_token_ids=[], prefix_len=0, vocab_size=vocab_size, input_len=query_len, offset=int(query_offsets[0]), index=0, allowed_tokens=allowed_tokens, ) ) requests = [] for i in range(num_requests): prompt, total_input_len, token_mismatch = self.generate_token_sequence( # noqa: E501 tokenizer=tokenizer, prefix_token_ids=[], prefix_len=0, vocab_size=vocab_size, input_len=int(doc_lens[i]), offset=int(doc_offsets[i]), index=i + 1, allowed_tokens=allowed_tokens, ) token_mismatch_total += token_mismatch requests.append((prompt, total_input_len)) batch_requests = [] # Create batched requests for i in range(0, num_requests, batchsize): batch = requests[i : i + batchsize] query_contrib = ( (query_input_len + n_sep_tokens) * len(batch) if is_reranker else query_input_len ) batch_requests.append( SampleRequest( prompt=[query_prompt] + [req[0] for req in batch], prompt_len=query_contrib + sum(req[1] for req in batch), expected_output_len=0, request_id=request_id_prefix + str(i // batchsize), ) ) if token_mismatch_total != 0: logger.warning( \"Across all generated prompts, there were %d %s tokens \" \"than expected after decoding and re-encoding. This is \" \"expected due to the imperfect nature of the sampling \" \"procedure.\", abs(token_mismatch_total), \"more\" if token_mismatch_total > 0 else \"fewer\", ) return batch_requests RandomMultiModalDataset ¶ Bases: RandomDataset Synthetic multimodal dataset (text + images) that extends RandomDataset. Status: - Images: supported via synthetic RGB data. - Video: supported via synthetic RGB data. - Audio: not yet supported. Sampling overview: 1) Number of items per request is sampled uniformly from the integer range [floor(n·(1−r)), ceil(n·(1+r))], where n is the base count and r is num_mm_items_range_ratio in [0, 1]. r=0 keeps it fixed; r=1 allows 0. The maximum is further clamped to the sum of per-modality limits. 2) Each item’s modality and shape is sampled from bucket_config, a dict mapping (height, width, num_frames) → probability. We treat num_frames=1 as image and num_frames > 1 as video. Entries with zero probability are removed and the rest are renormalized to sum to 1. 3) Per-modality hard caps are enforced via limit_mm_per_prompt. When a modality reaches its cap, all of its buckets are excluded and the remaining probabilities are renormalized. Example bucket configuration: {(256, 256, 1): 0.5, (720, 1280, 1): 0.4, (720, 1280, 16): 0.1} - Two image buckets (num_frames=1) and one video bucket (num_frames=16). OBS.: Only image sampling is supported for now. Source code in vllm/benchmarks/datasets.py 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203class RandomMultiModalDataset(RandomDataset): \"\"\" Synthetic multimodal dataset (text + images) that extends RandomDataset. Status: - Images: supported via synthetic RGB data. - Video: supported via synthetic RGB data. - Audio: not yet supported. Sampling overview: 1) Number of items per request is sampled uniformly from the integer range [floor(n·(1−r)), ceil(n·(1+r))], where n is the base count and r is `num_mm_items_range_ratio` in [0, 1]. r=0 keeps it fixed; r=1 allows 0. The maximum is further clamped to the sum of per-modality limits. 2) Each item’s modality and shape is sampled from `bucket_config`, a dict mapping (height, width, num_frames) → probability. We treat `num_frames`=1 as image and `num_frames` > 1 as video. Entries with zero probability are removed and the rest are renormalized to sum to 1. 3) Per-modality hard caps are enforced via `limit_mm_per_prompt`. When a modality reaches its cap, all of its buckets are excluded and the remaining probabilities are renormalized. Example bucket configuration: {(256, 256, 1): 0.5, (720, 1280, 1): 0.4, (720, 1280, 16): 0.1} - Two image buckets (`num_frames`=1) and one video bucket (`num_frames`=16). OBS.: Only image sampling is supported for now. \"\"\" IS_MULTIMODAL = True DEFAULT_LIMIT_MM_PER_PROMPT = {\"image\": 255, \"video\": 1} DEFAULT_BASE_ITEMS_PER_REQUEST = 1 DEFAULT_NUM_MM_ITEMS_RANGE_RATIO = 0.0 DEFAULT_MM_ITEM_BUCKET_CONFIG = { (256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0, } DEFAULT_ENABLE_MULTIMODAL_CHAT = False def __init__(self, **kwargs) -> None: super().__init__(**kwargs) def generate_synthetic_image(self, width: int, height: int) -> Image.Image: \"\"\"Generate synthetic PIL image with random RGB values. NOTE: iid pixel sampling results in worst-case compression (good for stressing I/O), but very unlike real photos. We could consider a “low-freq” mode (e.g., noise blur) to emulate network realism instead of max stress. \"\"\" random_pixels = self._rng.integers( 0, 256, (height, width, 3), dtype=np.uint8, ) return Image.fromarray(random_pixels) def generate_synthetic_video( self, width: int, height: int, num_frames: int ) -> dict: \"\"\"Generate synthetic video with random values. Creates a video with random pixel values, encodes it to MP4 format, and returns the content as bytes. \"\"\" import cv2 random_pixels = self._rng.integers( 0, 256, (num_frames, height, width, 3), dtype=np.uint8, ) # Create a temporary video file in memory fourcc = cv2.VideoWriter_fourcc(*\"mp4v\") fps = 30 # frames per second with NamedTemporaryFile(suffix=\".mp4\", delete_on_close=False) as temp_file: temp_path = temp_file.name # Create video writer video_writer = cv2.VideoWriter( temp_path, fourcc=fourcc, fps=fps, frameSize=(width, height) ) if not video_writer.isOpened(): raise RuntimeError(\"Failed to create video writer\") for frame in random_pixels: video_writer.write(frame) video_writer.release() temp_file.close() # Read the video file content with open(temp_path, \"rb\") as f: video_content = f.read() return {\"bytes\": video_content} def map_config_to_modality(self, config: tuple[int, int, int]) -> str: \"\"\"Map the configuration to the modality.\"\"\" if config[-1] == 1: return \"image\" elif config[-1] > 1: return \"video\" else: raise ValueError(f\"Invalid multimodal item configuration: {config}\") def normalize_bucket_config( self, bucket_config: dict[tuple[int, int, int], float] ) -> dict[tuple[int, int, int], float]: \"\"\" Remove zero probability entries and normalize the bucket config to sum to 1. \"\"\" # Raise error if value is negative if any(v < 0 for v in bucket_config.values()): raise ValueError(\"Bucket config values must be non-negative.\") # Remove zero probability entries bucket_config = {k: v for k, v in bucket_config.items() if v > 0} # if bucket config is empty, raise error if not bucket_config: raise ValueError( \"Got invalid bucket config. Bucket config values must be non-zero.\" ) # Normalize the remaining bucket config to sum to 1 total = sum(bucket_config.values()) return {k: v / total for k, v in bucket_config.items()} def generate_mm_item( self, mm_item_config: tuple[int, int, int], ) -> Mapping[str, Any]: \"\"\" Create synthetic images and videos and apply process_image/process_video respectively. This follows the OpenAI API chat completions https://github.com/openai/openai-python \"\"\" if self.map_config_to_modality(mm_item_config) == \"image\": return process_image( self.generate_synthetic_image(mm_item_config[1], mm_item_config[0]) ) elif self.map_config_to_modality(mm_item_config) == \"video\": return process_video( self.generate_synthetic_video( mm_item_config[1], mm_item_config[0], mm_item_config[2] ) ) else: raise ValueError(f\"Invalid multimodal item configuration: {mm_item_config}\") def get_mm_item_sampling_params( self, base_items_per_request: int, num_mm_items_range_ratio: float, limit_mm_per_prompt: dict[str, int], bucket_config: dict[tuple[int, int, int], float], ) -> tuple[int, int, dict[str, int], dict[tuple[int, int, int], float]]: \"\"\" Get the sampling parameters for the multimodal items. \"\"\" # Enforce num_mm_items_range_ratio <= 1 if not (0.0 <= num_mm_items_range_ratio <= 1.0): raise ValueError(\"num_mm_items_range_ratio must be in [0, 1].\") # Ensure modalities to sample are in limit_mm_per_prompt for k, v in bucket_config.items(): # get modality from bucket config modality = self.map_config_to_modality(k) if modality not in limit_mm_per_prompt: raise ValueError( f\"Modality {modality} is not in \" f\"limit_mm_per_prompt: \" f\"{limit_mm_per_prompt.keys()}\" ) # Remove zero probability entries # and normalize bucket config to sum to 1 bucket_config = self.normalize_bucket_config(bucket_config) logger.info( \"Normalized bucket config: %s\", bucket_config, ) # Only consider limit per prompt for modalities in bucket config allowed_modalities = {self.map_config_to_modality(cfg) for cfg in bucket_config} limit_mm_per_prompt = { k: v for k, v in limit_mm_per_prompt.items() if k in allowed_modalities } if not limit_mm_per_prompt: raise ValueError(\"No valid limits for modalities present in bucket_config.\") logger.info( \"Updated mm-limit-per-prompt: %s\", limit_mm_per_prompt, ) # Get max and min num mm items and ensure # it is at most the sum of limit_mm_per_prompt for all modalities max_num_mm_items = min( sum(limit_mm_per_prompt.values()), math.ceil(base_items_per_request * (1 + num_mm_items_range_ratio)), ) # Ensure min num mm items is at least 0 min_num_mm_items = max( 0, math.floor(base_items_per_request * (1 - num_mm_items_range_ratio)) ) # Raise error if min num mm items is greater than max num mm items if min_num_mm_items > max_num_mm_items: raise ValueError( f\"Min num mm items is greater than max mm items: \" f\"{min_num_mm_items} > {max_num_mm_items}\" ) logger.info( \"Sampling number of multimodal items from [%s, %s]\", min_num_mm_items, max_num_mm_items, ) return ( min_num_mm_items, max_num_mm_items, limit_mm_per_prompt, bucket_config, ) def get_mm_item_iterator( self, min_num_mm_items: int, max_num_mm_items: int, bucket_config: dict[tuple[int, int, int], float], limit_mm_per_prompt: dict[str, int], ) -> Iterator[tuple[int, int, int]]: \"\"\" Iterator over the multimodal items for each request whose size is between min_num_mm_items and max_num_mm_items. Loop over the bucket config and sample a multimodal item. Loop until the number of multimodal items sampled is equal to request_num_mm_items or limit of multimodal items per prompt for all modalities is reached. Note: - This function operates on a per-request shallow copy of `bucket_config` (tuple->float). The original dict passed to `sample` is not mutated. If this ever changes, a test is implemented and will fail. \"\"\" # Get the number of multimodal items to sample request_num_mm_items = int( self._rng.integers(min_num_mm_items, max_num_mm_items + 1) ) # If request_num_mm_items is 0, yield an empty iterator if request_num_mm_items == 0: return # Initialize modality counters modality_counter = {self.map_config_to_modality(k): 0 for k in bucket_config} # Copy the bucket config to avoid modifying the original bucket_config_copy = bucket_config.copy() # Loop over the number of multimodal items to sample while sum(modality_counter.values()) < request_num_mm_items: # Sample a multimodal item config mm_item_config = self._rng.choice( list(bucket_config_copy.keys()), p=list(bucket_config_copy.values()) ) modality = self.map_config_to_modality(mm_item_config) # Check that modality count is less than limit per prompt if modality_counter[modality] < limit_mm_per_prompt[modality]: modality_counter[modality] += 1 yield (mm_item_config) else: # If the counter is greater than the limit per prompt # set all multimodal items of this modality to 0 for k, v in bucket_config_copy.items(): if self.map_config_to_modality(k) == modality: bucket_config_copy[k] = 0 # If all configs are 0, break the loop # This should not happen as request_num_mm_items is at most # the sum of limit_mm_per_prompt for all modalities if all(v == 0 for v in bucket_config_copy.values()): logger.warning( \"Exhausted all multimodal items of modality %s\", modality ) break # Renormalize the bucket config bucket_config_copy = self.normalize_bucket_config(bucket_config_copy) def sample( self, tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, prefix_len: int = RandomDataset.DEFAULT_PREFIX_LEN, range_ratio: float = RandomDataset.DEFAULT_RANGE_RATIO, input_len: int = RandomDataset.DEFAULT_INPUT_LEN, output_len: int = RandomDataset.DEFAULT_OUTPUT_LEN, limit_mm_per_prompt: dict[str, int] = DEFAULT_LIMIT_MM_PER_PROMPT, base_items_per_request: int = DEFAULT_BASE_ITEMS_PER_REQUEST, num_mm_items_range_ratio: float = DEFAULT_NUM_MM_ITEMS_RANGE_RATIO, bucket_config: dict[ tuple[int, int, int], float ] = DEFAULT_MM_ITEM_BUCKET_CONFIG, enable_multimodal_chat: bool = DEFAULT_ENABLE_MULTIMODAL_CHAT, **kwargs, ) -> list[SampleRequest]: # Get the sampling parameters for the dataset input_lens, output_lens, offsets = self.get_sampling_params( num_requests, range_ratio, input_len, output_len, tokenizer ) ( min_num_mm_items, max_num_mm_items, limit_mm_per_prompt, bucket_config, ) = self.get_mm_item_sampling_params( base_items_per_request, num_mm_items_range_ratio, limit_mm_per_prompt, bucket_config, ) vocab_size = tokenizer.vocab_size # Can't use tokenizer.all_special_ids since # it returns ONLY ids from special_tokens_map.json # We want to exclude placeholder tokens and all # tokens that indicate start/end of image as it # may break prompt replacement logic. prohibited_tokens = list( tok_id for tok_id, token in tokenizer.added_tokens_decoder.items() if token.special ) all_tokens = np.arange(vocab_size) allowed_tokens = np.array(list(set(all_tokens) - set(prohibited_tokens))) logger.debug( \"Sampling from %d out of %d (vocab size)\", len(allowed_tokens), vocab_size ) # Generate prefix once prefix_token_ids = self.get_prefix(allowed_tokens, prefix_len) # Add synthetic multimodal items to each request mm_requests = [] token_mismatch_total = 0 for i in range(num_requests): prompt, total_input_len, token_mismatch = self.generate_token_sequence( # noqa: E501 tokenizer=tokenizer, prefix_token_ids=prefix_token_ids, prefix_len=prefix_len, vocab_size=vocab_size, input_len=int(input_lens[i]), offset=int(offsets[i]), index=i, allowed_tokens=allowed_tokens, ) token_mismatch_total += token_mismatch # Get multimodal item iterator for a given request mm_item_iterator = self.get_mm_item_iterator( min_num_mm_items, max_num_mm_items, bucket_config, limit_mm_per_prompt, ) mm_content = cast( list[dict[str, Any]], [ self.generate_mm_item(mm_item_config) for mm_item_config in mm_item_iterator ], ) if enable_multimodal_chat: # NOTE: For now this option is only provided for completeness # given that the serve.py benchmark currently does not use it. mm_chat_prompt: Any = prompt mm_chat_prompt = self.apply_multimodal_chat_transformation( prompt, mm_content ) sample_request = SampleRequest( prompt=mm_chat_prompt, prompt_len=total_input_len, expected_output_len=int(output_lens[i]), multi_modal_data=None, request_id=request_id_prefix + str(i), ) else: sample_request = SampleRequest( prompt=prompt, prompt_len=total_input_len, expected_output_len=int(output_lens[i]), multi_modal_data=mm_content, request_id=request_id_prefix + str(i), ) mm_requests.append(sample_request) if token_mismatch_total != 0: sign = \"more\" if token_mismatch_total > 0 else \"fewer\" logger.warning( \"Across all generated prompts, there were %d %s tokens \" \"than expected after decoding and re-encoding. This is \" \"expected due to the imperfect nature of the sampling \" \"procedure.\", abs(token_mismatch_total), sign, ) return mm_requests DEFAULT_BASE_ITEMS_PER_REQUEST class-attribute instance-attribute ¶ DEFAULT_BASE_ITEMS_PER_REQUEST = 1 DEFAULT_ENABLE_MULTIMODAL_CHAT class-attribute instance-attribute ¶ DEFAULT_ENABLE_MULTIMODAL_CHAT = False DEFAULT_LIMIT_MM_PER_PROMPT class-attribute instance-attribute ¶ DEFAULT_LIMIT_MM_PER_PROMPT = {'image': 255, 'video': 1} DEFAULT_MM_ITEM_BUCKET_CONFIG class-attribute instance-attribute ¶ DEFAULT_MM_ITEM_BUCKET_CONFIG = { (256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0, } DEFAULT_NUM_MM_ITEMS_RANGE_RATIO class-attribute instance-attribute ¶ DEFAULT_NUM_MM_ITEMS_RANGE_RATIO = 0.0 IS_MULTIMODAL class-attribute instance-attribute ¶ IS_MULTIMODAL = True __init__ ¶ __init__(**kwargs) -> None Source code in vllm/benchmarks/datasets.py 830 831def __init__(self, **kwargs) -> None: super().__init__(**kwargs) generate_mm_item ¶ generate_mm_item( mm_item_config: tuple[int, int, int], ) -> Mapping[str, Any] Create synthetic images and videos and apply process_image/process_video respectively. This follows the OpenAI API chat completions https://github.com/openai/openai-python Source code in vllm/benchmarks/datasets.py 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945def generate_mm_item( self, mm_item_config: tuple[int, int, int], ) -> Mapping[str, Any]: \"\"\" Create synthetic images and videos and apply process_image/process_video respectively. This follows the OpenAI API chat completions https://github.com/openai/openai-python \"\"\" if self.map_config_to_modality(mm_item_config) == \"image\": return process_image( self.generate_synthetic_image(mm_item_config[1], mm_item_config[0]) ) elif self.map_config_to_modality(mm_item_config) == \"video\": return process_video( self.generate_synthetic_video( mm_item_config[1], mm_item_config[0], mm_item_config[2] ) ) else: raise ValueError(f\"Invalid multimodal item configuration: {mm_item_config}\") generate_synthetic_image ¶ generate_synthetic_image(width: int, height: int) -> Image Generate synthetic PIL image with random RGB values. NOTE: iid pixel sampling results in worst-case compression (good for stressing I/O), but very unlike real photos. We could consider a “low-freq” mode (e.g., noise blur) to emulate network realism instead of max stress. Source code in vllm/benchmarks/datasets.py 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847def generate_synthetic_image(self, width: int, height: int) -> Image.Image: \"\"\"Generate synthetic PIL image with random RGB values. NOTE: iid pixel sampling results in worst-case compression (good for stressing I/O), but very unlike real photos. We could consider a “low-freq” mode (e.g., noise blur) to emulate network realism instead of max stress. \"\"\" random_pixels = self._rng.integers( 0, 256, (height, width, 3), dtype=np.uint8, ) return Image.fromarray(random_pixels) generate_synthetic_video ¶ generate_synthetic_video( width: int, height: int, num_frames: int ) -> dict Generate synthetic video with random values. Creates a video with random pixel values, encodes it to MP4 format, and returns the content as bytes. Source code in vllm/benchmarks/datasets.py 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891def generate_synthetic_video( self, width: int, height: int, num_frames: int ) -> dict: \"\"\"Generate synthetic video with random values. Creates a video with random pixel values, encodes it to MP4 format, and returns the content as bytes. \"\"\" import cv2 random_pixels = self._rng.integers( 0, 256, (num_frames, height, width, 3), dtype=np.uint8, ) # Create a temporary video file in memory fourcc = cv2.VideoWriter_fourcc(*\"mp4v\") fps = 30 # frames per second with NamedTemporaryFile(suffix=\".mp4\", delete_on_close=False) as temp_file: temp_path = temp_file.name # Create video writer video_writer = cv2.VideoWriter( temp_path, fourcc=fourcc, fps=fps, frameSize=(width, height) ) if not video_writer.isOpened(): raise RuntimeError(\"Failed to create video writer\") for frame in random_pixels: video_writer.write(frame) video_writer.release() temp_file.close() # Read the video file content with open(temp_path, \"rb\") as f: video_content = f.read() return {\"bytes\": video_content} get_mm_item_iterator ¶ get_mm_item_iterator( min_num_mm_items: int, max_num_mm_items: int, bucket_config: dict[tuple[int, int, int], float], limit_mm_per_prompt: dict[str, int], ) -> Iterator[tuple[int, int, int]] Iterator over the multimodal items for each request whose size is between min_num_mm_items and max_num_mm_items. Loop over the bucket config and sample a multimodal item. Loop until the number of multimodal items sampled is equal to request_num_mm_items or limit of multimodal items per prompt for all modalities is reached. Note: - This function operates on a per-request shallow copy of bucket_config (tuple->float). The original dict passed to sample is not mutated. If this ever changes, a test is implemented and will fail. Source code in vllm/benchmarks/datasets.py 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081def get_mm_item_iterator( self, min_num_mm_items: int, max_num_mm_items: int, bucket_config: dict[tuple[int, int, int], float], limit_mm_per_prompt: dict[str, int], ) -> Iterator[tuple[int, int, int]]: \"\"\" Iterator over the multimodal items for each request whose size is between min_num_mm_items and max_num_mm_items. Loop over the bucket config and sample a multimodal item. Loop until the number of multimodal items sampled is equal to request_num_mm_items or limit of multimodal items per prompt for all modalities is reached. Note: - This function operates on a per-request shallow copy of `bucket_config` (tuple->float). The original dict passed to `sample` is not mutated. If this ever changes, a test is implemented and will fail. \"\"\" # Get the number of multimodal items to sample request_num_mm_items = int( self._rng.integers(min_num_mm_items, max_num_mm_items + 1) ) # If request_num_mm_items is 0, yield an empty iterator if request_num_mm_items == 0: return # Initialize modality counters modality_counter = {self.map_config_to_modality(k): 0 for k in bucket_config} # Copy the bucket config to avoid modifying the original bucket_config_copy = bucket_config.copy() # Loop over the number of multimodal items to sample while sum(modality_counter.values()) < request_num_mm_items: # Sample a multimodal item config mm_item_config = self._rng.choice( list(bucket_config_copy.keys()), p=list(bucket_config_copy.values()) ) modality = self.map_config_to_modality(mm_item_config) # Check that modality count is less than limit per prompt if modality_counter[modality] < limit_mm_per_prompt[modality]: modality_counter[modality] += 1 yield (mm_item_config) else: # If the counter is greater than the limit per prompt # set all multimodal items of this modality to 0 for k, v in bucket_config_copy.items(): if self.map_config_to_modality(k) == modality: bucket_config_copy[k] = 0 # If all configs are 0, break the loop # This should not happen as request_num_mm_items is at most # the sum of limit_mm_per_prompt for all modalities if all(v == 0 for v in bucket_config_copy.values()): logger.warning( \"Exhausted all multimodal items of modality %s\", modality ) break # Renormalize the bucket config bucket_config_copy = self.normalize_bucket_config(bucket_config_copy) get_mm_item_sampling_params ¶ get_mm_item_sampling_params( base_items_per_request: int, num_mm_items_range_ratio: float, limit_mm_per_prompt: dict[str, int], bucket_config: dict[tuple[int, int, int], float], ) -> tuple[ int, int, dict[str, int], dict[tuple[int, int, int], float], ] Get the sampling parameters for the multimodal items. Source code in vllm/benchmarks/datasets.py 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020def get_mm_item_sampling_params( self, base_items_per_request: int, num_mm_items_range_ratio: float, limit_mm_per_prompt: dict[str, int], bucket_config: dict[tuple[int, int, int], float], ) -> tuple[int, int, dict[str, int], dict[tuple[int, int, int], float]]: \"\"\" Get the sampling parameters for the multimodal items. \"\"\" # Enforce num_mm_items_range_ratio <= 1 if not (0.0 <= num_mm_items_range_ratio <= 1.0): raise ValueError(\"num_mm_items_range_ratio must be in [0, 1].\") # Ensure modalities to sample are in limit_mm_per_prompt for k, v in bucket_config.items(): # get modality from bucket config modality = self.map_config_to_modality(k) if modality not in limit_mm_per_prompt: raise ValueError( f\"Modality {modality} is not in \" f\"limit_mm_per_prompt: \" f\"{limit_mm_per_prompt.keys()}\" ) # Remove zero probability entries # and normalize bucket config to sum to 1 bucket_config = self.normalize_bucket_config(bucket_config) logger.info( \"Normalized bucket config: %s\", bucket_config, ) # Only consider limit per prompt for modalities in bucket config allowed_modalities = {self.map_config_to_modality(cfg) for cfg in bucket_config} limit_mm_per_prompt = { k: v for k, v in limit_mm_per_prompt.items() if k in allowed_modalities } if not limit_mm_per_prompt: raise ValueError(\"No valid limits for modalities present in bucket_config.\") logger.info( \"Updated mm-limit-per-prompt: %s\", limit_mm_per_prompt, ) # Get max and min num mm items and ensure # it is at most the sum of limit_mm_per_prompt for all modalities max_num_mm_items = min( sum(limit_mm_per_prompt.values()), math.ceil(base_items_per_request * (1 + num_mm_items_range_ratio)), ) # Ensure min num mm items is at least 0 min_num_mm_items = max( 0, math.floor(base_items_per_request * (1 - num_mm_items_range_ratio)) ) # Raise error if min num mm items is greater than max num mm items if min_num_mm_items > max_num_mm_items: raise ValueError( f\"Min num mm items is greater than max mm items: \" f\"{min_num_mm_items} > {max_num_mm_items}\" ) logger.info( \"Sampling number of multimodal items from [%s, %s]\", min_num_mm_items, max_num_mm_items, ) return ( min_num_mm_items, max_num_mm_items, limit_mm_per_prompt, bucket_config, ) map_config_to_modality ¶ map_config_to_modality(config: tuple[int, int, int]) -> str Map the configuration to the modality. Source code in vllm/benchmarks/datasets.py 893 894 895 896 897 898 899 900def map_config_to_modality(self, config: tuple[int, int, int]) -> str: \"\"\"Map the configuration to the modality.\"\"\" if config[-1] == 1: return \"image\" elif config[-1] > 1: return \"video\" else: raise ValueError(f\"Invalid multimodal item configuration: {config}\") normalize_bucket_config ¶ normalize_bucket_config( bucket_config: dict[tuple[int, int, int], float], ) -> dict[tuple[int, int, int], float] Remove zero probability entries and normalize the bucket config to sum to 1. Source code in vllm/benchmarks/datasets.py 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921def normalize_bucket_config( self, bucket_config: dict[tuple[int, int, int], float] ) -> dict[tuple[int, int, int], float]: \"\"\" Remove zero probability entries and normalize the bucket config to sum to 1. \"\"\" # Raise error if value is negative if any(v < 0 for v in bucket_config.values()): raise ValueError(\"Bucket config values must be non-negative.\") # Remove zero probability entries bucket_config = {k: v for k, v in bucket_config.items() if v > 0} # if bucket config is empty, raise error if not bucket_config: raise ValueError( \"Got invalid bucket config. Bucket config values must be non-zero.\" ) # Normalize the remaining bucket config to sum to 1 total = sum(bucket_config.values()) return {k: v / total for k, v in bucket_config.items()} sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, prefix_len: int = DEFAULT_PREFIX_LEN, range_ratio: float = DEFAULT_RANGE_RATIO, input_len: int = DEFAULT_INPUT_LEN, output_len: int = DEFAULT_OUTPUT_LEN, limit_mm_per_prompt: dict[ str, int ] = DEFAULT_LIMIT_MM_PER_PROMPT, base_items_per_request: int = DEFAULT_BASE_ITEMS_PER_REQUEST, num_mm_items_range_ratio: float = DEFAULT_NUM_MM_ITEMS_RANGE_RATIO, bucket_config: dict[ tuple[int, int, int], float ] = DEFAULT_MM_ITEM_BUCKET_CONFIG, enable_multimodal_chat: bool = DEFAULT_ENABLE_MULTIMODAL_CHAT, **kwargs, ) -> list[SampleRequest] Source code in vllm/benchmarks/datasets.py 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203def sample( self, tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, prefix_len: int = RandomDataset.DEFAULT_PREFIX_LEN, range_ratio: float = RandomDataset.DEFAULT_RANGE_RATIO, input_len: int = RandomDataset.DEFAULT_INPUT_LEN, output_len: int = RandomDataset.DEFAULT_OUTPUT_LEN, limit_mm_per_prompt: dict[str, int] = DEFAULT_LIMIT_MM_PER_PROMPT, base_items_per_request: int = DEFAULT_BASE_ITEMS_PER_REQUEST, num_mm_items_range_ratio: float = DEFAULT_NUM_MM_ITEMS_RANGE_RATIO, bucket_config: dict[ tuple[int, int, int], float ] = DEFAULT_MM_ITEM_BUCKET_CONFIG, enable_multimodal_chat: bool = DEFAULT_ENABLE_MULTIMODAL_CHAT, **kwargs, ) -> list[SampleRequest]: # Get the sampling parameters for the dataset input_lens, output_lens, offsets = self.get_sampling_params( num_requests, range_ratio, input_len, output_len, tokenizer ) ( min_num_mm_items, max_num_mm_items, limit_mm_per_prompt, bucket_config, ) = self.get_mm_item_sampling_params( base_items_per_request, num_mm_items_range_ratio, limit_mm_per_prompt, bucket_config, ) vocab_size = tokenizer.vocab_size # Can't use tokenizer.all_special_ids since # it returns ONLY ids from special_tokens_map.json # We want to exclude placeholder tokens and all # tokens that indicate start/end of image as it # may break prompt replacement logic. prohibited_tokens = list( tok_id for tok_id, token in tokenizer.added_tokens_decoder.items() if token.special ) all_tokens = np.arange(vocab_size) allowed_tokens = np.array(list(set(all_tokens) - set(prohibited_tokens))) logger.debug( \"Sampling from %d out of %d (vocab size)\", len(allowed_tokens), vocab_size ) # Generate prefix once prefix_token_ids = self.get_prefix(allowed_tokens, prefix_len) # Add synthetic multimodal items to each request mm_requests = [] token_mismatch_total = 0 for i in range(num_requests): prompt, total_input_len, token_mismatch = self.generate_token_sequence( # noqa: E501 tokenizer=tokenizer, prefix_token_ids=prefix_token_ids, prefix_len=prefix_len, vocab_size=vocab_size, input_len=int(input_lens[i]), offset=int(offsets[i]), index=i, allowed_tokens=allowed_tokens, ) token_mismatch_total += token_mismatch # Get multimodal item iterator for a given request mm_item_iterator = self.get_mm_item_iterator( min_num_mm_items, max_num_mm_items, bucket_config, limit_mm_per_prompt, ) mm_content = cast( list[dict[str, Any]], [ self.generate_mm_item(mm_item_config) for mm_item_config in mm_item_iterator ], ) if enable_multimodal_chat: # NOTE: For now this option is only provided for completeness # given that the serve.py benchmark currently does not use it. mm_chat_prompt: Any = prompt mm_chat_prompt = self.apply_multimodal_chat_transformation( prompt, mm_content ) sample_request = SampleRequest( prompt=mm_chat_prompt, prompt_len=total_input_len, expected_output_len=int(output_lens[i]), multi_modal_data=None, request_id=request_id_prefix + str(i), ) else: sample_request = SampleRequest( prompt=prompt, prompt_len=total_input_len, expected_output_len=int(output_lens[i]), multi_modal_data=mm_content, request_id=request_id_prefix + str(i), ) mm_requests.append(sample_request) if token_mismatch_total != 0: sign = \"more\" if token_mismatch_total > 0 else \"fewer\" logger.warning( \"Across all generated prompts, there were %d %s tokens \" \"than expected after decoding and re-encoding. This is \" \"expected due to the imperfect nature of the sampling \" \"procedure.\", abs(token_mismatch_total), sign, ) return mm_requests SampleRequest dataclass ¶ Represents a single inference request for benchmarking. Source code in vllm/benchmarks/datasets.py 72 73 74 75 76 77 78 79 80 81 82 83@dataclass class SampleRequest: \"\"\" Represents a single inference request for benchmarking. \"\"\" prompt: str | list[str] prompt_len: int expected_output_len: int multi_modal_data: MultiModalDataDict | dict | list[dict] | None = None lora_request: LoRARequest | None = None request_id: str | None = None expected_output_len instance-attribute ¶ expected_output_len: int lora_request class-attribute instance-attribute ¶ lora_request: LoRARequest | None = None multi_modal_data class-attribute instance-attribute ¶ multi_modal_data: ( MultiModalDataDict | dict | list[dict] | None ) = None prompt instance-attribute ¶ prompt: str | list[str] prompt_len instance-attribute ¶ prompt_len: int request_id class-attribute instance-attribute ¶ request_id: str | None = None __init__ ¶ __init__( prompt: str | list[str], prompt_len: int, expected_output_len: int, multi_modal_data: MultiModalDataDict | dict | list[dict] | None = None, lora_request: LoRARequest | None = None, request_id: str | None = None, ) -> None ShareGPTDataset ¶ Bases: BenchmarkDataset Implements the ShareGPT dataset. Loads data from a JSON file and generates sample requests based on conversation turns. Source code in vllm/benchmarks/datasets.py 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294class ShareGPTDataset(BenchmarkDataset): \"\"\" Implements the ShareGPT dataset. Loads data from a JSON file and generates sample requests based on conversation turns. \"\"\" def __init__(self, **kwargs) -> None: super().__init__(**kwargs) self.load_data() def load_data(self) -> None: if self.dataset_path is None: raise ValueError(\"dataset_path must be provided for loading data.\") with open(self.dataset_path, encoding=\"utf-8\") as f: self.data = json.load(f) # Filter entries with at least two conversation turns. self.data = [ entry for entry in self.data if \"conversations\" in entry and len(entry[\"conversations\"]) >= 2 ] random.seed(self.random_seed) if not getattr(self, \"disable_shuffle\", False): random.shuffle(self.data) def sample( self, tokenizer: TokenizerLike, num_requests: int, lora_path: str | None = None, max_loras: int | None = None, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: samples: list = [] ind = 0 for entry in self.data: if len(samples) >= num_requests: break prompt, completion = ( entry[\"conversations\"][0][\"value\"], entry[\"conversations\"][1][\"value\"], ) lora_request = self.get_random_lora_request( max_loras=max_loras, lora_path=lora_path ) prompt_ids = tokenizer(prompt).input_ids completion_ids = tokenizer(completion).input_ids prompt_len = len(prompt_ids) new_output_len = len(completion_ids) if output_len is None else output_len if not is_valid_sequence( prompt_len, new_output_len, skip_min_output_len_check=output_len is not None, ): continue if image_path := entry.get(\"image\"): mm_content = process_image(image_path) elif video_path := entry.get(\"video\"): mm_content = process_video(video_path) else: mm_content = None if enable_multimodal_chat: prompt = self.apply_multimodal_chat_transformation(prompt, mm_content) samples.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=new_output_len, lora_request=lora_request, multi_modal_data=mm_content, request_id=request_id_prefix + str(ind), ) ) ind += 1 self.maybe_oversample_requests( samples, num_requests, request_id_prefix, no_oversample ) return samples __init__ ¶ __init__(**kwargs) -> None Source code in vllm/benchmarks/datasets.py 1217 1218 1219def __init__(self, **kwargs) -> None: super().__init__(**kwargs) self.load_data() load_data ¶ load_data() -> None Source code in vllm/benchmarks/datasets.py 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235def load_data(self) -> None: if self.dataset_path is None: raise ValueError(\"dataset_path must be provided for loading data.\") with open(self.dataset_path, encoding=\"utf-8\") as f: self.data = json.load(f) # Filter entries with at least two conversation turns. self.data = [ entry for entry in self.data if \"conversations\" in entry and len(entry[\"conversations\"]) >= 2 ] random.seed(self.random_seed) if not getattr(self, \"disable_shuffle\", False): random.shuffle(self.data) sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, lora_path: str | None = None, max_loras: int | None = None, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294def sample( self, tokenizer: TokenizerLike, num_requests: int, lora_path: str | None = None, max_loras: int | None = None, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: samples: list = [] ind = 0 for entry in self.data: if len(samples) >= num_requests: break prompt, completion = ( entry[\"conversations\"][0][\"value\"], entry[\"conversations\"][1][\"value\"], ) lora_request = self.get_random_lora_request( max_loras=max_loras, lora_path=lora_path ) prompt_ids = tokenizer(prompt).input_ids completion_ids = tokenizer(completion).input_ids prompt_len = len(prompt_ids) new_output_len = len(completion_ids) if output_len is None else output_len if not is_valid_sequence( prompt_len, new_output_len, skip_min_output_len_check=output_len is not None, ): continue if image_path := entry.get(\"image\"): mm_content = process_image(image_path) elif video_path := entry.get(\"video\"): mm_content = process_video(video_path) else: mm_content = None if enable_multimodal_chat: prompt = self.apply_multimodal_chat_transformation(prompt, mm_content) samples.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=new_output_len, lora_request=lora_request, multi_modal_data=mm_content, request_id=request_id_prefix + str(ind), ) ) ind += 1 self.maybe_oversample_requests( samples, num_requests, request_id_prefix, no_oversample ) return samples SonnetDataset ¶ Bases: BenchmarkDataset Simplified implementation of the Sonnet dataset. Loads poem lines from a text file and generates sample requests. Default values here copied from benchmark_serving.py for the sonnet dataset. Source code in vllm/benchmarks/datasets.py 2079 2080 2081 2082 2083 2084 2085 2086 2087 2088 2089 2090 2091 2092 2093 2094 2095 2096 2097 2098 2099 2100 2101 2102 2103 2104 2105 2106 2107 2108 2109 2110 2111 2112 2113 2114 2115 2116 2117 2118 2119 2120 2121 2122 2123 2124 2125 2126 2127 2128 2129 2130 2131 2132 2133 2134 2135 2136 2137 2138 2139 2140 2141 2142 2143 2144 2145 2146 2147 2148 2149 2150 2151 2152 2153 2154 2155 2156 2157 2158 2159 2160 2161 2162@deprecated( \"SonnetDataset is deprecated and will be removed in a future version.\", ) class SonnetDataset(BenchmarkDataset): \"\"\" Simplified implementation of the Sonnet dataset. Loads poem lines from a text file and generates sample requests. Default values here copied from `benchmark_serving.py` for the sonnet dataset. \"\"\" DEFAULT_PREFIX_LEN = 200 DEFAULT_INPUT_LEN = 550 DEFAULT_OUTPUT_LEN = 150 def __init__( self, **kwargs, ) -> None: super().__init__(**kwargs) self.load_data() def load_data(self) -> None: if not self.dataset_path: raise ValueError(\"dataset_path must be provided.\") with open(self.dataset_path, encoding=\"utf-8\") as f: self.data = f.readlines() def sample( self, tokenizer: TokenizerLike, num_requests: int, prefix_len: int = DEFAULT_PREFIX_LEN, input_len: int = DEFAULT_INPUT_LEN, output_len: int = DEFAULT_OUTPUT_LEN, return_prompt_formatted: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: # Calculate average token length for a poem line. tokenized_lines = [tokenizer(line).input_ids for line in self.data] avg_len = sum(len(tokens) for tokens in tokenized_lines) / len(tokenized_lines) # Build the base prompt. base_prompt = \"Pick as many lines as you can from these poem lines:\\n\" base_msg = [{\"role\": \"user\", \"content\": base_prompt}] base_fmt = tokenizer.apply_chat_template( base_msg, add_generation_prompt=True, tokenize=False ) base_offset = len(tokenizer(base_fmt).input_ids) if input_len <= base_offset: raise ValueError( f\"'input_len' must be higher than the base prompt length \" f\"({base_offset}).\" ) # Determine how many poem lines to use. num_input_lines = round((input_len - base_offset) / avg_len) num_prefix_lines = max(round((prefix_len - base_offset) / avg_len), 0) prefix_lines = self.data[:num_prefix_lines] samples = [] ind = 0 while len(samples) < num_requests: extra_lines = random.choices( self.data, k=num_input_lines - num_prefix_lines ) prompt = f\"{base_prompt}{''.join(prefix_lines + extra_lines)}\" msg = [{\"role\": \"user\", \"content\": prompt}] prompt_formatted = tokenizer.apply_chat_template( msg, add_generation_prompt=True, tokenize=False ) prompt_len = len(tokenizer(prompt_formatted).input_ids) if prompt_len <= input_len: samples.append( SampleRequest( prompt=prompt_formatted if return_prompt_formatted else prompt, prompt_len=prompt_len, expected_output_len=output_len, request_id=request_id_prefix + str(ind), ) ) ind += 1 return samples DEFAULT_INPUT_LEN class-attribute instance-attribute ¶ DEFAULT_INPUT_LEN = 550 DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶ DEFAULT_OUTPUT_LEN = 150 DEFAULT_PREFIX_LEN class-attribute instance-attribute ¶ DEFAULT_PREFIX_LEN = 200 __init__ ¶ __init__(**kwargs) -> None Source code in vllm/benchmarks/datasets.py 2093 2094 2095 2096 2097 2098def __init__( self, **kwargs, ) -> None: super().__init__(**kwargs) self.load_data() load_data ¶ load_data() -> None Source code in vllm/benchmarks/datasets.py 2100 2101 2102 2103 2104def load_data(self) -> None: if not self.dataset_path: raise ValueError(\"dataset_path must be provided.\") with open(self.dataset_path, encoding=\"utf-8\") as f: self.data = f.readlines() sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, prefix_len: int = DEFAULT_PREFIX_LEN, input_len: int = DEFAULT_INPUT_LEN, output_len: int = DEFAULT_OUTPUT_LEN, return_prompt_formatted: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 2106 2107 2108 2109 2110 2111 2112 2113 2114 2115 2116 2117 2118 2119 2120 2121 2122 2123 2124 2125 2126 2127 2128 2129 2130 2131 2132 2133 2134 2135 2136 2137 2138 2139 2140 2141 2142 2143 2144 2145 2146 2147 2148 2149 2150 2151 2152 2153 2154 2155 2156 2157 2158 2159 2160 2161 2162def sample( self, tokenizer: TokenizerLike, num_requests: int, prefix_len: int = DEFAULT_PREFIX_LEN, input_len: int = DEFAULT_INPUT_LEN, output_len: int = DEFAULT_OUTPUT_LEN, return_prompt_formatted: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: # Calculate average token length for a poem line. tokenized_lines = [tokenizer(line).input_ids for line in self.data] avg_len = sum(len(tokens) for tokens in tokenized_lines) / len(tokenized_lines) # Build the base prompt. base_prompt = \"Pick as many lines as you can from these poem lines:\\n\" base_msg = [{\"role\": \"user\", \"content\": base_prompt}] base_fmt = tokenizer.apply_chat_template( base_msg, add_generation_prompt=True, tokenize=False ) base_offset = len(tokenizer(base_fmt).input_ids) if input_len <= base_offset: raise ValueError( f\"'input_len' must be higher than the base prompt length \" f\"({base_offset}).\" ) # Determine how many poem lines to use. num_input_lines = round((input_len - base_offset) / avg_len) num_prefix_lines = max(round((prefix_len - base_offset) / avg_len), 0) prefix_lines = self.data[:num_prefix_lines] samples = [] ind = 0 while len(samples) < num_requests: extra_lines = random.choices( self.data, k=num_input_lines - num_prefix_lines ) prompt = f\"{base_prompt}{''.join(prefix_lines + extra_lines)}\" msg = [{\"role\": \"user\", \"content\": prompt}] prompt_formatted = tokenizer.apply_chat_template( msg, add_generation_prompt=True, tokenize=False ) prompt_len = len(tokenizer(prompt_formatted).input_ids) if prompt_len <= input_len: samples.append( SampleRequest( prompt=prompt_formatted if return_prompt_formatted else prompt, prompt_len=prompt_len, expected_output_len=output_len, request_id=request_id_prefix + str(ind), ) ) ind += 1 return samples SpecBench ¶ Bases: CustomDataset Implements the SpecBench dataset: https://github.com/hemingkx/Spec-Bench Download the dataset using: wget https://raw.githubusercontent.com/hemingkx/Spec-Bench/refs/heads/main/data/spec_bench/question.jsonl Source code in vllm/benchmarks/datasets.py 2034 2035 2036 2037 2038 2039 2040 2041 2042 2043 2044 2045 2046 2047 2048 2049 2050 2051 2052 2053 2054 2055 2056 2057 2058 2059 2060 2061 2062 2063 2064 2065 2066 2067 2068 2069 2070 2071class SpecBench(CustomDataset): \"\"\" Implements the SpecBench dataset: https://github.com/hemingkx/Spec-Bench Download the dataset using: wget https://raw.githubusercontent.com/hemingkx/Spec-Bench/refs/heads/main/data/spec_bench/question.jsonl \"\"\" # noqa: E501 def __init__(self, **kwargs) -> None: self.category = kwargs.pop(\"category\", None) super().__init__(**kwargs) self.load_data() def load_data(self) -> None: if self.dataset_path is None: raise ValueError(\"dataset_path must be provided for loading data.\") self.data = [] # Load the JSONL file jsonl_data = pd.read_json(path_or_buf=self.dataset_path, lines=True) # check if the JSONL file has a 'turns' column if \"turns\" not in jsonl_data.columns: raise ValueError(\"JSONL file must contain a 'turns' column.\") for _, row in jsonl_data.iterrows(): # sample only from a specific category if specified if (not self.category) or (self.category == row[\"category\"]): prompt = row[\"turns\"][0] self.data.append({\"prompt\": prompt}) random.seed(self.random_seed) if not getattr(self, \"disable_shuffle\", False): random.shuffle(self.data) def sample(self, **kwargs) -> list: # leverage CustomDataset sample return super().sample(**kwargs) category instance-attribute ¶ category = pop('category', None) __init__ ¶ __init__(**kwargs) -> None Source code in vllm/benchmarks/datasets.py 2041 2042 2043 2044def __init__(self, **kwargs) -> None: self.category = kwargs.pop(\"category\", None) super().__init__(**kwargs) self.load_data() load_data ¶ load_data() -> None Source code in vllm/benchmarks/datasets.py 2046 2047 2048 2049 2050 2051 2052 2053 2054 2055 2056 2057 2058 2059 2060 2061 2062 2063 2064 2065 2066 2067def load_data(self) -> None: if self.dataset_path is None: raise ValueError(\"dataset_path must be provided for loading data.\") self.data = [] # Load the JSONL file jsonl_data = pd.read_json(path_or_buf=self.dataset_path, lines=True) # check if the JSONL file has a 'turns' column if \"turns\" not in jsonl_data.columns: raise ValueError(\"JSONL file must contain a 'turns' column.\") for _, row in jsonl_data.iterrows(): # sample only from a specific category if specified if (not self.category) or (self.category == row[\"category\"]): prompt = row[\"turns\"][0] self.data.append({\"prompt\": prompt}) random.seed(self.random_seed) if not getattr(self, \"disable_shuffle\", False): random.shuffle(self.data) sample ¶ sample(**kwargs) -> list Source code in vllm/benchmarks/datasets.py 2069 2070 2071def sample(self, **kwargs) -> list: # leverage CustomDataset sample return super().sample(**kwargs) VisionArenaDataset ¶ Bases: HuggingFaceDataset Vision Arena Dataset. Source code in vllm/benchmarks/datasets.py 2409 2410 2411 2412 2413 2414 2415 2416 2417 2418 2419 2420 2421 2422 2423 2424 2425 2426 2427 2428 2429 2430 2431 2432 2433 2434 2435 2436 2437 2438 2439 2440 2441 2442 2443 2444 2445 2446 2447 2448 2449 2450 2451 2452 2453 2454 2455 2456 2457 2458 2459class VisionArenaDataset(HuggingFaceDataset): \"\"\" Vision Arena Dataset. \"\"\" DEFAULT_OUTPUT_LEN = 128 SUPPORTED_DATASET_PATHS = { \"lmarena-ai/VisionArena-Chat\": lambda x: x[\"conversation\"][0][0][\"content\"], \"lmarena-ai/vision-arena-bench-v0.1\": lambda x: x[\"turns\"][0][0][\"content\"], } IS_MULTIMODAL = True def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break parser_fn = self.SUPPORTED_DATASET_PATHS.get(self.hf_name) if parser_fn is None: raise ValueError(f\"Unsupported dataset path: {self.hf_name}\") prompt = parser_fn(item) mm_content = process_image(item[\"images\"][0]) prompt_len = len(tokenizer(prompt).input_ids) if enable_multimodal_chat: # Note: when chat is enabled the request prompt_len is no longer # accurate and we will be using request output to count the # actual prompt len prompt = self.apply_multimodal_chat_transformation(prompt, mm_content) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_content, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶ DEFAULT_OUTPUT_LEN = 128 IS_MULTIMODAL class-attribute instance-attribute ¶ IS_MULTIMODAL = True SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = { \"lmarena-ai/VisionArena-Chat\": lambda x: x[ \"conversation\" ][0][0][\"content\"], \"lmarena-ai/vision-arena-bench-v0.1\": lambda x: x[ \"turns\" ][0][0][\"content\"], } sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 2421 2422 2423 2424 2425 2426 2427 2428 2429 2430 2431 2432 2433 2434 2435 2436 2437 2438 2439 2440 2441 2442 2443 2444 2445 2446 2447 2448 2449 2450 2451 2452 2453 2454 2455 2456 2457 2458 2459def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break parser_fn = self.SUPPORTED_DATASET_PATHS.get(self.hf_name) if parser_fn is None: raise ValueError(f\"Unsupported dataset path: {self.hf_name}\") prompt = parser_fn(item) mm_content = process_image(item[\"images\"][0]) prompt_len = len(tokenizer(prompt).input_ids) if enable_multimodal_chat: # Note: when chat is enabled the request prompt_len is no longer # accurate and we will be using request output to count the # actual prompt len prompt = self.apply_multimodal_chat_transformation(prompt, mm_content) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_content, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests _ValidateDatasetArgs ¶ Bases: Action Argparse action to validate dataset name and path compatibility. Source code in vllm/benchmarks/datasets.py 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314class _ValidateDatasetArgs(argparse.Action): \"\"\"Argparse action to validate dataset name and path compatibility.\"\"\" def __call__(self, parser, namespace, values, option_string=None): setattr(namespace, self.dest, values) # Get current values of both dataset_name and dataset_path dataset_name = getattr(namespace, \"dataset_name\", \"random\") dataset_path = getattr(namespace, \"dataset_path\", None) # Validate the combination if dataset_name == \"random\" and dataset_path is not None: parser.error( \"Cannot use 'random' dataset with --dataset-path. \" \"Please specify the appropriate --dataset-name (e.g., \" \"'sharegpt', 'custom', 'sonnet') for your dataset file: \" f\"{dataset_path}\" ) __call__ ¶ __call__(parser, namespace, values, option_string=None) Source code in vllm/benchmarks/datasets.py 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314def __call__(self, parser, namespace, values, option_string=None): setattr(namespace, self.dest, values) # Get current values of both dataset_name and dataset_path dataset_name = getattr(namespace, \"dataset_name\", \"random\") dataset_path = getattr(namespace, \"dataset_path\", None) # Validate the combination if dataset_name == \"random\" and dataset_path is not None: parser.error( \"Cannot use 'random' dataset with --dataset-path. \" \"Please specify the appropriate --dataset-name (e.g., \" \"'sharegpt', 'custom', 'sonnet') for your dataset file: \" f\"{dataset_path}\" ) _format_zeta_prompt ¶ _format_zeta_prompt( sample: dict, original_start_marker: str = \"<|editable_region_start|>\", ) -> dict Format the zeta prompt for the Next Edit Prediction (NEP) dataset. This function formats examples from the NEP dataset into prompts and expected outputs. It could be further extended to support more NEP datasets. Parameters: Name Type Description Default sample dict The dataset sample containing events, inputs, and outputs. required original_start_marker str The marker indicating the start of the editable region. Defaults to \"<|editable_region_start|>\". '<|editable_region_start|>' Returns: Type Description dict A dictionary with the formatted prompts and expected outputs. Source code in vllm/benchmarks/datasets.py 2812 2813 2814 2815 2816 2817 2818 2819 2820 2821 2822 2823 2824 2825 2826 2827 2828 2829 2830 2831 2832 2833 2834 2835 2836 2837 2838 2839 2840 2841 2842def _format_zeta_prompt( sample: dict, original_start_marker: str = \"<|editable_region_start|>\" ) -> dict: \"\"\"Format the zeta prompt for the Next Edit Prediction (NEP) dataset. This function formats examples from the NEP dataset into prompts and expected outputs. It could be further extended to support more NEP datasets. Args: sample: The dataset sample containing events, inputs, and outputs. original_start_marker: The marker indicating the start of the editable region. Defaults to \"<|editable_region_start|>\". Returns: A dictionary with the formatted prompts and expected outputs. \"\"\" events = sample[\"events\"] input = sample[\"input\"] output = sample[\"output\"] prompt = zeta_prompt.format(events, input) # following the original implementation, extract the focused region # from the raw output output_start_index = output.find(original_start_marker) output_focused_region = output[output_start_index:] expected_output = output_focused_region return {\"prompt\": prompt, \"expected_output\": expected_output} add_dataset_parser ¶ add_dataset_parser(parser: ArgumentParser) Source code in vllm/benchmarks/datasets.py 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 1633 1634 1635 1636 1637 1638def add_dataset_parser(parser: FlexibleArgumentParser): parser.add_argument(\"--seed\", type=int, default=0) parser.add_argument( \"--num-prompts\", type=int, default=1000, help=\"Number of prompts to process.\", ) parser.add_argument( \"--dataset-name\", type=str, default=\"random\", action=_ValidateDatasetArgs, choices=[ \"sharegpt\", \"burstgpt\", \"sonnet\", \"random\", \"random-mm\", \"random-rerank\", \"hf\", \"custom\", \"prefix_repetition\", \"spec_bench\", ], help=\"Name of the dataset to benchmark on.\", ) parser.add_argument( \"--no-stream\", action=\"store_true\", help=\"Do not load the dataset in streaming mode.\", ) parser.add_argument( \"--dataset-path\", type=str, default=None, action=_ValidateDatasetArgs, help=\"Path to the sharegpt/sonnet dataset. \" \"Or the huggingface dataset ID if using HF dataset.\", ) parser.add_argument( \"--no-oversample\", action=\"store_true\", help=\"Do not oversample if the dataset has fewer samples than num-prompts.\", ) parser.add_argument( \"--skip-chat-template\", action=\"store_true\", help=\"Skip applying chat template to prompt for datasets that support it.\", ) parser.add_argument( \"--disable-shuffle\", action=\"store_true\", help=\"Disable shuffling of dataset samples for deterministic ordering.\", ) # group for dataset specific arguments custom_group = parser.add_argument_group(\"custom dataset options\") custom_group.add_argument( \"--custom-output-len\", type=int, default=256, help=\"Number of output tokens per request, used only for custom dataset.\", ) spec_bench_group = parser.add_argument_group(\"spec bench dataset options\") spec_bench_group.add_argument( \"--spec-bench-output-len\", type=int, default=256, help=\"Num of output tokens per request, used only for spec bench dataset.\", ) spec_bench_group.add_argument( \"--spec-bench-category\", type=str, default=None, help=\"Category for spec bench dataset. If None, use all categories.\", ) sonnet_group = parser.add_argument_group(\"sonnet dataset options\") sonnet_group.add_argument( \"--sonnet-input-len\", type=int, default=550, help=\"Number of input tokens per request, used only for sonnet dataset.\", ) sonnet_group.add_argument( \"--sonnet-output-len\", type=int, default=150, help=\"Number of output tokens per request, used only for sonnet dataset.\", ) sonnet_group.add_argument( \"--sonnet-prefix-len\", type=int, default=200, help=\"Number of prefix tokens per request, used only for sonnet dataset.\", ) sharegpt_group = parser.add_argument_group(\"sharegpt dataset options\") sharegpt_group.add_argument( \"--sharegpt-output-len\", type=int, default=None, help=\"Output length for each request. Overrides the output length \" \"from the ShareGPT dataset.\", ) blazedit_group = parser.add_argument_group(\"blazedit dataset options\") blazedit_group.add_argument( \"--blazedit-min-distance\", type=float, default=0.0, help=\"Minimum distance for blazedit dataset. Min: 0, Max: 1.0\", ) blazedit_group.add_argument( \"--blazedit-max-distance\", type=float, default=1.0, help=\"Maximum distance for blazedit dataset. Min: 0, Max: 1.0\", ) random_group = parser.add_argument_group(\"random dataset options\") random_group.add_argument( \"--random-input-len\", type=int, default=1024, help=\"Number of input tokens per request, used only for random sampling.\", ) random_group.add_argument( \"--random-output-len\", type=int, default=128, help=\"Number of output tokens per request, used only for random sampling.\", ) random_group.add_argument( \"--random-range-ratio\", type=float, default=0.0, help=\"Range ratio for sampling input/output length, \" \"used only for random sampling. Must be in the range [0, 1) to define \" \"a symmetric sampling range\" \"[length * (1 - range_ratio), length * (1 + range_ratio)].\", ) random_group.add_argument( \"--random-prefix-len\", type=int, default=0, help=( \"Number of fixed prefix tokens before the random context \" \"in a request. \" \"The total input length is the sum of `random-prefix-len` and \" \"a random \" \"context length sampled from [input_len * (1 - range_ratio), \" \"input_len * (1 + range_ratio)].\" ), ) random_group.add_argument( \"--random-batch-size\", type=int, default=1, help=(\"Batch size for random sampling. Only used for embeddings benchmark.\"), ) random_group.add_argument( \"--no-reranker\", action=\"store_true\", help=( \"Whether the model supports reranking natively.\" \" Only used for reranker benchmark.\" ), ) # random multimodal dataset options random_mm_group = parser.add_argument_group( \"random multimodal dataset options extended from random dataset\" ) random_mm_group.add_argument( \"--random-mm-base-items-per-request\", type=int, default=RandomMultiModalDataset.DEFAULT_BASE_ITEMS_PER_REQUEST, help=( \"Base number of multimodal items per request for random-mm. \" \"Actual per-request count is sampled around this base using \" \"--random-mm-num-mm-items-range-ratio.\" ), ) random_mm_group.add_argument( \"--random-mm-num-mm-items-range-ratio\", type=float, default=RandomMultiModalDataset.DEFAULT_NUM_MM_ITEMS_RANGE_RATIO, help=( \"Range ratio r in [0, 1] for sampling items per request. \" \"We sample uniformly from the closed integer range \" \"[floor(n*(1-r)), ceil(n*(1+r))] \" \"where n is the base items per request. \" \"r=0 keeps it fixed; r=1 allows 0 items. The maximum is clamped \" \"to the sum of per-modality limits from \" \"--random-mm-limit-mm-per-prompt. \" \"An error is raised if the computed min exceeds the max.\" ), ) random_mm_group.add_argument( \"--random-mm-limit-mm-per-prompt\", type=json.loads, default=RandomMultiModalDataset.DEFAULT_LIMIT_MM_PER_PROMPT, help=( \"Per-modality hard caps for items attached per request, e.g. \" '\\'{\"image\": 3, \"video\": 0}\\'. The sampled per-request item ' \"count is clamped to the sum of these limits. When a modality \" \"reaches its cap, its buckets are excluded and probabilities are \" \"renormalized.\" \"OBS.: Only image sampling is supported for now.\" ), ) def _parse_mm_bucket_config(v: object) -> dict[tuple[int, int, int], float]: # If already a dict (e.g., programmatic call), normalize keys def normalize(d: dict) -> dict[tuple[int, int, int], float]: out: dict[tuple[int, int, int], float] = {} for k, val in d.items(): key = k if isinstance(key, str): with suppress(Exception): key = ast.literal_eval(key) if not ( isinstance(key, tuple) and len(key) == 3 and all(isinstance(x, int) for x in key) ): raise ValueError( f\"Invalid bucket key {k!r}. Expected tuple (H, W, T).\" ) out[(int(key[0]), int(key[1]), int(key[2]))] = float(val) return out if isinstance(v, dict): return normalize(v) if isinstance(v, str): # Python literal (supports tuple keys) parsed = ast.literal_eval(v) if not isinstance(parsed, dict): raise ValueError(\"Bucket config must parse to a dict.\") return normalize(parsed) raise ValueError(\"Unsupported value for --random-mm-bucket-config.\") random_mm_group.add_argument( \"--random-mm-bucket-config\", type=_parse_mm_bucket_config, default=RandomMultiModalDataset.DEFAULT_MM_ITEM_BUCKET_CONFIG, help=( \"The bucket config is a dictionary mapping a multimodal item\" \"sampling configuration to a probability.\" \"Currently allows for 2 modalities: images and videos. \" \"An bucket key is a tuple of (height, width, num_frames)\" \"The value is the probability of sampling that specific item. \" \"Example: \" \"--random-mm-bucket-config \" \"{(256, 256, 1): 0.5, (720, 1280, 1): 0.4, (720, 1280, 16): 0.10} \" \"First item: images with resolution 256x256 w.p. 0.5\" \"Second item: images with resolution 720x1280 w.p. 0.4 \" \"Third item: videos with resolution 720x1280 and 16 frames w.p. 0.1\" \"OBS.: If the probabilities do not sum to 1, they are normalized.\" \"OBS bis.: Only image sampling is supported for now.\" ), ) hf_group = parser.add_argument_group(\"hf dataset options\") hf_group.add_argument( \"--hf-subset\", type=str, default=None, help=\"Subset of the HF dataset.\" ) hf_group.add_argument( \"--hf-split\", type=str, default=None, help=\"Split of the HF dataset.\" ) hf_group.add_argument( \"--hf-name\", type=str, default=None, help=( \"Name of the dataset on HuggingFace \" \"(e.g., 'lmarena-ai/VisionArena-Chat'). \" \"Specify this if your dataset-path is a local path.\" ), ) hf_group.add_argument( \"--hf-output-len\", type=int, default=None, help=\"Output length for each request. Overrides the output lengths \" \"from the sampled HF dataset.\", ) prefix_repetition_group = parser.add_argument_group( \"prefix repetition dataset options\" ) prefix_repetition_group.add_argument( \"--prefix-repetition-prefix-len\", type=int, default=256, help=\"Number of prefix tokens per request, used only for prefix \" \"repetition dataset.\", ) prefix_repetition_group.add_argument( \"--prefix-repetition-suffix-len\", type=int, default=256, help=\"Number of suffix tokens per request, used only for prefix \" \"repetition dataset. Total input length is prefix_len + suffix_len.\", ) prefix_repetition_group.add_argument( \"--prefix-repetition-num-prefixes\", type=int, default=10, help=\"Number of prefixes to generate, used only for prefix repetition \" \"dataset. Prompts per prefix is num_requests // num_prefixes.\", ) prefix_repetition_group.add_argument( \"--prefix-repetition-output-len\", type=int, default=128, help=\"Number of output tokens per request, used only for prefix \" \"repetition dataset.\", ) gen_prompt_decode_to_target_len ¶ gen_prompt_decode_to_target_len( tokenizer: TokenizerLike, token_sequence: list[int], target_token_len: int, max_retry: int = 10, add_special_tokens: bool = False, rng: Generator | None = None, ) -> tuple[str, list[int]] Ensure decoded-then-encoded prompt length matches the target token length. This function decodes an initial token sequence to text and re-encodes it , iteratively adjusting the token sequence length to match a target. This is necessary because some tokenizers do not guarantee a 1:1 mapping between consecutive tokens and the decoded-then-encoded sequence length. For example, for GPT2Tokenizer: [6880, 6881] -> ['Ġcalls', 'here'] -> [1650, 939, 486] -> ['Ġcall', 'sh', 'ere'] Returns a tuple of the final prompt string and the adjusted token sequence. Source code in vllm/benchmarks/datasets.py 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433def gen_prompt_decode_to_target_len( tokenizer: TokenizerLike, token_sequence: list[int], target_token_len: int, max_retry: int = 10, add_special_tokens: bool = False, rng: np.random.Generator | None = None, ) -> tuple[str, list[int]]: \"\"\" Ensure decoded-then-encoded prompt length matches the target token length. This function decodes an initial token sequence to text and re-encodes it , iteratively adjusting the token sequence length to match a target. This is necessary because some tokenizers do not guarantee a 1:1 mapping between consecutive tokens and the decoded-then-encoded sequence length. For example, for GPT2Tokenizer: [6880, 6881] -> ['Ġcalls', 'here'] -> [1650, 939, 486] -> ['Ġcall', 'sh', 'ere'] Returns a tuple of the final prompt string and the adjusted token sequence. \"\"\" remain_num_try = max_retry token_mismatch = 0 while True: prompt = tokenizer.decode(token_sequence) token_sequence = tokenizer.encode(prompt, add_special_tokens=add_special_tokens) if remain_num_try <= 0: if len(token_sequence) != target_token_len: token_mismatch = len(token_sequence) - target_token_len break if len(token_sequence) == target_token_len: break elif len(token_sequence) < target_token_len: if rng is not None: extra_tokens = rng.integers( 0, tokenizer.vocab_size, size=target_token_len - len(token_sequence), ).tolist() else: extra_tokens = np.random.randint( 0, tokenizer.vocab_size, size=target_token_len - len(token_sequence), ).tolist() token_sequence.extend(extra_tokens) elif len(token_sequence) > target_token_len: token_sequence = token_sequence[:target_token_len] remain_num_try -= 1 return prompt, token_sequence, token_mismatch get_samples ¶ get_samples( args, tokenizer: TokenizerLike ) -> list[SampleRequest] Source code in vllm/benchmarks/datasets.py 1641 1642 1643 1644 1645 1646 1647 1648 1649 1650 1651 1652 1653 1654 1655 1656 1657 1658 1659 1660 1661 1662 1663 1664 1665 1666 1667 1668 1669 1670 1671 1672 1673 1674 1675 1676 1677 1678 1679 1680 1681 1682 1683 1684 1685 1686 1687 1688 1689 1690 1691 1692 1693 1694 1695 1696 1697 1698 1699 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728 1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 1742 1743 1744 1745 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 1793 1794 1795 1796 1797 1798 1799 1800 1801 1802 1803 1804 1805 1806 1807 1808 1809 1810 1811 1812 1813 1814 1815 1816 1817 1818 1819 1820 1821 1822 1823 1824 1825 1826 1827 1828 1829 1830 1831 1832 1833 1834 1835 1836 1837 1838 1839 1840 1841 1842 1843 1844 1845 1846 1847 1848 1849 1850 1851 1852 1853 1854 1855 1856 1857 1858 1859 1860 1861 1862 1863 1864 1865 1866 1867 1868 1869 1870 1871 1872 1873 1874 1875 1876 1877 1878 1879 1880 1881 1882 1883 1884 1885 1886 1887 1888 1889 1890 1891 1892 1893 1894 1895 1896 1897 1898 1899 1900 1901 1902 1903 1904 1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919def get_samples(args, tokenizer: TokenizerLike) -> list[SampleRequest]: if not hasattr(args, \"request_id_prefix\"): args.request_id_prefix = \"\" if args.dataset_name == \"custom\": dataset = CustomDataset( dataset_path=args.dataset_path, disable_shuffle=args.disable_shuffle ) input_requests = dataset.sample( num_requests=args.num_prompts, tokenizer=tokenizer, output_len=args.custom_output_len, skip_chat_template=args.skip_chat_template, request_id_prefix=args.request_id_prefix, no_oversample=args.no_oversample, ) elif args.dataset_name == \"sonnet\": dataset = SonnetDataset( dataset_path=args.dataset_path, disable_shuffle=args.disable_shuffle ) # For the \"sonnet\" dataset, formatting depends on the backend. if args.backend == \"openai-chat\": input_requests = dataset.sample( num_requests=args.num_prompts, input_len=args.sonnet_input_len, output_len=args.sonnet_output_len, prefix_len=args.sonnet_prefix_len, tokenizer=tokenizer, return_prompt_formatted=False, request_id_prefix=args.request_id_prefix, no_oversample=args.no_oversample, ) else: assert tokenizer.chat_template or tokenizer.default_chat_template, ( \"Tokenizer/model must have chat template for sonnet dataset.\" ) input_requests = dataset.sample( num_requests=args.num_prompts, input_len=args.sonnet_input_len, output_len=args.sonnet_output_len, prefix_len=args.sonnet_prefix_len, tokenizer=tokenizer, return_prompt_formatted=True, request_id_prefix=args.request_id_prefix, no_oversample=args.no_oversample, ) elif args.dataset_name == \"hf\": # all following datasets are implemented from the # HuggingFaceDataset base class hf_kwargs = {} if ( args.dataset_path in VisionArenaDataset.SUPPORTED_DATASET_PATHS or args.hf_name in VisionArenaDataset.SUPPORTED_DATASET_PATHS ): dataset_class = VisionArenaDataset args.hf_split = \"train\" args.hf_subset = None elif ( args.dataset_path in MMVUDataset.SUPPORTED_DATASET_PATHS or args.hf_name in MMVUDataset.SUPPORTED_DATASET_PATHS ): dataset_class = MMVUDataset args.hf_split = \"validation\" args.hf_subset = None elif ( args.dataset_path in InstructCoderDataset.SUPPORTED_DATASET_PATHS or args.hf_name in InstructCoderDataset.SUPPORTED_DATASET_PATHS ): dataset_class = InstructCoderDataset args.hf_split = \"train\" elif ( args.dataset_path in MTBenchDataset.SUPPORTED_DATASET_PATHS or args.hf_name in MTBenchDataset.SUPPORTED_DATASET_PATHS ): dataset_class = MTBenchDataset args.hf_split = \"train\" elif ( args.dataset_path in MultiModalConversationDataset.SUPPORTED_DATASET_PATHS or args.hf_name in MultiModalConversationDataset.SUPPORTED_DATASET_PATHS ): dataset_class = MultiModalConversationDataset elif ( args.dataset_path in ConversationDataset.SUPPORTED_DATASET_PATHS or args.hf_name in ConversationDataset.SUPPORTED_DATASET_PATHS ): dataset_class = ConversationDataset elif ( args.dataset_path in AIMODataset.SUPPORTED_DATASET_PATHS or args.hf_name in AIMODataset.SUPPORTED_DATASET_PATHS ): dataset_class = AIMODataset args.hf_split = \"train\" elif ( args.dataset_path in NextEditPredictionDataset.SUPPORTED_DATASET_PATHS # noqa: E501 or args.hf_name in NextEditPredictionDataset.SUPPORTED_DATASET_PATHS ): dataset_class = NextEditPredictionDataset args.hf_split = \"train\" elif ( args.dataset_path in ASRDataset.SUPPORTED_DATASET_PATHS or args.hf_name in ASRDataset.SUPPORTED_DATASET_PATHS ): dataset_class = ASRDataset args.hf_split = \"train\" elif args.dataset_path in BlazeditDataset.SUPPORTED_DATASET_PATHS: dataset_class = BlazeditDataset args.hf_split = \"train\" hf_kwargs = { \"min_distance\": args.blazedit_min_distance, \"max_distance\": args.blazedit_max_distance, } elif ( args.dataset_path in MLPerfDataset.SUPPORTED_DATASET_PATHS or args.hf_name in MLPerfDataset.SUPPORTED_DATASET_PATHS ): dataset_class = MLPerfDataset args.hf_split = \"train\" elif ( args.dataset_path in MMStarDataset.SUPPORTED_DATASET_PATHS or args.hf_name in MMStarDataset.SUPPORTED_DATASET_PATHS ): dataset_class = MMStarDataset args.hf_split = \"val\" args.hf_subset = None else: supported_datasets = set( [ dataset_name for cls in HuggingFaceDataset.__subclasses__() for dataset_name in cls.SUPPORTED_DATASET_PATHS ] ) raise ValueError( f\"Unsupported dataset path: {args.dataset_path}. \" \"Huggingface dataset only supports dataset_path\" f\" from one of following: {supported_datasets}. \" \"Please consider contributing if you would \" \"like to add support for additional dataset formats.\" ) if dataset_class.IS_MULTIMODAL and not ( args.backend in (\"openai-chat\", \"openai-audio\") or \"embeddings-\" in args.backend ): # multi-modal benchmark is only available on OpenAI Chat # endpoint-type. raise ValueError( \"Multi-modal content is only supported on 'openai-chat' and \" \"'openai-audio' backends.\" ) input_requests = dataset_class( dataset_path=args.dataset_path, dataset_subset=args.hf_subset, dataset_split=args.hf_split, random_seed=args.seed, no_stream=args.no_stream, hf_name=args.hf_name, disable_shuffle=args.disable_shuffle, ).sample( num_requests=args.num_prompts, tokenizer=tokenizer, output_len=args.hf_output_len, request_id_prefix=args.request_id_prefix, no_oversample=args.no_oversample, skip_chat_template=args.skip_chat_template, **hf_kwargs, ) else: # For datasets that follow a similar structure, use a mapping. dataset_mapping = { \"spec_bench\": lambda: SpecBench( dataset_path=args.dataset_path, category=args.spec_bench_category, disable_shuffle=args.disable_shuffle, ).sample( num_requests=args.num_prompts, tokenizer=tokenizer, output_len=args.spec_bench_output_len, request_id_prefix=args.request_id_prefix, no_oversample=args.no_oversample, ), \"sharegpt\": lambda: ShareGPTDataset( random_seed=args.seed, dataset_path=args.dataset_path, disable_shuffle=args.disable_shuffle, ).sample( tokenizer=tokenizer, num_requests=args.num_prompts, output_len=args.sharegpt_output_len, request_id_prefix=args.request_id_prefix, no_oversample=args.no_oversample, ), \"burstgpt\": lambda: BurstGPTDataset( random_seed=args.seed, dataset_path=args.dataset_path, disable_shuffle=args.disable_shuffle, ).sample( tokenizer=tokenizer, num_requests=args.num_prompts, request_id_prefix=args.request_id_prefix, no_oversample=args.no_oversample, ), \"random\": lambda: RandomDataset( random_seed=args.seed, dataset_path=args.dataset_path, disable_shuffle=args.disable_shuffle, ).sample( tokenizer=tokenizer, num_requests=args.num_prompts, prefix_len=args.random_prefix_len, input_len=args.random_input_len, output_len=args.random_output_len, range_ratio=args.random_range_ratio, request_id_prefix=args.request_id_prefix, batchsize=args.random_batch_size, no_oversample=args.no_oversample, ), \"random-mm\": lambda: RandomMultiModalDataset( random_seed=args.seed, dataset_path=args.dataset_path, disable_shuffle=args.disable_shuffle, ).sample( tokenizer=tokenizer, num_requests=args.num_prompts, prefix_len=args.random_prefix_len, range_ratio=args.random_range_ratio, input_len=args.random_input_len, output_len=args.random_output_len, base_items_per_request=args.random_mm_base_items_per_request, limit_mm_per_prompt=args.random_mm_limit_mm_per_prompt, num_mm_items_range_ratio=args.random_mm_num_mm_items_range_ratio, bucket_config=args.random_mm_bucket_config, request_id_prefix=args.request_id_prefix, no_oversample=args.no_oversample, ), \"random-rerank\": lambda: RandomDatasetForReranking( random_seed=args.seed, dataset_path=args.dataset_path, disable_shuffle=args.disable_shuffle, ).sample( tokenizer=tokenizer, num_requests=args.num_prompts, input_len=args.random_input_len, range_ratio=args.random_range_ratio, request_id_prefix=args.request_id_prefix, batchsize=args.random_batch_size, is_reranker=not args.no_reranker, ), \"prefix_repetition\": lambda: PrefixRepetitionRandomDataset( random_seed=args.seed, dataset_path=args.dataset_path, disable_shuffle=args.disable_shuffle, ).sample( tokenizer=tokenizer, num_requests=args.num_prompts, prefix_len=args.prefix_repetition_prefix_len, suffix_len=args.prefix_repetition_suffix_len, num_prefixes=args.prefix_repetition_num_prefixes, output_len=args.prefix_repetition_output_len, request_id_prefix=args.request_id_prefix, no_oversample=args.no_oversample, ), } try: # Enforce endpoint compatibility for multimodal datasets. if args.dataset_name == \"random-mm\" and args.backend not in [\"openai-chat\"]: raise ValueError( \"Multi-modal content (images) is only supported on \" \"'openai-chat' backend.\" ) input_requests = dataset_mapping[args.dataset_name]() except KeyError as err: raise ValueError(f\"Unknown dataset: {args.dataset_name}\") from err return input_requests is_valid_sequence ¶ is_valid_sequence( prompt_len: int, output_len: int, min_len: int = 4, max_prompt_len: int = 1024, max_total_len: int = 2048, skip_min_output_len_check: bool = False, ) -> bool Validate a sequence based on prompt and output lengths. Default pruning criteria are copied from the original sample_hf_requests and sample_sharegpt_requests functions in benchmark_serving.py, as well as from sample_requests in benchmark_throughput.py. Source code in vllm/benchmarks/datasets.py 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286def is_valid_sequence( prompt_len: int, output_len: int, min_len: int = 4, max_prompt_len: int = 1024, max_total_len: int = 2048, skip_min_output_len_check: bool = False, ) -> bool: \"\"\" Validate a sequence based on prompt and output lengths. Default pruning criteria are copied from the original `sample_hf_requests` and `sample_sharegpt_requests` functions in benchmark_serving.py, as well as from `sample_requests` in benchmark_throughput.py. \"\"\" # Check for invalid conditions prompt_too_short = prompt_len < min_len output_too_short = (not skip_min_output_len_check) and (output_len < min_len) prompt_too_long = prompt_len > max_prompt_len combined_too_long = (prompt_len + output_len) > max_total_len # Return True if none of the invalid conditions are met return not ( prompt_too_short or output_too_short or prompt_too_long or combined_too_long ) lora_path_on_disk cached ¶ lora_path_on_disk(lora_path: str) -> str Source code in vllm/benchmarks/datasets.py 289 290 291@cache def lora_path_on_disk(lora_path: str) -> str: return get_adapter_absolute_path(lora_path) process_image ¶ process_image(image: Any) -> Mapping[str, Any] Process a single image input and return a multimedia content dictionary. Supports the following input types: Dictionary with raw image bytes: - Expects a dict with a 'bytes' key containing raw image data. - Loads the bytes as a PIL.Image.Image. PIL.Image.Image input: - Converts the image to RGB. - Saves the image as a JPEG in memory. - Encodes the JPEG data as a base64 string. - Returns a dictionary with the image as a base64 data URL. String input: - Treats the string as a URL or local file path. - Prepends \"file://\" if the string doesn't start with \"http://\" or \"file://\". - Returns a dictionary with the image URL. Raises: Type Description ValueError If the input is not a supported type. Source code in vllm/benchmarks/datasets.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341def process_image(image: Any) -> Mapping[str, Any]: \"\"\" Process a single image input and return a multimedia content dictionary. Supports the following input types: 1. Dictionary with raw image bytes: - Expects a dict with a 'bytes' key containing raw image data. - Loads the bytes as a PIL.Image.Image. 2. PIL.Image.Image input: - Converts the image to RGB. - Saves the image as a JPEG in memory. - Encodes the JPEG data as a base64 string. - Returns a dictionary with the image as a base64 data URL. 3. String input: - Treats the string as a URL or local file path. - Prepends \"file://\" if the string doesn't start with \"http://\" or \"file://\". - Returns a dictionary with the image URL. Raises: ValueError: If the input is not a supported type. \"\"\" if isinstance(image, dict) and \"bytes\" in image: image = Image.open(BytesIO(image[\"bytes\"])) if isinstance(image, Image.Image): image = convert_image_mode(image, \"RGB\") with io.BytesIO() as image_data: image.save(image_data, format=\"JPEG\") image_base64 = base64.b64encode(image_data.getvalue()).decode(\"utf-8\") return { \"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_base64}\"}, } if isinstance(image, str): image_url = ( image if image.startswith((\"http://\", \"https://\", \"file://\")) else f\"file://{image}\" ) return {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}} raise ValueError( f\"Invalid image input {image}. Must be a PIL.Image.Image\" \" or str or dictionary with raw image bytes.\" ) process_video ¶ process_video(video: Any) -> Mapping[str, Any] Process a single video input and return a multimedia content dictionary. Supports the following input types: Dictionary with raw video bytes: - Expects a dict with a 'bytes' key containing raw video data. String input: - Treats the string as a URL or local file path. - Prepends \"file://\" if the string doesn't start with \"http://\" or \"file://\". - Returns a dictionary with the image URL. Raises: Type Description ValueError If the input is not a supported type. Source code in vllm/benchmarks/datasets.py 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378def process_video(video: Any) -> Mapping[str, Any]: \"\"\" Process a single video input and return a multimedia content dictionary. Supports the following input types: 1. Dictionary with raw video bytes: - Expects a dict with a 'bytes' key containing raw video data. 2. String input: - Treats the string as a URL or local file path. - Prepends \"file://\" if the string doesn't start with \"http://\" or \"file://\". - Returns a dictionary with the image URL. Raises: ValueError: If the input is not a supported type. \"\"\" if isinstance(video, dict) and \"bytes\" in video: video_bytes = video[\"bytes\"] video_base64 = base64.b64encode(video_bytes).decode(\"utf-8\") return { \"type\": \"video_url\", \"video_url\": {\"url\": f\"data:video/mp4;base64,{video_base64}\"}, } if isinstance(video, str): video_url = ( video if video.startswith((\"http://\", \"https://\", \"file://\")) else f\"file://{video}\" ) return {\"type\": \"video_url\", \"video_url\": {\"url\": video_url}} raise ValueError( f\"Invalid video input {video}. Must be a string of local path/remote url, or a dictionary with raw video bytes in the form of `{{'bytes': raw_video_bytes}}`.\" # noqa: E501 ) December 25, 2025",
      "code": ""
    },
    {
      "description": "vllm.benchmarks.datasets ¶ This module defines a framework for sampling benchmark requests from various datasets. Each dataset subclass of BenchmarkDataset must implement sample generation. Supported dataset types include: - ShareGPT - Random (synthetic) - Sonnet - BurstGPT - HuggingFace - VisionArena datasets module-attribute ¶ datasets = PlaceholderModule('datasets') logger module-attribute ¶ logger = getLogger(__name__) lora_tokenizer_cache module-attribute ¶ lora_tokenizer_cache: dict[int, TokenizerLike] = {} zeta_prompt module-attribute ¶ zeta_prompt = \"### Instruction:\\nYou are a code completion assistant and your task is to analyze user edits and then rewrite an excerpt that the user provides, suggesting the appropriate edits within the excerpt, taking into account the cursor location.\\n\\n### User Edits:\\n\\n{}\\n\\n### User Excerpt:\\n\\n{}\\n\\n### Response:\\n\\n\" AIMODataset ¶ Bases: HuggingFaceDataset Dataset class for processing a AIMO dataset with reasoning questions. Source code in vllm/benchmarks/datasets.py 2736 2737 2738 2739 2740 2741 2742 2743 2744 2745 2746 2747 2748 2749 2750 2751 2752 2753 2754 2755 2756 2757 2758 2759 2760 2761 2762 2763 2764 2765 2766 2767 2768 2769 2770 2771 2772 2773 2774 2775 2776 2777 2778 2779 2780 2781 2782 2783 2784 2785 2786 2787 2788class AIMODataset(HuggingFaceDataset): \"\"\" Dataset class for processing a AIMO dataset with reasoning questions. \"\"\" SUPPORTED_DATASET_PATHS = { \"AI-MO/aimo-validation-aime\", \"AI-MO/NuminaMath-1.5\", \"AI-MO/NuminaMath-CoT\", } def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: sampled_requests = [] ind = 0 dynamic_output = output_len is None for item in self.data: if len(sampled_requests) >= num_requests: break prompt, completion = item[\"problem\"], item[\"solution\"] prompt_ids = tokenizer(prompt).input_ids completion_ids = tokenizer(completion).input_ids prompt_len = len(prompt_ids) completion_len = len(completion_ids) output_len = completion_len if dynamic_output else output_len assert isinstance(output_len, int) and output_len > 0 if dynamic_output and not is_valid_sequence( prompt_len, completion_len, max_prompt_len=2048, max_total_len=32000 ): continue sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=None, request_id=request_id_prefix + str(ind), ) ) ind += 1 self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = { \"AI-MO/aimo-validation-aime\", \"AI-MO/NuminaMath-1.5\", \"AI-MO/NuminaMath-CoT\", } sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 2747 2748 2749 2750 2751 2752 2753 2754 2755 2756 2757 2758 2759 2760 2761 2762 2763 2764 2765 2766 2767 2768 2769 2770 2771 2772 2773 2774 2775 2776 2777 2778 2779 2780 2781 2782 2783 2784 2785 2786 2787 2788def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: sampled_requests = [] ind = 0 dynamic_output = output_len is None for item in self.data: if len(sampled_requests) >= num_requests: break prompt, completion = item[\"problem\"], item[\"solution\"] prompt_ids = tokenizer(prompt).input_ids completion_ids = tokenizer(completion).input_ids prompt_len = len(prompt_ids) completion_len = len(completion_ids) output_len = completion_len if dynamic_output else output_len assert isinstance(output_len, int) and output_len > 0 if dynamic_output and not is_valid_sequence( prompt_len, completion_len, max_prompt_len=2048, max_total_len=32000 ): continue sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=None, request_id=request_id_prefix + str(ind), ) ) ind += 1 self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests ASRDataset ¶ Bases: HuggingFaceDataset Dataset class for processing a ASR dataset for transcription. Tested on the following set: +----------------+----------------------------------------+--------------------------+-----------------------------+ | Dataset | Domain | Speaking Style | hf-subset | +----------------+----------------------------------------+--------------------------+-----------------------------+ | TED-LIUM | TED talks | Oratory | release1, release2, release3| | | | | release3-speaker-adaptation | | VoxPopuli | European Parliament | Oratory | en, de, it, fr, ... | | LibriSpeech | Audiobook | Narrated | \"LIUM/tedlium\" | | GigaSpeech | Audiobook, podcast, YouTube | Narrated, spontaneous | xs, s, m, l, xl, dev, test | | SPGISpeech | Financial meetings | Oratory, spontaneous | S, M, L, dev, test | | AMI | Meetings | Spontaneous | ihm, sdm | +----------------+----------------------------------------+--------------------------+-----------------------------+ Source code in vllm/benchmarks/datasets.py 2894 2895 2896 2897 2898 2899 2900 2901 2902 2903 2904 2905 2906 2907 2908 2909 2910 2911 2912 2913 2914 2915 2916 2917 2918 2919 2920 2921 2922 2923 2924 2925 2926 2927 2928 2929 2930 2931 2932 2933 2934 2935 2936 2937 2938 2939 2940 2941 2942 2943 2944 2945 2946 2947 2948 2949 2950 2951 2952 2953 2954 2955 2956 2957 2958 2959 2960 2961 2962 2963 2964 2965 2966 2967 2968 2969 2970 2971 2972 2973 2974 2975 2976class ASRDataset(HuggingFaceDataset): \"\"\" Dataset class for processing a ASR dataset for transcription. Tested on the following set: +----------------+----------------------------------------+--------------------------+-----------------------------+ | Dataset | Domain | Speaking Style | hf-subset | +----------------+----------------------------------------+--------------------------+-----------------------------+ | TED-LIUM | TED talks | Oratory | release1, release2, release3| | | | | release3-speaker-adaptation | | VoxPopuli | European Parliament | Oratory | en, de, it, fr, ... | | LibriSpeech | Audiobook | Narrated | \"LIUM/tedlium\" | | GigaSpeech | Audiobook, podcast, YouTube | Narrated, spontaneous | xs, s, m, l, xl, dev, test | | SPGISpeech | Financial meetings | Oratory, spontaneous | S, M, L, dev, test | | AMI | Meetings | Spontaneous | ihm, sdm | +----------------+----------------------------------------+--------------------------+-----------------------------+ \"\"\" # noqa: E501 SUPPORTED_DATASET_PATHS = { \"openslr/librispeech_asr\", \"facebook/voxpopuli\", \"LIUM/tedlium\", \"edinburghcstr/ami\", \"speechcolab/gigaspeech\", \"kensho/spgispeech\", } DEFAULT_OUTPUT_LEN = 128 IS_MULTIMODAL = True # TODO Whisper-specific. Abstract interface when more models are supported. TRANSCRIPTION_PREAMBLE = \"<|startoftranscript|><|en|><|transcribe|><|notimestamps|>\" skip_long_audios: bool = True def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN prompt = ASRDataset.TRANSCRIPTION_PREAMBLE prompt_len = len(tokenizer(prompt).input_ids) sampled_requests = [] ind = 0 skipped = 0 for item in self.data: if len(sampled_requests) >= num_requests: break audio = item[\"audio\"] y, sr = audio[\"array\"], audio[\"sampling_rate\"] duration_s = librosa.get_duration(y=y, sr=sr) # Whisper max supported duration if self.skip_long_audios and duration_s > 30: skipped += 1 continue mm_content = {\"audio\": (y, sr)} sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_content, request_id=request_id_prefix + str(ind), ) ) ind += 1 if skipped: logger.warning( \"%d samples discarded from dataset due to\" \" their length being greater than\" \" what Whisper supports.\", skipped, ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶ DEFAULT_OUTPUT_LEN = 128 IS_MULTIMODAL class-attribute instance-attribute ¶ IS_MULTIMODAL = True SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = { \"openslr/librispeech_asr\", \"facebook/voxpopuli\", \"LIUM/tedlium\", \"edinburghcstr/ami\", \"speechcolab/gigaspeech\", \"kensho/spgispeech\", } TRANSCRIPTION_PREAMBLE class-attribute instance-attribute ¶ TRANSCRIPTION_PREAMBLE = \"<|startoftranscript|><|en|><|transcribe|><|notimestamps|>\" skip_long_audios class-attribute instance-attribute ¶ skip_long_audios: bool = True sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 2929 2930 2931 2932 2933 2934 2935 2936 2937 2938 2939 2940 2941 2942 2943 2944 2945 2946 2947 2948 2949 2950 2951 2952 2953 2954 2955 2956 2957 2958 2959 2960 2961 2962 2963 2964 2965 2966 2967 2968 2969 2970 2971 2972 2973 2974 2975 2976def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN prompt = ASRDataset.TRANSCRIPTION_PREAMBLE prompt_len = len(tokenizer(prompt).input_ids) sampled_requests = [] ind = 0 skipped = 0 for item in self.data: if len(sampled_requests) >= num_requests: break audio = item[\"audio\"] y, sr = audio[\"array\"], audio[\"sampling_rate\"] duration_s = librosa.get_duration(y=y, sr=sr) # Whisper max supported duration if self.skip_long_audios and duration_s > 30: skipped += 1 continue mm_content = {\"audio\": (y, sr)} sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_content, request_id=request_id_prefix + str(ind), ) ) ind += 1 if skipped: logger.warning( \"%d samples discarded from dataset due to\" \" their length being greater than\" \" what Whisper supports.\", skipped, ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests BenchmarkDataset ¶ Bases: ABC Source code in vllm/benchmarks/datasets.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254class BenchmarkDataset(ABC): DEFAULT_SEED = 0 IS_MULTIMODAL = False def __init__( self, dataset_path: str | None = None, random_seed: int = DEFAULT_SEED, disable_shuffle: bool = False, **kwargs, ) -> None: \"\"\" Initialize the BenchmarkDataset with an optional dataset path and random seed. Args: dataset_path (Optional[str]): Path to the dataset. If None, it indicates that a default or random dataset might be used. random_seed (int): Seed value for reproducible shuffling or sampling. Defaults to DEFAULT_SEED. \"\"\" self.dataset_path = dataset_path # Set the random seed, ensuring that a None value is replaced with the # default seed. self.random_seed = random_seed if random_seed is not None else self.DEFAULT_SEED self.disable_shuffle = disable_shuffle self.data = None def apply_multimodal_chat_transformation( self, prompt: str, mm_content: MultiModalDataDict | dict | list[dict] | None = None, ) -> list[dict]: \"\"\" Transform a prompt and optional multimodal content into a chat format. This method is used for chat models that expect a specific conversation format. \"\"\" content = [{\"text\": prompt, \"type\": \"text\"}] if mm_content is not None: if isinstance(mm_content, list): content.extend(cast(list[dict[str, Any]], mm_content)) elif isinstance(mm_content, dict): content.append(mm_content) else: raise TypeError( \"Could not process multimodal content of type: \" + f\"{type(mm_content)}\" ) return [{\"role\": \"user\", \"content\": content}] def load_data(self) -> None: \"\"\" Load data from the dataset path into self.data. This method must be overridden by subclasses since the method to load data will vary depending on the dataset format and source. Raises: NotImplementedError: If a subclass does not implement this method. \"\"\" # TODO (jenniferzhao): add support for downloading data raise NotImplementedError(\"load_data must be implemented in subclasses.\") def get_random_lora_request( self, max_loras: int | None = None, lora_path: str | None = None, ) -> LoRARequest | None: \"\"\" Optionally select a random LoRA request. This method is used when LoRA parameters are provided. It randomly selects a LoRA based on max_loras. Args: max_loras (Optional[int]): The maximum number of LoRAs available. If `None`, LoRA is not used. lora_path (Optional[str]): Path to the LoRA parameters on disk. If `None`, LoRA is not used. Returns: A new [`LoRARequest`][vllm.lora.request.LoRARequest] (or `None` if not applicable). \"\"\" if max_loras is None or lora_path is None: return None # Generate a random LoRA ID in the range [1, max_loras]. lora_id = random.randint(1, max_loras) lora_request = LoRARequest( lora_name=str(lora_id), lora_int_id=lora_id, lora_path=lora_path_on_disk(lora_path), ) return lora_request @abstractmethod def sample( self, tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, ) -> list[SampleRequest]: \"\"\" Abstract method to generate sample requests from the dataset. Subclasses must override this method to implement dataset-specific logic for generating a list of SampleRequest objects. Args: tokenizer (TokenizerLike): The tokenizer to be used for processing the dataset's text. num_requests (int): The number of sample requests to generate. request_id_prefix (str): The prefix of request_id. Returns: list[SampleRequest]: A list of sample requests generated from the dataset. \"\"\" raise NotImplementedError(\"sample must be implemented in subclasses.\") def maybe_oversample_requests( self, requests: list[SampleRequest], num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, ) -> None: \"\"\" Oversamples the list of requests if its size is less than the desired number. Args: requests (List[SampleRequest]): The current list of sampled requests. num_requests (int): The target number of requests. request_id_prefix (str): The prefix applied to generated request identifiers. \"\"\" if no_oversample: logger.info(\"Skipping oversampling. Total samples: %d.\", len(requests)) return if len(requests) < num_requests: random.seed(self.random_seed) needed = num_requests - len(requests) additional = [] for i in range(needed): req = deepcopy(random.choice(requests)) req.request_id = request_id_prefix + str(len(requests) + i) additional.append(req) requests.extend(additional) logger.info(\"Oversampled requests to reach %d total samples.\", num_requests) ids = [req.request_id for req in requests] if len(ids) != len(set(ids)): raise ValueError( \"Duplicate request_id found in the sampled \" \"requests. Please ensure that each request_id \" \"is unique.\" ) DEFAULT_SEED class-attribute instance-attribute ¶ DEFAULT_SEED = 0 IS_MULTIMODAL class-attribute instance-attribute ¶ IS_MULTIMODAL = False data instance-attribute ¶ data = None dataset_path instance-attribute ¶ dataset_path = dataset_path disable_shuffle instance-attribute ¶ disable_shuffle = disable_shuffle random_seed instance-attribute ¶ random_seed = ( random_seed if random_seed is not None else DEFAULT_SEED ) __init__ ¶ __init__( dataset_path: str | None = None, random_seed: int = DEFAULT_SEED, disable_shuffle: bool = False, **kwargs, ) -> None Initialize the BenchmarkDataset with an optional dataset path and random seed. Parameters: Name Type Description Default dataset_path Optional[str] Path to the dataset. If None, it indicates that a default or random dataset might be used. None random_seed int Seed value for reproducible shuffling or sampling. Defaults to DEFAULT_SEED. DEFAULT_SEED Source code in vllm/benchmarks/datasets.py 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117def __init__( self, dataset_path: str | None = None, random_seed: int = DEFAULT_SEED, disable_shuffle: bool = False, **kwargs, ) -> None: \"\"\" Initialize the BenchmarkDataset with an optional dataset path and random seed. Args: dataset_path (Optional[str]): Path to the dataset. If None, it indicates that a default or random dataset might be used. random_seed (int): Seed value for reproducible shuffling or sampling. Defaults to DEFAULT_SEED. \"\"\" self.dataset_path = dataset_path # Set the random seed, ensuring that a None value is replaced with the # default seed. self.random_seed = random_seed if random_seed is not None else self.DEFAULT_SEED self.disable_shuffle = disable_shuffle self.data = None apply_multimodal_chat_transformation ¶ apply_multimodal_chat_transformation( prompt: str, mm_content: MultiModalDataDict | dict | list[dict] | None = None, ) -> list[dict] Transform a prompt and optional multimodal content into a chat format. This method is used for chat models that expect a specific conversation format. Source code in vllm/benchmarks/datasets.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140def apply_multimodal_chat_transformation( self, prompt: str, mm_content: MultiModalDataDict | dict | list[dict] | None = None, ) -> list[dict]: \"\"\" Transform a prompt and optional multimodal content into a chat format. This method is used for chat models that expect a specific conversation format. \"\"\" content = [{\"text\": prompt, \"type\": \"text\"}] if mm_content is not None: if isinstance(mm_content, list): content.extend(cast(list[dict[str, Any]], mm_content)) elif isinstance(mm_content, dict): content.append(mm_content) else: raise TypeError( \"Could not process multimodal content of type: \" + f\"{type(mm_content)}\" ) return [{\"role\": \"user\", \"content\": content}] get_random_lora_request ¶ get_random_lora_request( max_loras: int | None = None, lora_path: str | None = None, ) -> LoRARequest | None Optionally select a random LoRA request. This method is used when LoRA parameters are provided. It randomly selects a LoRA based on max_loras. Parameters: Name Type Description Default max_loras Optional[int] The maximum number of LoRAs available. If None, LoRA is not used. None lora_path Optional[str] Path to the LoRA parameters on disk. If None, LoRA is not used. None Returns: Type Description LoRARequest | None A new LoRARequest LoRARequest | None (or None if not applicable). Source code in vllm/benchmarks/datasets.py 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186def get_random_lora_request( self, max_loras: int | None = None, lora_path: str | None = None, ) -> LoRARequest | None: \"\"\" Optionally select a random LoRA request. This method is used when LoRA parameters are provided. It randomly selects a LoRA based on max_loras. Args: max_loras (Optional[int]): The maximum number of LoRAs available. If `None`, LoRA is not used. lora_path (Optional[str]): Path to the LoRA parameters on disk. If `None`, LoRA is not used. Returns: A new [`LoRARequest`][vllm.lora.request.LoRARequest] (or `None` if not applicable). \"\"\" if max_loras is None or lora_path is None: return None # Generate a random LoRA ID in the range [1, max_loras]. lora_id = random.randint(1, max_loras) lora_request = LoRARequest( lora_name=str(lora_id), lora_int_id=lora_id, lora_path=lora_path_on_disk(lora_path), ) return lora_request load_data ¶ load_data() -> None Load data from the dataset path into self.data. This method must be overridden by subclasses since the method to load data will vary depending on the dataset format and source. Raises: Type Description NotImplementedError If a subclass does not implement this method. Source code in vllm/benchmarks/datasets.py 142 143 144 145 146 147 148 149 150 151 152 153def load_data(self) -> None: \"\"\" Load data from the dataset path into self.data. This method must be overridden by subclasses since the method to load data will vary depending on the dataset format and source. Raises: NotImplementedError: If a subclass does not implement this method. \"\"\" # TODO (jenniferzhao): add support for downloading data raise NotImplementedError(\"load_data must be implemented in subclasses.\") maybe_oversample_requests ¶ maybe_oversample_requests( requests: list[SampleRequest], num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, ) -> None Oversamples the list of requests if its size is less than the desired number. Parameters: Name Type Description Default requests List[SampleRequest] The current list of sampled requests. required num_requests int The target number of requests. required request_id_prefix str The prefix applied to generated request identifiers. '' Source code in vllm/benchmarks/datasets.py 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254def maybe_oversample_requests( self, requests: list[SampleRequest], num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, ) -> None: \"\"\" Oversamples the list of requests if its size is less than the desired number. Args: requests (List[SampleRequest]): The current list of sampled requests. num_requests (int): The target number of requests. request_id_prefix (str): The prefix applied to generated request identifiers. \"\"\" if no_oversample: logger.info(\"Skipping oversampling. Total samples: %d.\", len(requests)) return if len(requests) < num_requests: random.seed(self.random_seed) needed = num_requests - len(requests) additional = [] for i in range(needed): req = deepcopy(random.choice(requests)) req.request_id = request_id_prefix + str(len(requests) + i) additional.append(req) requests.extend(additional) logger.info(\"Oversampled requests to reach %d total samples.\", num_requests) ids = [req.request_id for req in requests] if len(ids) != len(set(ids)): raise ValueError( \"Duplicate request_id found in the sampled \" \"requests. Please ensure that each request_id \" \"is unique.\" ) sample abstractmethod ¶ sample( tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, ) -> list[SampleRequest] Abstract method to generate sample requests from the dataset. Subclasses must override this method to implement dataset-specific logic for generating a list of SampleRequest objects. Parameters: Name Type Description Default tokenizer TokenizerLike The tokenizer to be used for processing the dataset's text. required num_requests int The number of sample requests to generate. required request_id_prefix str The prefix of request_id. '' Returns: Type Description list[SampleRequest] list[SampleRequest]: A list of sample requests generated from the list[SampleRequest] dataset. Source code in vllm/benchmarks/datasets.py 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212@abstractmethod def sample( self, tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, ) -> list[SampleRequest]: \"\"\" Abstract method to generate sample requests from the dataset. Subclasses must override this method to implement dataset-specific logic for generating a list of SampleRequest objects. Args: tokenizer (TokenizerLike): The tokenizer to be used for processing the dataset's text. num_requests (int): The number of sample requests to generate. request_id_prefix (str): The prefix of request_id. Returns: list[SampleRequest]: A list of sample requests generated from the dataset. \"\"\" raise NotImplementedError(\"sample must be implemented in subclasses.\") BlazeditDataset ¶ Bases: HuggingFaceDataset Blazedit Dataset. https://github.com/ise-uiuc/blazedit 5k char version: vdaita/edit_5k_char 10k char version: vdaita/edit_10k_char Source code in vllm/benchmarks/datasets.py 2647 2648 2649 2650 2651 2652 2653 2654 2655 2656 2657 2658 2659 2660 2661 2662 2663 2664 2665 2666 2667 2668 2669 2670 2671 2672 2673 2674 2675 2676 2677 2678 2679 2680 2681 2682 2683 2684 2685 2686 2687 2688 2689 2690 2691 2692 2693 2694 2695 2696 2697 2698 2699 2700 2701 2702 2703 2704 2705 2706 2707 2708 2709 2710 2711 2712 2713 2714 2715 2716 2717 2718 2719 2720 2721 2722 2723 2724 2725 2726 2727 2728class BlazeditDataset(HuggingFaceDataset): \"\"\" Blazedit Dataset. https://github.com/ise-uiuc/blazedit 5k char version: vdaita/edit_5k_char 10k char version: vdaita/edit_10k_char \"\"\" # noqa: E501 # 5k char version will have output as ~5k chars # 10k char version will have output as ~10k chars # Assuming 3 char per token, 10k chars will be 3333 tokens # We set default to 4000 to be safe DEFAULT_OUTPUT_LEN = 4000 SUPPORTED_DATASET_PATHS = { \"vdaita/edit_5k_char\", \"vdaita/edit_10k_char\", } def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, min_distance: float = 0.0, max_distance: float = 1.0, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break code = item[\"code\"] change_request = item[\"change_request\"] norm_distance = item[\"norm_distance\"] # compare the levenshtein distance normalized by code length if norm_distance < min_distance or norm_distance > max_distance: continue # template copied from # https://github.com/ise-uiuc/blazedit/blob/7765137e656fd62de877422d2e4cf8de51228054/dataset/create_refined_dataset.py#L94-L105 # noqa: E501 prompt = f\"\"\"Given a code file, please apply the change requests and generate the new file. Original file: ```python {code} ``` Change request: {change_request} Please generate the new code file in the \"New file\" section below.\"\"\" # noqa: E501 # apply template if not skip_chat_template: prompt = tokenizer.apply_chat_template( [{\"role\": \"user\", \"content\": prompt}], add_generation_prompt=True, tokenize=False, ) prompt_len = len(tokenizer(prompt).input_ids) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶ DEFAULT_OUTPUT_LEN = 4000 SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = { \"vdaita/edit_5k_char\", \"vdaita/edit_10k_char\", } sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, min_distance: float = 0.0, max_distance: float = 1.0, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 2666 2667 2668 2669 2670 2671 2672 2673 2674 2675 2676 2677 2678 2679 2680 2681 2682 2683 2684 2685 2686 2687 2688 2689 2690 2691 2692 2693 2694 2695 2696 2697 2698 2699 2700 2701 2702 2703 2704 2705 2706 2707 2708 2709 2710 2711 2712 2713 2714 2715 2716 2717 2718 2719 2720 2721 2722 2723 2724 2725 2726 2727 2728 def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, min_distance: float = 0.0, max_distance: float = 1.0, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break code = item[\"code\"] change_request = item[\"change_request\"] norm_distance = item[\"norm_distance\"] # compare the levenshtein distance normalized by code length if norm_distance < min_distance or norm_distance > max_distance: continue # template copied from # https://github.com/ise-uiuc/blazedit/blob/7765137e656fd62de877422d2e4cf8de51228054/dataset/create_refined_dataset.py#L94-L105 # noqa: E501 prompt = f\"\"\"Given a code file, please apply the change requests and generate the new file. Original file: ```python {code} ``` Change request: {change_request} Please generate the new code file in the \"New file\" section below.\"\"\" # noqa: E501 # apply template if not skip_chat_template: prompt = tokenizer.apply_chat_template( [{\"role\": \"user\", \"content\": prompt}], add_generation_prompt=True, tokenize=False, ) prompt_len = len(tokenizer(prompt).input_ids) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests BurstGPTDataset ¶ Bases: BenchmarkDataset Implements the BurstGPT dataset. Loads data from a CSV file and generates sample requests based on synthetic prompt generation. Only rows with Model \"GPT-4\" and positive response tokens are used. Source code in vllm/benchmarks/datasets.py 2170 2171 2172 2173 2174 2175 2176 2177 2178 2179 2180 2181 2182 2183 2184 2185 2186 2187 2188 2189 2190 2191 2192 2193 2194 2195 2196 2197 2198 2199 2200 2201 2202 2203 2204 2205 2206 2207 2208 2209 2210 2211 2212 2213 2214 2215 2216 2217 2218 2219 2220 2221 2222 2223 2224 2225 2226 2227 2228 2229 2230 2231 2232 2233 2234 2235 2236 2237 2238 2239class BurstGPTDataset(BenchmarkDataset): \"\"\" Implements the BurstGPT dataset. Loads data from a CSV file and generates sample requests based on synthetic prompt generation. Only rows with Model \"GPT-4\" and positive response tokens are used. \"\"\" def __init__(self, **kwargs) -> None: super().__init__(**kwargs) self.load_data() def load_data( self, ): if self.dataset_path is None: raise ValueError(\"dataset_path must be provided for loading data.\") df = pd.read_csv(self.dataset_path) # Filter to keep only GPT-4 rows. gpt4_df = df[df[\"Model\"] == \"GPT-4\"] # Remove failed requests (where Response tokens is 0 or less). gpt4_df = gpt4_df[gpt4_df[\"Response tokens\"] > 0] # Sample the desired number of rows. self.data = gpt4_df def _sample_loaded_data(self, num_requests: int) -> list: if num_requests <= len(self.data): data = self.data.sample(n=num_requests, random_state=self.random_seed) else: data = self.data.sample( n=num_requests, random_state=self.random_seed, replace=True, ) # Convert the dataframe to a list of lists. return data.values.tolist() def sample( self, tokenizer: TokenizerLike, num_requests: int, max_loras: int | None = None, lora_path: str | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest]: samples = [] data = self._sample_loaded_data(num_requests=num_requests) for i in range(num_requests): input_len = int(data[i][2]) output_len = int(data[i][3]) lora_req = self.get_random_lora_request( max_loras=max_loras, lora_path=lora_path ) vocab_size = tokenizer.vocab_size # Generate a synthetic prompt: a list of token IDs computed as (i + # j) modulo vocab_size. token_ids = [(i + j) % vocab_size for j in range(input_len)] prompt = tokenizer.decode(token_ids) samples.append( SampleRequest( prompt=prompt, prompt_len=input_len, expected_output_len=output_len, lora_request=lora_req, request_id=request_id_prefix + str(i), ) ) return samples __init__ ¶ __init__(**kwargs) -> None Source code in vllm/benchmarks/datasets.py 2177 2178 2179def __init__(self, **kwargs) -> None: super().__init__(**kwargs) self.load_data() _sample_loaded_data ¶ _sample_loaded_data(num_requests: int) -> list Source code in vllm/benchmarks/datasets.py 2195 2196 2197 2198 2199 2200 2201 2202 2203 2204 2205def _sample_loaded_data(self, num_requests: int) -> list: if num_requests <= len(self.data): data = self.data.sample(n=num_requests, random_state=self.random_seed) else: data = self.data.sample( n=num_requests, random_state=self.random_seed, replace=True, ) # Convert the dataframe to a list of lists. return data.values.tolist() load_data ¶ load_data() Source code in vllm/benchmarks/datasets.py 2181 2182 2183 2184 2185 2186 2187 2188 2189 2190 2191 2192 2193def load_data( self, ): if self.dataset_path is None: raise ValueError(\"dataset_path must be provided for loading data.\") df = pd.read_csv(self.dataset_path) # Filter to keep only GPT-4 rows. gpt4_df = df[df[\"Model\"] == \"GPT-4\"] # Remove failed requests (where Response tokens is 0 or less). gpt4_df = gpt4_df[gpt4_df[\"Response tokens\"] > 0] # Sample the desired number of rows. self.data = gpt4_df sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, max_loras: int | None = None, lora_path: str | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest] Source code in vllm/benchmarks/datasets.py 2207 2208 2209 2210 2211 2212 2213 2214 2215 2216 2217 2218 2219 2220 2221 2222 2223 2224 2225 2226 2227 2228 2229 2230 2231 2232 2233 2234 2235 2236 2237 2238 2239def sample( self, tokenizer: TokenizerLike, num_requests: int, max_loras: int | None = None, lora_path: str | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest]: samples = [] data = self._sample_loaded_data(num_requests=num_requests) for i in range(num_requests): input_len = int(data[i][2]) output_len = int(data[i][3]) lora_req = self.get_random_lora_request( max_loras=max_loras, lora_path=lora_path ) vocab_size = tokenizer.vocab_size # Generate a synthetic prompt: a list of token IDs computed as (i + # j) modulo vocab_size. token_ids = [(i + j) % vocab_size for j in range(input_len)] prompt = tokenizer.decode(token_ids) samples.append( SampleRequest( prompt=prompt, prompt_len=input_len, expected_output_len=output_len, lora_request=lora_req, request_id=request_id_prefix + str(i), ) ) return samples ConversationDataset ¶ Bases: HuggingFaceDataset Dataset for text-only conversation data. Source code in vllm/benchmarks/datasets.py 2284 2285 2286 2287 2288 2289 2290 2291 2292 2293 2294 2295 2296 2297 2298 2299 2300 2301 2302 2303 2304 2305 2306 2307 2308 2309 2310 2311 2312 2313 2314 2315 2316 2317 2318 2319 2320 2321 2322 2323 2324 2325 2326 2327 2328 2329 2330 2331 2332 2333 2334 2335 2336 2337 2338 2339 2340 2341class ConversationDataset(HuggingFaceDataset): \"\"\"Dataset for text-only conversation data.\"\"\" SUPPORTED_DATASET_PATHS = { \"Aeala/ShareGPT_Vicuna_unfiltered\", } IS_MULTIMODAL = False def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: # Filter examples with at least 2 conversations filtered_data = self.data.filter(lambda x: len(x[\"conversations\"]) >= 2) sampled_requests = [] ind = 0 dynamic_output = output_len is None for item in filtered_data: if len(sampled_requests) >= num_requests: break conv = item[\"conversations\"] prompt, completion = conv[0][\"value\"], conv[1][\"value\"] prompt_ids = tokenizer(prompt).input_ids completion_ids = tokenizer(completion).input_ids prompt_len = len(prompt_ids) completion_len = len(completion_ids) output_len = completion_len if dynamic_output else output_len assert isinstance(output_len, int) and output_len > 0 if dynamic_output and not is_valid_sequence(prompt_len, completion_len): continue mm_content = process_image(item[\"image\"]) if \"image\" in item else None if enable_multimodal_chat: # Note: when chat is enabled the request prompt_len is no longer # accurate and we will be using request output to count the # actual prompt len and output len prompt = self.apply_multimodal_chat_transformation(prompt, mm_content) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_content, request_id=request_id_prefix + str(ind), ) ) ind += 1 self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests IS_MULTIMODAL class-attribute instance-attribute ¶ IS_MULTIMODAL = False SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = { \"Aeala/ShareGPT_Vicuna_unfiltered\" } sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 2292 2293 2294 2295 2296 2297 2298 2299 2300 2301 2302 2303 2304 2305 2306 2307 2308 2309 2310 2311 2312 2313 2314 2315 2316 2317 2318 2319 2320 2321 2322 2323 2324 2325 2326 2327 2328 2329 2330 2331 2332 2333 2334 2335 2336 2337 2338 2339 2340 2341def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: # Filter examples with at least 2 conversations filtered_data = self.data.filter(lambda x: len(x[\"conversations\"]) >= 2) sampled_requests = [] ind = 0 dynamic_output = output_len is None for item in filtered_data: if len(sampled_requests) >= num_requests: break conv = item[\"conversations\"] prompt, completion = conv[0][\"value\"], conv[1][\"value\"] prompt_ids = tokenizer(prompt).input_ids completion_ids = tokenizer(completion).input_ids prompt_len = len(prompt_ids) completion_len = len(completion_ids) output_len = completion_len if dynamic_output else output_len assert isinstance(output_len, int) and output_len > 0 if dynamic_output and not is_valid_sequence(prompt_len, completion_len): continue mm_content = process_image(item[\"image\"]) if \"image\" in item else None if enable_multimodal_chat: # Note: when chat is enabled the request prompt_len is no longer # accurate and we will be using request output to count the # actual prompt len and output len prompt = self.apply_multimodal_chat_transformation(prompt, mm_content) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_content, request_id=request_id_prefix + str(ind), ) ) ind += 1 self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests CustomDataset ¶ Bases: BenchmarkDataset Implements the Custom dataset. Loads data from a JSONL file and generates sample requests based on conversation turns. E.g., {\"prompt\": \"What is the capital of India?\"} {\"prompt\": \"What is the capital of Iran?\"} {\"prompt\": \"What is the capital of China?\"} Source code in vllm/benchmarks/datasets.py 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 1937 1938 1939 1940 1941 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025 2026class CustomDataset(BenchmarkDataset): \"\"\" Implements the Custom dataset. Loads data from a JSONL file and generates sample requests based on conversation turns. E.g., ``` {\"prompt\": \"What is the capital of India?\"} {\"prompt\": \"What is the capital of Iran?\"} {\"prompt\": \"What is the capital of China?\"} ``` \"\"\" def __init__(self, **kwargs) -> None: super().__init__(**kwargs) self.load_data() def load_data(self) -> None: if self.dataset_path is None: raise ValueError(\"dataset_path must be provided for loading data.\") # self.data will be a list of dictionaries # e.g., [{\"prompt\": \"What is the capital of India?\"}, ...] # This will be the standardized format which load_data() # has to convert into depending on the filetype of dataset_path. # sample() will assume this standardized format of self.data self.data = [] # Load the JSONL file if self.dataset_path.endswith(\".jsonl\"): jsonl_data = pd.read_json(path_or_buf=self.dataset_path, lines=True) # check if the JSONL file has a 'prompt' column if \"prompt\" not in jsonl_data.columns: raise ValueError(\"JSONL file must contain a 'prompt' column.\") # Convert each row to a dictionary and append to self.data # This will convert the DataFrame to a list of dictionaries # where each dictionary corresponds to a row in the DataFrame. # This is the standardized format we want for self.data for _, row in jsonl_data.iterrows(): self.data.append(row.to_dict()) else: raise NotImplementedError( \"Only JSONL format is supported for CustomDataset.\" ) random.seed(self.random_seed) if not getattr(self, \"disable_shuffle\", False): random.shuffle(self.data) def sample( self, tokenizer: TokenizerLike, num_requests: int, lora_path: str | None = None, max_loras: int | None = None, output_len: int | None = None, enable_multimodal_chat: bool = False, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: # load all data if needed self.num_available_samples = len(self.data) if num_requests <= 0: num_requests = self.num_available_samples logger.info( \"num_requests is set to 0 or negative, \" \"so using all available samples: %d\", num_requests, ) sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break prompt = item[\"prompt\"] # apply template if not skip_chat_template: prompt = tokenizer.apply_chat_template( [{\"role\": \"user\", \"content\": prompt}], add_generation_prompt=True, tokenize=False, ) prompt_len = len(tokenizer(prompt).input_ids) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests __init__ ¶ __init__(**kwargs) -> None Source code in vllm/benchmarks/datasets.py 1938 1939 1940def __init__(self, **kwargs) -> None: super().__init__(**kwargs) self.load_data() load_data ¶ load_data() -> None Source code in vllm/benchmarks/datasets.py 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974def load_data(self) -> None: if self.dataset_path is None: raise ValueError(\"dataset_path must be provided for loading data.\") # self.data will be a list of dictionaries # e.g., [{\"prompt\": \"What is the capital of India?\"}, ...] # This will be the standardized format which load_data() # has to convert into depending on the filetype of dataset_path. # sample() will assume this standardized format of self.data self.data = [] # Load the JSONL file if self.dataset_path.endswith(\".jsonl\"): jsonl_data = pd.read_json(path_or_buf=self.dataset_path, lines=True) # check if the JSONL file has a 'prompt' column if \"prompt\" not in jsonl_data.columns: raise ValueError(\"JSONL file must contain a 'prompt' column.\") # Convert each row to a dictionary and append to self.data # This will convert the DataFrame to a list of dictionaries # where each dictionary corresponds to a row in the DataFrame. # This is the standardized format we want for self.data for _, row in jsonl_data.iterrows(): self.data.append(row.to_dict()) else: raise NotImplementedError( \"Only JSONL format is supported for CustomDataset.\" ) random.seed(self.random_seed) if not getattr(self, \"disable_shuffle\", False): random.shuffle(self.data) sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, lora_path: str | None = None, max_loras: int | None = None, output_len: int | None = None, enable_multimodal_chat: bool = False, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 1976 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025 2026def sample( self, tokenizer: TokenizerLike, num_requests: int, lora_path: str | None = None, max_loras: int | None = None, output_len: int | None = None, enable_multimodal_chat: bool = False, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: # load all data if needed self.num_available_samples = len(self.data) if num_requests <= 0: num_requests = self.num_available_samples logger.info( \"num_requests is set to 0 or negative, \" \"so using all available samples: %d\", num_requests, ) sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break prompt = item[\"prompt\"] # apply template if not skip_chat_template: prompt = tokenizer.apply_chat_template( [{\"role\": \"user\", \"content\": prompt}], add_generation_prompt=True, tokenize=False, ) prompt_len = len(tokenizer(prompt).input_ids) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests HuggingFaceDataset ¶ Bases: BenchmarkDataset Base class for datasets hosted on HuggingFace. Source code in vllm/benchmarks/datasets.py 2245 2246 2247 2248 2249 2250 2251 2252 2253 2254 2255 2256 2257 2258 2259 2260 2261 2262 2263 2264 2265 2266 2267 2268 2269 2270 2271 2272 2273 2274 2275 2276class HuggingFaceDataset(BenchmarkDataset): \"\"\"Base class for datasets hosted on HuggingFace.\"\"\" SUPPORTED_DATASET_PATHS: set[str] | dict[str, Callable] = set() def __init__( self, dataset_path: str, dataset_split: str, no_stream: bool = False, dataset_subset: str | None = None, hf_name: str | None = None, **kwargs, ) -> None: super().__init__(dataset_path=dataset_path, **kwargs) self.dataset_split = dataset_split self.dataset_subset = dataset_subset self.load_stream = not no_stream self.hf_name = hf_name or dataset_path self.load_data() def load_data(self) -> None: \"\"\"Load data from HuggingFace datasets.\"\"\" self.data = load_dataset( self.dataset_path, name=self.dataset_subset, split=self.dataset_split, streaming=self.load_stream, ) if not getattr(self, \"disable_shuffle\", False): self.data = self.data.shuffle(seed=self.random_seed) SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS: set[str] | dict[str, Callable] = ( set() ) dataset_split instance-attribute ¶ dataset_split = dataset_split dataset_subset instance-attribute ¶ dataset_subset = dataset_subset hf_name instance-attribute ¶ hf_name = hf_name or dataset_path load_stream instance-attribute ¶ load_stream = not no_stream __init__ ¶ __init__( dataset_path: str, dataset_split: str, no_stream: bool = False, dataset_subset: str | None = None, hf_name: str | None = None, **kwargs, ) -> None Source code in vllm/benchmarks/datasets.py 2250 2251 2252 2253 2254 2255 2256 2257 2258 2259 2260 2261 2262 2263 2264 2265def __init__( self, dataset_path: str, dataset_split: str, no_stream: bool = False, dataset_subset: str | None = None, hf_name: str | None = None, **kwargs, ) -> None: super().__init__(dataset_path=dataset_path, **kwargs) self.dataset_split = dataset_split self.dataset_subset = dataset_subset self.load_stream = not no_stream self.hf_name = hf_name or dataset_path self.load_data() load_data ¶ load_data() -> None Load data from HuggingFace datasets. Source code in vllm/benchmarks/datasets.py 2267 2268 2269 2270 2271 2272 2273 2274 2275 2276def load_data(self) -> None: \"\"\"Load data from HuggingFace datasets.\"\"\" self.data = load_dataset( self.dataset_path, name=self.dataset_subset, split=self.dataset_split, streaming=self.load_stream, ) if not getattr(self, \"disable_shuffle\", False): self.data = self.data.shuffle(seed=self.random_seed) InstructCoderDataset ¶ Bases: HuggingFaceDataset InstructCoder Dataset. https://huggingface.co/datasets/likaixin/InstructCoder InstructCoder is the dataset designed for general code editing. It consists of 114,239 instruction-input-output triplets, and covers multiple distinct code editing scenario. Source code in vllm/benchmarks/datasets.py 2521 2522 2523 2524 2525 2526 2527 2528 2529 2530 2531 2532 2533 2534 2535 2536 2537 2538 2539 2540 2541 2542 2543 2544 2545 2546 2547 2548 2549 2550 2551 2552 2553 2554 2555 2556 2557 2558 2559 2560 2561 2562 2563 2564 2565 2566 2567 2568 2569 2570 2571 2572 2573 2574 2575 2576 2577class InstructCoderDataset(HuggingFaceDataset): \"\"\" InstructCoder Dataset. https://huggingface.co/datasets/likaixin/InstructCoder InstructCoder is the dataset designed for general code editing. It consists of 114,239 instruction-input-output triplets, and covers multiple distinct code editing scenario. \"\"\" DEFAULT_OUTPUT_LEN = 200 # this is the average default output length SUPPORTED_DATASET_PATHS = { \"likaixin/InstructCoder\", } def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break prompt = ( f\"{item['input']}\\n\\n{item['instruction']} Just output \" \"the code, do not include any explanation.\" ) # apply template if not skip_chat_template: prompt = tokenizer.apply_chat_template( [{\"role\": \"user\", \"content\": prompt}], add_generation_prompt=True, tokenize=False, ) prompt_len = len(tokenizer(prompt).input_ids) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶ DEFAULT_OUTPUT_LEN = 200 SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = {'likaixin/InstructCoder'} sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 2536 2537 2538 2539 2540 2541 2542 2543 2544 2545 2546 2547 2548 2549 2550 2551 2552 2553 2554 2555 2556 2557 2558 2559 2560 2561 2562 2563 2564 2565 2566 2567 2568 2569 2570 2571 2572 2573 2574 2575 2576 2577def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break prompt = ( f\"{item['input']}\\n\\n{item['instruction']} Just output \" \"the code, do not include any explanation.\" ) # apply template if not skip_chat_template: prompt = tokenizer.apply_chat_template( [{\"role\": \"user\", \"content\": prompt}], add_generation_prompt=True, tokenize=False, ) prompt_len = len(tokenizer(prompt).input_ids) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests MLPerfDataset ¶ Bases: HuggingFaceDataset MLPerf Inference Dataset. Dataset on HF: https://huggingface.co/datasets/mgoin/mlperf-inference-llama2-data https://huggingface.co/datasets/mgoin/mlperf-inference-llama3.1-data Each record contains \"system_prompt\": system role instruction. \"question\": user question. \"output\": reference answer. We combine the system prompt and question into a chat-formatted prompt (using the tokenizer's chat template) and set the expected output length to the tokenized length of the provided reference answer. Source code in vllm/benchmarks/datasets.py 2984 2985 2986 2987 2988 2989 2990 2991 2992 2993 2994 2995 2996 2997 2998 2999 3000 3001 3002 3003 3004 3005 3006 3007 3008 3009 3010 3011 3012 3013 3014 3015 3016 3017 3018 3019 3020 3021 3022 3023 3024 3025 3026 3027 3028 3029 3030 3031 3032 3033 3034 3035 3036 3037 3038 3039 3040 3041 3042 3043 3044 3045 3046 3047 3048 3049 3050 3051 3052 3053 3054 3055 3056 3057 3058 3059 3060 3061 3062class MLPerfDataset(HuggingFaceDataset): \"\"\" MLPerf Inference Dataset. Dataset on HF: https://huggingface.co/datasets/mgoin/mlperf-inference-llama2-data https://huggingface.co/datasets/mgoin/mlperf-inference-llama3.1-data Each record contains: - \"system_prompt\": system role instruction. - \"question\": user question. - \"output\": reference answer. We combine the system prompt and question into a chat-formatted prompt (using the tokenizer's chat template) and set the expected output length to the tokenized length of the provided reference answer. \"\"\" SUPPORTED_DATASET_PATHS = { \"mgoin/mlperf-inference-llama2-data\", \"mgoin/mlperf-inference-llama3.1-data\", } def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest]: # Force dynamic output length based on reference completion. dynamic_output = output_len is None sampled_requests: list[SampleRequest] = [] ind = 0 for item in self.data: if len(sampled_requests) >= num_requests: break system_prompt = item[\"system_prompt\"] question = item[\"question\"] reference_answer = item[\"output\"] # Build chat-style prompt using tokenizer template, if available. messages = [ {\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": question}, ] prompt_formatted = tokenizer.apply_chat_template( messages, add_generation_prompt=True, tokenize=False ) prompt_len = len(tokenizer(prompt_formatted).input_ids) # Determine output length from reference answer tokens. ref_out_len = len( tokenizer(reference_answer, add_special_tokens=False).input_ids ) expected_output_len = ref_out_len if dynamic_output else output_len # Validate sequence lengths. if not is_valid_sequence(prompt_len, expected_output_len): continue sampled_requests.append( SampleRequest( prompt=prompt_formatted, prompt_len=prompt_len, expected_output_len=expected_output_len, request_id=request_id_prefix + str(ind), ) ) ind += 1 self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = { \"mgoin/mlperf-inference-llama2-data\", \"mgoin/mlperf-inference-llama3.1-data\", } sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest] Source code in vllm/benchmarks/datasets.py 3007 3008 3009 3010 3011 3012 3013 3014 3015 3016 3017 3018 3019 3020 3021 3022 3023 3024 3025 3026 3027 3028 3029 3030 3031 3032 3033 3034 3035 3036 3037 3038 3039 3040 3041 3042 3043 3044 3045 3046 3047 3048 3049 3050 3051 3052 3053 3054 3055 3056 3057 3058 3059 3060 3061 3062def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest]: # Force dynamic output length based on reference completion. dynamic_output = output_len is None sampled_requests: list[SampleRequest] = [] ind = 0 for item in self.data: if len(sampled_requests) >= num_requests: break system_prompt = item[\"system_prompt\"] question = item[\"question\"] reference_answer = item[\"output\"] # Build chat-style prompt using tokenizer template, if available. messages = [ {\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": question}, ] prompt_formatted = tokenizer.apply_chat_template( messages, add_generation_prompt=True, tokenize=False ) prompt_len = len(tokenizer(prompt_formatted).input_ids) # Determine output length from reference answer tokens. ref_out_len = len( tokenizer(reference_answer, add_special_tokens=False).input_ids ) expected_output_len = ref_out_len if dynamic_output else output_len # Validate sequence lengths. if not is_valid_sequence(prompt_len, expected_output_len): continue sampled_requests.append( SampleRequest( prompt=prompt_formatted, prompt_len=prompt_len, expected_output_len=expected_output_len, request_id=request_id_prefix + str(ind), ) ) ind += 1 self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests MMStarDataset ¶ Bases: HuggingFaceDataset Lin-Chen/MMStar: https://huggingface.co/datasets/Lin-Chen/MMStar refer to: https://github.com/sgl-project/SpecForge/pull/106 Source code in vllm/benchmarks/datasets.py 3162 3163 3164 3165 3166 3167 3168 3169 3170 3171 3172 3173 3174 3175 3176 3177 3178 3179 3180 3181 3182 3183 3184 3185 3186 3187 3188 3189 3190 3191 3192 3193 3194 3195 3196 3197 3198 3199 3200 3201 3202 3203 3204 3205 3206 3207 3208 3209 3210 3211 3212 3213 3214 3215 3216 3217 3218 3219 3220 3221 3222 3223 3224 3225 3226 3227class MMStarDataset(HuggingFaceDataset): \"\"\" Lin-Chen/MMStar: https://huggingface.co/datasets/Lin-Chen/MMStar refer to: https://github.com/sgl-project/SpecForge/pull/106 \"\"\" DEFAULT_OUTPUT_LEN = 128 SUPPORTED_DATASET_PATHS = {\"Lin-Chen/MMStar\"} IS_MULTIMODAL = True def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest]: # If --hf-output-len is not set, use the default output length. output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests: list[SampleRequest] = [] for ind, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break # Split the question text from options # (keep only the part before \"Options:\"). full_q: str = item.get(\"question\", \"\") question_text = full_q.split(\"Options:\", 1)[0].strip() # Multimodal image content. mm_content = process_image(item[\"image\"]) # Compute prompt token length (note: this is plain text length # if enable_multimodal_chat is False). prompt_len = len(tokenizer(question_text).input_ids) if enable_multimodal_chat: # If multimodal content should be embedded in the chat message, # convert to [{\"role\":\"user\",\"content\":[...]}] prompt = self.apply_multimodal_chat_transformation( question_text, mm_content ) mm_for_request = None # Already embedded in chat content. else: # Default: prompt is plain text, # image is in mm_content for the bench to assemble. prompt = question_text mm_for_request = mm_content sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_for_request, request_id=request_id_prefix + str(ind), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶ DEFAULT_OUTPUT_LEN = 128 IS_MULTIMODAL class-attribute instance-attribute ¶ IS_MULTIMODAL = True SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = {'Lin-Chen/MMStar'} sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest] Source code in vllm/benchmarks/datasets.py 3172 3173 3174 3175 3176 3177 3178 3179 3180 3181 3182 3183 3184 3185 3186 3187 3188 3189 3190 3191 3192 3193 3194 3195 3196 3197 3198 3199 3200 3201 3202 3203 3204 3205 3206 3207 3208 3209 3210 3211 3212 3213 3214 3215 3216 3217 3218 3219 3220 3221 3222 3223 3224 3225 3226 3227def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest]: # If --hf-output-len is not set, use the default output length. output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests: list[SampleRequest] = [] for ind, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break # Split the question text from options # (keep only the part before \"Options:\"). full_q: str = item.get(\"question\", \"\") question_text = full_q.split(\"Options:\", 1)[0].strip() # Multimodal image content. mm_content = process_image(item[\"image\"]) # Compute prompt token length (note: this is plain text length # if enable_multimodal_chat is False). prompt_len = len(tokenizer(question_text).input_ids) if enable_multimodal_chat: # If multimodal content should be embedded in the chat message, # convert to [{\"role\":\"user\",\"content\":[...]}] prompt = self.apply_multimodal_chat_transformation( question_text, mm_content ) mm_for_request = None # Already embedded in chat content. else: # Default: prompt is plain text, # image is in mm_content for the bench to assemble. prompt = question_text mm_for_request = mm_content sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_for_request, request_id=request_id_prefix + str(ind), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests MMVUDataset ¶ Bases: HuggingFaceDataset MMVU Dataset. https://huggingface.co/datasets/yale-nlp/MMVU Source code in vllm/benchmarks/datasets.py 2462 2463 2464 2465 2466 2467 2468 2469 2470 2471 2472 2473 2474 2475 2476 2477 2478 2479 2480 2481 2482 2483 2484 2485 2486 2487 2488 2489 2490 2491 2492 2493 2494 2495 2496 2497 2498 2499 2500 2501 2502 2503 2504 2505 2506 2507 2508 2509 2510 2511 2512 2513class MMVUDataset(HuggingFaceDataset): \"\"\" MMVU Dataset. https://huggingface.co/datasets/yale-nlp/MMVU \"\"\" DEFAULT_OUTPUT_LEN = 128 SUPPORTED_DATASET_PATHS = { \"yale-nlp/MMVU\": lambda x: x[\"question\"] + \" \" + (\" \".join(f\"{k}.{v}\" for k, v in x[\"choices\"].items())), } def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break parser_fn = self.SUPPORTED_DATASET_PATHS.get(self.hf_name) if parser_fn is None: raise ValueError(f\"Unsupported dataset path: {self.hf_name}\") prompt = parser_fn(item) mm_content = process_video(item[\"video\"]) prompt_len = len(tokenizer(prompt).input_ids) if enable_multimodal_chat: # Note: when chat is enabled the request prompt_len is no longer # accurate and we will be using request output to count the # actual prompt len prompt = self.apply_multimodal_chat_transformation(prompt, mm_content) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_content, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶ DEFAULT_OUTPUT_LEN = 128 SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = { \"yale-nlp/MMVU\": lambda x: x[\"question\"] + \" \" + join(f\"{k}.{v}\" for k, v in (items())) } sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 2475 2476 2477 2478 2479 2480 2481 2482 2483 2484 2485 2486 2487 2488 2489 2490 2491 2492 2493 2494 2495 2496 2497 2498 2499 2500 2501 2502 2503 2504 2505 2506 2507 2508 2509 2510 2511 2512 2513def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break parser_fn = self.SUPPORTED_DATASET_PATHS.get(self.hf_name) if parser_fn is None: raise ValueError(f\"Unsupported dataset path: {self.hf_name}\") prompt = parser_fn(item) mm_content = process_video(item[\"video\"]) prompt_len = len(tokenizer(prompt).input_ids) if enable_multimodal_chat: # Note: when chat is enabled the request prompt_len is no longer # accurate and we will be using request output to count the # actual prompt len prompt = self.apply_multimodal_chat_transformation(prompt, mm_content) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_content, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests MTBenchDataset ¶ Bases: HuggingFaceDataset MT-Bench Dataset. https://huggingface.co/datasets/philschmid/mt-bench We create a single turn dataset for MT-Bench. This is similar to Spec decoding benchmark setup in vLLM https://github.com/vllm-project/vllm/blob/9d98ab5ec/examples/offline_inference/eagle.py#L14-L18 Source code in vllm/benchmarks/datasets.py 2585 2586 2587 2588 2589 2590 2591 2592 2593 2594 2595 2596 2597 2598 2599 2600 2601 2602 2603 2604 2605 2606 2607 2608 2609 2610 2611 2612 2613 2614 2615 2616 2617 2618 2619 2620 2621 2622 2623 2624 2625 2626 2627 2628 2629 2630 2631 2632 2633 2634 2635 2636 2637 2638 2639class MTBenchDataset(HuggingFaceDataset): \"\"\" MT-Bench Dataset. https://huggingface.co/datasets/philschmid/mt-bench We create a single turn dataset for MT-Bench. This is similar to Spec decoding benchmark setup in vLLM https://github.com/vllm-project/vllm/blob/9d98ab5ec/examples/offline_inference/eagle.py#L14-L18 \"\"\" # noqa: E501 DEFAULT_OUTPUT_LEN = 256 # avg len used in SD bench in vLLM SUPPORTED_DATASET_PATHS = { \"philschmid/mt-bench\", } def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break prompt = item[\"turns\"][0] # apply template if not skip_chat_template: prompt = tokenizer.apply_chat_template( [{\"role\": \"user\", \"content\": prompt}], add_generation_prompt=True, tokenize=False, ) prompt_len = len(tokenizer(prompt).input_ids) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶ DEFAULT_OUTPUT_LEN = 256 SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = {'philschmid/mt-bench'} sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 2600 2601 2602 2603 2604 2605 2606 2607 2608 2609 2610 2611 2612 2613 2614 2615 2616 2617 2618 2619 2620 2621 2622 2623 2624 2625 2626 2627 2628 2629 2630 2631 2632 2633 2634 2635 2636 2637 2638 2639def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break prompt = item[\"turns\"][0] # apply template if not skip_chat_template: prompt = tokenizer.apply_chat_template( [{\"role\": \"user\", \"content\": prompt}], add_generation_prompt=True, tokenize=False, ) prompt_len = len(tokenizer(prompt).input_ids) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests MultiModalConversationDataset ¶ Bases: HuggingFaceDataset Dataset for multimodal conversation data. Source code in vllm/benchmarks/datasets.py 2344 2345 2346 2347 2348 2349 2350 2351 2352 2353 2354 2355 2356 2357 2358 2359 2360 2361 2362 2363 2364 2365 2366 2367 2368 2369 2370 2371 2372 2373 2374 2375 2376 2377 2378 2379 2380 2381 2382 2383 2384 2385 2386 2387 2388 2389 2390 2391 2392 2393 2394 2395 2396 2397 2398 2399 2400 2401class MultiModalConversationDataset(HuggingFaceDataset): \"\"\"Dataset for multimodal conversation data.\"\"\" SUPPORTED_DATASET_PATHS = { \"lmms-lab/LLaVA-OneVision-Data\", } IS_MULTIMODAL = True def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: # Filter examples with at least 2 conversations filtered_data = self.data.filter(lambda x: len(x[\"conversations\"]) >= 2) sampled_requests = [] ind = 0 dynamic_output = output_len is None for item in filtered_data: if len(sampled_requests) >= num_requests: break conv = item[\"conversations\"] prompt, completion = conv[0][\"value\"], conv[1][\"value\"] prompt_ids = tokenizer(prompt).input_ids completion_ids = tokenizer(completion).input_ids prompt_len = len(prompt_ids) completion_len = len(completion_ids) output_len = completion_len if dynamic_output else output_len assert isinstance(output_len, int) and output_len > 0 if dynamic_output and not is_valid_sequence(prompt_len, completion_len): continue mm_content = process_image(item[\"image\"]) if \"image\" in item else None if enable_multimodal_chat: # Note: when chat is enabled the request prompt_len is no longer # accurate and we will be using request output to count the # actual prompt len and output len prompt = self.apply_multimodal_chat_transformation(prompt, mm_content) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_content, request_id=request_id_prefix + str(ind), ) ) ind += 1 self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests IS_MULTIMODAL class-attribute instance-attribute ¶ IS_MULTIMODAL = True SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = {'lmms-lab/LLaVA-OneVision-Data'} sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 2352 2353 2354 2355 2356 2357 2358 2359 2360 2361 2362 2363 2364 2365 2366 2367 2368 2369 2370 2371 2372 2373 2374 2375 2376 2377 2378 2379 2380 2381 2382 2383 2384 2385 2386 2387 2388 2389 2390 2391 2392 2393 2394 2395 2396 2397 2398 2399 2400 2401def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: # Filter examples with at least 2 conversations filtered_data = self.data.filter(lambda x: len(x[\"conversations\"]) >= 2) sampled_requests = [] ind = 0 dynamic_output = output_len is None for item in filtered_data: if len(sampled_requests) >= num_requests: break conv = item[\"conversations\"] prompt, completion = conv[0][\"value\"], conv[1][\"value\"] prompt_ids = tokenizer(prompt).input_ids completion_ids = tokenizer(completion).input_ids prompt_len = len(prompt_ids) completion_len = len(completion_ids) output_len = completion_len if dynamic_output else output_len assert isinstance(output_len, int) and output_len > 0 if dynamic_output and not is_valid_sequence(prompt_len, completion_len): continue mm_content = process_image(item[\"image\"]) if \"image\" in item else None if enable_multimodal_chat: # Note: when chat is enabled the request prompt_len is no longer # accurate and we will be using request output to count the # actual prompt len and output len prompt = self.apply_multimodal_chat_transformation(prompt, mm_content) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_content, request_id=request_id_prefix + str(ind), ) ) ind += 1 self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests NextEditPredictionDataset ¶ Bases: HuggingFaceDataset Dataset class for processing a Next Edit Prediction dataset. Source code in vllm/benchmarks/datasets.py 2845 2846 2847 2848 2849 2850 2851 2852 2853 2854 2855 2856 2857 2858 2859 2860 2861 2862 2863 2864 2865 2866 2867 2868 2869 2870 2871 2872 2873 2874 2875 2876 2877 2878 2879 2880 2881 2882 2883 2884 2885 2886class NextEditPredictionDataset(HuggingFaceDataset): \"\"\" Dataset class for processing a Next Edit Prediction dataset. \"\"\" SUPPORTED_DATASET_PATHS = { \"zed-industries/zeta\", } MAPPING_PROMPT_FUNCS = { \"zed-industries/zeta\": _format_zeta_prompt, } def sample( self, tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ): formatting_prompt_func = self.MAPPING_PROMPT_FUNCS.get(self.hf_name) if formatting_prompt_func is None: raise ValueError(f\"Unsupported dataset path: {self.hf_name}\") samples = [] for i, sample in enumerate(self.data): sample = formatting_prompt_func(sample) samples.append( SampleRequest( prompt=sample[\"prompt\"], prompt_len=len(tokenizer(sample[\"prompt\"]).input_ids), expected_output_len=len( tokenizer(sample[\"expected_output\"]).input_ids ), request_id=request_id_prefix + str(i), ) ) if len(samples) >= num_requests: break self.maybe_oversample_requests( samples, num_requests, request_id_prefix, no_oversample ) return samples MAPPING_PROMPT_FUNCS class-attribute instance-attribute ¶ MAPPING_PROMPT_FUNCS = { \"zed-industries/zeta\": _format_zeta_prompt } SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = {'zed-industries/zeta'} sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) Source code in vllm/benchmarks/datasets.py 2857 2858 2859 2860 2861 2862 2863 2864 2865 2866 2867 2868 2869 2870 2871 2872 2873 2874 2875 2876 2877 2878 2879 2880 2881 2882 2883 2884 2885 2886def sample( self, tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ): formatting_prompt_func = self.MAPPING_PROMPT_FUNCS.get(self.hf_name) if formatting_prompt_func is None: raise ValueError(f\"Unsupported dataset path: {self.hf_name}\") samples = [] for i, sample in enumerate(self.data): sample = formatting_prompt_func(sample) samples.append( SampleRequest( prompt=sample[\"prompt\"], prompt_len=len(tokenizer(sample[\"prompt\"]).input_ids), expected_output_len=len( tokenizer(sample[\"expected_output\"]).input_ids ), request_id=request_id_prefix + str(i), ) ) if len(samples) >= num_requests: break self.maybe_oversample_requests( samples, num_requests, request_id_prefix, no_oversample ) return samples PrefixRepetitionRandomDataset ¶ Bases: BenchmarkDataset Source code in vllm/benchmarks/datasets.py 3070 3071 3072 3073 3074 3075 3076 3077 3078 3079 3080 3081 3082 3083 3084 3085 3086 3087 3088 3089 3090 3091 3092 3093 3094 3095 3096 3097 3098 3099 3100 3101 3102 3103 3104 3105 3106 3107 3108 3109 3110 3111 3112 3113 3114 3115 3116 3117 3118 3119 3120 3121 3122 3123 3124 3125 3126 3127 3128 3129 3130 3131 3132 3133 3134 3135 3136 3137 3138 3139 3140 3141 3142 3143 3144 3145 3146 3147 3148 3149 3150 3151 3152 3153 3154class PrefixRepetitionRandomDataset(BenchmarkDataset): # Default values copied from benchmark_serving.py for the repeated prefix # dataset. DEFAULT_PREFIX_LEN = 256 DEFAULT_SUFFIX_LEN = 256 DEFAULT_NUM_PREFIXES = 10 DEFAULT_OUTPUT_LEN = 128 def __init__( self, **kwargs, ) -> None: super().__init__(**kwargs) random.seed(self.random_seed) np.random.seed(self.random_seed) def sample( self, tokenizer: TokenizerLike, num_requests: int, prefix_len: int = DEFAULT_PREFIX_LEN, suffix_len: int = DEFAULT_SUFFIX_LEN, num_prefixes: int = DEFAULT_NUM_PREFIXES, output_len: int = DEFAULT_OUTPUT_LEN, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest]: vocab_size = tokenizer.vocab_size prompts_per_prefix = num_requests // num_prefixes if prompts_per_prefix == 0: raise ValueError( f\"num_requests ({num_requests}) must be greater than or equal \" f\"to num_prefixes ({num_prefixes})\" ) def _generate_exact_length_tokens(target_length: int) -> list[int]: \"\"\"Generate tokens that decode and re-encode to exactly target_length.\"\"\" # Generate random tokens tokens = np.random.randint(0, vocab_size, size=target_length).tolist() _, adjusted_tokens, token_mismatch = gen_prompt_decode_to_target_len( # noqa: E501 tokenizer=tokenizer, token_sequence=tokens, target_token_len=target_length, add_special_tokens=False, ) return adjusted_tokens, token_mismatch requests = [] token_mismatch_total = 0 for _ in range(num_prefixes): prefix_tokens, prefix_mismatch = _generate_exact_length_tokens(prefix_len) token_mismatch_total += prefix_mismatch for _ in range(prompts_per_prefix): suffix_tokens, suffix_mismatch = _generate_exact_length_tokens( suffix_len ) token_mismatch_total += suffix_mismatch combined_tokens = prefix_tokens + suffix_tokens prompt = tokenizer.decode(combined_tokens) prompt_len = len(combined_tokens) requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, ) ) if token_mismatch_total != 0: sign = \"more\" if token_mismatch_total > 0 else \"fewer\" logger.warning( \"Across all generated prompts, there were %d %s tokens \" \"than expected after decoding and re-encoding. This is \" \"expected due to the imperfect nature of the sampling \" \"procedure.\", abs(token_mismatch_total), sign, ) if not getattr(self, \"disable_shuffle\", False): random.shuffle(requests) return requests DEFAULT_NUM_PREFIXES class-attribute instance-attribute ¶ DEFAULT_NUM_PREFIXES = 10 DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶ DEFAULT_OUTPUT_LEN = 128 DEFAULT_PREFIX_LEN class-attribute instance-attribute ¶ DEFAULT_PREFIX_LEN = 256 DEFAULT_SUFFIX_LEN class-attribute instance-attribute ¶ DEFAULT_SUFFIX_LEN = 256 __init__ ¶ __init__(**kwargs) -> None Source code in vllm/benchmarks/datasets.py 3078 3079 3080 3081 3082 3083 3084def __init__( self, **kwargs, ) -> None: super().__init__(**kwargs) random.seed(self.random_seed) np.random.seed(self.random_seed) sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, prefix_len: int = DEFAULT_PREFIX_LEN, suffix_len: int = DEFAULT_SUFFIX_LEN, num_prefixes: int = DEFAULT_NUM_PREFIXES, output_len: int = DEFAULT_OUTPUT_LEN, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest] Source code in vllm/benchmarks/datasets.py 3086 3087 3088 3089 3090 3091 3092 3093 3094 3095 3096 3097 3098 3099 3100 3101 3102 3103 3104 3105 3106 3107 3108 3109 3110 3111 3112 3113 3114 3115 3116 3117 3118 3119 3120 3121 3122 3123 3124 3125 3126 3127 3128 3129 3130 3131 3132 3133 3134 3135 3136 3137 3138 3139 3140 3141 3142 3143 3144 3145 3146 3147 3148 3149 3150 3151 3152 3153 3154def sample( self, tokenizer: TokenizerLike, num_requests: int, prefix_len: int = DEFAULT_PREFIX_LEN, suffix_len: int = DEFAULT_SUFFIX_LEN, num_prefixes: int = DEFAULT_NUM_PREFIXES, output_len: int = DEFAULT_OUTPUT_LEN, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest]: vocab_size = tokenizer.vocab_size prompts_per_prefix = num_requests // num_prefixes if prompts_per_prefix == 0: raise ValueError( f\"num_requests ({num_requests}) must be greater than or equal \" f\"to num_prefixes ({num_prefixes})\" ) def _generate_exact_length_tokens(target_length: int) -> list[int]: \"\"\"Generate tokens that decode and re-encode to exactly target_length.\"\"\" # Generate random tokens tokens = np.random.randint(0, vocab_size, size=target_length).tolist() _, adjusted_tokens, token_mismatch = gen_prompt_decode_to_target_len( # noqa: E501 tokenizer=tokenizer, token_sequence=tokens, target_token_len=target_length, add_special_tokens=False, ) return adjusted_tokens, token_mismatch requests = [] token_mismatch_total = 0 for _ in range(num_prefixes): prefix_tokens, prefix_mismatch = _generate_exact_length_tokens(prefix_len) token_mismatch_total += prefix_mismatch for _ in range(prompts_per_prefix): suffix_tokens, suffix_mismatch = _generate_exact_length_tokens( suffix_len ) token_mismatch_total += suffix_mismatch combined_tokens = prefix_tokens + suffix_tokens prompt = tokenizer.decode(combined_tokens) prompt_len = len(combined_tokens) requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, ) ) if token_mismatch_total != 0: sign = \"more\" if token_mismatch_total > 0 else \"fewer\" logger.warning( \"Across all generated prompts, there were %d %s tokens \" \"than expected after decoding and re-encoding. This is \" \"expected due to the imperfect nature of the sampling \" \"procedure.\", abs(token_mismatch_total), sign, ) if not getattr(self, \"disable_shuffle\", False): random.shuffle(requests) return requests RandomDataset ¶ Bases: BenchmarkDataset Synthetic text-only dataset for serving/throughput benchmarks. Strategy: - Sample input/output token lengths per request from integer-uniform ranges around configured means (controlled by range_ratio). - Prepend a fixed random prefix of length prefix_len. - Generate the remaining tokens as a reproducible sequence: (offset + index + arange(input_len)) % vocab_size. - Decode then re-encode/truncate to ensure prompt token counts match. - Uses numpy.default_rng seeded with random_seed for reproducible sampling. Source code in vllm/benchmarks/datasets.py 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668class RandomDataset(BenchmarkDataset): \"\"\" Synthetic text-only dataset for serving/throughput benchmarks. Strategy: - Sample input/output token lengths per request from integer-uniform ranges around configured means (controlled by range_ratio). - Prepend a fixed random prefix of length prefix_len. - Generate the remaining tokens as a reproducible sequence: (offset + index + arange(input_len)) % vocab_size. - Decode then re-encode/truncate to ensure prompt token counts match. - Uses numpy.default_rng seeded with random_seed for reproducible sampling. \"\"\" # Default values copied from benchmark_serving.py for the random dataset. DEFAULT_PREFIX_LEN = 0 DEFAULT_RANGE_RATIO = 0.0 DEFAULT_INPUT_LEN = 1024 DEFAULT_OUTPUT_LEN = 128 def __init__(self, **kwargs) -> None: super().__init__(**kwargs) # Use numpy's default_rng for deterministic sampling # Do not use random.seed() or np.random.seed() elsewhere in this class. # This ensures that the RNG is isolated from global RNG state. self._rng = np.random.default_rng(self.random_seed) def sample( self, tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, prefix_len: int = DEFAULT_PREFIX_LEN, range_ratio: float = DEFAULT_RANGE_RATIO, input_len: int = DEFAULT_INPUT_LEN, output_len: int = DEFAULT_OUTPUT_LEN, batchsize: int = 1, **kwargs, ) -> list[SampleRequest]: # validate total input tokens (prefix + sampled) is at least 1. num_special = int(tokenizer.num_special_tokens_to_add()) real_input_len = max(0, int(input_len) - num_special) min_sampled_input = math.floor(real_input_len * (1.0 - float(range_ratio))) min_total_input = int(prefix_len) + min_sampled_input if min_total_input < 1: raise ValueError( \"--random-input-len is too small: with tokenizer special \" f\"tokens {num_special} and --random-range-ratio {range_ratio}, \" \"the minimum possible total input tokens (prefix + sampled) is \" f\"{min_total_input}. Increase --random-input-len and/or \" \"--random-prefix-len, or decrease --random-range-ratio so that \" \"prefix_len + floor(max(0, random_input_len - num_special)) \" \"* (1 - range_ratio) >= 1.\" ) input_lens, output_lens, offsets = self.get_sampling_params( num_requests, range_ratio, input_len, output_len, tokenizer ) vocab_size = tokenizer.vocab_size prohibited_tokens = tokenizer.all_special_ids all_tokens = np.arange(vocab_size) allowed_tokens = np.array(list(set(all_tokens) - set(prohibited_tokens))) # Generate prefix once prefix_token_ids = self.get_prefix(allowed_tokens, prefix_len) requests = [] token_mismatch_total = 0 for i in range(num_requests): prompt, total_input_len, token_mismatch = self.generate_token_sequence( # noqa: E501 tokenizer=tokenizer, prefix_token_ids=prefix_token_ids, prefix_len=prefix_len, vocab_size=vocab_size, input_len=int(input_lens[i]), offset=int(offsets[i]), index=i, allowed_tokens=allowed_tokens, ) token_mismatch_total += token_mismatch requests.append( SampleRequest( prompt=prompt, prompt_len=total_input_len, expected_output_len=int(output_lens[i]), request_id=request_id_prefix + str(i), ) ) # only used for embeddings benchmark. if batchsize > 1: batch_requests = [] # Create batched requests for i in range(0, num_requests, batchsize): batch = requests[i : i + batchsize] batch_requests.append( SampleRequest( prompt=[req.prompt for req in batch], prompt_len=sum(req.prompt_len for req in batch), expected_output_len=0, request_id=request_id_prefix + str(i // batchsize), ) ) requests = batch_requests if token_mismatch_total != 0: sign = \"more\" if token_mismatch_total > 0 else \"fewer\" logger.warning( \"Across all generated prompts, there were %d %s tokens \" \"than expected after decoding and re-encoding. This is \" \"expected due to the imperfect nature of the sampling \" \"procedure.\", abs(token_mismatch_total), sign, ) return requests def get_prefix( self, allowed_tokens: np.ndarray, prefix_len: int, ) -> list[int]: \"\"\" Get the prefix for the dataset. \"\"\" return ( allowed_tokens[ self._rng.integers(0, len(allowed_tokens), size=prefix_len) ].tolist() if prefix_len > 0 else [] ) def get_sampling_params( self, num_requests: int, range_ratio: float, input_len: int, output_len: int, tokenizer: TokenizerLike, ) -> tuple[np.ndarray, np.ndarray, np.ndarray]: \"\"\" Get the sampling parameters for the dataset. \"\"\" # Enforce range_ratio < 1 if not (0.0 <= range_ratio < 1.0): raise ValueError(\"range_ratio must be in [0, 1).\") num_special_tokens = int(tokenizer.num_special_tokens_to_add()) real_input_len = max(0, int(input_len) - num_special_tokens) # Bounds use floor for low and ceil for high input_low = math.floor(real_input_len * (1 - range_ratio)) input_high = math.ceil(real_input_len * (1 + range_ratio)) output_low = math.floor(output_len * (1 - range_ratio)) output_high = math.ceil(output_len * (1 + range_ratio)) # Ensure the lower bound for output length is at least 1 to # prevent sampling 0 tokens. output_low = max(output_low, 1) output_high = max(output_high, 1) if input_low > input_high: raise ValueError( f\"Invalid input sampling interval: low={input_low} > high={input_high}\" ) if output_low > output_high: raise ValueError( \"Invalid output sampling interval: \" f\"low={output_low} > high={output_high}\" ) logger.info( \"Sampling input_len from [%s, %s] and output_len from [%s, %s]\", input_low, input_high, output_low, output_high, ) input_lens = self._rng.integers(input_low, input_high + 1, size=num_requests) output_lens = self._rng.integers(output_low, output_high + 1, size=num_requests) offsets = self._rng.integers(0, tokenizer.vocab_size, size=num_requests) return input_lens, output_lens, offsets def generate_token_sequence( self, *, tokenizer: TokenizerLike, prefix_token_ids: list[int], prefix_len: int, vocab_size: int, input_len: int, offset: int, index: int, allowed_tokens: np.ndarray, ) -> tuple[str, int, int]: \"\"\" Returns (prompt, total_input_len). NOTE: After decoding the prompt we have to encode and decode it again. This is done because in some cases N consecutive tokens give a string tokenized into != N number of tokens. For example for GPT2Tokenizer: [6880, 6881] -> ['Ġcalls', 'here'] -> [1650, 939, 486] -> ['Ġcall', 'sh', 'ere'] To avoid uncontrolled change of the prompt length, the encoded sequence is truncated before being decoded again. \"\"\" # Build the inner sequence by sampling # sequentially from the allowed tokens inner_seq = allowed_tokens[ (offset + index + np.arange(input_len)) % len(allowed_tokens) ].tolist() token_sequence = prefix_token_ids + inner_seq # Decode, then re-encode and truncate to preserve token count invariants total_input_len = prefix_len + int(input_len) prompt, adjusted_token_sequence, token_mismatch = ( gen_prompt_decode_to_target_len( tokenizer=tokenizer, token_sequence=token_sequence, target_token_len=total_input_len, add_special_tokens=False, rng=self._rng, ) ) total_input_len = len(adjusted_token_sequence) return prompt, total_input_len, token_mismatch DEFAULT_INPUT_LEN class-attribute instance-attribute ¶ DEFAULT_INPUT_LEN = 1024 DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶ DEFAULT_OUTPUT_LEN = 128 DEFAULT_PREFIX_LEN class-attribute instance-attribute ¶ DEFAULT_PREFIX_LEN = 0 DEFAULT_RANGE_RATIO class-attribute instance-attribute ¶ DEFAULT_RANGE_RATIO = 0.0 _rng instance-attribute ¶ _rng = default_rng(random_seed) __init__ ¶ __init__(**kwargs) -> None Source code in vllm/benchmarks/datasets.py 461 462 463 464 465 466def __init__(self, **kwargs) -> None: super().__init__(**kwargs) # Use numpy's default_rng for deterministic sampling # Do not use random.seed() or np.random.seed() elsewhere in this class. # This ensures that the RNG is isolated from global RNG state. self._rng = np.random.default_rng(self.random_seed) generate_token_sequence ¶ generate_token_sequence( *, tokenizer: TokenizerLike, prefix_token_ids: list[int], prefix_len: int, vocab_size: int, input_len: int, offset: int, index: int, allowed_tokens: ndarray, ) -> tuple[str, int, int] Returns (prompt, total_input_len). NOTE: After decoding the prompt we have to encode and decode it again. This is done because in some cases N consecutive tokens give a string tokenized into != N number of tokens. For example for GPT2Tokenizer: [6880, 6881] -> ['Ġcalls', 'here'] -> [1650, 939, 486] -> ['Ġcall', 'sh', 'ere'] To avoid uncontrolled change of the prompt length, the encoded sequence is truncated before being decoded again. Source code in vllm/benchmarks/datasets.py 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668def generate_token_sequence( self, *, tokenizer: TokenizerLike, prefix_token_ids: list[int], prefix_len: int, vocab_size: int, input_len: int, offset: int, index: int, allowed_tokens: np.ndarray, ) -> tuple[str, int, int]: \"\"\" Returns (prompt, total_input_len). NOTE: After decoding the prompt we have to encode and decode it again. This is done because in some cases N consecutive tokens give a string tokenized into != N number of tokens. For example for GPT2Tokenizer: [6880, 6881] -> ['Ġcalls', 'here'] -> [1650, 939, 486] -> ['Ġcall', 'sh', 'ere'] To avoid uncontrolled change of the prompt length, the encoded sequence is truncated before being decoded again. \"\"\" # Build the inner sequence by sampling # sequentially from the allowed tokens inner_seq = allowed_tokens[ (offset + index + np.arange(input_len)) % len(allowed_tokens) ].tolist() token_sequence = prefix_token_ids + inner_seq # Decode, then re-encode and truncate to preserve token count invariants total_input_len = prefix_len + int(input_len) prompt, adjusted_token_sequence, token_mismatch = ( gen_prompt_decode_to_target_len( tokenizer=tokenizer, token_sequence=token_sequence, target_token_len=total_input_len, add_special_tokens=False, rng=self._rng, ) ) total_input_len = len(adjusted_token_sequence) return prompt, total_input_len, token_mismatch get_prefix ¶ get_prefix( allowed_tokens: ndarray, prefix_len: int ) -> list[int] Get the prefix for the dataset. Source code in vllm/benchmarks/datasets.py 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574def get_prefix( self, allowed_tokens: np.ndarray, prefix_len: int, ) -> list[int]: \"\"\" Get the prefix for the dataset. \"\"\" return ( allowed_tokens[ self._rng.integers(0, len(allowed_tokens), size=prefix_len) ].tolist() if prefix_len > 0 else [] ) get_sampling_params ¶ get_sampling_params( num_requests: int, range_ratio: float, input_len: int, output_len: int, tokenizer: TokenizerLike, ) -> tuple[ndarray, ndarray, ndarray] Get the sampling parameters for the dataset. Source code in vllm/benchmarks/datasets.py 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623def get_sampling_params( self, num_requests: int, range_ratio: float, input_len: int, output_len: int, tokenizer: TokenizerLike, ) -> tuple[np.ndarray, np.ndarray, np.ndarray]: \"\"\" Get the sampling parameters for the dataset. \"\"\" # Enforce range_ratio < 1 if not (0.0 <= range_ratio < 1.0): raise ValueError(\"range_ratio must be in [0, 1).\") num_special_tokens = int(tokenizer.num_special_tokens_to_add()) real_input_len = max(0, int(input_len) - num_special_tokens) # Bounds use floor for low and ceil for high input_low = math.floor(real_input_len * (1 - range_ratio)) input_high = math.ceil(real_input_len * (1 + range_ratio)) output_low = math.floor(output_len * (1 - range_ratio)) output_high = math.ceil(output_len * (1 + range_ratio)) # Ensure the lower bound for output length is at least 1 to # prevent sampling 0 tokens. output_low = max(output_low, 1) output_high = max(output_high, 1) if input_low > input_high: raise ValueError( f\"Invalid input sampling interval: low={input_low} > high={input_high}\" ) if output_low > output_high: raise ValueError( \"Invalid output sampling interval: \" f\"low={output_low} > high={output_high}\" ) logger.info( \"Sampling input_len from [%s, %s] and output_len from [%s, %s]\", input_low, input_high, output_low, output_high, ) input_lens = self._rng.integers(input_low, input_high + 1, size=num_requests) output_lens = self._rng.integers(output_low, output_high + 1, size=num_requests) offsets = self._rng.integers(0, tokenizer.vocab_size, size=num_requests) return input_lens, output_lens, offsets sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, prefix_len: int = DEFAULT_PREFIX_LEN, range_ratio: float = DEFAULT_RANGE_RATIO, input_len: int = DEFAULT_INPUT_LEN, output_len: int = DEFAULT_OUTPUT_LEN, batchsize: int = 1, **kwargs, ) -> list[SampleRequest] Source code in vllm/benchmarks/datasets.py 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558def sample( self, tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, prefix_len: int = DEFAULT_PREFIX_LEN, range_ratio: float = DEFAULT_RANGE_RATIO, input_len: int = DEFAULT_INPUT_LEN, output_len: int = DEFAULT_OUTPUT_LEN, batchsize: int = 1, **kwargs, ) -> list[SampleRequest]: # validate total input tokens (prefix + sampled) is at least 1. num_special = int(tokenizer.num_special_tokens_to_add()) real_input_len = max(0, int(input_len) - num_special) min_sampled_input = math.floor(real_input_len * (1.0 - float(range_ratio))) min_total_input = int(prefix_len) + min_sampled_input if min_total_input < 1: raise ValueError( \"--random-input-len is too small: with tokenizer special \" f\"tokens {num_special} and --random-range-ratio {range_ratio}, \" \"the minimum possible total input tokens (prefix + sampled) is \" f\"{min_total_input}. Increase --random-input-len and/or \" \"--random-prefix-len, or decrease --random-range-ratio so that \" \"prefix_len + floor(max(0, random_input_len - num_special)) \" \"* (1 - range_ratio) >= 1.\" ) input_lens, output_lens, offsets = self.get_sampling_params( num_requests, range_ratio, input_len, output_len, tokenizer ) vocab_size = tokenizer.vocab_size prohibited_tokens = tokenizer.all_special_ids all_tokens = np.arange(vocab_size) allowed_tokens = np.array(list(set(all_tokens) - set(prohibited_tokens))) # Generate prefix once prefix_token_ids = self.get_prefix(allowed_tokens, prefix_len) requests = [] token_mismatch_total = 0 for i in range(num_requests): prompt, total_input_len, token_mismatch = self.generate_token_sequence( # noqa: E501 tokenizer=tokenizer, prefix_token_ids=prefix_token_ids, prefix_len=prefix_len, vocab_size=vocab_size, input_len=int(input_lens[i]), offset=int(offsets[i]), index=i, allowed_tokens=allowed_tokens, ) token_mismatch_total += token_mismatch requests.append( SampleRequest( prompt=prompt, prompt_len=total_input_len, expected_output_len=int(output_lens[i]), request_id=request_id_prefix + str(i), ) ) # only used for embeddings benchmark. if batchsize > 1: batch_requests = [] # Create batched requests for i in range(0, num_requests, batchsize): batch = requests[i : i + batchsize] batch_requests.append( SampleRequest( prompt=[req.prompt for req in batch], prompt_len=sum(req.prompt_len for req in batch), expected_output_len=0, request_id=request_id_prefix + str(i // batchsize), ) ) requests = batch_requests if token_mismatch_total != 0: sign = \"more\" if token_mismatch_total > 0 else \"fewer\" logger.warning( \"Across all generated prompts, there were %d %s tokens \" \"than expected after decoding and re-encoding. This is \" \"expected due to the imperfect nature of the sampling \" \"procedure.\", abs(token_mismatch_total), sign, ) return requests RandomDatasetForReranking ¶ Bases: RandomDataset Random dataset specialized for the needs of scoring: - Batches of inputs - Inputs composed of pairs Source code in vllm/benchmarks/datasets.py 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780class RandomDatasetForReranking(RandomDataset): \"\"\" Random dataset specialized for the needs of scoring: - Batches of inputs - Inputs composed of pairs \"\"\" def __init__(self, **kwargs) -> None: super().__init__(**kwargs) def sample( self, tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", range_ratio: float = RandomDataset.DEFAULT_RANGE_RATIO, input_len: int = RandomDataset.DEFAULT_INPUT_LEN, batchsize: int = 1, is_reranker: bool = True, **kwargs, ) -> list[SampleRequest]: n_sep_tokens = int(is_reranker) query_len_param = (input_len // 2) - n_sep_tokens if is_reranker else input_len query_lens, _, query_offsets = self.get_sampling_params( 1, range_ratio, query_len_param, 0, tokenizer ) query_len = int(query_lens[0]) if not is_reranker: assert num_requests > 1 and batchsize > 1 num_requests -= 1 batchsize -= 1 doc_len_param = input_len else: doc_len_param = input_len - query_len - n_sep_tokens doc_lens, _, doc_offsets = self.get_sampling_params( num_requests, range_ratio, doc_len_param, 0, tokenizer ) vocab_size = tokenizer.vocab_size prohibited_tokens = tokenizer.all_special_ids all_tokens = np.arange(vocab_size) allowed_tokens = np.array(list(set(all_tokens) - set(prohibited_tokens))) query_prompt, query_input_len, token_mismatch_total = ( self.generate_token_sequence( tokenizer=tokenizer, prefix_token_ids=[], prefix_len=0, vocab_size=vocab_size, input_len=query_len, offset=int(query_offsets[0]), index=0, allowed_tokens=allowed_tokens, ) ) requests = [] for i in range(num_requests): prompt, total_input_len, token_mismatch = self.generate_token_sequence( # noqa: E501 tokenizer=tokenizer, prefix_token_ids=[], prefix_len=0, vocab_size=vocab_size, input_len=int(doc_lens[i]), offset=int(doc_offsets[i]), index=i + 1, allowed_tokens=allowed_tokens, ) token_mismatch_total += token_mismatch requests.append((prompt, total_input_len)) batch_requests = [] # Create batched requests for i in range(0, num_requests, batchsize): batch = requests[i : i + batchsize] query_contrib = ( (query_input_len + n_sep_tokens) * len(batch) if is_reranker else query_input_len ) batch_requests.append( SampleRequest( prompt=[query_prompt] + [req[0] for req in batch], prompt_len=query_contrib + sum(req[1] for req in batch), expected_output_len=0, request_id=request_id_prefix + str(i // batchsize), ) ) if token_mismatch_total != 0: logger.warning( \"Across all generated prompts, there were %d %s tokens \" \"than expected after decoding and re-encoding. This is \" \"expected due to the imperfect nature of the sampling \" \"procedure.\", abs(token_mismatch_total), \"more\" if token_mismatch_total > 0 else \"fewer\", ) return batch_requests __init__ ¶ __init__(**kwargs) -> None Source code in vllm/benchmarks/datasets.py 683 684def __init__(self, **kwargs) -> None: super().__init__(**kwargs) sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", range_ratio: float = DEFAULT_RANGE_RATIO, input_len: int = DEFAULT_INPUT_LEN, batchsize: int = 1, is_reranker: bool = True, **kwargs, ) -> list[SampleRequest] Source code in vllm/benchmarks/datasets.py 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780def sample( self, tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", range_ratio: float = RandomDataset.DEFAULT_RANGE_RATIO, input_len: int = RandomDataset.DEFAULT_INPUT_LEN, batchsize: int = 1, is_reranker: bool = True, **kwargs, ) -> list[SampleRequest]: n_sep_tokens = int(is_reranker) query_len_param = (input_len // 2) - n_sep_tokens if is_reranker else input_len query_lens, _, query_offsets = self.get_sampling_params( 1, range_ratio, query_len_param, 0, tokenizer ) query_len = int(query_lens[0]) if not is_reranker: assert num_requests > 1 and batchsize > 1 num_requests -= 1 batchsize -= 1 doc_len_param = input_len else: doc_len_param = input_len - query_len - n_sep_tokens doc_lens, _, doc_offsets = self.get_sampling_params( num_requests, range_ratio, doc_len_param, 0, tokenizer ) vocab_size = tokenizer.vocab_size prohibited_tokens = tokenizer.all_special_ids all_tokens = np.arange(vocab_size) allowed_tokens = np.array(list(set(all_tokens) - set(prohibited_tokens))) query_prompt, query_input_len, token_mismatch_total = ( self.generate_token_sequence( tokenizer=tokenizer, prefix_token_ids=[], prefix_len=0, vocab_size=vocab_size, input_len=query_len, offset=int(query_offsets[0]), index=0, allowed_tokens=allowed_tokens, ) ) requests = [] for i in range(num_requests): prompt, total_input_len, token_mismatch = self.generate_token_sequence( # noqa: E501 tokenizer=tokenizer, prefix_token_ids=[], prefix_len=0, vocab_size=vocab_size, input_len=int(doc_lens[i]), offset=int(doc_offsets[i]), index=i + 1, allowed_tokens=allowed_tokens, ) token_mismatch_total += token_mismatch requests.append((prompt, total_input_len)) batch_requests = [] # Create batched requests for i in range(0, num_requests, batchsize): batch = requests[i : i + batchsize] query_contrib = ( (query_input_len + n_sep_tokens) * len(batch) if is_reranker else query_input_len ) batch_requests.append( SampleRequest( prompt=[query_prompt] + [req[0] for req in batch], prompt_len=query_contrib + sum(req[1] for req in batch), expected_output_len=0, request_id=request_id_prefix + str(i // batchsize), ) ) if token_mismatch_total != 0: logger.warning( \"Across all generated prompts, there were %d %s tokens \" \"than expected after decoding and re-encoding. This is \" \"expected due to the imperfect nature of the sampling \" \"procedure.\", abs(token_mismatch_total), \"more\" if token_mismatch_total > 0 else \"fewer\", ) return batch_requests RandomMultiModalDataset ¶ Bases: RandomDataset Synthetic multimodal dataset (text + images) that extends RandomDataset. Status: - Images: supported via synthetic RGB data. - Video: supported via synthetic RGB data. - Audio: not yet supported. Sampling overview: 1) Number of items per request is sampled uniformly from the integer range [floor(n·(1−r)), ceil(n·(1+r))], where n is the base count and r is num_mm_items_range_ratio in [0, 1]. r=0 keeps it fixed; r=1 allows 0. The maximum is further clamped to the sum of per-modality limits. 2) Each item’s modality and shape is sampled from bucket_config, a dict mapping (height, width, num_frames) → probability. We treat num_frames=1 as image and num_frames > 1 as video. Entries with zero probability are removed and the rest are renormalized to sum to 1. 3) Per-modality hard caps are enforced via limit_mm_per_prompt. When a modality reaches its cap, all of its buckets are excluded and the remaining probabilities are renormalized. Example bucket configuration: {(256, 256, 1): 0.5, (720, 1280, 1): 0.4, (720, 1280, 16): 0.1} - Two image buckets (num_frames=1) and one video bucket (num_frames=16). OBS.: Only image sampling is supported for now. Source code in vllm/benchmarks/datasets.py 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203class RandomMultiModalDataset(RandomDataset): \"\"\" Synthetic multimodal dataset (text + images) that extends RandomDataset. Status: - Images: supported via synthetic RGB data. - Video: supported via synthetic RGB data. - Audio: not yet supported. Sampling overview: 1) Number of items per request is sampled uniformly from the integer range [floor(n·(1−r)), ceil(n·(1+r))], where n is the base count and r is `num_mm_items_range_ratio` in [0, 1]. r=0 keeps it fixed; r=1 allows 0. The maximum is further clamped to the sum of per-modality limits. 2) Each item’s modality and shape is sampled from `bucket_config`, a dict mapping (height, width, num_frames) → probability. We treat `num_frames`=1 as image and `num_frames` > 1 as video. Entries with zero probability are removed and the rest are renormalized to sum to 1. 3) Per-modality hard caps are enforced via `limit_mm_per_prompt`. When a modality reaches its cap, all of its buckets are excluded and the remaining probabilities are renormalized. Example bucket configuration: {(256, 256, 1): 0.5, (720, 1280, 1): 0.4, (720, 1280, 16): 0.1} - Two image buckets (`num_frames`=1) and one video bucket (`num_frames`=16). OBS.: Only image sampling is supported for now. \"\"\" IS_MULTIMODAL = True DEFAULT_LIMIT_MM_PER_PROMPT = {\"image\": 255, \"video\": 1} DEFAULT_BASE_ITEMS_PER_REQUEST = 1 DEFAULT_NUM_MM_ITEMS_RANGE_RATIO = 0.0 DEFAULT_MM_ITEM_BUCKET_CONFIG = { (256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0, } DEFAULT_ENABLE_MULTIMODAL_CHAT = False def __init__(self, **kwargs) -> None: super().__init__(**kwargs) def generate_synthetic_image(self, width: int, height: int) -> Image.Image: \"\"\"Generate synthetic PIL image with random RGB values. NOTE: iid pixel sampling results in worst-case compression (good for stressing I/O), but very unlike real photos. We could consider a “low-freq” mode (e.g., noise blur) to emulate network realism instead of max stress. \"\"\" random_pixels = self._rng.integers( 0, 256, (height, width, 3), dtype=np.uint8, ) return Image.fromarray(random_pixels) def generate_synthetic_video( self, width: int, height: int, num_frames: int ) -> dict: \"\"\"Generate synthetic video with random values. Creates a video with random pixel values, encodes it to MP4 format, and returns the content as bytes. \"\"\" import cv2 random_pixels = self._rng.integers( 0, 256, (num_frames, height, width, 3), dtype=np.uint8, ) # Create a temporary video file in memory fourcc = cv2.VideoWriter_fourcc(*\"mp4v\") fps = 30 # frames per second with NamedTemporaryFile(suffix=\".mp4\", delete_on_close=False) as temp_file: temp_path = temp_file.name # Create video writer video_writer = cv2.VideoWriter( temp_path, fourcc=fourcc, fps=fps, frameSize=(width, height) ) if not video_writer.isOpened(): raise RuntimeError(\"Failed to create video writer\") for frame in random_pixels: video_writer.write(frame) video_writer.release() temp_file.close() # Read the video file content with open(temp_path, \"rb\") as f: video_content = f.read() return {\"bytes\": video_content} def map_config_to_modality(self, config: tuple[int, int, int]) -> str: \"\"\"Map the configuration to the modality.\"\"\" if config[-1] == 1: return \"image\" elif config[-1] > 1: return \"video\" else: raise ValueError(f\"Invalid multimodal item configuration: {config}\") def normalize_bucket_config( self, bucket_config: dict[tuple[int, int, int], float] ) -> dict[tuple[int, int, int], float]: \"\"\" Remove zero probability entries and normalize the bucket config to sum to 1. \"\"\" # Raise error if value is negative if any(v < 0 for v in bucket_config.values()): raise ValueError(\"Bucket config values must be non-negative.\") # Remove zero probability entries bucket_config = {k: v for k, v in bucket_config.items() if v > 0} # if bucket config is empty, raise error if not bucket_config: raise ValueError( \"Got invalid bucket config. Bucket config values must be non-zero.\" ) # Normalize the remaining bucket config to sum to 1 total = sum(bucket_config.values()) return {k: v / total for k, v in bucket_config.items()} def generate_mm_item( self, mm_item_config: tuple[int, int, int], ) -> Mapping[str, Any]: \"\"\" Create synthetic images and videos and apply process_image/process_video respectively. This follows the OpenAI API chat completions https://github.com/openai/openai-python \"\"\" if self.map_config_to_modality(mm_item_config) == \"image\": return process_image( self.generate_synthetic_image(mm_item_config[1], mm_item_config[0]) ) elif self.map_config_to_modality(mm_item_config) == \"video\": return process_video( self.generate_synthetic_video( mm_item_config[1], mm_item_config[0], mm_item_config[2] ) ) else: raise ValueError(f\"Invalid multimodal item configuration: {mm_item_config}\") def get_mm_item_sampling_params( self, base_items_per_request: int, num_mm_items_range_ratio: float, limit_mm_per_prompt: dict[str, int], bucket_config: dict[tuple[int, int, int], float], ) -> tuple[int, int, dict[str, int], dict[tuple[int, int, int], float]]: \"\"\" Get the sampling parameters for the multimodal items. \"\"\" # Enforce num_mm_items_range_ratio <= 1 if not (0.0 <= num_mm_items_range_ratio <= 1.0): raise ValueError(\"num_mm_items_range_ratio must be in [0, 1].\") # Ensure modalities to sample are in limit_mm_per_prompt for k, v in bucket_config.items(): # get modality from bucket config modality = self.map_config_to_modality(k) if modality not in limit_mm_per_prompt: raise ValueError( f\"Modality {modality} is not in \" f\"limit_mm_per_prompt: \" f\"{limit_mm_per_prompt.keys()}\" ) # Remove zero probability entries # and normalize bucket config to sum to 1 bucket_config = self.normalize_bucket_config(bucket_config) logger.info( \"Normalized bucket config: %s\", bucket_config, ) # Only consider limit per prompt for modalities in bucket config allowed_modalities = {self.map_config_to_modality(cfg) for cfg in bucket_config} limit_mm_per_prompt = { k: v for k, v in limit_mm_per_prompt.items() if k in allowed_modalities } if not limit_mm_per_prompt: raise ValueError(\"No valid limits for modalities present in bucket_config.\") logger.info( \"Updated mm-limit-per-prompt: %s\", limit_mm_per_prompt, ) # Get max and min num mm items and ensure # it is at most the sum of limit_mm_per_prompt for all modalities max_num_mm_items = min( sum(limit_mm_per_prompt.values()), math.ceil(base_items_per_request * (1 + num_mm_items_range_ratio)), ) # Ensure min num mm items is at least 0 min_num_mm_items = max( 0, math.floor(base_items_per_request * (1 - num_mm_items_range_ratio)) ) # Raise error if min num mm items is greater than max num mm items if min_num_mm_items > max_num_mm_items: raise ValueError( f\"Min num mm items is greater than max mm items: \" f\"{min_num_mm_items} > {max_num_mm_items}\" ) logger.info( \"Sampling number of multimodal items from [%s, %s]\", min_num_mm_items, max_num_mm_items, ) return ( min_num_mm_items, max_num_mm_items, limit_mm_per_prompt, bucket_config, ) def get_mm_item_iterator( self, min_num_mm_items: int, max_num_mm_items: int, bucket_config: dict[tuple[int, int, int], float], limit_mm_per_prompt: dict[str, int], ) -> Iterator[tuple[int, int, int]]: \"\"\" Iterator over the multimodal items for each request whose size is between min_num_mm_items and max_num_mm_items. Loop over the bucket config and sample a multimodal item. Loop until the number of multimodal items sampled is equal to request_num_mm_items or limit of multimodal items per prompt for all modalities is reached. Note: - This function operates on a per-request shallow copy of `bucket_config` (tuple->float). The original dict passed to `sample` is not mutated. If this ever changes, a test is implemented and will fail. \"\"\" # Get the number of multimodal items to sample request_num_mm_items = int( self._rng.integers(min_num_mm_items, max_num_mm_items + 1) ) # If request_num_mm_items is 0, yield an empty iterator if request_num_mm_items == 0: return # Initialize modality counters modality_counter = {self.map_config_to_modality(k): 0 for k in bucket_config} # Copy the bucket config to avoid modifying the original bucket_config_copy = bucket_config.copy() # Loop over the number of multimodal items to sample while sum(modality_counter.values()) < request_num_mm_items: # Sample a multimodal item config mm_item_config = self._rng.choice( list(bucket_config_copy.keys()), p=list(bucket_config_copy.values()) ) modality = self.map_config_to_modality(mm_item_config) # Check that modality count is less than limit per prompt if modality_counter[modality] < limit_mm_per_prompt[modality]: modality_counter[modality] += 1 yield (mm_item_config) else: # If the counter is greater than the limit per prompt # set all multimodal items of this modality to 0 for k, v in bucket_config_copy.items(): if self.map_config_to_modality(k) == modality: bucket_config_copy[k] = 0 # If all configs are 0, break the loop # This should not happen as request_num_mm_items is at most # the sum of limit_mm_per_prompt for all modalities if all(v == 0 for v in bucket_config_copy.values()): logger.warning( \"Exhausted all multimodal items of modality %s\", modality ) break # Renormalize the bucket config bucket_config_copy = self.normalize_bucket_config(bucket_config_copy) def sample( self, tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, prefix_len: int = RandomDataset.DEFAULT_PREFIX_LEN, range_ratio: float = RandomDataset.DEFAULT_RANGE_RATIO, input_len: int = RandomDataset.DEFAULT_INPUT_LEN, output_len: int = RandomDataset.DEFAULT_OUTPUT_LEN, limit_mm_per_prompt: dict[str, int] = DEFAULT_LIMIT_MM_PER_PROMPT, base_items_per_request: int = DEFAULT_BASE_ITEMS_PER_REQUEST, num_mm_items_range_ratio: float = DEFAULT_NUM_MM_ITEMS_RANGE_RATIO, bucket_config: dict[ tuple[int, int, int], float ] = DEFAULT_MM_ITEM_BUCKET_CONFIG, enable_multimodal_chat: bool = DEFAULT_ENABLE_MULTIMODAL_CHAT, **kwargs, ) -> list[SampleRequest]: # Get the sampling parameters for the dataset input_lens, output_lens, offsets = self.get_sampling_params( num_requests, range_ratio, input_len, output_len, tokenizer ) ( min_num_mm_items, max_num_mm_items, limit_mm_per_prompt, bucket_config, ) = self.get_mm_item_sampling_params( base_items_per_request, num_mm_items_range_ratio, limit_mm_per_prompt, bucket_config, ) vocab_size = tokenizer.vocab_size # Can't use tokenizer.all_special_ids since # it returns ONLY ids from special_tokens_map.json # We want to exclude placeholder tokens and all # tokens that indicate start/end of image as it # may break prompt replacement logic. prohibited_tokens = list( tok_id for tok_id, token in tokenizer.added_tokens_decoder.items() if token.special ) all_tokens = np.arange(vocab_size) allowed_tokens = np.array(list(set(all_tokens) - set(prohibited_tokens))) logger.debug( \"Sampling from %d out of %d (vocab size)\", len(allowed_tokens), vocab_size ) # Generate prefix once prefix_token_ids = self.get_prefix(allowed_tokens, prefix_len) # Add synthetic multimodal items to each request mm_requests = [] token_mismatch_total = 0 for i in range(num_requests): prompt, total_input_len, token_mismatch = self.generate_token_sequence( # noqa: E501 tokenizer=tokenizer, prefix_token_ids=prefix_token_ids, prefix_len=prefix_len, vocab_size=vocab_size, input_len=int(input_lens[i]), offset=int(offsets[i]), index=i, allowed_tokens=allowed_tokens, ) token_mismatch_total += token_mismatch # Get multimodal item iterator for a given request mm_item_iterator = self.get_mm_item_iterator( min_num_mm_items, max_num_mm_items, bucket_config, limit_mm_per_prompt, ) mm_content = cast( list[dict[str, Any]], [ self.generate_mm_item(mm_item_config) for mm_item_config in mm_item_iterator ], ) if enable_multimodal_chat: # NOTE: For now this option is only provided for completeness # given that the serve.py benchmark currently does not use it. mm_chat_prompt: Any = prompt mm_chat_prompt = self.apply_multimodal_chat_transformation( prompt, mm_content ) sample_request = SampleRequest( prompt=mm_chat_prompt, prompt_len=total_input_len, expected_output_len=int(output_lens[i]), multi_modal_data=None, request_id=request_id_prefix + str(i), ) else: sample_request = SampleRequest( prompt=prompt, prompt_len=total_input_len, expected_output_len=int(output_lens[i]), multi_modal_data=mm_content, request_id=request_id_prefix + str(i), ) mm_requests.append(sample_request) if token_mismatch_total != 0: sign = \"more\" if token_mismatch_total > 0 else \"fewer\" logger.warning( \"Across all generated prompts, there were %d %s tokens \" \"than expected after decoding and re-encoding. This is \" \"expected due to the imperfect nature of the sampling \" \"procedure.\", abs(token_mismatch_total), sign, ) return mm_requests DEFAULT_BASE_ITEMS_PER_REQUEST class-attribute instance-attribute ¶ DEFAULT_BASE_ITEMS_PER_REQUEST = 1 DEFAULT_ENABLE_MULTIMODAL_CHAT class-attribute instance-attribute ¶ DEFAULT_ENABLE_MULTIMODAL_CHAT = False DEFAULT_LIMIT_MM_PER_PROMPT class-attribute instance-attribute ¶ DEFAULT_LIMIT_MM_PER_PROMPT = {'image': 255, 'video': 1} DEFAULT_MM_ITEM_BUCKET_CONFIG class-attribute instance-attribute ¶ DEFAULT_MM_ITEM_BUCKET_CONFIG = { (256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0, } DEFAULT_NUM_MM_ITEMS_RANGE_RATIO class-attribute instance-attribute ¶ DEFAULT_NUM_MM_ITEMS_RANGE_RATIO = 0.0 IS_MULTIMODAL class-attribute instance-attribute ¶ IS_MULTIMODAL = True __init__ ¶ __init__(**kwargs) -> None Source code in vllm/benchmarks/datasets.py 830 831def __init__(self, **kwargs) -> None: super().__init__(**kwargs) generate_mm_item ¶ generate_mm_item( mm_item_config: tuple[int, int, int], ) -> Mapping[str, Any] Create synthetic images and videos and apply process_image/process_video respectively. This follows the OpenAI API chat completions https://github.com/openai/openai-python Source code in vllm/benchmarks/datasets.py 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945def generate_mm_item( self, mm_item_config: tuple[int, int, int], ) -> Mapping[str, Any]: \"\"\" Create synthetic images and videos and apply process_image/process_video respectively. This follows the OpenAI API chat completions https://github.com/openai/openai-python \"\"\" if self.map_config_to_modality(mm_item_config) == \"image\": return process_image( self.generate_synthetic_image(mm_item_config[1], mm_item_config[0]) ) elif self.map_config_to_modality(mm_item_config) == \"video\": return process_video( self.generate_synthetic_video( mm_item_config[1], mm_item_config[0], mm_item_config[2] ) ) else: raise ValueError(f\"Invalid multimodal item configuration: {mm_item_config}\") generate_synthetic_image ¶ generate_synthetic_image(width: int, height: int) -> Image Generate synthetic PIL image with random RGB values. NOTE: iid pixel sampling results in worst-case compression (good for stressing I/O), but very unlike real photos. We could consider a “low-freq” mode (e.g., noise blur) to emulate network realism instead of max stress. Source code in vllm/benchmarks/datasets.py 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847def generate_synthetic_image(self, width: int, height: int) -> Image.Image: \"\"\"Generate synthetic PIL image with random RGB values. NOTE: iid pixel sampling results in worst-case compression (good for stressing I/O), but very unlike real photos. We could consider a “low-freq” mode (e.g., noise blur) to emulate network realism instead of max stress. \"\"\" random_pixels = self._rng.integers( 0, 256, (height, width, 3), dtype=np.uint8, ) return Image.fromarray(random_pixels) generate_synthetic_video ¶ generate_synthetic_video( width: int, height: int, num_frames: int ) -> dict Generate synthetic video with random values. Creates a video with random pixel values, encodes it to MP4 format, and returns the content as bytes. Source code in vllm/benchmarks/datasets.py 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891def generate_synthetic_video( self, width: int, height: int, num_frames: int ) -> dict: \"\"\"Generate synthetic video with random values. Creates a video with random pixel values, encodes it to MP4 format, and returns the content as bytes. \"\"\" import cv2 random_pixels = self._rng.integers( 0, 256, (num_frames, height, width, 3), dtype=np.uint8, ) # Create a temporary video file in memory fourcc = cv2.VideoWriter_fourcc(*\"mp4v\") fps = 30 # frames per second with NamedTemporaryFile(suffix=\".mp4\", delete_on_close=False) as temp_file: temp_path = temp_file.name # Create video writer video_writer = cv2.VideoWriter( temp_path, fourcc=fourcc, fps=fps, frameSize=(width, height) ) if not video_writer.isOpened(): raise RuntimeError(\"Failed to create video writer\") for frame in random_pixels: video_writer.write(frame) video_writer.release() temp_file.close() # Read the video file content with open(temp_path, \"rb\") as f: video_content = f.read() return {\"bytes\": video_content} get_mm_item_iterator ¶ get_mm_item_iterator( min_num_mm_items: int, max_num_mm_items: int, bucket_config: dict[tuple[int, int, int], float], limit_mm_per_prompt: dict[str, int], ) -> Iterator[tuple[int, int, int]] Iterator over the multimodal items for each request whose size is between min_num_mm_items and max_num_mm_items. Loop over the bucket config and sample a multimodal item. Loop until the number of multimodal items sampled is equal to request_num_mm_items or limit of multimodal items per prompt for all modalities is reached. Note: - This function operates on a per-request shallow copy of bucket_config (tuple->float). The original dict passed to sample is not mutated. If this ever changes, a test is implemented and will fail. Source code in vllm/benchmarks/datasets.py 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081def get_mm_item_iterator( self, min_num_mm_items: int, max_num_mm_items: int, bucket_config: dict[tuple[int, int, int], float], limit_mm_per_prompt: dict[str, int], ) -> Iterator[tuple[int, int, int]]: \"\"\" Iterator over the multimodal items for each request whose size is between min_num_mm_items and max_num_mm_items. Loop over the bucket config and sample a multimodal item. Loop until the number of multimodal items sampled is equal to request_num_mm_items or limit of multimodal items per prompt for all modalities is reached. Note: - This function operates on a per-request shallow copy of `bucket_config` (tuple->float). The original dict passed to `sample` is not mutated. If this ever changes, a test is implemented and will fail. \"\"\" # Get the number of multimodal items to sample request_num_mm_items = int( self._rng.integers(min_num_mm_items, max_num_mm_items + 1) ) # If request_num_mm_items is 0, yield an empty iterator if request_num_mm_items == 0: return # Initialize modality counters modality_counter = {self.map_config_to_modality(k): 0 for k in bucket_config} # Copy the bucket config to avoid modifying the original bucket_config_copy = bucket_config.copy() # Loop over the number of multimodal items to sample while sum(modality_counter.values()) < request_num_mm_items: # Sample a multimodal item config mm_item_config = self._rng.choice( list(bucket_config_copy.keys()), p=list(bucket_config_copy.values()) ) modality = self.map_config_to_modality(mm_item_config) # Check that modality count is less than limit per prompt if modality_counter[modality] < limit_mm_per_prompt[modality]: modality_counter[modality] += 1 yield (mm_item_config) else: # If the counter is greater than the limit per prompt # set all multimodal items of this modality to 0 for k, v in bucket_config_copy.items(): if self.map_config_to_modality(k) == modality: bucket_config_copy[k] = 0 # If all configs are 0, break the loop # This should not happen as request_num_mm_items is at most # the sum of limit_mm_per_prompt for all modalities if all(v == 0 for v in bucket_config_copy.values()): logger.warning( \"Exhausted all multimodal items of modality %s\", modality ) break # Renormalize the bucket config bucket_config_copy = self.normalize_bucket_config(bucket_config_copy) get_mm_item_sampling_params ¶ get_mm_item_sampling_params( base_items_per_request: int, num_mm_items_range_ratio: float, limit_mm_per_prompt: dict[str, int], bucket_config: dict[tuple[int, int, int], float], ) -> tuple[ int, int, dict[str, int], dict[tuple[int, int, int], float], ] Get the sampling parameters for the multimodal items. Source code in vllm/benchmarks/datasets.py 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020def get_mm_item_sampling_params( self, base_items_per_request: int, num_mm_items_range_ratio: float, limit_mm_per_prompt: dict[str, int], bucket_config: dict[tuple[int, int, int], float], ) -> tuple[int, int, dict[str, int], dict[tuple[int, int, int], float]]: \"\"\" Get the sampling parameters for the multimodal items. \"\"\" # Enforce num_mm_items_range_ratio <= 1 if not (0.0 <= num_mm_items_range_ratio <= 1.0): raise ValueError(\"num_mm_items_range_ratio must be in [0, 1].\") # Ensure modalities to sample are in limit_mm_per_prompt for k, v in bucket_config.items(): # get modality from bucket config modality = self.map_config_to_modality(k) if modality not in limit_mm_per_prompt: raise ValueError( f\"Modality {modality} is not in \" f\"limit_mm_per_prompt: \" f\"{limit_mm_per_prompt.keys()}\" ) # Remove zero probability entries # and normalize bucket config to sum to 1 bucket_config = self.normalize_bucket_config(bucket_config) logger.info( \"Normalized bucket config: %s\", bucket_config, ) # Only consider limit per prompt for modalities in bucket config allowed_modalities = {self.map_config_to_modality(cfg) for cfg in bucket_config} limit_mm_per_prompt = { k: v for k, v in limit_mm_per_prompt.items() if k in allowed_modalities } if not limit_mm_per_prompt: raise ValueError(\"No valid limits for modalities present in bucket_config.\") logger.info( \"Updated mm-limit-per-prompt: %s\", limit_mm_per_prompt, ) # Get max and min num mm items and ensure # it is at most the sum of limit_mm_per_prompt for all modalities max_num_mm_items = min( sum(limit_mm_per_prompt.values()), math.ceil(base_items_per_request * (1 + num_mm_items_range_ratio)), ) # Ensure min num mm items is at least 0 min_num_mm_items = max( 0, math.floor(base_items_per_request * (1 - num_mm_items_range_ratio)) ) # Raise error if min num mm items is greater than max num mm items if min_num_mm_items > max_num_mm_items: raise ValueError( f\"Min num mm items is greater than max mm items: \" f\"{min_num_mm_items} > {max_num_mm_items}\" ) logger.info( \"Sampling number of multimodal items from [%s, %s]\", min_num_mm_items, max_num_mm_items, ) return ( min_num_mm_items, max_num_mm_items, limit_mm_per_prompt, bucket_config, ) map_config_to_modality ¶ map_config_to_modality(config: tuple[int, int, int]) -> str Map the configuration to the modality. Source code in vllm/benchmarks/datasets.py 893 894 895 896 897 898 899 900def map_config_to_modality(self, config: tuple[int, int, int]) -> str: \"\"\"Map the configuration to the modality.\"\"\" if config[-1] == 1: return \"image\" elif config[-1] > 1: return \"video\" else: raise ValueError(f\"Invalid multimodal item configuration: {config}\") normalize_bucket_config ¶ normalize_bucket_config( bucket_config: dict[tuple[int, int, int], float], ) -> dict[tuple[int, int, int], float] Remove zero probability entries and normalize the bucket config to sum to 1. Source code in vllm/benchmarks/datasets.py 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921def normalize_bucket_config( self, bucket_config: dict[tuple[int, int, int], float] ) -> dict[tuple[int, int, int], float]: \"\"\" Remove zero probability entries and normalize the bucket config to sum to 1. \"\"\" # Raise error if value is negative if any(v < 0 for v in bucket_config.values()): raise ValueError(\"Bucket config values must be non-negative.\") # Remove zero probability entries bucket_config = {k: v for k, v in bucket_config.items() if v > 0} # if bucket config is empty, raise error if not bucket_config: raise ValueError( \"Got invalid bucket config. Bucket config values must be non-zero.\" ) # Normalize the remaining bucket config to sum to 1 total = sum(bucket_config.values()) return {k: v / total for k, v in bucket_config.items()} sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, prefix_len: int = DEFAULT_PREFIX_LEN, range_ratio: float = DEFAULT_RANGE_RATIO, input_len: int = DEFAULT_INPUT_LEN, output_len: int = DEFAULT_OUTPUT_LEN, limit_mm_per_prompt: dict[ str, int ] = DEFAULT_LIMIT_MM_PER_PROMPT, base_items_per_request: int = DEFAULT_BASE_ITEMS_PER_REQUEST, num_mm_items_range_ratio: float = DEFAULT_NUM_MM_ITEMS_RANGE_RATIO, bucket_config: dict[ tuple[int, int, int], float ] = DEFAULT_MM_ITEM_BUCKET_CONFIG, enable_multimodal_chat: bool = DEFAULT_ENABLE_MULTIMODAL_CHAT, **kwargs, ) -> list[SampleRequest] Source code in vllm/benchmarks/datasets.py 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203def sample( self, tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, prefix_len: int = RandomDataset.DEFAULT_PREFIX_LEN, range_ratio: float = RandomDataset.DEFAULT_RANGE_RATIO, input_len: int = RandomDataset.DEFAULT_INPUT_LEN, output_len: int = RandomDataset.DEFAULT_OUTPUT_LEN, limit_mm_per_prompt: dict[str, int] = DEFAULT_LIMIT_MM_PER_PROMPT, base_items_per_request: int = DEFAULT_BASE_ITEMS_PER_REQUEST, num_mm_items_range_ratio: float = DEFAULT_NUM_MM_ITEMS_RANGE_RATIO, bucket_config: dict[ tuple[int, int, int], float ] = DEFAULT_MM_ITEM_BUCKET_CONFIG, enable_multimodal_chat: bool = DEFAULT_ENABLE_MULTIMODAL_CHAT, **kwargs, ) -> list[SampleRequest]: # Get the sampling parameters for the dataset input_lens, output_lens, offsets = self.get_sampling_params( num_requests, range_ratio, input_len, output_len, tokenizer ) ( min_num_mm_items, max_num_mm_items, limit_mm_per_prompt, bucket_config, ) = self.get_mm_item_sampling_params( base_items_per_request, num_mm_items_range_ratio, limit_mm_per_prompt, bucket_config, ) vocab_size = tokenizer.vocab_size # Can't use tokenizer.all_special_ids since # it returns ONLY ids from special_tokens_map.json # We want to exclude placeholder tokens and all # tokens that indicate start/end of image as it # may break prompt replacement logic. prohibited_tokens = list( tok_id for tok_id, token in tokenizer.added_tokens_decoder.items() if token.special ) all_tokens = np.arange(vocab_size) allowed_tokens = np.array(list(set(all_tokens) - set(prohibited_tokens))) logger.debug( \"Sampling from %d out of %d (vocab size)\", len(allowed_tokens), vocab_size ) # Generate prefix once prefix_token_ids = self.get_prefix(allowed_tokens, prefix_len) # Add synthetic multimodal items to each request mm_requests = [] token_mismatch_total = 0 for i in range(num_requests): prompt, total_input_len, token_mismatch = self.generate_token_sequence( # noqa: E501 tokenizer=tokenizer, prefix_token_ids=prefix_token_ids, prefix_len=prefix_len, vocab_size=vocab_size, input_len=int(input_lens[i]), offset=int(offsets[i]), index=i, allowed_tokens=allowed_tokens, ) token_mismatch_total += token_mismatch # Get multimodal item iterator for a given request mm_item_iterator = self.get_mm_item_iterator( min_num_mm_items, max_num_mm_items, bucket_config, limit_mm_per_prompt, ) mm_content = cast( list[dict[str, Any]], [ self.generate_mm_item(mm_item_config) for mm_item_config in mm_item_iterator ], ) if enable_multimodal_chat: # NOTE: For now this option is only provided for completeness # given that the serve.py benchmark currently does not use it. mm_chat_prompt: Any = prompt mm_chat_prompt = self.apply_multimodal_chat_transformation( prompt, mm_content ) sample_request = SampleRequest( prompt=mm_chat_prompt, prompt_len=total_input_len, expected_output_len=int(output_lens[i]), multi_modal_data=None, request_id=request_id_prefix + str(i), ) else: sample_request = SampleRequest( prompt=prompt, prompt_len=total_input_len, expected_output_len=int(output_lens[i]), multi_modal_data=mm_content, request_id=request_id_prefix + str(i), ) mm_requests.append(sample_request) if token_mismatch_total != 0: sign = \"more\" if token_mismatch_total > 0 else \"fewer\" logger.warning( \"Across all generated prompts, there were %d %s tokens \" \"than expected after decoding and re-encoding. This is \" \"expected due to the imperfect nature of the sampling \" \"procedure.\", abs(token_mismatch_total), sign, ) return mm_requests SampleRequest dataclass ¶ Represents a single inference request for benchmarking. Source code in vllm/benchmarks/datasets.py 72 73 74 75 76 77 78 79 80 81 82 83@dataclass class SampleRequest: \"\"\" Represents a single inference request for benchmarking. \"\"\" prompt: str | list[str] prompt_len: int expected_output_len: int multi_modal_data: MultiModalDataDict | dict | list[dict] | None = None lora_request: LoRARequest | None = None request_id: str | None = None expected_output_len instance-attribute ¶ expected_output_len: int lora_request class-attribute instance-attribute ¶ lora_request: LoRARequest | None = None multi_modal_data class-attribute instance-attribute ¶ multi_modal_data: ( MultiModalDataDict | dict | list[dict] | None ) = None prompt instance-attribute ¶ prompt: str | list[str] prompt_len instance-attribute ¶ prompt_len: int request_id class-attribute instance-attribute ¶ request_id: str | None = None __init__ ¶ __init__( prompt: str | list[str], prompt_len: int, expected_output_len: int, multi_modal_data: MultiModalDataDict | dict | list[dict] | None = None, lora_request: LoRARequest | None = None, request_id: str | None = None, ) -> None ShareGPTDataset ¶ Bases: BenchmarkDataset Implements the ShareGPT dataset. Loads data from a JSON file and generates sample requests based on conversation turns. Source code in vllm/benchmarks/datasets.py 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294class ShareGPTDataset(BenchmarkDataset): \"\"\" Implements the ShareGPT dataset. Loads data from a JSON file and generates sample requests based on conversation turns. \"\"\" def __init__(self, **kwargs) -> None: super().__init__(**kwargs) self.load_data() def load_data(self) -> None: if self.dataset_path is None: raise ValueError(\"dataset_path must be provided for loading data.\") with open(self.dataset_path, encoding=\"utf-8\") as f: self.data = json.load(f) # Filter entries with at least two conversation turns. self.data = [ entry for entry in self.data if \"conversations\" in entry and len(entry[\"conversations\"]) >= 2 ] random.seed(self.random_seed) if not getattr(self, \"disable_shuffle\", False): random.shuffle(self.data) def sample( self, tokenizer: TokenizerLike, num_requests: int, lora_path: str | None = None, max_loras: int | None = None, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: samples: list = [] ind = 0 for entry in self.data: if len(samples) >= num_requests: break prompt, completion = ( entry[\"conversations\"][0][\"value\"], entry[\"conversations\"][1][\"value\"], ) lora_request = self.get_random_lora_request( max_loras=max_loras, lora_path=lora_path ) prompt_ids = tokenizer(prompt).input_ids completion_ids = tokenizer(completion).input_ids prompt_len = len(prompt_ids) new_output_len = len(completion_ids) if output_len is None else output_len if not is_valid_sequence( prompt_len, new_output_len, skip_min_output_len_check=output_len is not None, ): continue if image_path := entry.get(\"image\"): mm_content = process_image(image_path) elif video_path := entry.get(\"video\"): mm_content = process_video(video_path) else: mm_content = None if enable_multimodal_chat: prompt = self.apply_multimodal_chat_transformation(prompt, mm_content) samples.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=new_output_len, lora_request=lora_request, multi_modal_data=mm_content, request_id=request_id_prefix + str(ind), ) ) ind += 1 self.maybe_oversample_requests( samples, num_requests, request_id_prefix, no_oversample ) return samples __init__ ¶ __init__(**kwargs) -> None Source code in vllm/benchmarks/datasets.py 1217 1218 1219def __init__(self, **kwargs) -> None: super().__init__(**kwargs) self.load_data() load_data ¶ load_data() -> None Source code in vllm/benchmarks/datasets.py 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235def load_data(self) -> None: if self.dataset_path is None: raise ValueError(\"dataset_path must be provided for loading data.\") with open(self.dataset_path, encoding=\"utf-8\") as f: self.data = json.load(f) # Filter entries with at least two conversation turns. self.data = [ entry for entry in self.data if \"conversations\" in entry and len(entry[\"conversations\"]) >= 2 ] random.seed(self.random_seed) if not getattr(self, \"disable_shuffle\", False): random.shuffle(self.data) sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, lora_path: str | None = None, max_loras: int | None = None, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294def sample( self, tokenizer: TokenizerLike, num_requests: int, lora_path: str | None = None, max_loras: int | None = None, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: samples: list = [] ind = 0 for entry in self.data: if len(samples) >= num_requests: break prompt, completion = ( entry[\"conversations\"][0][\"value\"], entry[\"conversations\"][1][\"value\"], ) lora_request = self.get_random_lora_request( max_loras=max_loras, lora_path=lora_path ) prompt_ids = tokenizer(prompt).input_ids completion_ids = tokenizer(completion).input_ids prompt_len = len(prompt_ids) new_output_len = len(completion_ids) if output_len is None else output_len if not is_valid_sequence( prompt_len, new_output_len, skip_min_output_len_check=output_len is not None, ): continue if image_path := entry.get(\"image\"): mm_content = process_image(image_path) elif video_path := entry.get(\"video\"): mm_content = process_video(video_path) else: mm_content = None if enable_multimodal_chat: prompt = self.apply_multimodal_chat_transformation(prompt, mm_content) samples.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=new_output_len, lora_request=lora_request, multi_modal_data=mm_content, request_id=request_id_prefix + str(ind), ) ) ind += 1 self.maybe_oversample_requests( samples, num_requests, request_id_prefix, no_oversample ) return samples SonnetDataset ¶ Bases: BenchmarkDataset Simplified implementation of the Sonnet dataset. Loads poem lines from a text file and generates sample requests. Default values here copied from benchmark_serving.py for the sonnet dataset. Source code in vllm/benchmarks/datasets.py 2079 2080 2081 2082 2083 2084 2085 2086 2087 2088 2089 2090 2091 2092 2093 2094 2095 2096 2097 2098 2099 2100 2101 2102 2103 2104 2105 2106 2107 2108 2109 2110 2111 2112 2113 2114 2115 2116 2117 2118 2119 2120 2121 2122 2123 2124 2125 2126 2127 2128 2129 2130 2131 2132 2133 2134 2135 2136 2137 2138 2139 2140 2141 2142 2143 2144 2145 2146 2147 2148 2149 2150 2151 2152 2153 2154 2155 2156 2157 2158 2159 2160 2161 2162@deprecated( \"SonnetDataset is deprecated and will be removed in a future version.\", ) class SonnetDataset(BenchmarkDataset): \"\"\" Simplified implementation of the Sonnet dataset. Loads poem lines from a text file and generates sample requests. Default values here copied from `benchmark_serving.py` for the sonnet dataset. \"\"\" DEFAULT_PREFIX_LEN = 200 DEFAULT_INPUT_LEN = 550 DEFAULT_OUTPUT_LEN = 150 def __init__( self, **kwargs, ) -> None: super().__init__(**kwargs) self.load_data() def load_data(self) -> None: if not self.dataset_path: raise ValueError(\"dataset_path must be provided.\") with open(self.dataset_path, encoding=\"utf-8\") as f: self.data = f.readlines() def sample( self, tokenizer: TokenizerLike, num_requests: int, prefix_len: int = DEFAULT_PREFIX_LEN, input_len: int = DEFAULT_INPUT_LEN, output_len: int = DEFAULT_OUTPUT_LEN, return_prompt_formatted: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: # Calculate average token length for a poem line. tokenized_lines = [tokenizer(line).input_ids for line in self.data] avg_len = sum(len(tokens) for tokens in tokenized_lines) / len(tokenized_lines) # Build the base prompt. base_prompt = \"Pick as many lines as you can from these poem lines:\\n\" base_msg = [{\"role\": \"user\", \"content\": base_prompt}] base_fmt = tokenizer.apply_chat_template( base_msg, add_generation_prompt=True, tokenize=False ) base_offset = len(tokenizer(base_fmt).input_ids) if input_len <= base_offset: raise ValueError( f\"'input_len' must be higher than the base prompt length \" f\"({base_offset}).\" ) # Determine how many poem lines to use. num_input_lines = round((input_len - base_offset) / avg_len) num_prefix_lines = max(round((prefix_len - base_offset) / avg_len), 0) prefix_lines = self.data[:num_prefix_lines] samples = [] ind = 0 while len(samples) < num_requests: extra_lines = random.choices( self.data, k=num_input_lines - num_prefix_lines ) prompt = f\"{base_prompt}{''.join(prefix_lines + extra_lines)}\" msg = [{\"role\": \"user\", \"content\": prompt}] prompt_formatted = tokenizer.apply_chat_template( msg, add_generation_prompt=True, tokenize=False ) prompt_len = len(tokenizer(prompt_formatted).input_ids) if prompt_len <= input_len: samples.append( SampleRequest( prompt=prompt_formatted if return_prompt_formatted else prompt, prompt_len=prompt_len, expected_output_len=output_len, request_id=request_id_prefix + str(ind), ) ) ind += 1 return samples DEFAULT_INPUT_LEN class-attribute instance-attribute ¶ DEFAULT_INPUT_LEN = 550 DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶ DEFAULT_OUTPUT_LEN = 150 DEFAULT_PREFIX_LEN class-attribute instance-attribute ¶ DEFAULT_PREFIX_LEN = 200 __init__ ¶ __init__(**kwargs) -> None Source code in vllm/benchmarks/datasets.py 2093 2094 2095 2096 2097 2098def __init__( self, **kwargs, ) -> None: super().__init__(**kwargs) self.load_data() load_data ¶ load_data() -> None Source code in vllm/benchmarks/datasets.py 2100 2101 2102 2103 2104def load_data(self) -> None: if not self.dataset_path: raise ValueError(\"dataset_path must be provided.\") with open(self.dataset_path, encoding=\"utf-8\") as f: self.data = f.readlines() sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, prefix_len: int = DEFAULT_PREFIX_LEN, input_len: int = DEFAULT_INPUT_LEN, output_len: int = DEFAULT_OUTPUT_LEN, return_prompt_formatted: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 2106 2107 2108 2109 2110 2111 2112 2113 2114 2115 2116 2117 2118 2119 2120 2121 2122 2123 2124 2125 2126 2127 2128 2129 2130 2131 2132 2133 2134 2135 2136 2137 2138 2139 2140 2141 2142 2143 2144 2145 2146 2147 2148 2149 2150 2151 2152 2153 2154 2155 2156 2157 2158 2159 2160 2161 2162def sample( self, tokenizer: TokenizerLike, num_requests: int, prefix_len: int = DEFAULT_PREFIX_LEN, input_len: int = DEFAULT_INPUT_LEN, output_len: int = DEFAULT_OUTPUT_LEN, return_prompt_formatted: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: # Calculate average token length for a poem line. tokenized_lines = [tokenizer(line).input_ids for line in self.data] avg_len = sum(len(tokens) for tokens in tokenized_lines) / len(tokenized_lines) # Build the base prompt. base_prompt = \"Pick as many lines as you can from these poem lines:\\n\" base_msg = [{\"role\": \"user\", \"content\": base_prompt}] base_fmt = tokenizer.apply_chat_template( base_msg, add_generation_prompt=True, tokenize=False ) base_offset = len(tokenizer(base_fmt).input_ids) if input_len <= base_offset: raise ValueError( f\"'input_len' must be higher than the base prompt length \" f\"({base_offset}).\" ) # Determine how many poem lines to use. num_input_lines = round((input_len - base_offset) / avg_len) num_prefix_lines = max(round((prefix_len - base_offset) / avg_len), 0) prefix_lines = self.data[:num_prefix_lines] samples = [] ind = 0 while len(samples) < num_requests: extra_lines = random.choices( self.data, k=num_input_lines - num_prefix_lines ) prompt = f\"{base_prompt}{''.join(prefix_lines + extra_lines)}\" msg = [{\"role\": \"user\", \"content\": prompt}] prompt_formatted = tokenizer.apply_chat_template( msg, add_generation_prompt=True, tokenize=False ) prompt_len = len(tokenizer(prompt_formatted).input_ids) if prompt_len <= input_len: samples.append( SampleRequest( prompt=prompt_formatted if return_prompt_formatted else prompt, prompt_len=prompt_len, expected_output_len=output_len, request_id=request_id_prefix + str(ind), ) ) ind += 1 return samples SpecBench ¶ Bases: CustomDataset Implements the SpecBench dataset: https://github.com/hemingkx/Spec-Bench Download the dataset using: wget https://raw.githubusercontent.com/hemingkx/Spec-Bench/refs/heads/main/data/spec_bench/question.jsonl Source code in vllm/benchmarks/datasets.py 2034 2035 2036 2037 2038 2039 2040 2041 2042 2043 2044 2045 2046 2047 2048 2049 2050 2051 2052 2053 2054 2055 2056 2057 2058 2059 2060 2061 2062 2063 2064 2065 2066 2067 2068 2069 2070 2071class SpecBench(CustomDataset): \"\"\" Implements the SpecBench dataset: https://github.com/hemingkx/Spec-Bench Download the dataset using: wget https://raw.githubusercontent.com/hemingkx/Spec-Bench/refs/heads/main/data/spec_bench/question.jsonl \"\"\" # noqa: E501 def __init__(self, **kwargs) -> None: self.category = kwargs.pop(\"category\", None) super().__init__(**kwargs) self.load_data() def load_data(self) -> None: if self.dataset_path is None: raise ValueError(\"dataset_path must be provided for loading data.\") self.data = [] # Load the JSONL file jsonl_data = pd.read_json(path_or_buf=self.dataset_path, lines=True) # check if the JSONL file has a 'turns' column if \"turns\" not in jsonl_data.columns: raise ValueError(\"JSONL file must contain a 'turns' column.\") for _, row in jsonl_data.iterrows(): # sample only from a specific category if specified if (not self.category) or (self.category == row[\"category\"]): prompt = row[\"turns\"][0] self.data.append({\"prompt\": prompt}) random.seed(self.random_seed) if not getattr(self, \"disable_shuffle\", False): random.shuffle(self.data) def sample(self, **kwargs) -> list: # leverage CustomDataset sample return super().sample(**kwargs) category instance-attribute ¶ category = pop('category', None) __init__ ¶ __init__(**kwargs) -> None Source code in vllm/benchmarks/datasets.py 2041 2042 2043 2044def __init__(self, **kwargs) -> None: self.category = kwargs.pop(\"category\", None) super().__init__(**kwargs) self.load_data() load_data ¶ load_data() -> None Source code in vllm/benchmarks/datasets.py 2046 2047 2048 2049 2050 2051 2052 2053 2054 2055 2056 2057 2058 2059 2060 2061 2062 2063 2064 2065 2066 2067def load_data(self) -> None: if self.dataset_path is None: raise ValueError(\"dataset_path must be provided for loading data.\") self.data = [] # Load the JSONL file jsonl_data = pd.read_json(path_or_buf=self.dataset_path, lines=True) # check if the JSONL file has a 'turns' column if \"turns\" not in jsonl_data.columns: raise ValueError(\"JSONL file must contain a 'turns' column.\") for _, row in jsonl_data.iterrows(): # sample only from a specific category if specified if (not self.category) or (self.category == row[\"category\"]): prompt = row[\"turns\"][0] self.data.append({\"prompt\": prompt}) random.seed(self.random_seed) if not getattr(self, \"disable_shuffle\", False): random.shuffle(self.data) sample ¶ sample(**kwargs) -> list Source code in vllm/benchmarks/datasets.py 2069 2070 2071def sample(self, **kwargs) -> list: # leverage CustomDataset sample return super().sample(**kwargs) VisionArenaDataset ¶ Bases: HuggingFaceDataset Vision Arena Dataset. Source code in vllm/benchmarks/datasets.py 2409 2410 2411 2412 2413 2414 2415 2416 2417 2418 2419 2420 2421 2422 2423 2424 2425 2426 2427 2428 2429 2430 2431 2432 2433 2434 2435 2436 2437 2438 2439 2440 2441 2442 2443 2444 2445 2446 2447 2448 2449 2450 2451 2452 2453 2454 2455 2456 2457 2458 2459class VisionArenaDataset(HuggingFaceDataset): \"\"\" Vision Arena Dataset. \"\"\" DEFAULT_OUTPUT_LEN = 128 SUPPORTED_DATASET_PATHS = { \"lmarena-ai/VisionArena-Chat\": lambda x: x[\"conversation\"][0][0][\"content\"], \"lmarena-ai/vision-arena-bench-v0.1\": lambda x: x[\"turns\"][0][0][\"content\"], } IS_MULTIMODAL = True def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break parser_fn = self.SUPPORTED_DATASET_PATHS.get(self.hf_name) if parser_fn is None: raise ValueError(f\"Unsupported dataset path: {self.hf_name}\") prompt = parser_fn(item) mm_content = process_image(item[\"images\"][0]) prompt_len = len(tokenizer(prompt).input_ids) if enable_multimodal_chat: # Note: when chat is enabled the request prompt_len is no longer # accurate and we will be using request output to count the # actual prompt len prompt = self.apply_multimodal_chat_transformation(prompt, mm_content) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_content, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶ DEFAULT_OUTPUT_LEN = 128 IS_MULTIMODAL class-attribute instance-attribute ¶ IS_MULTIMODAL = True SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = { \"lmarena-ai/VisionArena-Chat\": lambda x: x[ \"conversation\" ][0][0][\"content\"], \"lmarena-ai/vision-arena-bench-v0.1\": lambda x: x[ \"turns\" ][0][0][\"content\"], } sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 2421 2422 2423 2424 2425 2426 2427 2428 2429 2430 2431 2432 2433 2434 2435 2436 2437 2438 2439 2440 2441 2442 2443 2444 2445 2446 2447 2448 2449 2450 2451 2452 2453 2454 2455 2456 2457 2458 2459def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break parser_fn = self.SUPPORTED_DATASET_PATHS.get(self.hf_name) if parser_fn is None: raise ValueError(f\"Unsupported dataset path: {self.hf_name}\") prompt = parser_fn(item) mm_content = process_image(item[\"images\"][0]) prompt_len = len(tokenizer(prompt).input_ids) if enable_multimodal_chat: # Note: when chat is enabled the request prompt_len is no longer # accurate and we will be using request output to count the # actual prompt len prompt = self.apply_multimodal_chat_transformation(prompt, mm_content) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_content, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests _ValidateDatasetArgs ¶ Bases: Action Argparse action to validate dataset name and path compatibility. Source code in vllm/benchmarks/datasets.py 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314class _ValidateDatasetArgs(argparse.Action): \"\"\"Argparse action to validate dataset name and path compatibility.\"\"\" def __call__(self, parser, namespace, values, option_string=None): setattr(namespace, self.dest, values) # Get current values of both dataset_name and dataset_path dataset_name = getattr(namespace, \"dataset_name\", \"random\") dataset_path = getattr(namespace, \"dataset_path\", None) # Validate the combination if dataset_name == \"random\" and dataset_path is not None: parser.error( \"Cannot use 'random' dataset with --dataset-path. \" \"Please specify the appropriate --dataset-name (e.g., \" \"'sharegpt', 'custom', 'sonnet') for your dataset file: \" f\"{dataset_path}\" ) __call__ ¶ __call__(parser, namespace, values, option_string=None) Source code in vllm/benchmarks/datasets.py 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314def __call__(self, parser, namespace, values, option_string=None): setattr(namespace, self.dest, values) # Get current values of both dataset_name and dataset_path dataset_name = getattr(namespace, \"dataset_name\", \"random\") dataset_path = getattr(namespace, \"dataset_path\", None) # Validate the combination if dataset_name == \"random\" and dataset_path is not None: parser.error( \"Cannot use 'random' dataset with --dataset-path. \" \"Please specify the appropriate --dataset-name (e.g., \" \"'sharegpt', 'custom', 'sonnet') for your dataset file: \" f\"{dataset_path}\" ) _format_zeta_prompt ¶ _format_zeta_prompt( sample: dict, original_start_marker: str = \"<|editable_region_start|>\", ) -> dict Format the zeta prompt for the Next Edit Prediction (NEP) dataset. This function formats examples from the NEP dataset into prompts and expected outputs. It could be further extended to support more NEP datasets. Parameters: Name Type Description Default sample dict The dataset sample containing events, inputs, and outputs. required original_start_marker str The marker indicating the start of the editable region. Defaults to \"<|editable_region_start|>\". '<|editable_region_start|>' Returns: Type Description dict A dictionary with the formatted prompts and expected outputs. Source code in vllm/benchmarks/datasets.py 2812 2813 2814 2815 2816 2817 2818 2819 2820 2821 2822 2823 2824 2825 2826 2827 2828 2829 2830 2831 2832 2833 2834 2835 2836 2837 2838 2839 2840 2841 2842def _format_zeta_prompt( sample: dict, original_start_marker: str = \"<|editable_region_start|>\" ) -> dict: \"\"\"Format the zeta prompt for the Next Edit Prediction (NEP) dataset. This function formats examples from the NEP dataset into prompts and expected outputs. It could be further extended to support more NEP datasets. Args: sample: The dataset sample containing events, inputs, and outputs. original_start_marker: The marker indicating the start of the editable region. Defaults to \"<|editable_region_start|>\". Returns: A dictionary with the formatted prompts and expected outputs. \"\"\" events = sample[\"events\"] input = sample[\"input\"] output = sample[\"output\"] prompt = zeta_prompt.format(events, input) # following the original implementation, extract the focused region # from the raw output output_start_index = output.find(original_start_marker) output_focused_region = output[output_start_index:] expected_output = output_focused_region return {\"prompt\": prompt, \"expected_output\": expected_output} add_dataset_parser ¶ add_dataset_parser(parser: ArgumentParser) Source code in vllm/benchmarks/datasets.py 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 1633 1634 1635 1636 1637 1638def add_dataset_parser(parser: FlexibleArgumentParser): parser.add_argument(\"--seed\", type=int, default=0) parser.add_argument( \"--num-prompts\", type=int, default=1000, help=\"Number of prompts to process.\", ) parser.add_argument( \"--dataset-name\", type=str, default=\"random\", action=_ValidateDatasetArgs, choices=[ \"sharegpt\", \"burstgpt\", \"sonnet\", \"random\", \"random-mm\", \"random-rerank\", \"hf\", \"custom\", \"prefix_repetition\", \"spec_bench\", ], help=\"Name of the dataset to benchmark on.\", ) parser.add_argument( \"--no-stream\", action=\"store_true\", help=\"Do not load the dataset in streaming mode.\", ) parser.add_argument( \"--dataset-path\", type=str, default=None, action=_ValidateDatasetArgs, help=\"Path to the sharegpt/sonnet dataset. \" \"Or the huggingface dataset ID if using HF dataset.\", ) parser.add_argument( \"--no-oversample\", action=\"store_true\", help=\"Do not oversample if the dataset has fewer samples than num-prompts.\", ) parser.add_argument( \"--skip-chat-template\", action=\"store_true\", help=\"Skip applying chat template to prompt for datasets that support it.\", ) parser.add_argument( \"--disable-shuffle\", action=\"store_true\", help=\"Disable shuffling of dataset samples for deterministic ordering.\", ) # group for dataset specific arguments custom_group = parser.add_argument_group(\"custom dataset options\") custom_group.add_argument( \"--custom-output-len\", type=int, default=256, help=\"Number of output tokens per request, used only for custom dataset.\", ) spec_bench_group = parser.add_argument_group(\"spec bench dataset options\") spec_bench_group.add_argument( \"--spec-bench-output-len\", type=int, default=256, help=\"Num of output tokens per request, used only for spec bench dataset.\", ) spec_bench_group.add_argument( \"--spec-bench-category\", type=str, default=None, help=\"Category for spec bench dataset. If None, use all categories.\", ) sonnet_group = parser.add_argument_group(\"sonnet dataset options\") sonnet_group.add_argument( \"--sonnet-input-len\", type=int, default=550, help=\"Number of input tokens per request, used only for sonnet dataset.\", ) sonnet_group.add_argument( \"--sonnet-output-len\", type=int, default=150, help=\"Number of output tokens per request, used only for sonnet dataset.\", ) sonnet_group.add_argument( \"--sonnet-prefix-len\", type=int, default=200, help=\"Number of prefix tokens per request, used only for sonnet dataset.\", ) sharegpt_group = parser.add_argument_group(\"sharegpt dataset options\") sharegpt_group.add_argument( \"--sharegpt-output-len\", type=int, default=None, help=\"Output length for each request. Overrides the output length \" \"from the ShareGPT dataset.\", ) blazedit_group = parser.add_argument_group(\"blazedit dataset options\") blazedit_group.add_argument( \"--blazedit-min-distance\", type=float, default=0.0, help=\"Minimum distance for blazedit dataset. Min: 0, Max: 1.0\", ) blazedit_group.add_argument( \"--blazedit-max-distance\", type=float, default=1.0, help=\"Maximum distance for blazedit dataset. Min: 0, Max: 1.0\", ) random_group = parser.add_argument_group(\"random dataset options\") random_group.add_argument( \"--random-input-len\", type=int, default=1024, help=\"Number of input tokens per request, used only for random sampling.\", ) random_group.add_argument( \"--random-output-len\", type=int, default=128, help=\"Number of output tokens per request, used only for random sampling.\", ) random_group.add_argument( \"--random-range-ratio\", type=float, default=0.0, help=\"Range ratio for sampling input/output length, \" \"used only for random sampling. Must be in the range [0, 1) to define \" \"a symmetric sampling range\" \"[length * (1 - range_ratio), length * (1 + range_ratio)].\", ) random_group.add_argument( \"--random-prefix-len\", type=int, default=0, help=( \"Number of fixed prefix tokens before the random context \" \"in a request. \" \"The total input length is the sum of `random-prefix-len` and \" \"a random \" \"context length sampled from [input_len * (1 - range_ratio), \" \"input_len * (1 + range_ratio)].\" ), ) random_group.add_argument( \"--random-batch-size\", type=int, default=1, help=(\"Batch size for random sampling. Only used for embeddings benchmark.\"), ) random_group.add_argument( \"--no-reranker\", action=\"store_true\", help=( \"Whether the model supports reranking natively.\" \" Only used for reranker benchmark.\" ), ) # random multimodal dataset options random_mm_group = parser.add_argument_group( \"random multimodal dataset options extended from random dataset\" ) random_mm_group.add_argument( \"--random-mm-base-items-per-request\", type=int, default=RandomMultiModalDataset.DEFAULT_BASE_ITEMS_PER_REQUEST, help=( \"Base number of multimodal items per request for random-mm. \" \"Actual per-request count is sampled around this base using \" \"--random-mm-num-mm-items-range-ratio.\" ), ) random_mm_group.add_argument( \"--random-mm-num-mm-items-range-ratio\", type=float, default=RandomMultiModalDataset.DEFAULT_NUM_MM_ITEMS_RANGE_RATIO, help=( \"Range ratio r in [0, 1] for sampling items per request. \" \"We sample uniformly from the closed integer range \" \"[floor(n*(1-r)), ceil(n*(1+r))] \" \"where n is the base items per request. \" \"r=0 keeps it fixed; r=1 allows 0 items. The maximum is clamped \" \"to the sum of per-modality limits from \" \"--random-mm-limit-mm-per-prompt. \" \"An error is raised if the computed min exceeds the max.\" ), ) random_mm_group.add_argument( \"--random-mm-limit-mm-per-prompt\", type=json.loads, default=RandomMultiModalDataset.DEFAULT_LIMIT_MM_PER_PROMPT, help=( \"Per-modality hard caps for items attached per request, e.g. \" '\\'{\"image\": 3, \"video\": 0}\\'. The sampled per-request item ' \"count is clamped to the sum of these limits. When a modality \" \"reaches its cap, its buckets are excluded and probabilities are \" \"renormalized.\" \"OBS.: Only image sampling is supported for now.\" ), ) def _parse_mm_bucket_config(v: object) -> dict[tuple[int, int, int], float]: # If already a dict (e.g., programmatic call), normalize keys def normalize(d: dict) -> dict[tuple[int, int, int], float]: out: dict[tuple[int, int, int], float] = {} for k, val in d.items(): key = k if isinstance(key, str): with suppress(Exception): key = ast.literal_eval(key) if not ( isinstance(key, tuple) and len(key) == 3 and all(isinstance(x, int) for x in key) ): raise ValueError( f\"Invalid bucket key {k!r}. Expected tuple (H, W, T).\" ) out[(int(key[0]), int(key[1]), int(key[2]))] = float(val) return out if isinstance(v, dict): return normalize(v) if isinstance(v, str): # Python literal (supports tuple keys) parsed = ast.literal_eval(v) if not isinstance(parsed, dict): raise ValueError(\"Bucket config must parse to a dict.\") return normalize(parsed) raise ValueError(\"Unsupported value for --random-mm-bucket-config.\") random_mm_group.add_argument( \"--random-mm-bucket-config\", type=_parse_mm_bucket_config, default=RandomMultiModalDataset.DEFAULT_MM_ITEM_BUCKET_CONFIG, help=( \"The bucket config is a dictionary mapping a multimodal item\" \"sampling configuration to a probability.\" \"Currently allows for 2 modalities: images and videos. \" \"An bucket key is a tuple of (height, width, num_frames)\" \"The value is the probability of sampling that specific item. \" \"Example: \" \"--random-mm-bucket-config \" \"{(256, 256, 1): 0.5, (720, 1280, 1): 0.4, (720, 1280, 16): 0.10} \" \"First item: images with resolution 256x256 w.p. 0.5\" \"Second item: images with resolution 720x1280 w.p. 0.4 \" \"Third item: videos with resolution 720x1280 and 16 frames w.p. 0.1\" \"OBS.: If the probabilities do not sum to 1, they are normalized.\" \"OBS bis.: Only image sampling is supported for now.\" ), ) hf_group = parser.add_argument_group(\"hf dataset options\") hf_group.add_argument( \"--hf-subset\", type=str, default=None, help=\"Subset of the HF dataset.\" ) hf_group.add_argument( \"--hf-split\", type=str, default=None, help=\"Split of the HF dataset.\" ) hf_group.add_argument( \"--hf-name\", type=str, default=None, help=( \"Name of the dataset on HuggingFace \" \"(e.g., 'lmarena-ai/VisionArena-Chat'). \" \"Specify this if your dataset-path is a local path.\" ), ) hf_group.add_argument( \"--hf-output-len\", type=int, default=None, help=\"Output length for each request. Overrides the output lengths \" \"from the sampled HF dataset.\", ) prefix_repetition_group = parser.add_argument_group( \"prefix repetition dataset options\" ) prefix_repetition_group.add_argument( \"--prefix-repetition-prefix-len\", type=int, default=256, help=\"Number of prefix tokens per request, used only for prefix \" \"repetition dataset.\", ) prefix_repetition_group.add_argument( \"--prefix-repetition-suffix-len\", type=int, default=256, help=\"Number of suffix tokens per request, used only for prefix \" \"repetition dataset. Total input length is prefix_len + suffix_len.\", ) prefix_repetition_group.add_argument( \"--prefix-repetition-num-prefixes\", type=int, default=10, help=\"Number of prefixes to generate, used only for prefix repetition \" \"dataset. Prompts per prefix is num_requests // num_prefixes.\", ) prefix_repetition_group.add_argument( \"--prefix-repetition-output-len\", type=int, default=128, help=\"Number of output tokens per request, used only for prefix \" \"repetition dataset.\", ) gen_prompt_decode_to_target_len ¶ gen_prompt_decode_to_target_len( tokenizer: TokenizerLike, token_sequence: list[int], target_token_len: int, max_retry: int = 10, add_special_tokens: bool = False, rng: Generator | None = None, ) -> tuple[str, list[int]] Ensure decoded-then-encoded prompt length matches the target token length. This function decodes an initial token sequence to text and re-encodes it , iteratively adjusting the token sequence length to match a target. This is necessary because some tokenizers do not guarantee a 1:1 mapping between consecutive tokens and the decoded-then-encoded sequence length. For example, for GPT2Tokenizer: [6880, 6881] -> ['Ġcalls', 'here'] -> [1650, 939, 486] -> ['Ġcall', 'sh', 'ere'] Returns a tuple of the final prompt string and the adjusted token sequence. Source code in vllm/benchmarks/datasets.py 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433def gen_prompt_decode_to_target_len( tokenizer: TokenizerLike, token_sequence: list[int], target_token_len: int, max_retry: int = 10, add_special_tokens: bool = False, rng: np.random.Generator | None = None, ) -> tuple[str, list[int]]: \"\"\" Ensure decoded-then-encoded prompt length matches the target token length. This function decodes an initial token sequence to text and re-encodes it , iteratively adjusting the token sequence length to match a target. This is necessary because some tokenizers do not guarantee a 1:1 mapping between consecutive tokens and the decoded-then-encoded sequence length. For example, for GPT2Tokenizer: [6880, 6881] -> ['Ġcalls', 'here'] -> [1650, 939, 486] -> ['Ġcall', 'sh', 'ere'] Returns a tuple of the final prompt string and the adjusted token sequence. \"\"\" remain_num_try = max_retry token_mismatch = 0 while True: prompt = tokenizer.decode(token_sequence) token_sequence = tokenizer.encode(prompt, add_special_tokens=add_special_tokens) if remain_num_try <= 0: if len(token_sequence) != target_token_len: token_mismatch = len(token_sequence) - target_token_len break if len(token_sequence) == target_token_len: break elif len(token_sequence) < target_token_len: if rng is not None: extra_tokens = rng.integers( 0, tokenizer.vocab_size, size=target_token_len - len(token_sequence), ).tolist() else: extra_tokens = np.random.randint( 0, tokenizer.vocab_size, size=target_token_len - len(token_sequence), ).tolist() token_sequence.extend(extra_tokens) elif len(token_sequence) > target_token_len: token_sequence = token_sequence[:target_token_len] remain_num_try -= 1 return prompt, token_sequence, token_mismatch get_samples ¶ get_samples( args, tokenizer: TokenizerLike ) -> list[SampleRequest] Source code in vllm/benchmarks/datasets.py 1641 1642 1643 1644 1645 1646 1647 1648 1649 1650 1651 1652 1653 1654 1655 1656 1657 1658 1659 1660 1661 1662 1663 1664 1665 1666 1667 1668 1669 1670 1671 1672 1673 1674 1675 1676 1677 1678 1679 1680 1681 1682 1683 1684 1685 1686 1687 1688 1689 1690 1691 1692 1693 1694 1695 1696 1697 1698 1699 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728 1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 1742 1743 1744 1745 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 1793 1794 1795 1796 1797 1798 1799 1800 1801 1802 1803 1804 1805 1806 1807 1808 1809 1810 1811 1812 1813 1814 1815 1816 1817 1818 1819 1820 1821 1822 1823 1824 1825 1826 1827 1828 1829 1830 1831 1832 1833 1834 1835 1836 1837 1838 1839 1840 1841 1842 1843 1844 1845 1846 1847 1848 1849 1850 1851 1852 1853 1854 1855 1856 1857 1858 1859 1860 1861 1862 1863 1864 1865 1866 1867 1868 1869 1870 1871 1872 1873 1874 1875 1876 1877 1878 1879 1880 1881 1882 1883 1884 1885 1886 1887 1888 1889 1890 1891 1892 1893 1894 1895 1896 1897 1898 1899 1900 1901 1902 1903 1904 1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919def get_samples(args, tokenizer: TokenizerLike) -> list[SampleRequest]: if not hasattr(args, \"request_id_prefix\"): args.request_id_prefix = \"\" if args.dataset_name == \"custom\": dataset = CustomDataset( dataset_path=args.dataset_path, disable_shuffle=args.disable_shuffle ) input_requests = dataset.sample( num_requests=args.num_prompts, tokenizer=tokenizer, output_len=args.custom_output_len, skip_chat_template=args.skip_chat_template, request_id_prefix=args.request_id_prefix, no_oversample=args.no_oversample, ) elif args.dataset_name == \"sonnet\": dataset = SonnetDataset( dataset_path=args.dataset_path, disable_shuffle=args.disable_shuffle ) # For the \"sonnet\" dataset, formatting depends on the backend. if args.backend == \"openai-chat\": input_requests = dataset.sample( num_requests=args.num_prompts, input_len=args.sonnet_input_len, output_len=args.sonnet_output_len, prefix_len=args.sonnet_prefix_len, tokenizer=tokenizer, return_prompt_formatted=False, request_id_prefix=args.request_id_prefix, no_oversample=args.no_oversample, ) else: assert tokenizer.chat_template or tokenizer.default_chat_template, ( \"Tokenizer/model must have chat template for sonnet dataset.\" ) input_requests = dataset.sample( num_requests=args.num_prompts, input_len=args.sonnet_input_len, output_len=args.sonnet_output_len, prefix_len=args.sonnet_prefix_len, tokenizer=tokenizer, return_prompt_formatted=True, request_id_prefix=args.request_id_prefix, no_oversample=args.no_oversample, ) elif args.dataset_name == \"hf\": # all following datasets are implemented from the # HuggingFaceDataset base class hf_kwargs = {} if ( args.dataset_path in VisionArenaDataset.SUPPORTED_DATASET_PATHS or args.hf_name in VisionArenaDataset.SUPPORTED_DATASET_PATHS ): dataset_class = VisionArenaDataset args.hf_split = \"train\" args.hf_subset = None elif ( args.dataset_path in MMVUDataset.SUPPORTED_DATASET_PATHS or args.hf_name in MMVUDataset.SUPPORTED_DATASET_PATHS ): dataset_class = MMVUDataset args.hf_split = \"validation\" args.hf_subset = None elif ( args.dataset_path in InstructCoderDataset.SUPPORTED_DATASET_PATHS or args.hf_name in InstructCoderDataset.SUPPORTED_DATASET_PATHS ): dataset_class = InstructCoderDataset args.hf_split = \"train\" elif ( args.dataset_path in MTBenchDataset.SUPPORTED_DATASET_PATHS or args.hf_name in MTBenchDataset.SUPPORTED_DATASET_PATHS ): dataset_class = MTBenchDataset args.hf_split = \"train\" elif ( args.dataset_path in MultiModalConversationDataset.SUPPORTED_DATASET_PATHS or args.hf_name in MultiModalConversationDataset.SUPPORTED_DATASET_PATHS ): dataset_class = MultiModalConversationDataset elif ( args.dataset_path in ConversationDataset.SUPPORTED_DATASET_PATHS or args.hf_name in ConversationDataset.SUPPORTED_DATASET_PATHS ): dataset_class = ConversationDataset elif ( args.dataset_path in AIMODataset.SUPPORTED_DATASET_PATHS or args.hf_name in AIMODataset.SUPPORTED_DATASET_PATHS ): dataset_class = AIMODataset args.hf_split = \"train\" elif ( args.dataset_path in NextEditPredictionDataset.SUPPORTED_DATASET_PATHS # noqa: E501 or args.hf_name in NextEditPredictionDataset.SUPPORTED_DATASET_PATHS ): dataset_class = NextEditPredictionDataset args.hf_split = \"train\" elif ( args.dataset_path in ASRDataset.SUPPORTED_DATASET_PATHS or args.hf_name in ASRDataset.SUPPORTED_DATASET_PATHS ): dataset_class = ASRDataset args.hf_split = \"train\" elif args.dataset_path in BlazeditDataset.SUPPORTED_DATASET_PATHS: dataset_class = BlazeditDataset args.hf_split = \"train\" hf_kwargs = { \"min_distance\": args.blazedit_min_distance, \"max_distance\": args.blazedit_max_distance, } elif ( args.dataset_path in MLPerfDataset.SUPPORTED_DATASET_PATHS or args.hf_name in MLPerfDataset.SUPPORTED_DATASET_PATHS ): dataset_class = MLPerfDataset args.hf_split = \"train\" elif ( args.dataset_path in MMStarDataset.SUPPORTED_DATASET_PATHS or args.hf_name in MMStarDataset.SUPPORTED_DATASET_PATHS ): dataset_class = MMStarDataset args.hf_split = \"val\" args.hf_subset = None else: supported_datasets = set( [ dataset_name for cls in HuggingFaceDataset.__subclasses__() for dataset_name in cls.SUPPORTED_DATASET_PATHS ] ) raise ValueError( f\"Unsupported dataset path: {args.dataset_path}. \" \"Huggingface dataset only supports dataset_path\" f\" from one of following: {supported_datasets}. \" \"Please consider contributing if you would \" \"like to add support for additional dataset formats.\" ) if dataset_class.IS_MULTIMODAL and not ( args.backend in (\"openai-chat\", \"openai-audio\") or \"embeddings-\" in args.backend ): # multi-modal benchmark is only available on OpenAI Chat # endpoint-type. raise ValueError( \"Multi-modal content is only supported on 'openai-chat' and \" \"'openai-audio' backends.\" ) input_requests = dataset_class( dataset_path=args.dataset_path, dataset_subset=args.hf_subset, dataset_split=args.hf_split, random_seed=args.seed, no_stream=args.no_stream, hf_name=args.hf_name, disable_shuffle=args.disable_shuffle, ).sample( num_requests=args.num_prompts, tokenizer=tokenizer, output_len=args.hf_output_len, request_id_prefix=args.request_id_prefix, no_oversample=args.no_oversample, skip_chat_template=args.skip_chat_template, **hf_kwargs, ) else: # For datasets that follow a similar structure, use a mapping. dataset_mapping = { \"spec_bench\": lambda: SpecBench( dataset_path=args.dataset_path, category=args.spec_bench_category, disable_shuffle=args.disable_shuffle, ).sample( num_requests=args.num_prompts, tokenizer=tokenizer, output_len=args.spec_bench_output_len, request_id_prefix=args.request_id_prefix, no_oversample=args.no_oversample, ), \"sharegpt\": lambda: ShareGPTDataset( random_seed=args.seed, dataset_path=args.dataset_path, disable_shuffle=args.disable_shuffle, ).sample( tokenizer=tokenizer, num_requests=args.num_prompts, output_len=args.sharegpt_output_len, request_id_prefix=args.request_id_prefix, no_oversample=args.no_oversample, ), \"burstgpt\": lambda: BurstGPTDataset( random_seed=args.seed, dataset_path=args.dataset_path, disable_shuffle=args.disable_shuffle, ).sample( tokenizer=tokenizer, num_requests=args.num_prompts, request_id_prefix=args.request_id_prefix, no_oversample=args.no_oversample, ), \"random\": lambda: RandomDataset( random_seed=args.seed, dataset_path=args.dataset_path, disable_shuffle=args.disable_shuffle, ).sample( tokenizer=tokenizer, num_requests=args.num_prompts, prefix_len=args.random_prefix_len, input_len=args.random_input_len, output_len=args.random_output_len, range_ratio=args.random_range_ratio, request_id_prefix=args.request_id_prefix, batchsize=args.random_batch_size, no_oversample=args.no_oversample, ), \"random-mm\": lambda: RandomMultiModalDataset( random_seed=args.seed, dataset_path=args.dataset_path, disable_shuffle=args.disable_shuffle, ).sample( tokenizer=tokenizer, num_requests=args.num_prompts, prefix_len=args.random_prefix_len, range_ratio=args.random_range_ratio, input_len=args.random_input_len, output_len=args.random_output_len, base_items_per_request=args.random_mm_base_items_per_request, limit_mm_per_prompt=args.random_mm_limit_mm_per_prompt, num_mm_items_range_ratio=args.random_mm_num_mm_items_range_ratio, bucket_config=args.random_mm_bucket_config, request_id_prefix=args.request_id_prefix, no_oversample=args.no_oversample, ), \"random-rerank\": lambda: RandomDatasetForReranking( random_seed=args.seed, dataset_path=args.dataset_path, disable_shuffle=args.disable_shuffle, ).sample( tokenizer=tokenizer, num_requests=args.num_prompts, input_len=args.random_input_len, range_ratio=args.random_range_ratio, request_id_prefix=args.request_id_prefix, batchsize=args.random_batch_size, is_reranker=not args.no_reranker, ), \"prefix_repetition\": lambda: PrefixRepetitionRandomDataset( random_seed=args.seed, dataset_path=args.dataset_path, disable_shuffle=args.disable_shuffle, ).sample( tokenizer=tokenizer, num_requests=args.num_prompts, prefix_len=args.prefix_repetition_prefix_len, suffix_len=args.prefix_repetition_suffix_len, num_prefixes=args.prefix_repetition_num_prefixes, output_len=args.prefix_repetition_output_len, request_id_prefix=args.request_id_prefix, no_oversample=args.no_oversample, ), } try: # Enforce endpoint compatibility for multimodal datasets. if args.dataset_name == \"random-mm\" and args.backend not in [\"openai-chat\"]: raise ValueError( \"Multi-modal content (images) is only supported on \" \"'openai-chat' backend.\" ) input_requests = dataset_mapping[args.dataset_name]() except KeyError as err: raise ValueError(f\"Unknown dataset: {args.dataset_name}\") from err return input_requests is_valid_sequence ¶ is_valid_sequence( prompt_len: int, output_len: int, min_len: int = 4, max_prompt_len: int = 1024, max_total_len: int = 2048, skip_min_output_len_check: bool = False, ) -> bool Validate a sequence based on prompt and output lengths. Default pruning criteria are copied from the original sample_hf_requests and sample_sharegpt_requests functions in benchmark_serving.py, as well as from sample_requests in benchmark_throughput.py. Source code in vllm/benchmarks/datasets.py 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286def is_valid_sequence( prompt_len: int, output_len: int, min_len: int = 4, max_prompt_len: int = 1024, max_total_len: int = 2048, skip_min_output_len_check: bool = False, ) -> bool: \"\"\" Validate a sequence based on prompt and output lengths. Default pruning criteria are copied from the original `sample_hf_requests` and `sample_sharegpt_requests` functions in benchmark_serving.py, as well as from `sample_requests` in benchmark_throughput.py. \"\"\" # Check for invalid conditions prompt_too_short = prompt_len < min_len output_too_short = (not skip_min_output_len_check) and (output_len < min_len) prompt_too_long = prompt_len > max_prompt_len combined_too_long = (prompt_len + output_len) > max_total_len # Return True if none of the invalid conditions are met return not ( prompt_too_short or output_too_short or prompt_too_long or combined_too_long ) lora_path_on_disk cached ¶ lora_path_on_disk(lora_path: str) -> str Source code in vllm/benchmarks/datasets.py 289 290 291@cache def lora_path_on_disk(lora_path: str) -> str: return get_adapter_absolute_path(lora_path) process_image ¶ process_image(image: Any) -> Mapping[str, Any] Process a single image input and return a multimedia content dictionary. Supports the following input types: Dictionary with raw image bytes: - Expects a dict with a 'bytes' key containing raw image data. - Loads the bytes as a PIL.Image.Image. PIL.Image.Image input: - Converts the image to RGB. - Saves the image as a JPEG in memory. - Encodes the JPEG data as a base64 string. - Returns a dictionary with the image as a base64 data URL. String input: - Treats the string as a URL or local file path. - Prepends \"file://\" if the string doesn't start with \"http://\" or \"file://\". - Returns a dictionary with the image URL. Raises: Type Description ValueError If the input is not a supported type. Source code in vllm/benchmarks/datasets.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341def process_image(image: Any) -> Mapping[str, Any]: \"\"\" Process a single image input and return a multimedia content dictionary. Supports the following input types: 1. Dictionary with raw image bytes: - Expects a dict with a 'bytes' key containing raw image data. - Loads the bytes as a PIL.Image.Image. 2. PIL.Image.Image input: - Converts the image to RGB. - Saves the image as a JPEG in memory. - Encodes the JPEG data as a base64 string. - Returns a dictionary with the image as a base64 data URL. 3. String input: - Treats the string as a URL or local file path. - Prepends \"file://\" if the string doesn't start with \"http://\" or \"file://\". - Returns a dictionary with the image URL. Raises: ValueError: If the input is not a supported type. \"\"\" if isinstance(image, dict) and \"bytes\" in image: image = Image.open(BytesIO(image[\"bytes\"])) if isinstance(image, Image.Image): image = convert_image_mode(image, \"RGB\") with io.BytesIO() as image_data: image.save(image_data, format=\"JPEG\") image_base64 = base64.b64encode(image_data.getvalue()).decode(\"utf-8\") return { \"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_base64}\"}, } if isinstance(image, str): image_url = ( image if image.startswith((\"http://\", \"https://\", \"file://\")) else f\"file://{image}\" ) return {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}} raise ValueError( f\"Invalid image input {image}. Must be a PIL.Image.Image\" \" or str or dictionary with raw image bytes.\" ) process_video ¶ process_video(video: Any) -> Mapping[str, Any] Process a single video input and return a multimedia content dictionary. Supports the following input types: Dictionary with raw video bytes: - Expects a dict with a 'bytes' key containing raw video data. String input: - Treats the string as a URL or local file path. - Prepends \"file://\" if the string doesn't start with \"http://\" or \"file://\". - Returns a dictionary with the image URL. Raises: Type Description ValueError If the input is not a supported type. Source code in vllm/benchmarks/datasets.py 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378def process_video(video: Any) -> Mapping[str, Any]: \"\"\" Process a single video input and return a multimedia content dictionary. Supports the following input types: 1. Dictionary with raw video bytes: - Expects a dict with a 'bytes' key containing raw video data. 2. String input: - Treats the string as a URL or local file path. - Prepends \"file://\" if the string doesn't start with \"http://\" or \"file://\". - Returns a dictionary with the image URL. Raises: ValueError: If the input is not a supported type. \"\"\" if isinstance(video, dict) and \"bytes\" in video: video_bytes = video[\"bytes\"] video_base64 = base64.b64encode(video_bytes).decode(\"utf-8\") return { \"type\": \"video_url\", \"video_url\": {\"url\": f\"data:video/mp4;base64,{video_base64}\"}, } if isinstance(video, str): video_url = ( video if video.startswith((\"http://\", \"https://\", \"file://\")) else f\"file://{video}\" ) return {\"type\": \"video_url\", \"video_url\": {\"url\": video_url}} raise ValueError( f\"Invalid video input {video}. Must be a string of local path/remote url, or a dictionary with raw video bytes in the form of `{{'bytes': raw_video_bytes}}`.\" # noqa: E501 ) December 25, 2025",
      "code": ""
    },
    {
      "description": "vllm.benchmarks.datasets ¶ This module defines a framework for sampling benchmark requests from various datasets. Each dataset subclass of BenchmarkDataset must implement sample generation. Supported dataset types include: - ShareGPT - Random (synthetic) - Sonnet - BurstGPT - HuggingFace - VisionArena datasets module-attribute ¶ datasets = PlaceholderModule('datasets') logger module-attribute ¶ logger = getLogger(__name__) lora_tokenizer_cache module-attribute ¶ lora_tokenizer_cache: dict[int, TokenizerLike] = {} zeta_prompt module-attribute ¶ zeta_prompt = \"### Instruction:\\nYou are a code completion assistant and your task is to analyze user edits and then rewrite an excerpt that the user provides, suggesting the appropriate edits within the excerpt, taking into account the cursor location.\\n\\n### User Edits:\\n\\n{}\\n\\n### User Excerpt:\\n\\n{}\\n\\n### Response:\\n\\n\" AIMODataset ¶ Bases: HuggingFaceDataset Dataset class for processing a AIMO dataset with reasoning questions. Source code in vllm/benchmarks/datasets.py 2736 2737 2738 2739 2740 2741 2742 2743 2744 2745 2746 2747 2748 2749 2750 2751 2752 2753 2754 2755 2756 2757 2758 2759 2760 2761 2762 2763 2764 2765 2766 2767 2768 2769 2770 2771 2772 2773 2774 2775 2776 2777 2778 2779 2780 2781 2782 2783 2784 2785 2786 2787 2788class AIMODataset(HuggingFaceDataset): \"\"\" Dataset class for processing a AIMO dataset with reasoning questions. \"\"\" SUPPORTED_DATASET_PATHS = { \"AI-MO/aimo-validation-aime\", \"AI-MO/NuminaMath-1.5\", \"AI-MO/NuminaMath-CoT\", } def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: sampled_requests = [] ind = 0 dynamic_output = output_len is None for item in self.data: if len(sampled_requests) >= num_requests: break prompt, completion = item[\"problem\"], item[\"solution\"] prompt_ids = tokenizer(prompt).input_ids completion_ids = tokenizer(completion).input_ids prompt_len = len(prompt_ids) completion_len = len(completion_ids) output_len = completion_len if dynamic_output else output_len assert isinstance(output_len, int) and output_len > 0 if dynamic_output and not is_valid_sequence( prompt_len, completion_len, max_prompt_len=2048, max_total_len=32000 ): continue sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=None, request_id=request_id_prefix + str(ind), ) ) ind += 1 self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = { \"AI-MO/aimo-validation-aime\", \"AI-MO/NuminaMath-1.5\", \"AI-MO/NuminaMath-CoT\", } sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 2747 2748 2749 2750 2751 2752 2753 2754 2755 2756 2757 2758 2759 2760 2761 2762 2763 2764 2765 2766 2767 2768 2769 2770 2771 2772 2773 2774 2775 2776 2777 2778 2779 2780 2781 2782 2783 2784 2785 2786 2787 2788def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: sampled_requests = [] ind = 0 dynamic_output = output_len is None for item in self.data: if len(sampled_requests) >= num_requests: break prompt, completion = item[\"problem\"], item[\"solution\"] prompt_ids = tokenizer(prompt).input_ids completion_ids = tokenizer(completion).input_ids prompt_len = len(prompt_ids) completion_len = len(completion_ids) output_len = completion_len if dynamic_output else output_len assert isinstance(output_len, int) and output_len > 0 if dynamic_output and not is_valid_sequence( prompt_len, completion_len, max_prompt_len=2048, max_total_len=32000 ): continue sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=None, request_id=request_id_prefix + str(ind), ) ) ind += 1 self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests ASRDataset ¶ Bases: HuggingFaceDataset Dataset class for processing a ASR dataset for transcription. Tested on the following set: +----------------+----------------------------------------+--------------------------+-----------------------------+ | Dataset | Domain | Speaking Style | hf-subset | +----------------+----------------------------------------+--------------------------+-----------------------------+ | TED-LIUM | TED talks | Oratory | release1, release2, release3| | | | | release3-speaker-adaptation | | VoxPopuli | European Parliament | Oratory | en, de, it, fr, ... | | LibriSpeech | Audiobook | Narrated | \"LIUM/tedlium\" | | GigaSpeech | Audiobook, podcast, YouTube | Narrated, spontaneous | xs, s, m, l, xl, dev, test | | SPGISpeech | Financial meetings | Oratory, spontaneous | S, M, L, dev, test | | AMI | Meetings | Spontaneous | ihm, sdm | +----------------+----------------------------------------+--------------------------+-----------------------------+ Source code in vllm/benchmarks/datasets.py 2894 2895 2896 2897 2898 2899 2900 2901 2902 2903 2904 2905 2906 2907 2908 2909 2910 2911 2912 2913 2914 2915 2916 2917 2918 2919 2920 2921 2922 2923 2924 2925 2926 2927 2928 2929 2930 2931 2932 2933 2934 2935 2936 2937 2938 2939 2940 2941 2942 2943 2944 2945 2946 2947 2948 2949 2950 2951 2952 2953 2954 2955 2956 2957 2958 2959 2960 2961 2962 2963 2964 2965 2966 2967 2968 2969 2970 2971 2972 2973 2974 2975 2976class ASRDataset(HuggingFaceDataset): \"\"\" Dataset class for processing a ASR dataset for transcription. Tested on the following set: +----------------+----------------------------------------+--------------------------+-----------------------------+ | Dataset | Domain | Speaking Style | hf-subset | +----------------+----------------------------------------+--------------------------+-----------------------------+ | TED-LIUM | TED talks | Oratory | release1, release2, release3| | | | | release3-speaker-adaptation | | VoxPopuli | European Parliament | Oratory | en, de, it, fr, ... | | LibriSpeech | Audiobook | Narrated | \"LIUM/tedlium\" | | GigaSpeech | Audiobook, podcast, YouTube | Narrated, spontaneous | xs, s, m, l, xl, dev, test | | SPGISpeech | Financial meetings | Oratory, spontaneous | S, M, L, dev, test | | AMI | Meetings | Spontaneous | ihm, sdm | +----------------+----------------------------------------+--------------------------+-----------------------------+ \"\"\" # noqa: E501 SUPPORTED_DATASET_PATHS = { \"openslr/librispeech_asr\", \"facebook/voxpopuli\", \"LIUM/tedlium\", \"edinburghcstr/ami\", \"speechcolab/gigaspeech\", \"kensho/spgispeech\", } DEFAULT_OUTPUT_LEN = 128 IS_MULTIMODAL = True # TODO Whisper-specific. Abstract interface when more models are supported. TRANSCRIPTION_PREAMBLE = \"<|startoftranscript|><|en|><|transcribe|><|notimestamps|>\" skip_long_audios: bool = True def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN prompt = ASRDataset.TRANSCRIPTION_PREAMBLE prompt_len = len(tokenizer(prompt).input_ids) sampled_requests = [] ind = 0 skipped = 0 for item in self.data: if len(sampled_requests) >= num_requests: break audio = item[\"audio\"] y, sr = audio[\"array\"], audio[\"sampling_rate\"] duration_s = librosa.get_duration(y=y, sr=sr) # Whisper max supported duration if self.skip_long_audios and duration_s > 30: skipped += 1 continue mm_content = {\"audio\": (y, sr)} sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_content, request_id=request_id_prefix + str(ind), ) ) ind += 1 if skipped: logger.warning( \"%d samples discarded from dataset due to\" \" their length being greater than\" \" what Whisper supports.\", skipped, ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶ DEFAULT_OUTPUT_LEN = 128 IS_MULTIMODAL class-attribute instance-attribute ¶ IS_MULTIMODAL = True SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = { \"openslr/librispeech_asr\", \"facebook/voxpopuli\", \"LIUM/tedlium\", \"edinburghcstr/ami\", \"speechcolab/gigaspeech\", \"kensho/spgispeech\", } TRANSCRIPTION_PREAMBLE class-attribute instance-attribute ¶ TRANSCRIPTION_PREAMBLE = \"<|startoftranscript|><|en|><|transcribe|><|notimestamps|>\" skip_long_audios class-attribute instance-attribute ¶ skip_long_audios: bool = True sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 2929 2930 2931 2932 2933 2934 2935 2936 2937 2938 2939 2940 2941 2942 2943 2944 2945 2946 2947 2948 2949 2950 2951 2952 2953 2954 2955 2956 2957 2958 2959 2960 2961 2962 2963 2964 2965 2966 2967 2968 2969 2970 2971 2972 2973 2974 2975 2976def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN prompt = ASRDataset.TRANSCRIPTION_PREAMBLE prompt_len = len(tokenizer(prompt).input_ids) sampled_requests = [] ind = 0 skipped = 0 for item in self.data: if len(sampled_requests) >= num_requests: break audio = item[\"audio\"] y, sr = audio[\"array\"], audio[\"sampling_rate\"] duration_s = librosa.get_duration(y=y, sr=sr) # Whisper max supported duration if self.skip_long_audios and duration_s > 30: skipped += 1 continue mm_content = {\"audio\": (y, sr)} sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_content, request_id=request_id_prefix + str(ind), ) ) ind += 1 if skipped: logger.warning( \"%d samples discarded from dataset due to\" \" their length being greater than\" \" what Whisper supports.\", skipped, ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests BenchmarkDataset ¶ Bases: ABC Source code in vllm/benchmarks/datasets.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254class BenchmarkDataset(ABC): DEFAULT_SEED = 0 IS_MULTIMODAL = False def __init__( self, dataset_path: str | None = None, random_seed: int = DEFAULT_SEED, disable_shuffle: bool = False, **kwargs, ) -> None: \"\"\" Initialize the BenchmarkDataset with an optional dataset path and random seed. Args: dataset_path (Optional[str]): Path to the dataset. If None, it indicates that a default or random dataset might be used. random_seed (int): Seed value for reproducible shuffling or sampling. Defaults to DEFAULT_SEED. \"\"\" self.dataset_path = dataset_path # Set the random seed, ensuring that a None value is replaced with the # default seed. self.random_seed = random_seed if random_seed is not None else self.DEFAULT_SEED self.disable_shuffle = disable_shuffle self.data = None def apply_multimodal_chat_transformation( self, prompt: str, mm_content: MultiModalDataDict | dict | list[dict] | None = None, ) -> list[dict]: \"\"\" Transform a prompt and optional multimodal content into a chat format. This method is used for chat models that expect a specific conversation format. \"\"\" content = [{\"text\": prompt, \"type\": \"text\"}] if mm_content is not None: if isinstance(mm_content, list): content.extend(cast(list[dict[str, Any]], mm_content)) elif isinstance(mm_content, dict): content.append(mm_content) else: raise TypeError( \"Could not process multimodal content of type: \" + f\"{type(mm_content)}\" ) return [{\"role\": \"user\", \"content\": content}] def load_data(self) -> None: \"\"\" Load data from the dataset path into self.data. This method must be overridden by subclasses since the method to load data will vary depending on the dataset format and source. Raises: NotImplementedError: If a subclass does not implement this method. \"\"\" # TODO (jenniferzhao): add support for downloading data raise NotImplementedError(\"load_data must be implemented in subclasses.\") def get_random_lora_request( self, max_loras: int | None = None, lora_path: str | None = None, ) -> LoRARequest | None: \"\"\" Optionally select a random LoRA request. This method is used when LoRA parameters are provided. It randomly selects a LoRA based on max_loras. Args: max_loras (Optional[int]): The maximum number of LoRAs available. If `None`, LoRA is not used. lora_path (Optional[str]): Path to the LoRA parameters on disk. If `None`, LoRA is not used. Returns: A new [`LoRARequest`][vllm.lora.request.LoRARequest] (or `None` if not applicable). \"\"\" if max_loras is None or lora_path is None: return None # Generate a random LoRA ID in the range [1, max_loras]. lora_id = random.randint(1, max_loras) lora_request = LoRARequest( lora_name=str(lora_id), lora_int_id=lora_id, lora_path=lora_path_on_disk(lora_path), ) return lora_request @abstractmethod def sample( self, tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, ) -> list[SampleRequest]: \"\"\" Abstract method to generate sample requests from the dataset. Subclasses must override this method to implement dataset-specific logic for generating a list of SampleRequest objects. Args: tokenizer (TokenizerLike): The tokenizer to be used for processing the dataset's text. num_requests (int): The number of sample requests to generate. request_id_prefix (str): The prefix of request_id. Returns: list[SampleRequest]: A list of sample requests generated from the dataset. \"\"\" raise NotImplementedError(\"sample must be implemented in subclasses.\") def maybe_oversample_requests( self, requests: list[SampleRequest], num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, ) -> None: \"\"\" Oversamples the list of requests if its size is less than the desired number. Args: requests (List[SampleRequest]): The current list of sampled requests. num_requests (int): The target number of requests. request_id_prefix (str): The prefix applied to generated request identifiers. \"\"\" if no_oversample: logger.info(\"Skipping oversampling. Total samples: %d.\", len(requests)) return if len(requests) < num_requests: random.seed(self.random_seed) needed = num_requests - len(requests) additional = [] for i in range(needed): req = deepcopy(random.choice(requests)) req.request_id = request_id_prefix + str(len(requests) + i) additional.append(req) requests.extend(additional) logger.info(\"Oversampled requests to reach %d total samples.\", num_requests) ids = [req.request_id for req in requests] if len(ids) != len(set(ids)): raise ValueError( \"Duplicate request_id found in the sampled \" \"requests. Please ensure that each request_id \" \"is unique.\" ) DEFAULT_SEED class-attribute instance-attribute ¶ DEFAULT_SEED = 0 IS_MULTIMODAL class-attribute instance-attribute ¶ IS_MULTIMODAL = False data instance-attribute ¶ data = None dataset_path instance-attribute ¶ dataset_path = dataset_path disable_shuffle instance-attribute ¶ disable_shuffle = disable_shuffle random_seed instance-attribute ¶ random_seed = ( random_seed if random_seed is not None else DEFAULT_SEED ) __init__ ¶ __init__( dataset_path: str | None = None, random_seed: int = DEFAULT_SEED, disable_shuffle: bool = False, **kwargs, ) -> None Initialize the BenchmarkDataset with an optional dataset path and random seed. Parameters: Name Type Description Default dataset_path Optional[str] Path to the dataset. If None, it indicates that a default or random dataset might be used. None random_seed int Seed value for reproducible shuffling or sampling. Defaults to DEFAULT_SEED. DEFAULT_SEED Source code in vllm/benchmarks/datasets.py 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117def __init__( self, dataset_path: str | None = None, random_seed: int = DEFAULT_SEED, disable_shuffle: bool = False, **kwargs, ) -> None: \"\"\" Initialize the BenchmarkDataset with an optional dataset path and random seed. Args: dataset_path (Optional[str]): Path to the dataset. If None, it indicates that a default or random dataset might be used. random_seed (int): Seed value for reproducible shuffling or sampling. Defaults to DEFAULT_SEED. \"\"\" self.dataset_path = dataset_path # Set the random seed, ensuring that a None value is replaced with the # default seed. self.random_seed = random_seed if random_seed is not None else self.DEFAULT_SEED self.disable_shuffle = disable_shuffle self.data = None apply_multimodal_chat_transformation ¶ apply_multimodal_chat_transformation( prompt: str, mm_content: MultiModalDataDict | dict | list[dict] | None = None, ) -> list[dict] Transform a prompt and optional multimodal content into a chat format. This method is used for chat models that expect a specific conversation format. Source code in vllm/benchmarks/datasets.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140def apply_multimodal_chat_transformation( self, prompt: str, mm_content: MultiModalDataDict | dict | list[dict] | None = None, ) -> list[dict]: \"\"\" Transform a prompt and optional multimodal content into a chat format. This method is used for chat models that expect a specific conversation format. \"\"\" content = [{\"text\": prompt, \"type\": \"text\"}] if mm_content is not None: if isinstance(mm_content, list): content.extend(cast(list[dict[str, Any]], mm_content)) elif isinstance(mm_content, dict): content.append(mm_content) else: raise TypeError( \"Could not process multimodal content of type: \" + f\"{type(mm_content)}\" ) return [{\"role\": \"user\", \"content\": content}] get_random_lora_request ¶ get_random_lora_request( max_loras: int | None = None, lora_path: str | None = None, ) -> LoRARequest | None Optionally select a random LoRA request. This method is used when LoRA parameters are provided. It randomly selects a LoRA based on max_loras. Parameters: Name Type Description Default max_loras Optional[int] The maximum number of LoRAs available. If None, LoRA is not used. None lora_path Optional[str] Path to the LoRA parameters on disk. If None, LoRA is not used. None Returns: Type Description LoRARequest | None A new LoRARequest LoRARequest | None (or None if not applicable). Source code in vllm/benchmarks/datasets.py 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186def get_random_lora_request( self, max_loras: int | None = None, lora_path: str | None = None, ) -> LoRARequest | None: \"\"\" Optionally select a random LoRA request. This method is used when LoRA parameters are provided. It randomly selects a LoRA based on max_loras. Args: max_loras (Optional[int]): The maximum number of LoRAs available. If `None`, LoRA is not used. lora_path (Optional[str]): Path to the LoRA parameters on disk. If `None`, LoRA is not used. Returns: A new [`LoRARequest`][vllm.lora.request.LoRARequest] (or `None` if not applicable). \"\"\" if max_loras is None or lora_path is None: return None # Generate a random LoRA ID in the range [1, max_loras]. lora_id = random.randint(1, max_loras) lora_request = LoRARequest( lora_name=str(lora_id), lora_int_id=lora_id, lora_path=lora_path_on_disk(lora_path), ) return lora_request load_data ¶ load_data() -> None Load data from the dataset path into self.data. This method must be overridden by subclasses since the method to load data will vary depending on the dataset format and source. Raises: Type Description NotImplementedError If a subclass does not implement this method. Source code in vllm/benchmarks/datasets.py 142 143 144 145 146 147 148 149 150 151 152 153def load_data(self) -> None: \"\"\" Load data from the dataset path into self.data. This method must be overridden by subclasses since the method to load data will vary depending on the dataset format and source. Raises: NotImplementedError: If a subclass does not implement this method. \"\"\" # TODO (jenniferzhao): add support for downloading data raise NotImplementedError(\"load_data must be implemented in subclasses.\") maybe_oversample_requests ¶ maybe_oversample_requests( requests: list[SampleRequest], num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, ) -> None Oversamples the list of requests if its size is less than the desired number. Parameters: Name Type Description Default requests List[SampleRequest] The current list of sampled requests. required num_requests int The target number of requests. required request_id_prefix str The prefix applied to generated request identifiers. '' Source code in vllm/benchmarks/datasets.py 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254def maybe_oversample_requests( self, requests: list[SampleRequest], num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, ) -> None: \"\"\" Oversamples the list of requests if its size is less than the desired number. Args: requests (List[SampleRequest]): The current list of sampled requests. num_requests (int): The target number of requests. request_id_prefix (str): The prefix applied to generated request identifiers. \"\"\" if no_oversample: logger.info(\"Skipping oversampling. Total samples: %d.\", len(requests)) return if len(requests) < num_requests: random.seed(self.random_seed) needed = num_requests - len(requests) additional = [] for i in range(needed): req = deepcopy(random.choice(requests)) req.request_id = request_id_prefix + str(len(requests) + i) additional.append(req) requests.extend(additional) logger.info(\"Oversampled requests to reach %d total samples.\", num_requests) ids = [req.request_id for req in requests] if len(ids) != len(set(ids)): raise ValueError( \"Duplicate request_id found in the sampled \" \"requests. Please ensure that each request_id \" \"is unique.\" ) sample abstractmethod ¶ sample( tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, ) -> list[SampleRequest] Abstract method to generate sample requests from the dataset. Subclasses must override this method to implement dataset-specific logic for generating a list of SampleRequest objects. Parameters: Name Type Description Default tokenizer TokenizerLike The tokenizer to be used for processing the dataset's text. required num_requests int The number of sample requests to generate. required request_id_prefix str The prefix of request_id. '' Returns: Type Description list[SampleRequest] list[SampleRequest]: A list of sample requests generated from the list[SampleRequest] dataset. Source code in vllm/benchmarks/datasets.py 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212@abstractmethod def sample( self, tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, ) -> list[SampleRequest]: \"\"\" Abstract method to generate sample requests from the dataset. Subclasses must override this method to implement dataset-specific logic for generating a list of SampleRequest objects. Args: tokenizer (TokenizerLike): The tokenizer to be used for processing the dataset's text. num_requests (int): The number of sample requests to generate. request_id_prefix (str): The prefix of request_id. Returns: list[SampleRequest]: A list of sample requests generated from the dataset. \"\"\" raise NotImplementedError(\"sample must be implemented in subclasses.\") BlazeditDataset ¶ Bases: HuggingFaceDataset Blazedit Dataset. https://github.com/ise-uiuc/blazedit 5k char version: vdaita/edit_5k_char 10k char version: vdaita/edit_10k_char Source code in vllm/benchmarks/datasets.py 2647 2648 2649 2650 2651 2652 2653 2654 2655 2656 2657 2658 2659 2660 2661 2662 2663 2664 2665 2666 2667 2668 2669 2670 2671 2672 2673 2674 2675 2676 2677 2678 2679 2680 2681 2682 2683 2684 2685 2686 2687 2688 2689 2690 2691 2692 2693 2694 2695 2696 2697 2698 2699 2700 2701 2702 2703 2704 2705 2706 2707 2708 2709 2710 2711 2712 2713 2714 2715 2716 2717 2718 2719 2720 2721 2722 2723 2724 2725 2726 2727 2728class BlazeditDataset(HuggingFaceDataset): \"\"\" Blazedit Dataset. https://github.com/ise-uiuc/blazedit 5k char version: vdaita/edit_5k_char 10k char version: vdaita/edit_10k_char \"\"\" # noqa: E501 # 5k char version will have output as ~5k chars # 10k char version will have output as ~10k chars # Assuming 3 char per token, 10k chars will be 3333 tokens # We set default to 4000 to be safe DEFAULT_OUTPUT_LEN = 4000 SUPPORTED_DATASET_PATHS = { \"vdaita/edit_5k_char\", \"vdaita/edit_10k_char\", } def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, min_distance: float = 0.0, max_distance: float = 1.0, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break code = item[\"code\"] change_request = item[\"change_request\"] norm_distance = item[\"norm_distance\"] # compare the levenshtein distance normalized by code length if norm_distance < min_distance or norm_distance > max_distance: continue # template copied from # https://github.com/ise-uiuc/blazedit/blob/7765137e656fd62de877422d2e4cf8de51228054/dataset/create_refined_dataset.py#L94-L105 # noqa: E501 prompt = f\"\"\"Given a code file, please apply the change requests and generate the new file. Original file: ```python {code} ``` Change request: {change_request} Please generate the new code file in the \"New file\" section below.\"\"\" # noqa: E501 # apply template if not skip_chat_template: prompt = tokenizer.apply_chat_template( [{\"role\": \"user\", \"content\": prompt}], add_generation_prompt=True, tokenize=False, ) prompt_len = len(tokenizer(prompt).input_ids) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶ DEFAULT_OUTPUT_LEN = 4000 SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = { \"vdaita/edit_5k_char\", \"vdaita/edit_10k_char\", } sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, min_distance: float = 0.0, max_distance: float = 1.0, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 2666 2667 2668 2669 2670 2671 2672 2673 2674 2675 2676 2677 2678 2679 2680 2681 2682 2683 2684 2685 2686 2687 2688 2689 2690 2691 2692 2693 2694 2695 2696 2697 2698 2699 2700 2701 2702 2703 2704 2705 2706 2707 2708 2709 2710 2711 2712 2713 2714 2715 2716 2717 2718 2719 2720 2721 2722 2723 2724 2725 2726 2727 2728 def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, min_distance: float = 0.0, max_distance: float = 1.0, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break code = item[\"code\"] change_request = item[\"change_request\"] norm_distance = item[\"norm_distance\"] # compare the levenshtein distance normalized by code length if norm_distance < min_distance or norm_distance > max_distance: continue # template copied from # https://github.com/ise-uiuc/blazedit/blob/7765137e656fd62de877422d2e4cf8de51228054/dataset/create_refined_dataset.py#L94-L105 # noqa: E501 prompt = f\"\"\"Given a code file, please apply the change requests and generate the new file. Original file: ```python {code} ``` Change request: {change_request} Please generate the new code file in the \"New file\" section below.\"\"\" # noqa: E501 # apply template if not skip_chat_template: prompt = tokenizer.apply_chat_template( [{\"role\": \"user\", \"content\": prompt}], add_generation_prompt=True, tokenize=False, ) prompt_len = len(tokenizer(prompt).input_ids) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests BurstGPTDataset ¶ Bases: BenchmarkDataset Implements the BurstGPT dataset. Loads data from a CSV file and generates sample requests based on synthetic prompt generation. Only rows with Model \"GPT-4\" and positive response tokens are used. Source code in vllm/benchmarks/datasets.py 2170 2171 2172 2173 2174 2175 2176 2177 2178 2179 2180 2181 2182 2183 2184 2185 2186 2187 2188 2189 2190 2191 2192 2193 2194 2195 2196 2197 2198 2199 2200 2201 2202 2203 2204 2205 2206 2207 2208 2209 2210 2211 2212 2213 2214 2215 2216 2217 2218 2219 2220 2221 2222 2223 2224 2225 2226 2227 2228 2229 2230 2231 2232 2233 2234 2235 2236 2237 2238 2239class BurstGPTDataset(BenchmarkDataset): \"\"\" Implements the BurstGPT dataset. Loads data from a CSV file and generates sample requests based on synthetic prompt generation. Only rows with Model \"GPT-4\" and positive response tokens are used. \"\"\" def __init__(self, **kwargs) -> None: super().__init__(**kwargs) self.load_data() def load_data( self, ): if self.dataset_path is None: raise ValueError(\"dataset_path must be provided for loading data.\") df = pd.read_csv(self.dataset_path) # Filter to keep only GPT-4 rows. gpt4_df = df[df[\"Model\"] == \"GPT-4\"] # Remove failed requests (where Response tokens is 0 or less). gpt4_df = gpt4_df[gpt4_df[\"Response tokens\"] > 0] # Sample the desired number of rows. self.data = gpt4_df def _sample_loaded_data(self, num_requests: int) -> list: if num_requests <= len(self.data): data = self.data.sample(n=num_requests, random_state=self.random_seed) else: data = self.data.sample( n=num_requests, random_state=self.random_seed, replace=True, ) # Convert the dataframe to a list of lists. return data.values.tolist() def sample( self, tokenizer: TokenizerLike, num_requests: int, max_loras: int | None = None, lora_path: str | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest]: samples = [] data = self._sample_loaded_data(num_requests=num_requests) for i in range(num_requests): input_len = int(data[i][2]) output_len = int(data[i][3]) lora_req = self.get_random_lora_request( max_loras=max_loras, lora_path=lora_path ) vocab_size = tokenizer.vocab_size # Generate a synthetic prompt: a list of token IDs computed as (i + # j) modulo vocab_size. token_ids = [(i + j) % vocab_size for j in range(input_len)] prompt = tokenizer.decode(token_ids) samples.append( SampleRequest( prompt=prompt, prompt_len=input_len, expected_output_len=output_len, lora_request=lora_req, request_id=request_id_prefix + str(i), ) ) return samples __init__ ¶ __init__(**kwargs) -> None Source code in vllm/benchmarks/datasets.py 2177 2178 2179def __init__(self, **kwargs) -> None: super().__init__(**kwargs) self.load_data() _sample_loaded_data ¶ _sample_loaded_data(num_requests: int) -> list Source code in vllm/benchmarks/datasets.py 2195 2196 2197 2198 2199 2200 2201 2202 2203 2204 2205def _sample_loaded_data(self, num_requests: int) -> list: if num_requests <= len(self.data): data = self.data.sample(n=num_requests, random_state=self.random_seed) else: data = self.data.sample( n=num_requests, random_state=self.random_seed, replace=True, ) # Convert the dataframe to a list of lists. return data.values.tolist() load_data ¶ load_data() Source code in vllm/benchmarks/datasets.py 2181 2182 2183 2184 2185 2186 2187 2188 2189 2190 2191 2192 2193def load_data( self, ): if self.dataset_path is None: raise ValueError(\"dataset_path must be provided for loading data.\") df = pd.read_csv(self.dataset_path) # Filter to keep only GPT-4 rows. gpt4_df = df[df[\"Model\"] == \"GPT-4\"] # Remove failed requests (where Response tokens is 0 or less). gpt4_df = gpt4_df[gpt4_df[\"Response tokens\"] > 0] # Sample the desired number of rows. self.data = gpt4_df sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, max_loras: int | None = None, lora_path: str | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest] Source code in vllm/benchmarks/datasets.py 2207 2208 2209 2210 2211 2212 2213 2214 2215 2216 2217 2218 2219 2220 2221 2222 2223 2224 2225 2226 2227 2228 2229 2230 2231 2232 2233 2234 2235 2236 2237 2238 2239def sample( self, tokenizer: TokenizerLike, num_requests: int, max_loras: int | None = None, lora_path: str | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest]: samples = [] data = self._sample_loaded_data(num_requests=num_requests) for i in range(num_requests): input_len = int(data[i][2]) output_len = int(data[i][3]) lora_req = self.get_random_lora_request( max_loras=max_loras, lora_path=lora_path ) vocab_size = tokenizer.vocab_size # Generate a synthetic prompt: a list of token IDs computed as (i + # j) modulo vocab_size. token_ids = [(i + j) % vocab_size for j in range(input_len)] prompt = tokenizer.decode(token_ids) samples.append( SampleRequest( prompt=prompt, prompt_len=input_len, expected_output_len=output_len, lora_request=lora_req, request_id=request_id_prefix + str(i), ) ) return samples ConversationDataset ¶ Bases: HuggingFaceDataset Dataset for text-only conversation data. Source code in vllm/benchmarks/datasets.py 2284 2285 2286 2287 2288 2289 2290 2291 2292 2293 2294 2295 2296 2297 2298 2299 2300 2301 2302 2303 2304 2305 2306 2307 2308 2309 2310 2311 2312 2313 2314 2315 2316 2317 2318 2319 2320 2321 2322 2323 2324 2325 2326 2327 2328 2329 2330 2331 2332 2333 2334 2335 2336 2337 2338 2339 2340 2341class ConversationDataset(HuggingFaceDataset): \"\"\"Dataset for text-only conversation data.\"\"\" SUPPORTED_DATASET_PATHS = { \"Aeala/ShareGPT_Vicuna_unfiltered\", } IS_MULTIMODAL = False def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: # Filter examples with at least 2 conversations filtered_data = self.data.filter(lambda x: len(x[\"conversations\"]) >= 2) sampled_requests = [] ind = 0 dynamic_output = output_len is None for item in filtered_data: if len(sampled_requests) >= num_requests: break conv = item[\"conversations\"] prompt, completion = conv[0][\"value\"], conv[1][\"value\"] prompt_ids = tokenizer(prompt).input_ids completion_ids = tokenizer(completion).input_ids prompt_len = len(prompt_ids) completion_len = len(completion_ids) output_len = completion_len if dynamic_output else output_len assert isinstance(output_len, int) and output_len > 0 if dynamic_output and not is_valid_sequence(prompt_len, completion_len): continue mm_content = process_image(item[\"image\"]) if \"image\" in item else None if enable_multimodal_chat: # Note: when chat is enabled the request prompt_len is no longer # accurate and we will be using request output to count the # actual prompt len and output len prompt = self.apply_multimodal_chat_transformation(prompt, mm_content) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_content, request_id=request_id_prefix + str(ind), ) ) ind += 1 self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests IS_MULTIMODAL class-attribute instance-attribute ¶ IS_MULTIMODAL = False SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = { \"Aeala/ShareGPT_Vicuna_unfiltered\" } sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 2292 2293 2294 2295 2296 2297 2298 2299 2300 2301 2302 2303 2304 2305 2306 2307 2308 2309 2310 2311 2312 2313 2314 2315 2316 2317 2318 2319 2320 2321 2322 2323 2324 2325 2326 2327 2328 2329 2330 2331 2332 2333 2334 2335 2336 2337 2338 2339 2340 2341def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: # Filter examples with at least 2 conversations filtered_data = self.data.filter(lambda x: len(x[\"conversations\"]) >= 2) sampled_requests = [] ind = 0 dynamic_output = output_len is None for item in filtered_data: if len(sampled_requests) >= num_requests: break conv = item[\"conversations\"] prompt, completion = conv[0][\"value\"], conv[1][\"value\"] prompt_ids = tokenizer(prompt).input_ids completion_ids = tokenizer(completion).input_ids prompt_len = len(prompt_ids) completion_len = len(completion_ids) output_len = completion_len if dynamic_output else output_len assert isinstance(output_len, int) and output_len > 0 if dynamic_output and not is_valid_sequence(prompt_len, completion_len): continue mm_content = process_image(item[\"image\"]) if \"image\" in item else None if enable_multimodal_chat: # Note: when chat is enabled the request prompt_len is no longer # accurate and we will be using request output to count the # actual prompt len and output len prompt = self.apply_multimodal_chat_transformation(prompt, mm_content) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_content, request_id=request_id_prefix + str(ind), ) ) ind += 1 self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests CustomDataset ¶ Bases: BenchmarkDataset Implements the Custom dataset. Loads data from a JSONL file and generates sample requests based on conversation turns. E.g., {\"prompt\": \"What is the capital of India?\"} {\"prompt\": \"What is the capital of Iran?\"} {\"prompt\": \"What is the capital of China?\"} Source code in vllm/benchmarks/datasets.py 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 1937 1938 1939 1940 1941 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025 2026class CustomDataset(BenchmarkDataset): \"\"\" Implements the Custom dataset. Loads data from a JSONL file and generates sample requests based on conversation turns. E.g., ``` {\"prompt\": \"What is the capital of India?\"} {\"prompt\": \"What is the capital of Iran?\"} {\"prompt\": \"What is the capital of China?\"} ``` \"\"\" def __init__(self, **kwargs) -> None: super().__init__(**kwargs) self.load_data() def load_data(self) -> None: if self.dataset_path is None: raise ValueError(\"dataset_path must be provided for loading data.\") # self.data will be a list of dictionaries # e.g., [{\"prompt\": \"What is the capital of India?\"}, ...] # This will be the standardized format which load_data() # has to convert into depending on the filetype of dataset_path. # sample() will assume this standardized format of self.data self.data = [] # Load the JSONL file if self.dataset_path.endswith(\".jsonl\"): jsonl_data = pd.read_json(path_or_buf=self.dataset_path, lines=True) # check if the JSONL file has a 'prompt' column if \"prompt\" not in jsonl_data.columns: raise ValueError(\"JSONL file must contain a 'prompt' column.\") # Convert each row to a dictionary and append to self.data # This will convert the DataFrame to a list of dictionaries # where each dictionary corresponds to a row in the DataFrame. # This is the standardized format we want for self.data for _, row in jsonl_data.iterrows(): self.data.append(row.to_dict()) else: raise NotImplementedError( \"Only JSONL format is supported for CustomDataset.\" ) random.seed(self.random_seed) if not getattr(self, \"disable_shuffle\", False): random.shuffle(self.data) def sample( self, tokenizer: TokenizerLike, num_requests: int, lora_path: str | None = None, max_loras: int | None = None, output_len: int | None = None, enable_multimodal_chat: bool = False, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: # load all data if needed self.num_available_samples = len(self.data) if num_requests <= 0: num_requests = self.num_available_samples logger.info( \"num_requests is set to 0 or negative, \" \"so using all available samples: %d\", num_requests, ) sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break prompt = item[\"prompt\"] # apply template if not skip_chat_template: prompt = tokenizer.apply_chat_template( [{\"role\": \"user\", \"content\": prompt}], add_generation_prompt=True, tokenize=False, ) prompt_len = len(tokenizer(prompt).input_ids) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests __init__ ¶ __init__(**kwargs) -> None Source code in vllm/benchmarks/datasets.py 1938 1939 1940def __init__(self, **kwargs) -> None: super().__init__(**kwargs) self.load_data() load_data ¶ load_data() -> None Source code in vllm/benchmarks/datasets.py 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974def load_data(self) -> None: if self.dataset_path is None: raise ValueError(\"dataset_path must be provided for loading data.\") # self.data will be a list of dictionaries # e.g., [{\"prompt\": \"What is the capital of India?\"}, ...] # This will be the standardized format which load_data() # has to convert into depending on the filetype of dataset_path. # sample() will assume this standardized format of self.data self.data = [] # Load the JSONL file if self.dataset_path.endswith(\".jsonl\"): jsonl_data = pd.read_json(path_or_buf=self.dataset_path, lines=True) # check if the JSONL file has a 'prompt' column if \"prompt\" not in jsonl_data.columns: raise ValueError(\"JSONL file must contain a 'prompt' column.\") # Convert each row to a dictionary and append to self.data # This will convert the DataFrame to a list of dictionaries # where each dictionary corresponds to a row in the DataFrame. # This is the standardized format we want for self.data for _, row in jsonl_data.iterrows(): self.data.append(row.to_dict()) else: raise NotImplementedError( \"Only JSONL format is supported for CustomDataset.\" ) random.seed(self.random_seed) if not getattr(self, \"disable_shuffle\", False): random.shuffle(self.data) sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, lora_path: str | None = None, max_loras: int | None = None, output_len: int | None = None, enable_multimodal_chat: bool = False, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 1976 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025 2026def sample( self, tokenizer: TokenizerLike, num_requests: int, lora_path: str | None = None, max_loras: int | None = None, output_len: int | None = None, enable_multimodal_chat: bool = False, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: # load all data if needed self.num_available_samples = len(self.data) if num_requests <= 0: num_requests = self.num_available_samples logger.info( \"num_requests is set to 0 or negative, \" \"so using all available samples: %d\", num_requests, ) sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break prompt = item[\"prompt\"] # apply template if not skip_chat_template: prompt = tokenizer.apply_chat_template( [{\"role\": \"user\", \"content\": prompt}], add_generation_prompt=True, tokenize=False, ) prompt_len = len(tokenizer(prompt).input_ids) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests HuggingFaceDataset ¶ Bases: BenchmarkDataset Base class for datasets hosted on HuggingFace. Source code in vllm/benchmarks/datasets.py 2245 2246 2247 2248 2249 2250 2251 2252 2253 2254 2255 2256 2257 2258 2259 2260 2261 2262 2263 2264 2265 2266 2267 2268 2269 2270 2271 2272 2273 2274 2275 2276class HuggingFaceDataset(BenchmarkDataset): \"\"\"Base class for datasets hosted on HuggingFace.\"\"\" SUPPORTED_DATASET_PATHS: set[str] | dict[str, Callable] = set() def __init__( self, dataset_path: str, dataset_split: str, no_stream: bool = False, dataset_subset: str | None = None, hf_name: str | None = None, **kwargs, ) -> None: super().__init__(dataset_path=dataset_path, **kwargs) self.dataset_split = dataset_split self.dataset_subset = dataset_subset self.load_stream = not no_stream self.hf_name = hf_name or dataset_path self.load_data() def load_data(self) -> None: \"\"\"Load data from HuggingFace datasets.\"\"\" self.data = load_dataset( self.dataset_path, name=self.dataset_subset, split=self.dataset_split, streaming=self.load_stream, ) if not getattr(self, \"disable_shuffle\", False): self.data = self.data.shuffle(seed=self.random_seed) SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS: set[str] | dict[str, Callable] = ( set() ) dataset_split instance-attribute ¶ dataset_split = dataset_split dataset_subset instance-attribute ¶ dataset_subset = dataset_subset hf_name instance-attribute ¶ hf_name = hf_name or dataset_path load_stream instance-attribute ¶ load_stream = not no_stream __init__ ¶ __init__( dataset_path: str, dataset_split: str, no_stream: bool = False, dataset_subset: str | None = None, hf_name: str | None = None, **kwargs, ) -> None Source code in vllm/benchmarks/datasets.py 2250 2251 2252 2253 2254 2255 2256 2257 2258 2259 2260 2261 2262 2263 2264 2265def __init__( self, dataset_path: str, dataset_split: str, no_stream: bool = False, dataset_subset: str | None = None, hf_name: str | None = None, **kwargs, ) -> None: super().__init__(dataset_path=dataset_path, **kwargs) self.dataset_split = dataset_split self.dataset_subset = dataset_subset self.load_stream = not no_stream self.hf_name = hf_name or dataset_path self.load_data() load_data ¶ load_data() -> None Load data from HuggingFace datasets. Source code in vllm/benchmarks/datasets.py 2267 2268 2269 2270 2271 2272 2273 2274 2275 2276def load_data(self) -> None: \"\"\"Load data from HuggingFace datasets.\"\"\" self.data = load_dataset( self.dataset_path, name=self.dataset_subset, split=self.dataset_split, streaming=self.load_stream, ) if not getattr(self, \"disable_shuffle\", False): self.data = self.data.shuffle(seed=self.random_seed) InstructCoderDataset ¶ Bases: HuggingFaceDataset InstructCoder Dataset. https://huggingface.co/datasets/likaixin/InstructCoder InstructCoder is the dataset designed for general code editing. It consists of 114,239 instruction-input-output triplets, and covers multiple distinct code editing scenario. Source code in vllm/benchmarks/datasets.py 2521 2522 2523 2524 2525 2526 2527 2528 2529 2530 2531 2532 2533 2534 2535 2536 2537 2538 2539 2540 2541 2542 2543 2544 2545 2546 2547 2548 2549 2550 2551 2552 2553 2554 2555 2556 2557 2558 2559 2560 2561 2562 2563 2564 2565 2566 2567 2568 2569 2570 2571 2572 2573 2574 2575 2576 2577class InstructCoderDataset(HuggingFaceDataset): \"\"\" InstructCoder Dataset. https://huggingface.co/datasets/likaixin/InstructCoder InstructCoder is the dataset designed for general code editing. It consists of 114,239 instruction-input-output triplets, and covers multiple distinct code editing scenario. \"\"\" DEFAULT_OUTPUT_LEN = 200 # this is the average default output length SUPPORTED_DATASET_PATHS = { \"likaixin/InstructCoder\", } def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break prompt = ( f\"{item['input']}\\n\\n{item['instruction']} Just output \" \"the code, do not include any explanation.\" ) # apply template if not skip_chat_template: prompt = tokenizer.apply_chat_template( [{\"role\": \"user\", \"content\": prompt}], add_generation_prompt=True, tokenize=False, ) prompt_len = len(tokenizer(prompt).input_ids) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶ DEFAULT_OUTPUT_LEN = 200 SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = {'likaixin/InstructCoder'} sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 2536 2537 2538 2539 2540 2541 2542 2543 2544 2545 2546 2547 2548 2549 2550 2551 2552 2553 2554 2555 2556 2557 2558 2559 2560 2561 2562 2563 2564 2565 2566 2567 2568 2569 2570 2571 2572 2573 2574 2575 2576 2577def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break prompt = ( f\"{item['input']}\\n\\n{item['instruction']} Just output \" \"the code, do not include any explanation.\" ) # apply template if not skip_chat_template: prompt = tokenizer.apply_chat_template( [{\"role\": \"user\", \"content\": prompt}], add_generation_prompt=True, tokenize=False, ) prompt_len = len(tokenizer(prompt).input_ids) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests MLPerfDataset ¶ Bases: HuggingFaceDataset MLPerf Inference Dataset. Dataset on HF: https://huggingface.co/datasets/mgoin/mlperf-inference-llama2-data https://huggingface.co/datasets/mgoin/mlperf-inference-llama3.1-data Each record contains \"system_prompt\": system role instruction. \"question\": user question. \"output\": reference answer. We combine the system prompt and question into a chat-formatted prompt (using the tokenizer's chat template) and set the expected output length to the tokenized length of the provided reference answer. Source code in vllm/benchmarks/datasets.py 2984 2985 2986 2987 2988 2989 2990 2991 2992 2993 2994 2995 2996 2997 2998 2999 3000 3001 3002 3003 3004 3005 3006 3007 3008 3009 3010 3011 3012 3013 3014 3015 3016 3017 3018 3019 3020 3021 3022 3023 3024 3025 3026 3027 3028 3029 3030 3031 3032 3033 3034 3035 3036 3037 3038 3039 3040 3041 3042 3043 3044 3045 3046 3047 3048 3049 3050 3051 3052 3053 3054 3055 3056 3057 3058 3059 3060 3061 3062class MLPerfDataset(HuggingFaceDataset): \"\"\" MLPerf Inference Dataset. Dataset on HF: https://huggingface.co/datasets/mgoin/mlperf-inference-llama2-data https://huggingface.co/datasets/mgoin/mlperf-inference-llama3.1-data Each record contains: - \"system_prompt\": system role instruction. - \"question\": user question. - \"output\": reference answer. We combine the system prompt and question into a chat-formatted prompt (using the tokenizer's chat template) and set the expected output length to the tokenized length of the provided reference answer. \"\"\" SUPPORTED_DATASET_PATHS = { \"mgoin/mlperf-inference-llama2-data\", \"mgoin/mlperf-inference-llama3.1-data\", } def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest]: # Force dynamic output length based on reference completion. dynamic_output = output_len is None sampled_requests: list[SampleRequest] = [] ind = 0 for item in self.data: if len(sampled_requests) >= num_requests: break system_prompt = item[\"system_prompt\"] question = item[\"question\"] reference_answer = item[\"output\"] # Build chat-style prompt using tokenizer template, if available. messages = [ {\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": question}, ] prompt_formatted = tokenizer.apply_chat_template( messages, add_generation_prompt=True, tokenize=False ) prompt_len = len(tokenizer(prompt_formatted).input_ids) # Determine output length from reference answer tokens. ref_out_len = len( tokenizer(reference_answer, add_special_tokens=False).input_ids ) expected_output_len = ref_out_len if dynamic_output else output_len # Validate sequence lengths. if not is_valid_sequence(prompt_len, expected_output_len): continue sampled_requests.append( SampleRequest( prompt=prompt_formatted, prompt_len=prompt_len, expected_output_len=expected_output_len, request_id=request_id_prefix + str(ind), ) ) ind += 1 self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = { \"mgoin/mlperf-inference-llama2-data\", \"mgoin/mlperf-inference-llama3.1-data\", } sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest] Source code in vllm/benchmarks/datasets.py 3007 3008 3009 3010 3011 3012 3013 3014 3015 3016 3017 3018 3019 3020 3021 3022 3023 3024 3025 3026 3027 3028 3029 3030 3031 3032 3033 3034 3035 3036 3037 3038 3039 3040 3041 3042 3043 3044 3045 3046 3047 3048 3049 3050 3051 3052 3053 3054 3055 3056 3057 3058 3059 3060 3061 3062def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest]: # Force dynamic output length based on reference completion. dynamic_output = output_len is None sampled_requests: list[SampleRequest] = [] ind = 0 for item in self.data: if len(sampled_requests) >= num_requests: break system_prompt = item[\"system_prompt\"] question = item[\"question\"] reference_answer = item[\"output\"] # Build chat-style prompt using tokenizer template, if available. messages = [ {\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": question}, ] prompt_formatted = tokenizer.apply_chat_template( messages, add_generation_prompt=True, tokenize=False ) prompt_len = len(tokenizer(prompt_formatted).input_ids) # Determine output length from reference answer tokens. ref_out_len = len( tokenizer(reference_answer, add_special_tokens=False).input_ids ) expected_output_len = ref_out_len if dynamic_output else output_len # Validate sequence lengths. if not is_valid_sequence(prompt_len, expected_output_len): continue sampled_requests.append( SampleRequest( prompt=prompt_formatted, prompt_len=prompt_len, expected_output_len=expected_output_len, request_id=request_id_prefix + str(ind), ) ) ind += 1 self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests MMStarDataset ¶ Bases: HuggingFaceDataset Lin-Chen/MMStar: https://huggingface.co/datasets/Lin-Chen/MMStar refer to: https://github.com/sgl-project/SpecForge/pull/106 Source code in vllm/benchmarks/datasets.py 3162 3163 3164 3165 3166 3167 3168 3169 3170 3171 3172 3173 3174 3175 3176 3177 3178 3179 3180 3181 3182 3183 3184 3185 3186 3187 3188 3189 3190 3191 3192 3193 3194 3195 3196 3197 3198 3199 3200 3201 3202 3203 3204 3205 3206 3207 3208 3209 3210 3211 3212 3213 3214 3215 3216 3217 3218 3219 3220 3221 3222 3223 3224 3225 3226 3227class MMStarDataset(HuggingFaceDataset): \"\"\" Lin-Chen/MMStar: https://huggingface.co/datasets/Lin-Chen/MMStar refer to: https://github.com/sgl-project/SpecForge/pull/106 \"\"\" DEFAULT_OUTPUT_LEN = 128 SUPPORTED_DATASET_PATHS = {\"Lin-Chen/MMStar\"} IS_MULTIMODAL = True def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest]: # If --hf-output-len is not set, use the default output length. output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests: list[SampleRequest] = [] for ind, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break # Split the question text from options # (keep only the part before \"Options:\"). full_q: str = item.get(\"question\", \"\") question_text = full_q.split(\"Options:\", 1)[0].strip() # Multimodal image content. mm_content = process_image(item[\"image\"]) # Compute prompt token length (note: this is plain text length # if enable_multimodal_chat is False). prompt_len = len(tokenizer(question_text).input_ids) if enable_multimodal_chat: # If multimodal content should be embedded in the chat message, # convert to [{\"role\":\"user\",\"content\":[...]}] prompt = self.apply_multimodal_chat_transformation( question_text, mm_content ) mm_for_request = None # Already embedded in chat content. else: # Default: prompt is plain text, # image is in mm_content for the bench to assemble. prompt = question_text mm_for_request = mm_content sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_for_request, request_id=request_id_prefix + str(ind), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶ DEFAULT_OUTPUT_LEN = 128 IS_MULTIMODAL class-attribute instance-attribute ¶ IS_MULTIMODAL = True SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = {'Lin-Chen/MMStar'} sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest] Source code in vllm/benchmarks/datasets.py 3172 3173 3174 3175 3176 3177 3178 3179 3180 3181 3182 3183 3184 3185 3186 3187 3188 3189 3190 3191 3192 3193 3194 3195 3196 3197 3198 3199 3200 3201 3202 3203 3204 3205 3206 3207 3208 3209 3210 3211 3212 3213 3214 3215 3216 3217 3218 3219 3220 3221 3222 3223 3224 3225 3226 3227def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest]: # If --hf-output-len is not set, use the default output length. output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests: list[SampleRequest] = [] for ind, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break # Split the question text from options # (keep only the part before \"Options:\"). full_q: str = item.get(\"question\", \"\") question_text = full_q.split(\"Options:\", 1)[0].strip() # Multimodal image content. mm_content = process_image(item[\"image\"]) # Compute prompt token length (note: this is plain text length # if enable_multimodal_chat is False). prompt_len = len(tokenizer(question_text).input_ids) if enable_multimodal_chat: # If multimodal content should be embedded in the chat message, # convert to [{\"role\":\"user\",\"content\":[...]}] prompt = self.apply_multimodal_chat_transformation( question_text, mm_content ) mm_for_request = None # Already embedded in chat content. else: # Default: prompt is plain text, # image is in mm_content for the bench to assemble. prompt = question_text mm_for_request = mm_content sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_for_request, request_id=request_id_prefix + str(ind), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests MMVUDataset ¶ Bases: HuggingFaceDataset MMVU Dataset. https://huggingface.co/datasets/yale-nlp/MMVU Source code in vllm/benchmarks/datasets.py 2462 2463 2464 2465 2466 2467 2468 2469 2470 2471 2472 2473 2474 2475 2476 2477 2478 2479 2480 2481 2482 2483 2484 2485 2486 2487 2488 2489 2490 2491 2492 2493 2494 2495 2496 2497 2498 2499 2500 2501 2502 2503 2504 2505 2506 2507 2508 2509 2510 2511 2512 2513class MMVUDataset(HuggingFaceDataset): \"\"\" MMVU Dataset. https://huggingface.co/datasets/yale-nlp/MMVU \"\"\" DEFAULT_OUTPUT_LEN = 128 SUPPORTED_DATASET_PATHS = { \"yale-nlp/MMVU\": lambda x: x[\"question\"] + \" \" + (\" \".join(f\"{k}.{v}\" for k, v in x[\"choices\"].items())), } def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break parser_fn = self.SUPPORTED_DATASET_PATHS.get(self.hf_name) if parser_fn is None: raise ValueError(f\"Unsupported dataset path: {self.hf_name}\") prompt = parser_fn(item) mm_content = process_video(item[\"video\"]) prompt_len = len(tokenizer(prompt).input_ids) if enable_multimodal_chat: # Note: when chat is enabled the request prompt_len is no longer # accurate and we will be using request output to count the # actual prompt len prompt = self.apply_multimodal_chat_transformation(prompt, mm_content) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_content, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶ DEFAULT_OUTPUT_LEN = 128 SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = { \"yale-nlp/MMVU\": lambda x: x[\"question\"] + \" \" + join(f\"{k}.{v}\" for k, v in (items())) } sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 2475 2476 2477 2478 2479 2480 2481 2482 2483 2484 2485 2486 2487 2488 2489 2490 2491 2492 2493 2494 2495 2496 2497 2498 2499 2500 2501 2502 2503 2504 2505 2506 2507 2508 2509 2510 2511 2512 2513def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break parser_fn = self.SUPPORTED_DATASET_PATHS.get(self.hf_name) if parser_fn is None: raise ValueError(f\"Unsupported dataset path: {self.hf_name}\") prompt = parser_fn(item) mm_content = process_video(item[\"video\"]) prompt_len = len(tokenizer(prompt).input_ids) if enable_multimodal_chat: # Note: when chat is enabled the request prompt_len is no longer # accurate and we will be using request output to count the # actual prompt len prompt = self.apply_multimodal_chat_transformation(prompt, mm_content) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_content, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests MTBenchDataset ¶ Bases: HuggingFaceDataset MT-Bench Dataset. https://huggingface.co/datasets/philschmid/mt-bench We create a single turn dataset for MT-Bench. This is similar to Spec decoding benchmark setup in vLLM https://github.com/vllm-project/vllm/blob/9d98ab5ec/examples/offline_inference/eagle.py#L14-L18 Source code in vllm/benchmarks/datasets.py 2585 2586 2587 2588 2589 2590 2591 2592 2593 2594 2595 2596 2597 2598 2599 2600 2601 2602 2603 2604 2605 2606 2607 2608 2609 2610 2611 2612 2613 2614 2615 2616 2617 2618 2619 2620 2621 2622 2623 2624 2625 2626 2627 2628 2629 2630 2631 2632 2633 2634 2635 2636 2637 2638 2639class MTBenchDataset(HuggingFaceDataset): \"\"\" MT-Bench Dataset. https://huggingface.co/datasets/philschmid/mt-bench We create a single turn dataset for MT-Bench. This is similar to Spec decoding benchmark setup in vLLM https://github.com/vllm-project/vllm/blob/9d98ab5ec/examples/offline_inference/eagle.py#L14-L18 \"\"\" # noqa: E501 DEFAULT_OUTPUT_LEN = 256 # avg len used in SD bench in vLLM SUPPORTED_DATASET_PATHS = { \"philschmid/mt-bench\", } def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break prompt = item[\"turns\"][0] # apply template if not skip_chat_template: prompt = tokenizer.apply_chat_template( [{\"role\": \"user\", \"content\": prompt}], add_generation_prompt=True, tokenize=False, ) prompt_len = len(tokenizer(prompt).input_ids) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶ DEFAULT_OUTPUT_LEN = 256 SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = {'philschmid/mt-bench'} sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 2600 2601 2602 2603 2604 2605 2606 2607 2608 2609 2610 2611 2612 2613 2614 2615 2616 2617 2618 2619 2620 2621 2622 2623 2624 2625 2626 2627 2628 2629 2630 2631 2632 2633 2634 2635 2636 2637 2638 2639def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break prompt = item[\"turns\"][0] # apply template if not skip_chat_template: prompt = tokenizer.apply_chat_template( [{\"role\": \"user\", \"content\": prompt}], add_generation_prompt=True, tokenize=False, ) prompt_len = len(tokenizer(prompt).input_ids) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests MultiModalConversationDataset ¶ Bases: HuggingFaceDataset Dataset for multimodal conversation data. Source code in vllm/benchmarks/datasets.py 2344 2345 2346 2347 2348 2349 2350 2351 2352 2353 2354 2355 2356 2357 2358 2359 2360 2361 2362 2363 2364 2365 2366 2367 2368 2369 2370 2371 2372 2373 2374 2375 2376 2377 2378 2379 2380 2381 2382 2383 2384 2385 2386 2387 2388 2389 2390 2391 2392 2393 2394 2395 2396 2397 2398 2399 2400 2401class MultiModalConversationDataset(HuggingFaceDataset): \"\"\"Dataset for multimodal conversation data.\"\"\" SUPPORTED_DATASET_PATHS = { \"lmms-lab/LLaVA-OneVision-Data\", } IS_MULTIMODAL = True def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: # Filter examples with at least 2 conversations filtered_data = self.data.filter(lambda x: len(x[\"conversations\"]) >= 2) sampled_requests = [] ind = 0 dynamic_output = output_len is None for item in filtered_data: if len(sampled_requests) >= num_requests: break conv = item[\"conversations\"] prompt, completion = conv[0][\"value\"], conv[1][\"value\"] prompt_ids = tokenizer(prompt).input_ids completion_ids = tokenizer(completion).input_ids prompt_len = len(prompt_ids) completion_len = len(completion_ids) output_len = completion_len if dynamic_output else output_len assert isinstance(output_len, int) and output_len > 0 if dynamic_output and not is_valid_sequence(prompt_len, completion_len): continue mm_content = process_image(item[\"image\"]) if \"image\" in item else None if enable_multimodal_chat: # Note: when chat is enabled the request prompt_len is no longer # accurate and we will be using request output to count the # actual prompt len and output len prompt = self.apply_multimodal_chat_transformation(prompt, mm_content) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_content, request_id=request_id_prefix + str(ind), ) ) ind += 1 self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests IS_MULTIMODAL class-attribute instance-attribute ¶ IS_MULTIMODAL = True SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = {'lmms-lab/LLaVA-OneVision-Data'} sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 2352 2353 2354 2355 2356 2357 2358 2359 2360 2361 2362 2363 2364 2365 2366 2367 2368 2369 2370 2371 2372 2373 2374 2375 2376 2377 2378 2379 2380 2381 2382 2383 2384 2385 2386 2387 2388 2389 2390 2391 2392 2393 2394 2395 2396 2397 2398 2399 2400 2401def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: # Filter examples with at least 2 conversations filtered_data = self.data.filter(lambda x: len(x[\"conversations\"]) >= 2) sampled_requests = [] ind = 0 dynamic_output = output_len is None for item in filtered_data: if len(sampled_requests) >= num_requests: break conv = item[\"conversations\"] prompt, completion = conv[0][\"value\"], conv[1][\"value\"] prompt_ids = tokenizer(prompt).input_ids completion_ids = tokenizer(completion).input_ids prompt_len = len(prompt_ids) completion_len = len(completion_ids) output_len = completion_len if dynamic_output else output_len assert isinstance(output_len, int) and output_len > 0 if dynamic_output and not is_valid_sequence(prompt_len, completion_len): continue mm_content = process_image(item[\"image\"]) if \"image\" in item else None if enable_multimodal_chat: # Note: when chat is enabled the request prompt_len is no longer # accurate and we will be using request output to count the # actual prompt len and output len prompt = self.apply_multimodal_chat_transformation(prompt, mm_content) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_content, request_id=request_id_prefix + str(ind), ) ) ind += 1 self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests NextEditPredictionDataset ¶ Bases: HuggingFaceDataset Dataset class for processing a Next Edit Prediction dataset. Source code in vllm/benchmarks/datasets.py 2845 2846 2847 2848 2849 2850 2851 2852 2853 2854 2855 2856 2857 2858 2859 2860 2861 2862 2863 2864 2865 2866 2867 2868 2869 2870 2871 2872 2873 2874 2875 2876 2877 2878 2879 2880 2881 2882 2883 2884 2885 2886class NextEditPredictionDataset(HuggingFaceDataset): \"\"\" Dataset class for processing a Next Edit Prediction dataset. \"\"\" SUPPORTED_DATASET_PATHS = { \"zed-industries/zeta\", } MAPPING_PROMPT_FUNCS = { \"zed-industries/zeta\": _format_zeta_prompt, } def sample( self, tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ): formatting_prompt_func = self.MAPPING_PROMPT_FUNCS.get(self.hf_name) if formatting_prompt_func is None: raise ValueError(f\"Unsupported dataset path: {self.hf_name}\") samples = [] for i, sample in enumerate(self.data): sample = formatting_prompt_func(sample) samples.append( SampleRequest( prompt=sample[\"prompt\"], prompt_len=len(tokenizer(sample[\"prompt\"]).input_ids), expected_output_len=len( tokenizer(sample[\"expected_output\"]).input_ids ), request_id=request_id_prefix + str(i), ) ) if len(samples) >= num_requests: break self.maybe_oversample_requests( samples, num_requests, request_id_prefix, no_oversample ) return samples MAPPING_PROMPT_FUNCS class-attribute instance-attribute ¶ MAPPING_PROMPT_FUNCS = { \"zed-industries/zeta\": _format_zeta_prompt } SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = {'zed-industries/zeta'} sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) Source code in vllm/benchmarks/datasets.py 2857 2858 2859 2860 2861 2862 2863 2864 2865 2866 2867 2868 2869 2870 2871 2872 2873 2874 2875 2876 2877 2878 2879 2880 2881 2882 2883 2884 2885 2886def sample( self, tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ): formatting_prompt_func = self.MAPPING_PROMPT_FUNCS.get(self.hf_name) if formatting_prompt_func is None: raise ValueError(f\"Unsupported dataset path: {self.hf_name}\") samples = [] for i, sample in enumerate(self.data): sample = formatting_prompt_func(sample) samples.append( SampleRequest( prompt=sample[\"prompt\"], prompt_len=len(tokenizer(sample[\"prompt\"]).input_ids), expected_output_len=len( tokenizer(sample[\"expected_output\"]).input_ids ), request_id=request_id_prefix + str(i), ) ) if len(samples) >= num_requests: break self.maybe_oversample_requests( samples, num_requests, request_id_prefix, no_oversample ) return samples PrefixRepetitionRandomDataset ¶ Bases: BenchmarkDataset Source code in vllm/benchmarks/datasets.py 3070 3071 3072 3073 3074 3075 3076 3077 3078 3079 3080 3081 3082 3083 3084 3085 3086 3087 3088 3089 3090 3091 3092 3093 3094 3095 3096 3097 3098 3099 3100 3101 3102 3103 3104 3105 3106 3107 3108 3109 3110 3111 3112 3113 3114 3115 3116 3117 3118 3119 3120 3121 3122 3123 3124 3125 3126 3127 3128 3129 3130 3131 3132 3133 3134 3135 3136 3137 3138 3139 3140 3141 3142 3143 3144 3145 3146 3147 3148 3149 3150 3151 3152 3153 3154class PrefixRepetitionRandomDataset(BenchmarkDataset): # Default values copied from benchmark_serving.py for the repeated prefix # dataset. DEFAULT_PREFIX_LEN = 256 DEFAULT_SUFFIX_LEN = 256 DEFAULT_NUM_PREFIXES = 10 DEFAULT_OUTPUT_LEN = 128 def __init__( self, **kwargs, ) -> None: super().__init__(**kwargs) random.seed(self.random_seed) np.random.seed(self.random_seed) def sample( self, tokenizer: TokenizerLike, num_requests: int, prefix_len: int = DEFAULT_PREFIX_LEN, suffix_len: int = DEFAULT_SUFFIX_LEN, num_prefixes: int = DEFAULT_NUM_PREFIXES, output_len: int = DEFAULT_OUTPUT_LEN, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest]: vocab_size = tokenizer.vocab_size prompts_per_prefix = num_requests // num_prefixes if prompts_per_prefix == 0: raise ValueError( f\"num_requests ({num_requests}) must be greater than or equal \" f\"to num_prefixes ({num_prefixes})\" ) def _generate_exact_length_tokens(target_length: int) -> list[int]: \"\"\"Generate tokens that decode and re-encode to exactly target_length.\"\"\" # Generate random tokens tokens = np.random.randint(0, vocab_size, size=target_length).tolist() _, adjusted_tokens, token_mismatch = gen_prompt_decode_to_target_len( # noqa: E501 tokenizer=tokenizer, token_sequence=tokens, target_token_len=target_length, add_special_tokens=False, ) return adjusted_tokens, token_mismatch requests = [] token_mismatch_total = 0 for _ in range(num_prefixes): prefix_tokens, prefix_mismatch = _generate_exact_length_tokens(prefix_len) token_mismatch_total += prefix_mismatch for _ in range(prompts_per_prefix): suffix_tokens, suffix_mismatch = _generate_exact_length_tokens( suffix_len ) token_mismatch_total += suffix_mismatch combined_tokens = prefix_tokens + suffix_tokens prompt = tokenizer.decode(combined_tokens) prompt_len = len(combined_tokens) requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, ) ) if token_mismatch_total != 0: sign = \"more\" if token_mismatch_total > 0 else \"fewer\" logger.warning( \"Across all generated prompts, there were %d %s tokens \" \"than expected after decoding and re-encoding. This is \" \"expected due to the imperfect nature of the sampling \" \"procedure.\", abs(token_mismatch_total), sign, ) if not getattr(self, \"disable_shuffle\", False): random.shuffle(requests) return requests DEFAULT_NUM_PREFIXES class-attribute instance-attribute ¶ DEFAULT_NUM_PREFIXES = 10 DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶ DEFAULT_OUTPUT_LEN = 128 DEFAULT_PREFIX_LEN class-attribute instance-attribute ¶ DEFAULT_PREFIX_LEN = 256 DEFAULT_SUFFIX_LEN class-attribute instance-attribute ¶ DEFAULT_SUFFIX_LEN = 256 __init__ ¶ __init__(**kwargs) -> None Source code in vllm/benchmarks/datasets.py 3078 3079 3080 3081 3082 3083 3084def __init__( self, **kwargs, ) -> None: super().__init__(**kwargs) random.seed(self.random_seed) np.random.seed(self.random_seed) sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, prefix_len: int = DEFAULT_PREFIX_LEN, suffix_len: int = DEFAULT_SUFFIX_LEN, num_prefixes: int = DEFAULT_NUM_PREFIXES, output_len: int = DEFAULT_OUTPUT_LEN, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest] Source code in vllm/benchmarks/datasets.py 3086 3087 3088 3089 3090 3091 3092 3093 3094 3095 3096 3097 3098 3099 3100 3101 3102 3103 3104 3105 3106 3107 3108 3109 3110 3111 3112 3113 3114 3115 3116 3117 3118 3119 3120 3121 3122 3123 3124 3125 3126 3127 3128 3129 3130 3131 3132 3133 3134 3135 3136 3137 3138 3139 3140 3141 3142 3143 3144 3145 3146 3147 3148 3149 3150 3151 3152 3153 3154def sample( self, tokenizer: TokenizerLike, num_requests: int, prefix_len: int = DEFAULT_PREFIX_LEN, suffix_len: int = DEFAULT_SUFFIX_LEN, num_prefixes: int = DEFAULT_NUM_PREFIXES, output_len: int = DEFAULT_OUTPUT_LEN, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest]: vocab_size = tokenizer.vocab_size prompts_per_prefix = num_requests // num_prefixes if prompts_per_prefix == 0: raise ValueError( f\"num_requests ({num_requests}) must be greater than or equal \" f\"to num_prefixes ({num_prefixes})\" ) def _generate_exact_length_tokens(target_length: int) -> list[int]: \"\"\"Generate tokens that decode and re-encode to exactly target_length.\"\"\" # Generate random tokens tokens = np.random.randint(0, vocab_size, size=target_length).tolist() _, adjusted_tokens, token_mismatch = gen_prompt_decode_to_target_len( # noqa: E501 tokenizer=tokenizer, token_sequence=tokens, target_token_len=target_length, add_special_tokens=False, ) return adjusted_tokens, token_mismatch requests = [] token_mismatch_total = 0 for _ in range(num_prefixes): prefix_tokens, prefix_mismatch = _generate_exact_length_tokens(prefix_len) token_mismatch_total += prefix_mismatch for _ in range(prompts_per_prefix): suffix_tokens, suffix_mismatch = _generate_exact_length_tokens( suffix_len ) token_mismatch_total += suffix_mismatch combined_tokens = prefix_tokens + suffix_tokens prompt = tokenizer.decode(combined_tokens) prompt_len = len(combined_tokens) requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, ) ) if token_mismatch_total != 0: sign = \"more\" if token_mismatch_total > 0 else \"fewer\" logger.warning( \"Across all generated prompts, there were %d %s tokens \" \"than expected after decoding and re-encoding. This is \" \"expected due to the imperfect nature of the sampling \" \"procedure.\", abs(token_mismatch_total), sign, ) if not getattr(self, \"disable_shuffle\", False): random.shuffle(requests) return requests RandomDataset ¶ Bases: BenchmarkDataset Synthetic text-only dataset for serving/throughput benchmarks. Strategy: - Sample input/output token lengths per request from integer-uniform ranges around configured means (controlled by range_ratio). - Prepend a fixed random prefix of length prefix_len. - Generate the remaining tokens as a reproducible sequence: (offset + index + arange(input_len)) % vocab_size. - Decode then re-encode/truncate to ensure prompt token counts match. - Uses numpy.default_rng seeded with random_seed for reproducible sampling. Source code in vllm/benchmarks/datasets.py 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668class RandomDataset(BenchmarkDataset): \"\"\" Synthetic text-only dataset for serving/throughput benchmarks. Strategy: - Sample input/output token lengths per request from integer-uniform ranges around configured means (controlled by range_ratio). - Prepend a fixed random prefix of length prefix_len. - Generate the remaining tokens as a reproducible sequence: (offset + index + arange(input_len)) % vocab_size. - Decode then re-encode/truncate to ensure prompt token counts match. - Uses numpy.default_rng seeded with random_seed for reproducible sampling. \"\"\" # Default values copied from benchmark_serving.py for the random dataset. DEFAULT_PREFIX_LEN = 0 DEFAULT_RANGE_RATIO = 0.0 DEFAULT_INPUT_LEN = 1024 DEFAULT_OUTPUT_LEN = 128 def __init__(self, **kwargs) -> None: super().__init__(**kwargs) # Use numpy's default_rng for deterministic sampling # Do not use random.seed() or np.random.seed() elsewhere in this class. # This ensures that the RNG is isolated from global RNG state. self._rng = np.random.default_rng(self.random_seed) def sample( self, tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, prefix_len: int = DEFAULT_PREFIX_LEN, range_ratio: float = DEFAULT_RANGE_RATIO, input_len: int = DEFAULT_INPUT_LEN, output_len: int = DEFAULT_OUTPUT_LEN, batchsize: int = 1, **kwargs, ) -> list[SampleRequest]: # validate total input tokens (prefix + sampled) is at least 1. num_special = int(tokenizer.num_special_tokens_to_add()) real_input_len = max(0, int(input_len) - num_special) min_sampled_input = math.floor(real_input_len * (1.0 - float(range_ratio))) min_total_input = int(prefix_len) + min_sampled_input if min_total_input < 1: raise ValueError( \"--random-input-len is too small: with tokenizer special \" f\"tokens {num_special} and --random-range-ratio {range_ratio}, \" \"the minimum possible total input tokens (prefix + sampled) is \" f\"{min_total_input}. Increase --random-input-len and/or \" \"--random-prefix-len, or decrease --random-range-ratio so that \" \"prefix_len + floor(max(0, random_input_len - num_special)) \" \"* (1 - range_ratio) >= 1.\" ) input_lens, output_lens, offsets = self.get_sampling_params( num_requests, range_ratio, input_len, output_len, tokenizer ) vocab_size = tokenizer.vocab_size prohibited_tokens = tokenizer.all_special_ids all_tokens = np.arange(vocab_size) allowed_tokens = np.array(list(set(all_tokens) - set(prohibited_tokens))) # Generate prefix once prefix_token_ids = self.get_prefix(allowed_tokens, prefix_len) requests = [] token_mismatch_total = 0 for i in range(num_requests): prompt, total_input_len, token_mismatch = self.generate_token_sequence( # noqa: E501 tokenizer=tokenizer, prefix_token_ids=prefix_token_ids, prefix_len=prefix_len, vocab_size=vocab_size, input_len=int(input_lens[i]), offset=int(offsets[i]), index=i, allowed_tokens=allowed_tokens, ) token_mismatch_total += token_mismatch requests.append( SampleRequest( prompt=prompt, prompt_len=total_input_len, expected_output_len=int(output_lens[i]), request_id=request_id_prefix + str(i), ) ) # only used for embeddings benchmark. if batchsize > 1: batch_requests = [] # Create batched requests for i in range(0, num_requests, batchsize): batch = requests[i : i + batchsize] batch_requests.append( SampleRequest( prompt=[req.prompt for req in batch], prompt_len=sum(req.prompt_len for req in batch), expected_output_len=0, request_id=request_id_prefix + str(i // batchsize), ) ) requests = batch_requests if token_mismatch_total != 0: sign = \"more\" if token_mismatch_total > 0 else \"fewer\" logger.warning( \"Across all generated prompts, there were %d %s tokens \" \"than expected after decoding and re-encoding. This is \" \"expected due to the imperfect nature of the sampling \" \"procedure.\", abs(token_mismatch_total), sign, ) return requests def get_prefix( self, allowed_tokens: np.ndarray, prefix_len: int, ) -> list[int]: \"\"\" Get the prefix for the dataset. \"\"\" return ( allowed_tokens[ self._rng.integers(0, len(allowed_tokens), size=prefix_len) ].tolist() if prefix_len > 0 else [] ) def get_sampling_params( self, num_requests: int, range_ratio: float, input_len: int, output_len: int, tokenizer: TokenizerLike, ) -> tuple[np.ndarray, np.ndarray, np.ndarray]: \"\"\" Get the sampling parameters for the dataset. \"\"\" # Enforce range_ratio < 1 if not (0.0 <= range_ratio < 1.0): raise ValueError(\"range_ratio must be in [0, 1).\") num_special_tokens = int(tokenizer.num_special_tokens_to_add()) real_input_len = max(0, int(input_len) - num_special_tokens) # Bounds use floor for low and ceil for high input_low = math.floor(real_input_len * (1 - range_ratio)) input_high = math.ceil(real_input_len * (1 + range_ratio)) output_low = math.floor(output_len * (1 - range_ratio)) output_high = math.ceil(output_len * (1 + range_ratio)) # Ensure the lower bound for output length is at least 1 to # prevent sampling 0 tokens. output_low = max(output_low, 1) output_high = max(output_high, 1) if input_low > input_high: raise ValueError( f\"Invalid input sampling interval: low={input_low} > high={input_high}\" ) if output_low > output_high: raise ValueError( \"Invalid output sampling interval: \" f\"low={output_low} > high={output_high}\" ) logger.info( \"Sampling input_len from [%s, %s] and output_len from [%s, %s]\", input_low, input_high, output_low, output_high, ) input_lens = self._rng.integers(input_low, input_high + 1, size=num_requests) output_lens = self._rng.integers(output_low, output_high + 1, size=num_requests) offsets = self._rng.integers(0, tokenizer.vocab_size, size=num_requests) return input_lens, output_lens, offsets def generate_token_sequence( self, *, tokenizer: TokenizerLike, prefix_token_ids: list[int], prefix_len: int, vocab_size: int, input_len: int, offset: int, index: int, allowed_tokens: np.ndarray, ) -> tuple[str, int, int]: \"\"\" Returns (prompt, total_input_len). NOTE: After decoding the prompt we have to encode and decode it again. This is done because in some cases N consecutive tokens give a string tokenized into != N number of tokens. For example for GPT2Tokenizer: [6880, 6881] -> ['Ġcalls', 'here'] -> [1650, 939, 486] -> ['Ġcall', 'sh', 'ere'] To avoid uncontrolled change of the prompt length, the encoded sequence is truncated before being decoded again. \"\"\" # Build the inner sequence by sampling # sequentially from the allowed tokens inner_seq = allowed_tokens[ (offset + index + np.arange(input_len)) % len(allowed_tokens) ].tolist() token_sequence = prefix_token_ids + inner_seq # Decode, then re-encode and truncate to preserve token count invariants total_input_len = prefix_len + int(input_len) prompt, adjusted_token_sequence, token_mismatch = ( gen_prompt_decode_to_target_len( tokenizer=tokenizer, token_sequence=token_sequence, target_token_len=total_input_len, add_special_tokens=False, rng=self._rng, ) ) total_input_len = len(adjusted_token_sequence) return prompt, total_input_len, token_mismatch DEFAULT_INPUT_LEN class-attribute instance-attribute ¶ DEFAULT_INPUT_LEN = 1024 DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶ DEFAULT_OUTPUT_LEN = 128 DEFAULT_PREFIX_LEN class-attribute instance-attribute ¶ DEFAULT_PREFIX_LEN = 0 DEFAULT_RANGE_RATIO class-attribute instance-attribute ¶ DEFAULT_RANGE_RATIO = 0.0 _rng instance-attribute ¶ _rng = default_rng(random_seed) __init__ ¶ __init__(**kwargs) -> None Source code in vllm/benchmarks/datasets.py 461 462 463 464 465 466def __init__(self, **kwargs) -> None: super().__init__(**kwargs) # Use numpy's default_rng for deterministic sampling # Do not use random.seed() or np.random.seed() elsewhere in this class. # This ensures that the RNG is isolated from global RNG state. self._rng = np.random.default_rng(self.random_seed) generate_token_sequence ¶ generate_token_sequence( *, tokenizer: TokenizerLike, prefix_token_ids: list[int], prefix_len: int, vocab_size: int, input_len: int, offset: int, index: int, allowed_tokens: ndarray, ) -> tuple[str, int, int] Returns (prompt, total_input_len). NOTE: After decoding the prompt we have to encode and decode it again. This is done because in some cases N consecutive tokens give a string tokenized into != N number of tokens. For example for GPT2Tokenizer: [6880, 6881] -> ['Ġcalls', 'here'] -> [1650, 939, 486] -> ['Ġcall', 'sh', 'ere'] To avoid uncontrolled change of the prompt length, the encoded sequence is truncated before being decoded again. Source code in vllm/benchmarks/datasets.py 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668def generate_token_sequence( self, *, tokenizer: TokenizerLike, prefix_token_ids: list[int], prefix_len: int, vocab_size: int, input_len: int, offset: int, index: int, allowed_tokens: np.ndarray, ) -> tuple[str, int, int]: \"\"\" Returns (prompt, total_input_len). NOTE: After decoding the prompt we have to encode and decode it again. This is done because in some cases N consecutive tokens give a string tokenized into != N number of tokens. For example for GPT2Tokenizer: [6880, 6881] -> ['Ġcalls', 'here'] -> [1650, 939, 486] -> ['Ġcall', 'sh', 'ere'] To avoid uncontrolled change of the prompt length, the encoded sequence is truncated before being decoded again. \"\"\" # Build the inner sequence by sampling # sequentially from the allowed tokens inner_seq = allowed_tokens[ (offset + index + np.arange(input_len)) % len(allowed_tokens) ].tolist() token_sequence = prefix_token_ids + inner_seq # Decode, then re-encode and truncate to preserve token count invariants total_input_len = prefix_len + int(input_len) prompt, adjusted_token_sequence, token_mismatch = ( gen_prompt_decode_to_target_len( tokenizer=tokenizer, token_sequence=token_sequence, target_token_len=total_input_len, add_special_tokens=False, rng=self._rng, ) ) total_input_len = len(adjusted_token_sequence) return prompt, total_input_len, token_mismatch get_prefix ¶ get_prefix( allowed_tokens: ndarray, prefix_len: int ) -> list[int] Get the prefix for the dataset. Source code in vllm/benchmarks/datasets.py 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574def get_prefix( self, allowed_tokens: np.ndarray, prefix_len: int, ) -> list[int]: \"\"\" Get the prefix for the dataset. \"\"\" return ( allowed_tokens[ self._rng.integers(0, len(allowed_tokens), size=prefix_len) ].tolist() if prefix_len > 0 else [] ) get_sampling_params ¶ get_sampling_params( num_requests: int, range_ratio: float, input_len: int, output_len: int, tokenizer: TokenizerLike, ) -> tuple[ndarray, ndarray, ndarray] Get the sampling parameters for the dataset. Source code in vllm/benchmarks/datasets.py 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623def get_sampling_params( self, num_requests: int, range_ratio: float, input_len: int, output_len: int, tokenizer: TokenizerLike, ) -> tuple[np.ndarray, np.ndarray, np.ndarray]: \"\"\" Get the sampling parameters for the dataset. \"\"\" # Enforce range_ratio < 1 if not (0.0 <= range_ratio < 1.0): raise ValueError(\"range_ratio must be in [0, 1).\") num_special_tokens = int(tokenizer.num_special_tokens_to_add()) real_input_len = max(0, int(input_len) - num_special_tokens) # Bounds use floor for low and ceil for high input_low = math.floor(real_input_len * (1 - range_ratio)) input_high = math.ceil(real_input_len * (1 + range_ratio)) output_low = math.floor(output_len * (1 - range_ratio)) output_high = math.ceil(output_len * (1 + range_ratio)) # Ensure the lower bound for output length is at least 1 to # prevent sampling 0 tokens. output_low = max(output_low, 1) output_high = max(output_high, 1) if input_low > input_high: raise ValueError( f\"Invalid input sampling interval: low={input_low} > high={input_high}\" ) if output_low > output_high: raise ValueError( \"Invalid output sampling interval: \" f\"low={output_low} > high={output_high}\" ) logger.info( \"Sampling input_len from [%s, %s] and output_len from [%s, %s]\", input_low, input_high, output_low, output_high, ) input_lens = self._rng.integers(input_low, input_high + 1, size=num_requests) output_lens = self._rng.integers(output_low, output_high + 1, size=num_requests) offsets = self._rng.integers(0, tokenizer.vocab_size, size=num_requests) return input_lens, output_lens, offsets sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, prefix_len: int = DEFAULT_PREFIX_LEN, range_ratio: float = DEFAULT_RANGE_RATIO, input_len: int = DEFAULT_INPUT_LEN, output_len: int = DEFAULT_OUTPUT_LEN, batchsize: int = 1, **kwargs, ) -> list[SampleRequest] Source code in vllm/benchmarks/datasets.py 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558def sample( self, tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, prefix_len: int = DEFAULT_PREFIX_LEN, range_ratio: float = DEFAULT_RANGE_RATIO, input_len: int = DEFAULT_INPUT_LEN, output_len: int = DEFAULT_OUTPUT_LEN, batchsize: int = 1, **kwargs, ) -> list[SampleRequest]: # validate total input tokens (prefix + sampled) is at least 1. num_special = int(tokenizer.num_special_tokens_to_add()) real_input_len = max(0, int(input_len) - num_special) min_sampled_input = math.floor(real_input_len * (1.0 - float(range_ratio))) min_total_input = int(prefix_len) + min_sampled_input if min_total_input < 1: raise ValueError( \"--random-input-len is too small: with tokenizer special \" f\"tokens {num_special} and --random-range-ratio {range_ratio}, \" \"the minimum possible total input tokens (prefix + sampled) is \" f\"{min_total_input}. Increase --random-input-len and/or \" \"--random-prefix-len, or decrease --random-range-ratio so that \" \"prefix_len + floor(max(0, random_input_len - num_special)) \" \"* (1 - range_ratio) >= 1.\" ) input_lens, output_lens, offsets = self.get_sampling_params( num_requests, range_ratio, input_len, output_len, tokenizer ) vocab_size = tokenizer.vocab_size prohibited_tokens = tokenizer.all_special_ids all_tokens = np.arange(vocab_size) allowed_tokens = np.array(list(set(all_tokens) - set(prohibited_tokens))) # Generate prefix once prefix_token_ids = self.get_prefix(allowed_tokens, prefix_len) requests = [] token_mismatch_total = 0 for i in range(num_requests): prompt, total_input_len, token_mismatch = self.generate_token_sequence( # noqa: E501 tokenizer=tokenizer, prefix_token_ids=prefix_token_ids, prefix_len=prefix_len, vocab_size=vocab_size, input_len=int(input_lens[i]), offset=int(offsets[i]), index=i, allowed_tokens=allowed_tokens, ) token_mismatch_total += token_mismatch requests.append( SampleRequest( prompt=prompt, prompt_len=total_input_len, expected_output_len=int(output_lens[i]), request_id=request_id_prefix + str(i), ) ) # only used for embeddings benchmark. if batchsize > 1: batch_requests = [] # Create batched requests for i in range(0, num_requests, batchsize): batch = requests[i : i + batchsize] batch_requests.append( SampleRequest( prompt=[req.prompt for req in batch], prompt_len=sum(req.prompt_len for req in batch), expected_output_len=0, request_id=request_id_prefix + str(i // batchsize), ) ) requests = batch_requests if token_mismatch_total != 0: sign = \"more\" if token_mismatch_total > 0 else \"fewer\" logger.warning( \"Across all generated prompts, there were %d %s tokens \" \"than expected after decoding and re-encoding. This is \" \"expected due to the imperfect nature of the sampling \" \"procedure.\", abs(token_mismatch_total), sign, ) return requests RandomDatasetForReranking ¶ Bases: RandomDataset Random dataset specialized for the needs of scoring: - Batches of inputs - Inputs composed of pairs Source code in vllm/benchmarks/datasets.py 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780class RandomDatasetForReranking(RandomDataset): \"\"\" Random dataset specialized for the needs of scoring: - Batches of inputs - Inputs composed of pairs \"\"\" def __init__(self, **kwargs) -> None: super().__init__(**kwargs) def sample( self, tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", range_ratio: float = RandomDataset.DEFAULT_RANGE_RATIO, input_len: int = RandomDataset.DEFAULT_INPUT_LEN, batchsize: int = 1, is_reranker: bool = True, **kwargs, ) -> list[SampleRequest]: n_sep_tokens = int(is_reranker) query_len_param = (input_len // 2) - n_sep_tokens if is_reranker else input_len query_lens, _, query_offsets = self.get_sampling_params( 1, range_ratio, query_len_param, 0, tokenizer ) query_len = int(query_lens[0]) if not is_reranker: assert num_requests > 1 and batchsize > 1 num_requests -= 1 batchsize -= 1 doc_len_param = input_len else: doc_len_param = input_len - query_len - n_sep_tokens doc_lens, _, doc_offsets = self.get_sampling_params( num_requests, range_ratio, doc_len_param, 0, tokenizer ) vocab_size = tokenizer.vocab_size prohibited_tokens = tokenizer.all_special_ids all_tokens = np.arange(vocab_size) allowed_tokens = np.array(list(set(all_tokens) - set(prohibited_tokens))) query_prompt, query_input_len, token_mismatch_total = ( self.generate_token_sequence( tokenizer=tokenizer, prefix_token_ids=[], prefix_len=0, vocab_size=vocab_size, input_len=query_len, offset=int(query_offsets[0]), index=0, allowed_tokens=allowed_tokens, ) ) requests = [] for i in range(num_requests): prompt, total_input_len, token_mismatch = self.generate_token_sequence( # noqa: E501 tokenizer=tokenizer, prefix_token_ids=[], prefix_len=0, vocab_size=vocab_size, input_len=int(doc_lens[i]), offset=int(doc_offsets[i]), index=i + 1, allowed_tokens=allowed_tokens, ) token_mismatch_total += token_mismatch requests.append((prompt, total_input_len)) batch_requests = [] # Create batched requests for i in range(0, num_requests, batchsize): batch = requests[i : i + batchsize] query_contrib = ( (query_input_len + n_sep_tokens) * len(batch) if is_reranker else query_input_len ) batch_requests.append( SampleRequest( prompt=[query_prompt] + [req[0] for req in batch], prompt_len=query_contrib + sum(req[1] for req in batch), expected_output_len=0, request_id=request_id_prefix + str(i // batchsize), ) ) if token_mismatch_total != 0: logger.warning( \"Across all generated prompts, there were %d %s tokens \" \"than expected after decoding and re-encoding. This is \" \"expected due to the imperfect nature of the sampling \" \"procedure.\", abs(token_mismatch_total), \"more\" if token_mismatch_total > 0 else \"fewer\", ) return batch_requests __init__ ¶ __init__(**kwargs) -> None Source code in vllm/benchmarks/datasets.py 683 684def __init__(self, **kwargs) -> None: super().__init__(**kwargs) sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", range_ratio: float = DEFAULT_RANGE_RATIO, input_len: int = DEFAULT_INPUT_LEN, batchsize: int = 1, is_reranker: bool = True, **kwargs, ) -> list[SampleRequest] Source code in vllm/benchmarks/datasets.py 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780def sample( self, tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", range_ratio: float = RandomDataset.DEFAULT_RANGE_RATIO, input_len: int = RandomDataset.DEFAULT_INPUT_LEN, batchsize: int = 1, is_reranker: bool = True, **kwargs, ) -> list[SampleRequest]: n_sep_tokens = int(is_reranker) query_len_param = (input_len // 2) - n_sep_tokens if is_reranker else input_len query_lens, _, query_offsets = self.get_sampling_params( 1, range_ratio, query_len_param, 0, tokenizer ) query_len = int(query_lens[0]) if not is_reranker: assert num_requests > 1 and batchsize > 1 num_requests -= 1 batchsize -= 1 doc_len_param = input_len else: doc_len_param = input_len - query_len - n_sep_tokens doc_lens, _, doc_offsets = self.get_sampling_params( num_requests, range_ratio, doc_len_param, 0, tokenizer ) vocab_size = tokenizer.vocab_size prohibited_tokens = tokenizer.all_special_ids all_tokens = np.arange(vocab_size) allowed_tokens = np.array(list(set(all_tokens) - set(prohibited_tokens))) query_prompt, query_input_len, token_mismatch_total = ( self.generate_token_sequence( tokenizer=tokenizer, prefix_token_ids=[], prefix_len=0, vocab_size=vocab_size, input_len=query_len, offset=int(query_offsets[0]), index=0, allowed_tokens=allowed_tokens, ) ) requests = [] for i in range(num_requests): prompt, total_input_len, token_mismatch = self.generate_token_sequence( # noqa: E501 tokenizer=tokenizer, prefix_token_ids=[], prefix_len=0, vocab_size=vocab_size, input_len=int(doc_lens[i]), offset=int(doc_offsets[i]), index=i + 1, allowed_tokens=allowed_tokens, ) token_mismatch_total += token_mismatch requests.append((prompt, total_input_len)) batch_requests = [] # Create batched requests for i in range(0, num_requests, batchsize): batch = requests[i : i + batchsize] query_contrib = ( (query_input_len + n_sep_tokens) * len(batch) if is_reranker else query_input_len ) batch_requests.append( SampleRequest( prompt=[query_prompt] + [req[0] for req in batch], prompt_len=query_contrib + sum(req[1] for req in batch), expected_output_len=0, request_id=request_id_prefix + str(i // batchsize), ) ) if token_mismatch_total != 0: logger.warning( \"Across all generated prompts, there were %d %s tokens \" \"than expected after decoding and re-encoding. This is \" \"expected due to the imperfect nature of the sampling \" \"procedure.\", abs(token_mismatch_total), \"more\" if token_mismatch_total > 0 else \"fewer\", ) return batch_requests RandomMultiModalDataset ¶ Bases: RandomDataset Synthetic multimodal dataset (text + images) that extends RandomDataset. Status: - Images: supported via synthetic RGB data. - Video: supported via synthetic RGB data. - Audio: not yet supported. Sampling overview: 1) Number of items per request is sampled uniformly from the integer range [floor(n·(1−r)), ceil(n·(1+r))], where n is the base count and r is num_mm_items_range_ratio in [0, 1]. r=0 keeps it fixed; r=1 allows 0. The maximum is further clamped to the sum of per-modality limits. 2) Each item’s modality and shape is sampled from bucket_config, a dict mapping (height, width, num_frames) → probability. We treat num_frames=1 as image and num_frames > 1 as video. Entries with zero probability are removed and the rest are renormalized to sum to 1. 3) Per-modality hard caps are enforced via limit_mm_per_prompt. When a modality reaches its cap, all of its buckets are excluded and the remaining probabilities are renormalized. Example bucket configuration: {(256, 256, 1): 0.5, (720, 1280, 1): 0.4, (720, 1280, 16): 0.1} - Two image buckets (num_frames=1) and one video bucket (num_frames=16). OBS.: Only image sampling is supported for now. Source code in vllm/benchmarks/datasets.py 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203class RandomMultiModalDataset(RandomDataset): \"\"\" Synthetic multimodal dataset (text + images) that extends RandomDataset. Status: - Images: supported via synthetic RGB data. - Video: supported via synthetic RGB data. - Audio: not yet supported. Sampling overview: 1) Number of items per request is sampled uniformly from the integer range [floor(n·(1−r)), ceil(n·(1+r))], where n is the base count and r is `num_mm_items_range_ratio` in [0, 1]. r=0 keeps it fixed; r=1 allows 0. The maximum is further clamped to the sum of per-modality limits. 2) Each item’s modality and shape is sampled from `bucket_config`, a dict mapping (height, width, num_frames) → probability. We treat `num_frames`=1 as image and `num_frames` > 1 as video. Entries with zero probability are removed and the rest are renormalized to sum to 1. 3) Per-modality hard caps are enforced via `limit_mm_per_prompt`. When a modality reaches its cap, all of its buckets are excluded and the remaining probabilities are renormalized. Example bucket configuration: {(256, 256, 1): 0.5, (720, 1280, 1): 0.4, (720, 1280, 16): 0.1} - Two image buckets (`num_frames`=1) and one video bucket (`num_frames`=16). OBS.: Only image sampling is supported for now. \"\"\" IS_MULTIMODAL = True DEFAULT_LIMIT_MM_PER_PROMPT = {\"image\": 255, \"video\": 1} DEFAULT_BASE_ITEMS_PER_REQUEST = 1 DEFAULT_NUM_MM_ITEMS_RANGE_RATIO = 0.0 DEFAULT_MM_ITEM_BUCKET_CONFIG = { (256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0, } DEFAULT_ENABLE_MULTIMODAL_CHAT = False def __init__(self, **kwargs) -> None: super().__init__(**kwargs) def generate_synthetic_image(self, width: int, height: int) -> Image.Image: \"\"\"Generate synthetic PIL image with random RGB values. NOTE: iid pixel sampling results in worst-case compression (good for stressing I/O), but very unlike real photos. We could consider a “low-freq” mode (e.g., noise blur) to emulate network realism instead of max stress. \"\"\" random_pixels = self._rng.integers( 0, 256, (height, width, 3), dtype=np.uint8, ) return Image.fromarray(random_pixels) def generate_synthetic_video( self, width: int, height: int, num_frames: int ) -> dict: \"\"\"Generate synthetic video with random values. Creates a video with random pixel values, encodes it to MP4 format, and returns the content as bytes. \"\"\" import cv2 random_pixels = self._rng.integers( 0, 256, (num_frames, height, width, 3), dtype=np.uint8, ) # Create a temporary video file in memory fourcc = cv2.VideoWriter_fourcc(*\"mp4v\") fps = 30 # frames per second with NamedTemporaryFile(suffix=\".mp4\", delete_on_close=False) as temp_file: temp_path = temp_file.name # Create video writer video_writer = cv2.VideoWriter( temp_path, fourcc=fourcc, fps=fps, frameSize=(width, height) ) if not video_writer.isOpened(): raise RuntimeError(\"Failed to create video writer\") for frame in random_pixels: video_writer.write(frame) video_writer.release() temp_file.close() # Read the video file content with open(temp_path, \"rb\") as f: video_content = f.read() return {\"bytes\": video_content} def map_config_to_modality(self, config: tuple[int, int, int]) -> str: \"\"\"Map the configuration to the modality.\"\"\" if config[-1] == 1: return \"image\" elif config[-1] > 1: return \"video\" else: raise ValueError(f\"Invalid multimodal item configuration: {config}\") def normalize_bucket_config( self, bucket_config: dict[tuple[int, int, int], float] ) -> dict[tuple[int, int, int], float]: \"\"\" Remove zero probability entries and normalize the bucket config to sum to 1. \"\"\" # Raise error if value is negative if any(v < 0 for v in bucket_config.values()): raise ValueError(\"Bucket config values must be non-negative.\") # Remove zero probability entries bucket_config = {k: v for k, v in bucket_config.items() if v > 0} # if bucket config is empty, raise error if not bucket_config: raise ValueError( \"Got invalid bucket config. Bucket config values must be non-zero.\" ) # Normalize the remaining bucket config to sum to 1 total = sum(bucket_config.values()) return {k: v / total for k, v in bucket_config.items()} def generate_mm_item( self, mm_item_config: tuple[int, int, int], ) -> Mapping[str, Any]: \"\"\" Create synthetic images and videos and apply process_image/process_video respectively. This follows the OpenAI API chat completions https://github.com/openai/openai-python \"\"\" if self.map_config_to_modality(mm_item_config) == \"image\": return process_image( self.generate_synthetic_image(mm_item_config[1], mm_item_config[0]) ) elif self.map_config_to_modality(mm_item_config) == \"video\": return process_video( self.generate_synthetic_video( mm_item_config[1], mm_item_config[0], mm_item_config[2] ) ) else: raise ValueError(f\"Invalid multimodal item configuration: {mm_item_config}\") def get_mm_item_sampling_params( self, base_items_per_request: int, num_mm_items_range_ratio: float, limit_mm_per_prompt: dict[str, int], bucket_config: dict[tuple[int, int, int], float], ) -> tuple[int, int, dict[str, int], dict[tuple[int, int, int], float]]: \"\"\" Get the sampling parameters for the multimodal items. \"\"\" # Enforce num_mm_items_range_ratio <= 1 if not (0.0 <= num_mm_items_range_ratio <= 1.0): raise ValueError(\"num_mm_items_range_ratio must be in [0, 1].\") # Ensure modalities to sample are in limit_mm_per_prompt for k, v in bucket_config.items(): # get modality from bucket config modality = self.map_config_to_modality(k) if modality not in limit_mm_per_prompt: raise ValueError( f\"Modality {modality} is not in \" f\"limit_mm_per_prompt: \" f\"{limit_mm_per_prompt.keys()}\" ) # Remove zero probability entries # and normalize bucket config to sum to 1 bucket_config = self.normalize_bucket_config(bucket_config) logger.info( \"Normalized bucket config: %s\", bucket_config, ) # Only consider limit per prompt for modalities in bucket config allowed_modalities = {self.map_config_to_modality(cfg) for cfg in bucket_config} limit_mm_per_prompt = { k: v for k, v in limit_mm_per_prompt.items() if k in allowed_modalities } if not limit_mm_per_prompt: raise ValueError(\"No valid limits for modalities present in bucket_config.\") logger.info( \"Updated mm-limit-per-prompt: %s\", limit_mm_per_prompt, ) # Get max and min num mm items and ensure # it is at most the sum of limit_mm_per_prompt for all modalities max_num_mm_items = min( sum(limit_mm_per_prompt.values()), math.ceil(base_items_per_request * (1 + num_mm_items_range_ratio)), ) # Ensure min num mm items is at least 0 min_num_mm_items = max( 0, math.floor(base_items_per_request * (1 - num_mm_items_range_ratio)) ) # Raise error if min num mm items is greater than max num mm items if min_num_mm_items > max_num_mm_items: raise ValueError( f\"Min num mm items is greater than max mm items: \" f\"{min_num_mm_items} > {max_num_mm_items}\" ) logger.info( \"Sampling number of multimodal items from [%s, %s]\", min_num_mm_items, max_num_mm_items, ) return ( min_num_mm_items, max_num_mm_items, limit_mm_per_prompt, bucket_config, ) def get_mm_item_iterator( self, min_num_mm_items: int, max_num_mm_items: int, bucket_config: dict[tuple[int, int, int], float], limit_mm_per_prompt: dict[str, int], ) -> Iterator[tuple[int, int, int]]: \"\"\" Iterator over the multimodal items for each request whose size is between min_num_mm_items and max_num_mm_items. Loop over the bucket config and sample a multimodal item. Loop until the number of multimodal items sampled is equal to request_num_mm_items or limit of multimodal items per prompt for all modalities is reached. Note: - This function operates on a per-request shallow copy of `bucket_config` (tuple->float). The original dict passed to `sample` is not mutated. If this ever changes, a test is implemented and will fail. \"\"\" # Get the number of multimodal items to sample request_num_mm_items = int( self._rng.integers(min_num_mm_items, max_num_mm_items + 1) ) # If request_num_mm_items is 0, yield an empty iterator if request_num_mm_items == 0: return # Initialize modality counters modality_counter = {self.map_config_to_modality(k): 0 for k in bucket_config} # Copy the bucket config to avoid modifying the original bucket_config_copy = bucket_config.copy() # Loop over the number of multimodal items to sample while sum(modality_counter.values()) < request_num_mm_items: # Sample a multimodal item config mm_item_config = self._rng.choice( list(bucket_config_copy.keys()), p=list(bucket_config_copy.values()) ) modality = self.map_config_to_modality(mm_item_config) # Check that modality count is less than limit per prompt if modality_counter[modality] < limit_mm_per_prompt[modality]: modality_counter[modality] += 1 yield (mm_item_config) else: # If the counter is greater than the limit per prompt # set all multimodal items of this modality to 0 for k, v in bucket_config_copy.items(): if self.map_config_to_modality(k) == modality: bucket_config_copy[k] = 0 # If all configs are 0, break the loop # This should not happen as request_num_mm_items is at most # the sum of limit_mm_per_prompt for all modalities if all(v == 0 for v in bucket_config_copy.values()): logger.warning( \"Exhausted all multimodal items of modality %s\", modality ) break # Renormalize the bucket config bucket_config_copy = self.normalize_bucket_config(bucket_config_copy) def sample( self, tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, prefix_len: int = RandomDataset.DEFAULT_PREFIX_LEN, range_ratio: float = RandomDataset.DEFAULT_RANGE_RATIO, input_len: int = RandomDataset.DEFAULT_INPUT_LEN, output_len: int = RandomDataset.DEFAULT_OUTPUT_LEN, limit_mm_per_prompt: dict[str, int] = DEFAULT_LIMIT_MM_PER_PROMPT, base_items_per_request: int = DEFAULT_BASE_ITEMS_PER_REQUEST, num_mm_items_range_ratio: float = DEFAULT_NUM_MM_ITEMS_RANGE_RATIO, bucket_config: dict[ tuple[int, int, int], float ] = DEFAULT_MM_ITEM_BUCKET_CONFIG, enable_multimodal_chat: bool = DEFAULT_ENABLE_MULTIMODAL_CHAT, **kwargs, ) -> list[SampleRequest]: # Get the sampling parameters for the dataset input_lens, output_lens, offsets = self.get_sampling_params( num_requests, range_ratio, input_len, output_len, tokenizer ) ( min_num_mm_items, max_num_mm_items, limit_mm_per_prompt, bucket_config, ) = self.get_mm_item_sampling_params( base_items_per_request, num_mm_items_range_ratio, limit_mm_per_prompt, bucket_config, ) vocab_size = tokenizer.vocab_size # Can't use tokenizer.all_special_ids since # it returns ONLY ids from special_tokens_map.json # We want to exclude placeholder tokens and all # tokens that indicate start/end of image as it # may break prompt replacement logic. prohibited_tokens = list( tok_id for tok_id, token in tokenizer.added_tokens_decoder.items() if token.special ) all_tokens = np.arange(vocab_size) allowed_tokens = np.array(list(set(all_tokens) - set(prohibited_tokens))) logger.debug( \"Sampling from %d out of %d (vocab size)\", len(allowed_tokens), vocab_size ) # Generate prefix once prefix_token_ids = self.get_prefix(allowed_tokens, prefix_len) # Add synthetic multimodal items to each request mm_requests = [] token_mismatch_total = 0 for i in range(num_requests): prompt, total_input_len, token_mismatch = self.generate_token_sequence( # noqa: E501 tokenizer=tokenizer, prefix_token_ids=prefix_token_ids, prefix_len=prefix_len, vocab_size=vocab_size, input_len=int(input_lens[i]), offset=int(offsets[i]), index=i, allowed_tokens=allowed_tokens, ) token_mismatch_total += token_mismatch # Get multimodal item iterator for a given request mm_item_iterator = self.get_mm_item_iterator( min_num_mm_items, max_num_mm_items, bucket_config, limit_mm_per_prompt, ) mm_content = cast( list[dict[str, Any]], [ self.generate_mm_item(mm_item_config) for mm_item_config in mm_item_iterator ], ) if enable_multimodal_chat: # NOTE: For now this option is only provided for completeness # given that the serve.py benchmark currently does not use it. mm_chat_prompt: Any = prompt mm_chat_prompt = self.apply_multimodal_chat_transformation( prompt, mm_content ) sample_request = SampleRequest( prompt=mm_chat_prompt, prompt_len=total_input_len, expected_output_len=int(output_lens[i]), multi_modal_data=None, request_id=request_id_prefix + str(i), ) else: sample_request = SampleRequest( prompt=prompt, prompt_len=total_input_len, expected_output_len=int(output_lens[i]), multi_modal_data=mm_content, request_id=request_id_prefix + str(i), ) mm_requests.append(sample_request) if token_mismatch_total != 0: sign = \"more\" if token_mismatch_total > 0 else \"fewer\" logger.warning( \"Across all generated prompts, there were %d %s tokens \" \"than expected after decoding and re-encoding. This is \" \"expected due to the imperfect nature of the sampling \" \"procedure.\", abs(token_mismatch_total), sign, ) return mm_requests DEFAULT_BASE_ITEMS_PER_REQUEST class-attribute instance-attribute ¶ DEFAULT_BASE_ITEMS_PER_REQUEST = 1 DEFAULT_ENABLE_MULTIMODAL_CHAT class-attribute instance-attribute ¶ DEFAULT_ENABLE_MULTIMODAL_CHAT = False DEFAULT_LIMIT_MM_PER_PROMPT class-attribute instance-attribute ¶ DEFAULT_LIMIT_MM_PER_PROMPT = {'image': 255, 'video': 1} DEFAULT_MM_ITEM_BUCKET_CONFIG class-attribute instance-attribute ¶ DEFAULT_MM_ITEM_BUCKET_CONFIG = { (256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0, } DEFAULT_NUM_MM_ITEMS_RANGE_RATIO class-attribute instance-attribute ¶ DEFAULT_NUM_MM_ITEMS_RANGE_RATIO = 0.0 IS_MULTIMODAL class-attribute instance-attribute ¶ IS_MULTIMODAL = True __init__ ¶ __init__(**kwargs) -> None Source code in vllm/benchmarks/datasets.py 830 831def __init__(self, **kwargs) -> None: super().__init__(**kwargs) generate_mm_item ¶ generate_mm_item( mm_item_config: tuple[int, int, int], ) -> Mapping[str, Any] Create synthetic images and videos and apply process_image/process_video respectively. This follows the OpenAI API chat completions https://github.com/openai/openai-python Source code in vllm/benchmarks/datasets.py 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945def generate_mm_item( self, mm_item_config: tuple[int, int, int], ) -> Mapping[str, Any]: \"\"\" Create synthetic images and videos and apply process_image/process_video respectively. This follows the OpenAI API chat completions https://github.com/openai/openai-python \"\"\" if self.map_config_to_modality(mm_item_config) == \"image\": return process_image( self.generate_synthetic_image(mm_item_config[1], mm_item_config[0]) ) elif self.map_config_to_modality(mm_item_config) == \"video\": return process_video( self.generate_synthetic_video( mm_item_config[1], mm_item_config[0], mm_item_config[2] ) ) else: raise ValueError(f\"Invalid multimodal item configuration: {mm_item_config}\") generate_synthetic_image ¶ generate_synthetic_image(width: int, height: int) -> Image Generate synthetic PIL image with random RGB values. NOTE: iid pixel sampling results in worst-case compression (good for stressing I/O), but very unlike real photos. We could consider a “low-freq” mode (e.g., noise blur) to emulate network realism instead of max stress. Source code in vllm/benchmarks/datasets.py 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847def generate_synthetic_image(self, width: int, height: int) -> Image.Image: \"\"\"Generate synthetic PIL image with random RGB values. NOTE: iid pixel sampling results in worst-case compression (good for stressing I/O), but very unlike real photos. We could consider a “low-freq” mode (e.g., noise blur) to emulate network realism instead of max stress. \"\"\" random_pixels = self._rng.integers( 0, 256, (height, width, 3), dtype=np.uint8, ) return Image.fromarray(random_pixels) generate_synthetic_video ¶ generate_synthetic_video( width: int, height: int, num_frames: int ) -> dict Generate synthetic video with random values. Creates a video with random pixel values, encodes it to MP4 format, and returns the content as bytes. Source code in vllm/benchmarks/datasets.py 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891def generate_synthetic_video( self, width: int, height: int, num_frames: int ) -> dict: \"\"\"Generate synthetic video with random values. Creates a video with random pixel values, encodes it to MP4 format, and returns the content as bytes. \"\"\" import cv2 random_pixels = self._rng.integers( 0, 256, (num_frames, height, width, 3), dtype=np.uint8, ) # Create a temporary video file in memory fourcc = cv2.VideoWriter_fourcc(*\"mp4v\") fps = 30 # frames per second with NamedTemporaryFile(suffix=\".mp4\", delete_on_close=False) as temp_file: temp_path = temp_file.name # Create video writer video_writer = cv2.VideoWriter( temp_path, fourcc=fourcc, fps=fps, frameSize=(width, height) ) if not video_writer.isOpened(): raise RuntimeError(\"Failed to create video writer\") for frame in random_pixels: video_writer.write(frame) video_writer.release() temp_file.close() # Read the video file content with open(temp_path, \"rb\") as f: video_content = f.read() return {\"bytes\": video_content} get_mm_item_iterator ¶ get_mm_item_iterator( min_num_mm_items: int, max_num_mm_items: int, bucket_config: dict[tuple[int, int, int], float], limit_mm_per_prompt: dict[str, int], ) -> Iterator[tuple[int, int, int]] Iterator over the multimodal items for each request whose size is between min_num_mm_items and max_num_mm_items. Loop over the bucket config and sample a multimodal item. Loop until the number of multimodal items sampled is equal to request_num_mm_items or limit of multimodal items per prompt for all modalities is reached. Note: - This function operates on a per-request shallow copy of bucket_config (tuple->float). The original dict passed to sample is not mutated. If this ever changes, a test is implemented and will fail. Source code in vllm/benchmarks/datasets.py 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081def get_mm_item_iterator( self, min_num_mm_items: int, max_num_mm_items: int, bucket_config: dict[tuple[int, int, int], float], limit_mm_per_prompt: dict[str, int], ) -> Iterator[tuple[int, int, int]]: \"\"\" Iterator over the multimodal items for each request whose size is between min_num_mm_items and max_num_mm_items. Loop over the bucket config and sample a multimodal item. Loop until the number of multimodal items sampled is equal to request_num_mm_items or limit of multimodal items per prompt for all modalities is reached. Note: - This function operates on a per-request shallow copy of `bucket_config` (tuple->float). The original dict passed to `sample` is not mutated. If this ever changes, a test is implemented and will fail. \"\"\" # Get the number of multimodal items to sample request_num_mm_items = int( self._rng.integers(min_num_mm_items, max_num_mm_items + 1) ) # If request_num_mm_items is 0, yield an empty iterator if request_num_mm_items == 0: return # Initialize modality counters modality_counter = {self.map_config_to_modality(k): 0 for k in bucket_config} # Copy the bucket config to avoid modifying the original bucket_config_copy = bucket_config.copy() # Loop over the number of multimodal items to sample while sum(modality_counter.values()) < request_num_mm_items: # Sample a multimodal item config mm_item_config = self._rng.choice( list(bucket_config_copy.keys()), p=list(bucket_config_copy.values()) ) modality = self.map_config_to_modality(mm_item_config) # Check that modality count is less than limit per prompt if modality_counter[modality] < limit_mm_per_prompt[modality]: modality_counter[modality] += 1 yield (mm_item_config) else: # If the counter is greater than the limit per prompt # set all multimodal items of this modality to 0 for k, v in bucket_config_copy.items(): if self.map_config_to_modality(k) == modality: bucket_config_copy[k] = 0 # If all configs are 0, break the loop # This should not happen as request_num_mm_items is at most # the sum of limit_mm_per_prompt for all modalities if all(v == 0 for v in bucket_config_copy.values()): logger.warning( \"Exhausted all multimodal items of modality %s\", modality ) break # Renormalize the bucket config bucket_config_copy = self.normalize_bucket_config(bucket_config_copy) get_mm_item_sampling_params ¶ get_mm_item_sampling_params( base_items_per_request: int, num_mm_items_range_ratio: float, limit_mm_per_prompt: dict[str, int], bucket_config: dict[tuple[int, int, int], float], ) -> tuple[ int, int, dict[str, int], dict[tuple[int, int, int], float], ] Get the sampling parameters for the multimodal items. Source code in vllm/benchmarks/datasets.py 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020def get_mm_item_sampling_params( self, base_items_per_request: int, num_mm_items_range_ratio: float, limit_mm_per_prompt: dict[str, int], bucket_config: dict[tuple[int, int, int], float], ) -> tuple[int, int, dict[str, int], dict[tuple[int, int, int], float]]: \"\"\" Get the sampling parameters for the multimodal items. \"\"\" # Enforce num_mm_items_range_ratio <= 1 if not (0.0 <= num_mm_items_range_ratio <= 1.0): raise ValueError(\"num_mm_items_range_ratio must be in [0, 1].\") # Ensure modalities to sample are in limit_mm_per_prompt for k, v in bucket_config.items(): # get modality from bucket config modality = self.map_config_to_modality(k) if modality not in limit_mm_per_prompt: raise ValueError( f\"Modality {modality} is not in \" f\"limit_mm_per_prompt: \" f\"{limit_mm_per_prompt.keys()}\" ) # Remove zero probability entries # and normalize bucket config to sum to 1 bucket_config = self.normalize_bucket_config(bucket_config) logger.info( \"Normalized bucket config: %s\", bucket_config, ) # Only consider limit per prompt for modalities in bucket config allowed_modalities = {self.map_config_to_modality(cfg) for cfg in bucket_config} limit_mm_per_prompt = { k: v for k, v in limit_mm_per_prompt.items() if k in allowed_modalities } if not limit_mm_per_prompt: raise ValueError(\"No valid limits for modalities present in bucket_config.\") logger.info( \"Updated mm-limit-per-prompt: %s\", limit_mm_per_prompt, ) # Get max and min num mm items and ensure # it is at most the sum of limit_mm_per_prompt for all modalities max_num_mm_items = min( sum(limit_mm_per_prompt.values()), math.ceil(base_items_per_request * (1 + num_mm_items_range_ratio)), ) # Ensure min num mm items is at least 0 min_num_mm_items = max( 0, math.floor(base_items_per_request * (1 - num_mm_items_range_ratio)) ) # Raise error if min num mm items is greater than max num mm items if min_num_mm_items > max_num_mm_items: raise ValueError( f\"Min num mm items is greater than max mm items: \" f\"{min_num_mm_items} > {max_num_mm_items}\" ) logger.info( \"Sampling number of multimodal items from [%s, %s]\", min_num_mm_items, max_num_mm_items, ) return ( min_num_mm_items, max_num_mm_items, limit_mm_per_prompt, bucket_config, ) map_config_to_modality ¶ map_config_to_modality(config: tuple[int, int, int]) -> str Map the configuration to the modality. Source code in vllm/benchmarks/datasets.py 893 894 895 896 897 898 899 900def map_config_to_modality(self, config: tuple[int, int, int]) -> str: \"\"\"Map the configuration to the modality.\"\"\" if config[-1] == 1: return \"image\" elif config[-1] > 1: return \"video\" else: raise ValueError(f\"Invalid multimodal item configuration: {config}\") normalize_bucket_config ¶ normalize_bucket_config( bucket_config: dict[tuple[int, int, int], float], ) -> dict[tuple[int, int, int], float] Remove zero probability entries and normalize the bucket config to sum to 1. Source code in vllm/benchmarks/datasets.py 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921def normalize_bucket_config( self, bucket_config: dict[tuple[int, int, int], float] ) -> dict[tuple[int, int, int], float]: \"\"\" Remove zero probability entries and normalize the bucket config to sum to 1. \"\"\" # Raise error if value is negative if any(v < 0 for v in bucket_config.values()): raise ValueError(\"Bucket config values must be non-negative.\") # Remove zero probability entries bucket_config = {k: v for k, v in bucket_config.items() if v > 0} # if bucket config is empty, raise error if not bucket_config: raise ValueError( \"Got invalid bucket config. Bucket config values must be non-zero.\" ) # Normalize the remaining bucket config to sum to 1 total = sum(bucket_config.values()) return {k: v / total for k, v in bucket_config.items()} sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, prefix_len: int = DEFAULT_PREFIX_LEN, range_ratio: float = DEFAULT_RANGE_RATIO, input_len: int = DEFAULT_INPUT_LEN, output_len: int = DEFAULT_OUTPUT_LEN, limit_mm_per_prompt: dict[ str, int ] = DEFAULT_LIMIT_MM_PER_PROMPT, base_items_per_request: int = DEFAULT_BASE_ITEMS_PER_REQUEST, num_mm_items_range_ratio: float = DEFAULT_NUM_MM_ITEMS_RANGE_RATIO, bucket_config: dict[ tuple[int, int, int], float ] = DEFAULT_MM_ITEM_BUCKET_CONFIG, enable_multimodal_chat: bool = DEFAULT_ENABLE_MULTIMODAL_CHAT, **kwargs, ) -> list[SampleRequest] Source code in vllm/benchmarks/datasets.py 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203def sample( self, tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, prefix_len: int = RandomDataset.DEFAULT_PREFIX_LEN, range_ratio: float = RandomDataset.DEFAULT_RANGE_RATIO, input_len: int = RandomDataset.DEFAULT_INPUT_LEN, output_len: int = RandomDataset.DEFAULT_OUTPUT_LEN, limit_mm_per_prompt: dict[str, int] = DEFAULT_LIMIT_MM_PER_PROMPT, base_items_per_request: int = DEFAULT_BASE_ITEMS_PER_REQUEST, num_mm_items_range_ratio: float = DEFAULT_NUM_MM_ITEMS_RANGE_RATIO, bucket_config: dict[ tuple[int, int, int], float ] = DEFAULT_MM_ITEM_BUCKET_CONFIG, enable_multimodal_chat: bool = DEFAULT_ENABLE_MULTIMODAL_CHAT, **kwargs, ) -> list[SampleRequest]: # Get the sampling parameters for the dataset input_lens, output_lens, offsets = self.get_sampling_params( num_requests, range_ratio, input_len, output_len, tokenizer ) ( min_num_mm_items, max_num_mm_items, limit_mm_per_prompt, bucket_config, ) = self.get_mm_item_sampling_params( base_items_per_request, num_mm_items_range_ratio, limit_mm_per_prompt, bucket_config, ) vocab_size = tokenizer.vocab_size # Can't use tokenizer.all_special_ids since # it returns ONLY ids from special_tokens_map.json # We want to exclude placeholder tokens and all # tokens that indicate start/end of image as it # may break prompt replacement logic. prohibited_tokens = list( tok_id for tok_id, token in tokenizer.added_tokens_decoder.items() if token.special ) all_tokens = np.arange(vocab_size) allowed_tokens = np.array(list(set(all_tokens) - set(prohibited_tokens))) logger.debug( \"Sampling from %d out of %d (vocab size)\", len(allowed_tokens), vocab_size ) # Generate prefix once prefix_token_ids = self.get_prefix(allowed_tokens, prefix_len) # Add synthetic multimodal items to each request mm_requests = [] token_mismatch_total = 0 for i in range(num_requests): prompt, total_input_len, token_mismatch = self.generate_token_sequence( # noqa: E501 tokenizer=tokenizer, prefix_token_ids=prefix_token_ids, prefix_len=prefix_len, vocab_size=vocab_size, input_len=int(input_lens[i]), offset=int(offsets[i]), index=i, allowed_tokens=allowed_tokens, ) token_mismatch_total += token_mismatch # Get multimodal item iterator for a given request mm_item_iterator = self.get_mm_item_iterator( min_num_mm_items, max_num_mm_items, bucket_config, limit_mm_per_prompt, ) mm_content = cast( list[dict[str, Any]], [ self.generate_mm_item(mm_item_config) for mm_item_config in mm_item_iterator ], ) if enable_multimodal_chat: # NOTE: For now this option is only provided for completeness # given that the serve.py benchmark currently does not use it. mm_chat_prompt: Any = prompt mm_chat_prompt = self.apply_multimodal_chat_transformation( prompt, mm_content ) sample_request = SampleRequest( prompt=mm_chat_prompt, prompt_len=total_input_len, expected_output_len=int(output_lens[i]), multi_modal_data=None, request_id=request_id_prefix + str(i), ) else: sample_request = SampleRequest( prompt=prompt, prompt_len=total_input_len, expected_output_len=int(output_lens[i]), multi_modal_data=mm_content, request_id=request_id_prefix + str(i), ) mm_requests.append(sample_request) if token_mismatch_total != 0: sign = \"more\" if token_mismatch_total > 0 else \"fewer\" logger.warning( \"Across all generated prompts, there were %d %s tokens \" \"than expected after decoding and re-encoding. This is \" \"expected due to the imperfect nature of the sampling \" \"procedure.\", abs(token_mismatch_total), sign, ) return mm_requests SampleRequest dataclass ¶ Represents a single inference request for benchmarking. Source code in vllm/benchmarks/datasets.py 72 73 74 75 76 77 78 79 80 81 82 83@dataclass class SampleRequest: \"\"\" Represents a single inference request for benchmarking. \"\"\" prompt: str | list[str] prompt_len: int expected_output_len: int multi_modal_data: MultiModalDataDict | dict | list[dict] | None = None lora_request: LoRARequest | None = None request_id: str | None = None expected_output_len instance-attribute ¶ expected_output_len: int lora_request class-attribute instance-attribute ¶ lora_request: LoRARequest | None = None multi_modal_data class-attribute instance-attribute ¶ multi_modal_data: ( MultiModalDataDict | dict | list[dict] | None ) = None prompt instance-attribute ¶ prompt: str | list[str] prompt_len instance-attribute ¶ prompt_len: int request_id class-attribute instance-attribute ¶ request_id: str | None = None __init__ ¶ __init__( prompt: str | list[str], prompt_len: int, expected_output_len: int, multi_modal_data: MultiModalDataDict | dict | list[dict] | None = None, lora_request: LoRARequest | None = None, request_id: str | None = None, ) -> None ShareGPTDataset ¶ Bases: BenchmarkDataset Implements the ShareGPT dataset. Loads data from a JSON file and generates sample requests based on conversation turns. Source code in vllm/benchmarks/datasets.py 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294class ShareGPTDataset(BenchmarkDataset): \"\"\" Implements the ShareGPT dataset. Loads data from a JSON file and generates sample requests based on conversation turns. \"\"\" def __init__(self, **kwargs) -> None: super().__init__(**kwargs) self.load_data() def load_data(self) -> None: if self.dataset_path is None: raise ValueError(\"dataset_path must be provided for loading data.\") with open(self.dataset_path, encoding=\"utf-8\") as f: self.data = json.load(f) # Filter entries with at least two conversation turns. self.data = [ entry for entry in self.data if \"conversations\" in entry and len(entry[\"conversations\"]) >= 2 ] random.seed(self.random_seed) if not getattr(self, \"disable_shuffle\", False): random.shuffle(self.data) def sample( self, tokenizer: TokenizerLike, num_requests: int, lora_path: str | None = None, max_loras: int | None = None, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: samples: list = [] ind = 0 for entry in self.data: if len(samples) >= num_requests: break prompt, completion = ( entry[\"conversations\"][0][\"value\"], entry[\"conversations\"][1][\"value\"], ) lora_request = self.get_random_lora_request( max_loras=max_loras, lora_path=lora_path ) prompt_ids = tokenizer(prompt).input_ids completion_ids = tokenizer(completion).input_ids prompt_len = len(prompt_ids) new_output_len = len(completion_ids) if output_len is None else output_len if not is_valid_sequence( prompt_len, new_output_len, skip_min_output_len_check=output_len is not None, ): continue if image_path := entry.get(\"image\"): mm_content = process_image(image_path) elif video_path := entry.get(\"video\"): mm_content = process_video(video_path) else: mm_content = None if enable_multimodal_chat: prompt = self.apply_multimodal_chat_transformation(prompt, mm_content) samples.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=new_output_len, lora_request=lora_request, multi_modal_data=mm_content, request_id=request_id_prefix + str(ind), ) ) ind += 1 self.maybe_oversample_requests( samples, num_requests, request_id_prefix, no_oversample ) return samples __init__ ¶ __init__(**kwargs) -> None Source code in vllm/benchmarks/datasets.py 1217 1218 1219def __init__(self, **kwargs) -> None: super().__init__(**kwargs) self.load_data() load_data ¶ load_data() -> None Source code in vllm/benchmarks/datasets.py 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235def load_data(self) -> None: if self.dataset_path is None: raise ValueError(\"dataset_path must be provided for loading data.\") with open(self.dataset_path, encoding=\"utf-8\") as f: self.data = json.load(f) # Filter entries with at least two conversation turns. self.data = [ entry for entry in self.data if \"conversations\" in entry and len(entry[\"conversations\"]) >= 2 ] random.seed(self.random_seed) if not getattr(self, \"disable_shuffle\", False): random.shuffle(self.data) sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, lora_path: str | None = None, max_loras: int | None = None, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294def sample( self, tokenizer: TokenizerLike, num_requests: int, lora_path: str | None = None, max_loras: int | None = None, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: samples: list = [] ind = 0 for entry in self.data: if len(samples) >= num_requests: break prompt, completion = ( entry[\"conversations\"][0][\"value\"], entry[\"conversations\"][1][\"value\"], ) lora_request = self.get_random_lora_request( max_loras=max_loras, lora_path=lora_path ) prompt_ids = tokenizer(prompt).input_ids completion_ids = tokenizer(completion).input_ids prompt_len = len(prompt_ids) new_output_len = len(completion_ids) if output_len is None else output_len if not is_valid_sequence( prompt_len, new_output_len, skip_min_output_len_check=output_len is not None, ): continue if image_path := entry.get(\"image\"): mm_content = process_image(image_path) elif video_path := entry.get(\"video\"): mm_content = process_video(video_path) else: mm_content = None if enable_multimodal_chat: prompt = self.apply_multimodal_chat_transformation(prompt, mm_content) samples.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=new_output_len, lora_request=lora_request, multi_modal_data=mm_content, request_id=request_id_prefix + str(ind), ) ) ind += 1 self.maybe_oversample_requests( samples, num_requests, request_id_prefix, no_oversample ) return samples SonnetDataset ¶ Bases: BenchmarkDataset Simplified implementation of the Sonnet dataset. Loads poem lines from a text file and generates sample requests. Default values here copied from benchmark_serving.py for the sonnet dataset. Source code in vllm/benchmarks/datasets.py 2079 2080 2081 2082 2083 2084 2085 2086 2087 2088 2089 2090 2091 2092 2093 2094 2095 2096 2097 2098 2099 2100 2101 2102 2103 2104 2105 2106 2107 2108 2109 2110 2111 2112 2113 2114 2115 2116 2117 2118 2119 2120 2121 2122 2123 2124 2125 2126 2127 2128 2129 2130 2131 2132 2133 2134 2135 2136 2137 2138 2139 2140 2141 2142 2143 2144 2145 2146 2147 2148 2149 2150 2151 2152 2153 2154 2155 2156 2157 2158 2159 2160 2161 2162@deprecated( \"SonnetDataset is deprecated and will be removed in a future version.\", ) class SonnetDataset(BenchmarkDataset): \"\"\" Simplified implementation of the Sonnet dataset. Loads poem lines from a text file and generates sample requests. Default values here copied from `benchmark_serving.py` for the sonnet dataset. \"\"\" DEFAULT_PREFIX_LEN = 200 DEFAULT_INPUT_LEN = 550 DEFAULT_OUTPUT_LEN = 150 def __init__( self, **kwargs, ) -> None: super().__init__(**kwargs) self.load_data() def load_data(self) -> None: if not self.dataset_path: raise ValueError(\"dataset_path must be provided.\") with open(self.dataset_path, encoding=\"utf-8\") as f: self.data = f.readlines() def sample( self, tokenizer: TokenizerLike, num_requests: int, prefix_len: int = DEFAULT_PREFIX_LEN, input_len: int = DEFAULT_INPUT_LEN, output_len: int = DEFAULT_OUTPUT_LEN, return_prompt_formatted: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: # Calculate average token length for a poem line. tokenized_lines = [tokenizer(line).input_ids for line in self.data] avg_len = sum(len(tokens) for tokens in tokenized_lines) / len(tokenized_lines) # Build the base prompt. base_prompt = \"Pick as many lines as you can from these poem lines:\\n\" base_msg = [{\"role\": \"user\", \"content\": base_prompt}] base_fmt = tokenizer.apply_chat_template( base_msg, add_generation_prompt=True, tokenize=False ) base_offset = len(tokenizer(base_fmt).input_ids) if input_len <= base_offset: raise ValueError( f\"'input_len' must be higher than the base prompt length \" f\"({base_offset}).\" ) # Determine how many poem lines to use. num_input_lines = round((input_len - base_offset) / avg_len) num_prefix_lines = max(round((prefix_len - base_offset) / avg_len), 0) prefix_lines = self.data[:num_prefix_lines] samples = [] ind = 0 while len(samples) < num_requests: extra_lines = random.choices( self.data, k=num_input_lines - num_prefix_lines ) prompt = f\"{base_prompt}{''.join(prefix_lines + extra_lines)}\" msg = [{\"role\": \"user\", \"content\": prompt}] prompt_formatted = tokenizer.apply_chat_template( msg, add_generation_prompt=True, tokenize=False ) prompt_len = len(tokenizer(prompt_formatted).input_ids) if prompt_len <= input_len: samples.append( SampleRequest( prompt=prompt_formatted if return_prompt_formatted else prompt, prompt_len=prompt_len, expected_output_len=output_len, request_id=request_id_prefix + str(ind), ) ) ind += 1 return samples DEFAULT_INPUT_LEN class-attribute instance-attribute ¶ DEFAULT_INPUT_LEN = 550 DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶ DEFAULT_OUTPUT_LEN = 150 DEFAULT_PREFIX_LEN class-attribute instance-attribute ¶ DEFAULT_PREFIX_LEN = 200 __init__ ¶ __init__(**kwargs) -> None Source code in vllm/benchmarks/datasets.py 2093 2094 2095 2096 2097 2098def __init__( self, **kwargs, ) -> None: super().__init__(**kwargs) self.load_data() load_data ¶ load_data() -> None Source code in vllm/benchmarks/datasets.py 2100 2101 2102 2103 2104def load_data(self) -> None: if not self.dataset_path: raise ValueError(\"dataset_path must be provided.\") with open(self.dataset_path, encoding=\"utf-8\") as f: self.data = f.readlines() sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, prefix_len: int = DEFAULT_PREFIX_LEN, input_len: int = DEFAULT_INPUT_LEN, output_len: int = DEFAULT_OUTPUT_LEN, return_prompt_formatted: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 2106 2107 2108 2109 2110 2111 2112 2113 2114 2115 2116 2117 2118 2119 2120 2121 2122 2123 2124 2125 2126 2127 2128 2129 2130 2131 2132 2133 2134 2135 2136 2137 2138 2139 2140 2141 2142 2143 2144 2145 2146 2147 2148 2149 2150 2151 2152 2153 2154 2155 2156 2157 2158 2159 2160 2161 2162def sample( self, tokenizer: TokenizerLike, num_requests: int, prefix_len: int = DEFAULT_PREFIX_LEN, input_len: int = DEFAULT_INPUT_LEN, output_len: int = DEFAULT_OUTPUT_LEN, return_prompt_formatted: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: # Calculate average token length for a poem line. tokenized_lines = [tokenizer(line).input_ids for line in self.data] avg_len = sum(len(tokens) for tokens in tokenized_lines) / len(tokenized_lines) # Build the base prompt. base_prompt = \"Pick as many lines as you can from these poem lines:\\n\" base_msg = [{\"role\": \"user\", \"content\": base_prompt}] base_fmt = tokenizer.apply_chat_template( base_msg, add_generation_prompt=True, tokenize=False ) base_offset = len(tokenizer(base_fmt).input_ids) if input_len <= base_offset: raise ValueError( f\"'input_len' must be higher than the base prompt length \" f\"({base_offset}).\" ) # Determine how many poem lines to use. num_input_lines = round((input_len - base_offset) / avg_len) num_prefix_lines = max(round((prefix_len - base_offset) / avg_len), 0) prefix_lines = self.data[:num_prefix_lines] samples = [] ind = 0 while len(samples) < num_requests: extra_lines = random.choices( self.data, k=num_input_lines - num_prefix_lines ) prompt = f\"{base_prompt}{''.join(prefix_lines + extra_lines)}\" msg = [{\"role\": \"user\", \"content\": prompt}] prompt_formatted = tokenizer.apply_chat_template( msg, add_generation_prompt=True, tokenize=False ) prompt_len = len(tokenizer(prompt_formatted).input_ids) if prompt_len <= input_len: samples.append( SampleRequest( prompt=prompt_formatted if return_prompt_formatted else prompt, prompt_len=prompt_len, expected_output_len=output_len, request_id=request_id_prefix + str(ind), ) ) ind += 1 return samples SpecBench ¶ Bases: CustomDataset Implements the SpecBench dataset: https://github.com/hemingkx/Spec-Bench Download the dataset using: wget https://raw.githubusercontent.com/hemingkx/Spec-Bench/refs/heads/main/data/spec_bench/question.jsonl Source code in vllm/benchmarks/datasets.py 2034 2035 2036 2037 2038 2039 2040 2041 2042 2043 2044 2045 2046 2047 2048 2049 2050 2051 2052 2053 2054 2055 2056 2057 2058 2059 2060 2061 2062 2063 2064 2065 2066 2067 2068 2069 2070 2071class SpecBench(CustomDataset): \"\"\" Implements the SpecBench dataset: https://github.com/hemingkx/Spec-Bench Download the dataset using: wget https://raw.githubusercontent.com/hemingkx/Spec-Bench/refs/heads/main/data/spec_bench/question.jsonl \"\"\" # noqa: E501 def __init__(self, **kwargs) -> None: self.category = kwargs.pop(\"category\", None) super().__init__(**kwargs) self.load_data() def load_data(self) -> None: if self.dataset_path is None: raise ValueError(\"dataset_path must be provided for loading data.\") self.data = [] # Load the JSONL file jsonl_data = pd.read_json(path_or_buf=self.dataset_path, lines=True) # check if the JSONL file has a 'turns' column if \"turns\" not in jsonl_data.columns: raise ValueError(\"JSONL file must contain a 'turns' column.\") for _, row in jsonl_data.iterrows(): # sample only from a specific category if specified if (not self.category) or (self.category == row[\"category\"]): prompt = row[\"turns\"][0] self.data.append({\"prompt\": prompt}) random.seed(self.random_seed) if not getattr(self, \"disable_shuffle\", False): random.shuffle(self.data) def sample(self, **kwargs) -> list: # leverage CustomDataset sample return super().sample(**kwargs) category instance-attribute ¶ category = pop('category', None) __init__ ¶ __init__(**kwargs) -> None Source code in vllm/benchmarks/datasets.py 2041 2042 2043 2044def __init__(self, **kwargs) -> None: self.category = kwargs.pop(\"category\", None) super().__init__(**kwargs) self.load_data() load_data ¶ load_data() -> None Source code in vllm/benchmarks/datasets.py 2046 2047 2048 2049 2050 2051 2052 2053 2054 2055 2056 2057 2058 2059 2060 2061 2062 2063 2064 2065 2066 2067def load_data(self) -> None: if self.dataset_path is None: raise ValueError(\"dataset_path must be provided for loading data.\") self.data = [] # Load the JSONL file jsonl_data = pd.read_json(path_or_buf=self.dataset_path, lines=True) # check if the JSONL file has a 'turns' column if \"turns\" not in jsonl_data.columns: raise ValueError(\"JSONL file must contain a 'turns' column.\") for _, row in jsonl_data.iterrows(): # sample only from a specific category if specified if (not self.category) or (self.category == row[\"category\"]): prompt = row[\"turns\"][0] self.data.append({\"prompt\": prompt}) random.seed(self.random_seed) if not getattr(self, \"disable_shuffle\", False): random.shuffle(self.data) sample ¶ sample(**kwargs) -> list Source code in vllm/benchmarks/datasets.py 2069 2070 2071def sample(self, **kwargs) -> list: # leverage CustomDataset sample return super().sample(**kwargs) VisionArenaDataset ¶ Bases: HuggingFaceDataset Vision Arena Dataset. Source code in vllm/benchmarks/datasets.py 2409 2410 2411 2412 2413 2414 2415 2416 2417 2418 2419 2420 2421 2422 2423 2424 2425 2426 2427 2428 2429 2430 2431 2432 2433 2434 2435 2436 2437 2438 2439 2440 2441 2442 2443 2444 2445 2446 2447 2448 2449 2450 2451 2452 2453 2454 2455 2456 2457 2458 2459class VisionArenaDataset(HuggingFaceDataset): \"\"\" Vision Arena Dataset. \"\"\" DEFAULT_OUTPUT_LEN = 128 SUPPORTED_DATASET_PATHS = { \"lmarena-ai/VisionArena-Chat\": lambda x: x[\"conversation\"][0][0][\"content\"], \"lmarena-ai/vision-arena-bench-v0.1\": lambda x: x[\"turns\"][0][0][\"content\"], } IS_MULTIMODAL = True def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break parser_fn = self.SUPPORTED_DATASET_PATHS.get(self.hf_name) if parser_fn is None: raise ValueError(f\"Unsupported dataset path: {self.hf_name}\") prompt = parser_fn(item) mm_content = process_image(item[\"images\"][0]) prompt_len = len(tokenizer(prompt).input_ids) if enable_multimodal_chat: # Note: when chat is enabled the request prompt_len is no longer # accurate and we will be using request output to count the # actual prompt len prompt = self.apply_multimodal_chat_transformation(prompt, mm_content) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_content, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶ DEFAULT_OUTPUT_LEN = 128 IS_MULTIMODAL class-attribute instance-attribute ¶ IS_MULTIMODAL = True SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = { \"lmarena-ai/VisionArena-Chat\": lambda x: x[ \"conversation\" ][0][0][\"content\"], \"lmarena-ai/vision-arena-bench-v0.1\": lambda x: x[ \"turns\" ][0][0][\"content\"], } sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 2421 2422 2423 2424 2425 2426 2427 2428 2429 2430 2431 2432 2433 2434 2435 2436 2437 2438 2439 2440 2441 2442 2443 2444 2445 2446 2447 2448 2449 2450 2451 2452 2453 2454 2455 2456 2457 2458 2459def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break parser_fn = self.SUPPORTED_DATASET_PATHS.get(self.hf_name) if parser_fn is None: raise ValueError(f\"Unsupported dataset path: {self.hf_name}\") prompt = parser_fn(item) mm_content = process_image(item[\"images\"][0]) prompt_len = len(tokenizer(prompt).input_ids) if enable_multimodal_chat: # Note: when chat is enabled the request prompt_len is no longer # accurate and we will be using request output to count the # actual prompt len prompt = self.apply_multimodal_chat_transformation(prompt, mm_content) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_content, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests _ValidateDatasetArgs ¶ Bases: Action Argparse action to validate dataset name and path compatibility. Source code in vllm/benchmarks/datasets.py 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314class _ValidateDatasetArgs(argparse.Action): \"\"\"Argparse action to validate dataset name and path compatibility.\"\"\" def __call__(self, parser, namespace, values, option_string=None): setattr(namespace, self.dest, values) # Get current values of both dataset_name and dataset_path dataset_name = getattr(namespace, \"dataset_name\", \"random\") dataset_path = getattr(namespace, \"dataset_path\", None) # Validate the combination if dataset_name == \"random\" and dataset_path is not None: parser.error( \"Cannot use 'random' dataset with --dataset-path. \" \"Please specify the appropriate --dataset-name (e.g., \" \"'sharegpt', 'custom', 'sonnet') for your dataset file: \" f\"{dataset_path}\" ) __call__ ¶ __call__(parser, namespace, values, option_string=None) Source code in vllm/benchmarks/datasets.py 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314def __call__(self, parser, namespace, values, option_string=None): setattr(namespace, self.dest, values) # Get current values of both dataset_name and dataset_path dataset_name = getattr(namespace, \"dataset_name\", \"random\") dataset_path = getattr(namespace, \"dataset_path\", None) # Validate the combination if dataset_name == \"random\" and dataset_path is not None: parser.error( \"Cannot use 'random' dataset with --dataset-path. \" \"Please specify the appropriate --dataset-name (e.g., \" \"'sharegpt', 'custom', 'sonnet') for your dataset file: \" f\"{dataset_path}\" ) _format_zeta_prompt ¶ _format_zeta_prompt( sample: dict, original_start_marker: str = \"<|editable_region_start|>\", ) -> dict Format the zeta prompt for the Next Edit Prediction (NEP) dataset. This function formats examples from the NEP dataset into prompts and expected outputs. It could be further extended to support more NEP datasets. Parameters: Name Type Description Default sample dict The dataset sample containing events, inputs, and outputs. required original_start_marker str The marker indicating the start of the editable region. Defaults to \"<|editable_region_start|>\". '<|editable_region_start|>' Returns: Type Description dict A dictionary with the formatted prompts and expected outputs. Source code in vllm/benchmarks/datasets.py 2812 2813 2814 2815 2816 2817 2818 2819 2820 2821 2822 2823 2824 2825 2826 2827 2828 2829 2830 2831 2832 2833 2834 2835 2836 2837 2838 2839 2840 2841 2842def _format_zeta_prompt( sample: dict, original_start_marker: str = \"<|editable_region_start|>\" ) -> dict: \"\"\"Format the zeta prompt for the Next Edit Prediction (NEP) dataset. This function formats examples from the NEP dataset into prompts and expected outputs. It could be further extended to support more NEP datasets. Args: sample: The dataset sample containing events, inputs, and outputs. original_start_marker: The marker indicating the start of the editable region. Defaults to \"<|editable_region_start|>\". Returns: A dictionary with the formatted prompts and expected outputs. \"\"\" events = sample[\"events\"] input = sample[\"input\"] output = sample[\"output\"] prompt = zeta_prompt.format(events, input) # following the original implementation, extract the focused region # from the raw output output_start_index = output.find(original_start_marker) output_focused_region = output[output_start_index:] expected_output = output_focused_region return {\"prompt\": prompt, \"expected_output\": expected_output} add_dataset_parser ¶ add_dataset_parser(parser: ArgumentParser) Source code in vllm/benchmarks/datasets.py 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 1633 1634 1635 1636 1637 1638def add_dataset_parser(parser: FlexibleArgumentParser): parser.add_argument(\"--seed\", type=int, default=0) parser.add_argument( \"--num-prompts\", type=int, default=1000, help=\"Number of prompts to process.\", ) parser.add_argument( \"--dataset-name\", type=str, default=\"random\", action=_ValidateDatasetArgs, choices=[ \"sharegpt\", \"burstgpt\", \"sonnet\", \"random\", \"random-mm\", \"random-rerank\", \"hf\", \"custom\", \"prefix_repetition\", \"spec_bench\", ], help=\"Name of the dataset to benchmark on.\", ) parser.add_argument( \"--no-stream\", action=\"store_true\", help=\"Do not load the dataset in streaming mode.\", ) parser.add_argument( \"--dataset-path\", type=str, default=None, action=_ValidateDatasetArgs, help=\"Path to the sharegpt/sonnet dataset. \" \"Or the huggingface dataset ID if using HF dataset.\", ) parser.add_argument( \"--no-oversample\", action=\"store_true\", help=\"Do not oversample if the dataset has fewer samples than num-prompts.\", ) parser.add_argument( \"--skip-chat-template\", action=\"store_true\", help=\"Skip applying chat template to prompt for datasets that support it.\", ) parser.add_argument( \"--disable-shuffle\", action=\"store_true\", help=\"Disable shuffling of dataset samples for deterministic ordering.\", ) # group for dataset specific arguments custom_group = parser.add_argument_group(\"custom dataset options\") custom_group.add_argument( \"--custom-output-len\", type=int, default=256, help=\"Number of output tokens per request, used only for custom dataset.\", ) spec_bench_group = parser.add_argument_group(\"spec bench dataset options\") spec_bench_group.add_argument( \"--spec-bench-output-len\", type=int, default=256, help=\"Num of output tokens per request, used only for spec bench dataset.\", ) spec_bench_group.add_argument( \"--spec-bench-category\", type=str, default=None, help=\"Category for spec bench dataset. If None, use all categories.\", ) sonnet_group = parser.add_argument_group(\"sonnet dataset options\") sonnet_group.add_argument( \"--sonnet-input-len\", type=int, default=550, help=\"Number of input tokens per request, used only for sonnet dataset.\", ) sonnet_group.add_argument( \"--sonnet-output-len\", type=int, default=150, help=\"Number of output tokens per request, used only for sonnet dataset.\", ) sonnet_group.add_argument( \"--sonnet-prefix-len\", type=int, default=200, help=\"Number of prefix tokens per request, used only for sonnet dataset.\", ) sharegpt_group = parser.add_argument_group(\"sharegpt dataset options\") sharegpt_group.add_argument( \"--sharegpt-output-len\", type=int, default=None, help=\"Output length for each request. Overrides the output length \" \"from the ShareGPT dataset.\", ) blazedit_group = parser.add_argument_group(\"blazedit dataset options\") blazedit_group.add_argument( \"--blazedit-min-distance\", type=float, default=0.0, help=\"Minimum distance for blazedit dataset. Min: 0, Max: 1.0\", ) blazedit_group.add_argument( \"--blazedit-max-distance\", type=float, default=1.0, help=\"Maximum distance for blazedit dataset. Min: 0, Max: 1.0\", ) random_group = parser.add_argument_group(\"random dataset options\") random_group.add_argument( \"--random-input-len\", type=int, default=1024, help=\"Number of input tokens per request, used only for random sampling.\", ) random_group.add_argument( \"--random-output-len\", type=int, default=128, help=\"Number of output tokens per request, used only for random sampling.\", ) random_group.add_argument( \"--random-range-ratio\", type=float, default=0.0, help=\"Range ratio for sampling input/output length, \" \"used only for random sampling. Must be in the range [0, 1) to define \" \"a symmetric sampling range\" \"[length * (1 - range_ratio), length * (1 + range_ratio)].\", ) random_group.add_argument( \"--random-prefix-len\", type=int, default=0, help=( \"Number of fixed prefix tokens before the random context \" \"in a request. \" \"The total input length is the sum of `random-prefix-len` and \" \"a random \" \"context length sampled from [input_len * (1 - range_ratio), \" \"input_len * (1 + range_ratio)].\" ), ) random_group.add_argument( \"--random-batch-size\", type=int, default=1, help=(\"Batch size for random sampling. Only used for embeddings benchmark.\"), ) random_group.add_argument( \"--no-reranker\", action=\"store_true\", help=( \"Whether the model supports reranking natively.\" \" Only used for reranker benchmark.\" ), ) # random multimodal dataset options random_mm_group = parser.add_argument_group( \"random multimodal dataset options extended from random dataset\" ) random_mm_group.add_argument( \"--random-mm-base-items-per-request\", type=int, default=RandomMultiModalDataset.DEFAULT_BASE_ITEMS_PER_REQUEST, help=( \"Base number of multimodal items per request for random-mm. \" \"Actual per-request count is sampled around this base using \" \"--random-mm-num-mm-items-range-ratio.\" ), ) random_mm_group.add_argument( \"--random-mm-num-mm-items-range-ratio\", type=float, default=RandomMultiModalDataset.DEFAULT_NUM_MM_ITEMS_RANGE_RATIO, help=( \"Range ratio r in [0, 1] for sampling items per request. \" \"We sample uniformly from the closed integer range \" \"[floor(n*(1-r)), ceil(n*(1+r))] \" \"where n is the base items per request. \" \"r=0 keeps it fixed; r=1 allows 0 items. The maximum is clamped \" \"to the sum of per-modality limits from \" \"--random-mm-limit-mm-per-prompt. \" \"An error is raised if the computed min exceeds the max.\" ), ) random_mm_group.add_argument( \"--random-mm-limit-mm-per-prompt\", type=json.loads, default=RandomMultiModalDataset.DEFAULT_LIMIT_MM_PER_PROMPT, help=( \"Per-modality hard caps for items attached per request, e.g. \" '\\'{\"image\": 3, \"video\": 0}\\'. The sampled per-request item ' \"count is clamped to the sum of these limits. When a modality \" \"reaches its cap, its buckets are excluded and probabilities are \" \"renormalized.\" \"OBS.: Only image sampling is supported for now.\" ), ) def _parse_mm_bucket_config(v: object) -> dict[tuple[int, int, int], float]: # If already a dict (e.g., programmatic call), normalize keys def normalize(d: dict) -> dict[tuple[int, int, int], float]: out: dict[tuple[int, int, int], float] = {} for k, val in d.items(): key = k if isinstance(key, str): with suppress(Exception): key = ast.literal_eval(key) if not ( isinstance(key, tuple) and len(key) == 3 and all(isinstance(x, int) for x in key) ): raise ValueError( f\"Invalid bucket key {k!r}. Expected tuple (H, W, T).\" ) out[(int(key[0]), int(key[1]), int(key[2]))] = float(val) return out if isinstance(v, dict): return normalize(v) if isinstance(v, str): # Python literal (supports tuple keys) parsed = ast.literal_eval(v) if not isinstance(parsed, dict): raise ValueError(\"Bucket config must parse to a dict.\") return normalize(parsed) raise ValueError(\"Unsupported value for --random-mm-bucket-config.\") random_mm_group.add_argument( \"--random-mm-bucket-config\", type=_parse_mm_bucket_config, default=RandomMultiModalDataset.DEFAULT_MM_ITEM_BUCKET_CONFIG, help=( \"The bucket config is a dictionary mapping a multimodal item\" \"sampling configuration to a probability.\" \"Currently allows for 2 modalities: images and videos. \" \"An bucket key is a tuple of (height, width, num_frames)\" \"The value is the probability of sampling that specific item. \" \"Example: \" \"--random-mm-bucket-config \" \"{(256, 256, 1): 0.5, (720, 1280, 1): 0.4, (720, 1280, 16): 0.10} \" \"First item: images with resolution 256x256 w.p. 0.5\" \"Second item: images with resolution 720x1280 w.p. 0.4 \" \"Third item: videos with resolution 720x1280 and 16 frames w.p. 0.1\" \"OBS.: If the probabilities do not sum to 1, they are normalized.\" \"OBS bis.: Only image sampling is supported for now.\" ), ) hf_group = parser.add_argument_group(\"hf dataset options\") hf_group.add_argument( \"--hf-subset\", type=str, default=None, help=\"Subset of the HF dataset.\" ) hf_group.add_argument( \"--hf-split\", type=str, default=None, help=\"Split of the HF dataset.\" ) hf_group.add_argument( \"--hf-name\", type=str, default=None, help=( \"Name of the dataset on HuggingFace \" \"(e.g., 'lmarena-ai/VisionArena-Chat'). \" \"Specify this if your dataset-path is a local path.\" ), ) hf_group.add_argument( \"--hf-output-len\", type=int, default=None, help=\"Output length for each request. Overrides the output lengths \" \"from the sampled HF dataset.\", ) prefix_repetition_group = parser.add_argument_group( \"prefix repetition dataset options\" ) prefix_repetition_group.add_argument( \"--prefix-repetition-prefix-len\", type=int, default=256, help=\"Number of prefix tokens per request, used only for prefix \" \"repetition dataset.\", ) prefix_repetition_group.add_argument( \"--prefix-repetition-suffix-len\", type=int, default=256, help=\"Number of suffix tokens per request, used only for prefix \" \"repetition dataset. Total input length is prefix_len + suffix_len.\", ) prefix_repetition_group.add_argument( \"--prefix-repetition-num-prefixes\", type=int, default=10, help=\"Number of prefixes to generate, used only for prefix repetition \" \"dataset. Prompts per prefix is num_requests // num_prefixes.\", ) prefix_repetition_group.add_argument( \"--prefix-repetition-output-len\", type=int, default=128, help=\"Number of output tokens per request, used only for prefix \" \"repetition dataset.\", ) gen_prompt_decode_to_target_len ¶ gen_prompt_decode_to_target_len( tokenizer: TokenizerLike, token_sequence: list[int], target_token_len: int, max_retry: int = 10, add_special_tokens: bool = False, rng: Generator | None = None, ) -> tuple[str, list[int]] Ensure decoded-then-encoded prompt length matches the target token length. This function decodes an initial token sequence to text and re-encodes it , iteratively adjusting the token sequence length to match a target. This is necessary because some tokenizers do not guarantee a 1:1 mapping between consecutive tokens and the decoded-then-encoded sequence length. For example, for GPT2Tokenizer: [6880, 6881] -> ['Ġcalls', 'here'] -> [1650, 939, 486] -> ['Ġcall', 'sh', 'ere'] Returns a tuple of the final prompt string and the adjusted token sequence. Source code in vllm/benchmarks/datasets.py 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433def gen_prompt_decode_to_target_len( tokenizer: TokenizerLike, token_sequence: list[int], target_token_len: int, max_retry: int = 10, add_special_tokens: bool = False, rng: np.random.Generator | None = None, ) -> tuple[str, list[int]]: \"\"\" Ensure decoded-then-encoded prompt length matches the target token length. This function decodes an initial token sequence to text and re-encodes it , iteratively adjusting the token sequence length to match a target. This is necessary because some tokenizers do not guarantee a 1:1 mapping between consecutive tokens and the decoded-then-encoded sequence length. For example, for GPT2Tokenizer: [6880, 6881] -> ['Ġcalls', 'here'] -> [1650, 939, 486] -> ['Ġcall', 'sh', 'ere'] Returns a tuple of the final prompt string and the adjusted token sequence. \"\"\" remain_num_try = max_retry token_mismatch = 0 while True: prompt = tokenizer.decode(token_sequence) token_sequence = tokenizer.encode(prompt, add_special_tokens=add_special_tokens) if remain_num_try <= 0: if len(token_sequence) != target_token_len: token_mismatch = len(token_sequence) - target_token_len break if len(token_sequence) == target_token_len: break elif len(token_sequence) < target_token_len: if rng is not None: extra_tokens = rng.integers( 0, tokenizer.vocab_size, size=target_token_len - len(token_sequence), ).tolist() else: extra_tokens = np.random.randint( 0, tokenizer.vocab_size, size=target_token_len - len(token_sequence), ).tolist() token_sequence.extend(extra_tokens) elif len(token_sequence) > target_token_len: token_sequence = token_sequence[:target_token_len] remain_num_try -= 1 return prompt, token_sequence, token_mismatch get_samples ¶ get_samples( args, tokenizer: TokenizerLike ) -> list[SampleRequest] Source code in vllm/benchmarks/datasets.py 1641 1642 1643 1644 1645 1646 1647 1648 1649 1650 1651 1652 1653 1654 1655 1656 1657 1658 1659 1660 1661 1662 1663 1664 1665 1666 1667 1668 1669 1670 1671 1672 1673 1674 1675 1676 1677 1678 1679 1680 1681 1682 1683 1684 1685 1686 1687 1688 1689 1690 1691 1692 1693 1694 1695 1696 1697 1698 1699 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728 1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 1742 1743 1744 1745 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 1793 1794 1795 1796 1797 1798 1799 1800 1801 1802 1803 1804 1805 1806 1807 1808 1809 1810 1811 1812 1813 1814 1815 1816 1817 1818 1819 1820 1821 1822 1823 1824 1825 1826 1827 1828 1829 1830 1831 1832 1833 1834 1835 1836 1837 1838 1839 1840 1841 1842 1843 1844 1845 1846 1847 1848 1849 1850 1851 1852 1853 1854 1855 1856 1857 1858 1859 1860 1861 1862 1863 1864 1865 1866 1867 1868 1869 1870 1871 1872 1873 1874 1875 1876 1877 1878 1879 1880 1881 1882 1883 1884 1885 1886 1887 1888 1889 1890 1891 1892 1893 1894 1895 1896 1897 1898 1899 1900 1901 1902 1903 1904 1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919def get_samples(args, tokenizer: TokenizerLike) -> list[SampleRequest]: if not hasattr(args, \"request_id_prefix\"): args.request_id_prefix = \"\" if args.dataset_name == \"custom\": dataset = CustomDataset( dataset_path=args.dataset_path, disable_shuffle=args.disable_shuffle ) input_requests = dataset.sample( num_requests=args.num_prompts, tokenizer=tokenizer, output_len=args.custom_output_len, skip_chat_template=args.skip_chat_template, request_id_prefix=args.request_id_prefix, no_oversample=args.no_oversample, ) elif args.dataset_name == \"sonnet\": dataset = SonnetDataset( dataset_path=args.dataset_path, disable_shuffle=args.disable_shuffle ) # For the \"sonnet\" dataset, formatting depends on the backend. if args.backend == \"openai-chat\": input_requests = dataset.sample( num_requests=args.num_prompts, input_len=args.sonnet_input_len, output_len=args.sonnet_output_len, prefix_len=args.sonnet_prefix_len, tokenizer=tokenizer, return_prompt_formatted=False, request_id_prefix=args.request_id_prefix, no_oversample=args.no_oversample, ) else: assert tokenizer.chat_template or tokenizer.default_chat_template, ( \"Tokenizer/model must have chat template for sonnet dataset.\" ) input_requests = dataset.sample( num_requests=args.num_prompts, input_len=args.sonnet_input_len, output_len=args.sonnet_output_len, prefix_len=args.sonnet_prefix_len, tokenizer=tokenizer, return_prompt_formatted=True, request_id_prefix=args.request_id_prefix, no_oversample=args.no_oversample, ) elif args.dataset_name == \"hf\": # all following datasets are implemented from the # HuggingFaceDataset base class hf_kwargs = {} if ( args.dataset_path in VisionArenaDataset.SUPPORTED_DATASET_PATHS or args.hf_name in VisionArenaDataset.SUPPORTED_DATASET_PATHS ): dataset_class = VisionArenaDataset args.hf_split = \"train\" args.hf_subset = None elif ( args.dataset_path in MMVUDataset.SUPPORTED_DATASET_PATHS or args.hf_name in MMVUDataset.SUPPORTED_DATASET_PATHS ): dataset_class = MMVUDataset args.hf_split = \"validation\" args.hf_subset = None elif ( args.dataset_path in InstructCoderDataset.SUPPORTED_DATASET_PATHS or args.hf_name in InstructCoderDataset.SUPPORTED_DATASET_PATHS ): dataset_class = InstructCoderDataset args.hf_split = \"train\" elif ( args.dataset_path in MTBenchDataset.SUPPORTED_DATASET_PATHS or args.hf_name in MTBenchDataset.SUPPORTED_DATASET_PATHS ): dataset_class = MTBenchDataset args.hf_split = \"train\" elif ( args.dataset_path in MultiModalConversationDataset.SUPPORTED_DATASET_PATHS or args.hf_name in MultiModalConversationDataset.SUPPORTED_DATASET_PATHS ): dataset_class = MultiModalConversationDataset elif ( args.dataset_path in ConversationDataset.SUPPORTED_DATASET_PATHS or args.hf_name in ConversationDataset.SUPPORTED_DATASET_PATHS ): dataset_class = ConversationDataset elif ( args.dataset_path in AIMODataset.SUPPORTED_DATASET_PATHS or args.hf_name in AIMODataset.SUPPORTED_DATASET_PATHS ): dataset_class = AIMODataset args.hf_split = \"train\" elif ( args.dataset_path in NextEditPredictionDataset.SUPPORTED_DATASET_PATHS # noqa: E501 or args.hf_name in NextEditPredictionDataset.SUPPORTED_DATASET_PATHS ): dataset_class = NextEditPredictionDataset args.hf_split = \"train\" elif ( args.dataset_path in ASRDataset.SUPPORTED_DATASET_PATHS or args.hf_name in ASRDataset.SUPPORTED_DATASET_PATHS ): dataset_class = ASRDataset args.hf_split = \"train\" elif args.dataset_path in BlazeditDataset.SUPPORTED_DATASET_PATHS: dataset_class = BlazeditDataset args.hf_split = \"train\" hf_kwargs = { \"min_distance\": args.blazedit_min_distance, \"max_distance\": args.blazedit_max_distance, } elif ( args.dataset_path in MLPerfDataset.SUPPORTED_DATASET_PATHS or args.hf_name in MLPerfDataset.SUPPORTED_DATASET_PATHS ): dataset_class = MLPerfDataset args.hf_split = \"train\" elif ( args.dataset_path in MMStarDataset.SUPPORTED_DATASET_PATHS or args.hf_name in MMStarDataset.SUPPORTED_DATASET_PATHS ): dataset_class = MMStarDataset args.hf_split = \"val\" args.hf_subset = None else: supported_datasets = set( [ dataset_name for cls in HuggingFaceDataset.__subclasses__() for dataset_name in cls.SUPPORTED_DATASET_PATHS ] ) raise ValueError( f\"Unsupported dataset path: {args.dataset_path}. \" \"Huggingface dataset only supports dataset_path\" f\" from one of following: {supported_datasets}. \" \"Please consider contributing if you would \" \"like to add support for additional dataset formats.\" ) if dataset_class.IS_MULTIMODAL and not ( args.backend in (\"openai-chat\", \"openai-audio\") or \"embeddings-\" in args.backend ): # multi-modal benchmark is only available on OpenAI Chat # endpoint-type. raise ValueError( \"Multi-modal content is only supported on 'openai-chat' and \" \"'openai-audio' backends.\" ) input_requests = dataset_class( dataset_path=args.dataset_path, dataset_subset=args.hf_subset, dataset_split=args.hf_split, random_seed=args.seed, no_stream=args.no_stream, hf_name=args.hf_name, disable_shuffle=args.disable_shuffle, ).sample( num_requests=args.num_prompts, tokenizer=tokenizer, output_len=args.hf_output_len, request_id_prefix=args.request_id_prefix, no_oversample=args.no_oversample, skip_chat_template=args.skip_chat_template, **hf_kwargs, ) else: # For datasets that follow a similar structure, use a mapping. dataset_mapping = { \"spec_bench\": lambda: SpecBench( dataset_path=args.dataset_path, category=args.spec_bench_category, disable_shuffle=args.disable_shuffle, ).sample( num_requests=args.num_prompts, tokenizer=tokenizer, output_len=args.spec_bench_output_len, request_id_prefix=args.request_id_prefix, no_oversample=args.no_oversample, ), \"sharegpt\": lambda: ShareGPTDataset( random_seed=args.seed, dataset_path=args.dataset_path, disable_shuffle=args.disable_shuffle, ).sample( tokenizer=tokenizer, num_requests=args.num_prompts, output_len=args.sharegpt_output_len, request_id_prefix=args.request_id_prefix, no_oversample=args.no_oversample, ), \"burstgpt\": lambda: BurstGPTDataset( random_seed=args.seed, dataset_path=args.dataset_path, disable_shuffle=args.disable_shuffle, ).sample( tokenizer=tokenizer, num_requests=args.num_prompts, request_id_prefix=args.request_id_prefix, no_oversample=args.no_oversample, ), \"random\": lambda: RandomDataset( random_seed=args.seed, dataset_path=args.dataset_path, disable_shuffle=args.disable_shuffle, ).sample( tokenizer=tokenizer, num_requests=args.num_prompts, prefix_len=args.random_prefix_len, input_len=args.random_input_len, output_len=args.random_output_len, range_ratio=args.random_range_ratio, request_id_prefix=args.request_id_prefix, batchsize=args.random_batch_size, no_oversample=args.no_oversample, ), \"random-mm\": lambda: RandomMultiModalDataset( random_seed=args.seed, dataset_path=args.dataset_path, disable_shuffle=args.disable_shuffle, ).sample( tokenizer=tokenizer, num_requests=args.num_prompts, prefix_len=args.random_prefix_len, range_ratio=args.random_range_ratio, input_len=args.random_input_len, output_len=args.random_output_len, base_items_per_request=args.random_mm_base_items_per_request, limit_mm_per_prompt=args.random_mm_limit_mm_per_prompt, num_mm_items_range_ratio=args.random_mm_num_mm_items_range_ratio, bucket_config=args.random_mm_bucket_config, request_id_prefix=args.request_id_prefix, no_oversample=args.no_oversample, ), \"random-rerank\": lambda: RandomDatasetForReranking( random_seed=args.seed, dataset_path=args.dataset_path, disable_shuffle=args.disable_shuffle, ).sample( tokenizer=tokenizer, num_requests=args.num_prompts, input_len=args.random_input_len, range_ratio=args.random_range_ratio, request_id_prefix=args.request_id_prefix, batchsize=args.random_batch_size, is_reranker=not args.no_reranker, ), \"prefix_repetition\": lambda: PrefixRepetitionRandomDataset( random_seed=args.seed, dataset_path=args.dataset_path, disable_shuffle=args.disable_shuffle, ).sample( tokenizer=tokenizer, num_requests=args.num_prompts, prefix_len=args.prefix_repetition_prefix_len, suffix_len=args.prefix_repetition_suffix_len, num_prefixes=args.prefix_repetition_num_prefixes, output_len=args.prefix_repetition_output_len, request_id_prefix=args.request_id_prefix, no_oversample=args.no_oversample, ), } try: # Enforce endpoint compatibility for multimodal datasets. if args.dataset_name == \"random-mm\" and args.backend not in [\"openai-chat\"]: raise ValueError( \"Multi-modal content (images) is only supported on \" \"'openai-chat' backend.\" ) input_requests = dataset_mapping[args.dataset_name]() except KeyError as err: raise ValueError(f\"Unknown dataset: {args.dataset_name}\") from err return input_requests is_valid_sequence ¶ is_valid_sequence( prompt_len: int, output_len: int, min_len: int = 4, max_prompt_len: int = 1024, max_total_len: int = 2048, skip_min_output_len_check: bool = False, ) -> bool Validate a sequence based on prompt and output lengths. Default pruning criteria are copied from the original sample_hf_requests and sample_sharegpt_requests functions in benchmark_serving.py, as well as from sample_requests in benchmark_throughput.py. Source code in vllm/benchmarks/datasets.py 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286def is_valid_sequence( prompt_len: int, output_len: int, min_len: int = 4, max_prompt_len: int = 1024, max_total_len: int = 2048, skip_min_output_len_check: bool = False, ) -> bool: \"\"\" Validate a sequence based on prompt and output lengths. Default pruning criteria are copied from the original `sample_hf_requests` and `sample_sharegpt_requests` functions in benchmark_serving.py, as well as from `sample_requests` in benchmark_throughput.py. \"\"\" # Check for invalid conditions prompt_too_short = prompt_len < min_len output_too_short = (not skip_min_output_len_check) and (output_len < min_len) prompt_too_long = prompt_len > max_prompt_len combined_too_long = (prompt_len + output_len) > max_total_len # Return True if none of the invalid conditions are met return not ( prompt_too_short or output_too_short or prompt_too_long or combined_too_long ) lora_path_on_disk cached ¶ lora_path_on_disk(lora_path: str) -> str Source code in vllm/benchmarks/datasets.py 289 290 291@cache def lora_path_on_disk(lora_path: str) -> str: return get_adapter_absolute_path(lora_path) process_image ¶ process_image(image: Any) -> Mapping[str, Any] Process a single image input and return a multimedia content dictionary. Supports the following input types: Dictionary with raw image bytes: - Expects a dict with a 'bytes' key containing raw image data. - Loads the bytes as a PIL.Image.Image. PIL.Image.Image input: - Converts the image to RGB. - Saves the image as a JPEG in memory. - Encodes the JPEG data as a base64 string. - Returns a dictionary with the image as a base64 data URL. String input: - Treats the string as a URL or local file path. - Prepends \"file://\" if the string doesn't start with \"http://\" or \"file://\". - Returns a dictionary with the image URL. Raises: Type Description ValueError If the input is not a supported type. Source code in vllm/benchmarks/datasets.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341def process_image(image: Any) -> Mapping[str, Any]: \"\"\" Process a single image input and return a multimedia content dictionary. Supports the following input types: 1. Dictionary with raw image bytes: - Expects a dict with a 'bytes' key containing raw image data. - Loads the bytes as a PIL.Image.Image. 2. PIL.Image.Image input: - Converts the image to RGB. - Saves the image as a JPEG in memory. - Encodes the JPEG data as a base64 string. - Returns a dictionary with the image as a base64 data URL. 3. String input: - Treats the string as a URL or local file path. - Prepends \"file://\" if the string doesn't start with \"http://\" or \"file://\". - Returns a dictionary with the image URL. Raises: ValueError: If the input is not a supported type. \"\"\" if isinstance(image, dict) and \"bytes\" in image: image = Image.open(BytesIO(image[\"bytes\"])) if isinstance(image, Image.Image): image = convert_image_mode(image, \"RGB\") with io.BytesIO() as image_data: image.save(image_data, format=\"JPEG\") image_base64 = base64.b64encode(image_data.getvalue()).decode(\"utf-8\") return { \"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_base64}\"}, } if isinstance(image, str): image_url = ( image if image.startswith((\"http://\", \"https://\", \"file://\")) else f\"file://{image}\" ) return {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}} raise ValueError( f\"Invalid image input {image}. Must be a PIL.Image.Image\" \" or str or dictionary with raw image bytes.\" ) process_video ¶ process_video(video: Any) -> Mapping[str, Any] Process a single video input and return a multimedia content dictionary. Supports the following input types: Dictionary with raw video bytes: - Expects a dict with a 'bytes' key containing raw video data. String input: - Treats the string as a URL or local file path. - Prepends \"file://\" if the string doesn't start with \"http://\" or \"file://\". - Returns a dictionary with the image URL. Raises: Type Description ValueError If the input is not a supported type. Source code in vllm/benchmarks/datasets.py 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378def process_video(video: Any) -> Mapping[str, Any]: \"\"\" Process a single video input and return a multimedia content dictionary. Supports the following input types: 1. Dictionary with raw video bytes: - Expects a dict with a 'bytes' key containing raw video data. 2. String input: - Treats the string as a URL or local file path. - Prepends \"file://\" if the string doesn't start with \"http://\" or \"file://\". - Returns a dictionary with the image URL. Raises: ValueError: If the input is not a supported type. \"\"\" if isinstance(video, dict) and \"bytes\" in video: video_bytes = video[\"bytes\"] video_base64 = base64.b64encode(video_bytes).decode(\"utf-8\") return { \"type\": \"video_url\", \"video_url\": {\"url\": f\"data:video/mp4;base64,{video_base64}\"}, } if isinstance(video, str): video_url = ( video if video.startswith((\"http://\", \"https://\", \"file://\")) else f\"file://{video}\" ) return {\"type\": \"video_url\", \"video_url\": {\"url\": video_url}} raise ValueError( f\"Invalid video input {video}. Must be a string of local path/remote url, or a dictionary with raw video bytes in the form of `{{'bytes': raw_video_bytes}}`.\" # noqa: E501 )",
      "code": ""
    },
    {
      "description": "This module defines a framework for sampling benchmark requests from various datasets. Each dataset subclass of BenchmarkDataset must implement sample generation. Supported dataset types include: - ShareGPT - Random (synthetic) - Sonnet - BurstGPT - HuggingFace - VisionArena datasets module-attribute ¶ datasets = PlaceholderModule('datasets') logger module-attribute ¶ logger = getLogger(__name__) lora_tokenizer_cache module-attribute ¶ lora_tokenizer_cache: dict[int, TokenizerLike] = {} zeta_prompt module-attribute ¶ zeta_prompt = \"### Instruction:\\nYou are a code completion assistant and your task is to analyze user edits and then rewrite an excerpt that the user provides, suggesting the appropriate edits within the excerpt, taking into account the cursor location.\\n\\n### User Edits:\\n\\n{}\\n\\n### User Excerpt:\\n\\n{}\\n\\n### Response:\\n\\n\" AIMODataset ¶ Bases: HuggingFaceDataset Dataset class for processing a AIMO dataset with reasoning questions. Source code in vllm/benchmarks/datasets.py 2736 2737 2738 2739 2740 2741 2742 2743 2744 2745 2746 2747 2748 2749 2750 2751 2752 2753 2754 2755 2756 2757 2758 2759 2760 2761 2762 2763 2764 2765 2766 2767 2768 2769 2770 2771 2772 2773 2774 2775 2776 2777 2778 2779 2780 2781 2782 2783 2784 2785 2786 2787 2788class AIMODataset(HuggingFaceDataset): \"\"\" Dataset class for processing a AIMO dataset with reasoning questions. \"\"\" SUPPORTED_DATASET_PATHS = { \"AI-MO/aimo-validation-aime\", \"AI-MO/NuminaMath-1.5\", \"AI-MO/NuminaMath-CoT\", } def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: sampled_requests = [] ind = 0 dynamic_output = output_len is None for item in self.data: if len(sampled_requests) >= num_requests: break prompt, completion = item[\"problem\"], item[\"solution\"] prompt_ids = tokenizer(prompt).input_ids completion_ids = tokenizer(completion).input_ids prompt_len = len(prompt_ids) completion_len = len(completion_ids) output_len = completion_len if dynamic_output else output_len assert isinstance(output_len, int) and output_len > 0 if dynamic_output and not is_valid_sequence( prompt_len, completion_len, max_prompt_len=2048, max_total_len=32000 ): continue sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=None, request_id=request_id_prefix + str(ind), ) ) ind += 1 self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = { \"AI-MO/aimo-validation-aime\", \"AI-MO/NuminaMath-1.5\", \"AI-MO/NuminaMath-CoT\", } sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 2747 2748 2749 2750 2751 2752 2753 2754 2755 2756 2757 2758 2759 2760 2761 2762 2763 2764 2765 2766 2767 2768 2769 2770 2771 2772 2773 2774 2775 2776 2777 2778 2779 2780 2781 2782 2783 2784 2785 2786 2787 2788def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: sampled_requests = [] ind = 0 dynamic_output = output_len is None for item in self.data: if len(sampled_requests) >= num_requests: break prompt, completion = item[\"problem\"], item[\"solution\"] prompt_ids = tokenizer(prompt).input_ids completion_ids = tokenizer(completion).input_ids prompt_len = len(prompt_ids) completion_len = len(completion_ids) output_len = completion_len if dynamic_output else output_len assert isinstance(output_len, int) and output_len > 0 if dynamic_output and not is_valid_sequence( prompt_len, completion_len, max_prompt_len=2048, max_total_len=32000 ): continue sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=None, request_id=request_id_prefix + str(ind), ) ) ind += 1 self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests ASRDataset ¶ Bases: HuggingFaceDataset Dataset class for processing a ASR dataset for transcription. Tested on the following set: +----------------+----------------------------------------+--------------------------+-----------------------------+ | Dataset | Domain | Speaking Style | hf-subset | +----------------+----------------------------------------+--------------------------+-----------------------------+ | TED-LIUM | TED talks | Oratory | release1, release2, release3| | | | | release3-speaker-adaptation | | VoxPopuli | European Parliament | Oratory | en, de, it, fr, ... | | LibriSpeech | Audiobook | Narrated | \"LIUM/tedlium\" | | GigaSpeech | Audiobook, podcast, YouTube | Narrated, spontaneous | xs, s, m, l, xl, dev, test | | SPGISpeech | Financial meetings | Oratory, spontaneous | S, M, L, dev, test | | AMI | Meetings | Spontaneous | ihm, sdm | +----------------+----------------------------------------+--------------------------+-----------------------------+ Source code in vllm/benchmarks/datasets.py 2894 2895 2896 2897 2898 2899 2900 2901 2902 2903 2904 2905 2906 2907 2908 2909 2910 2911 2912 2913 2914 2915 2916 2917 2918 2919 2920 2921 2922 2923 2924 2925 2926 2927 2928 2929 2930 2931 2932 2933 2934 2935 2936 2937 2938 2939 2940 2941 2942 2943 2944 2945 2946 2947 2948 2949 2950 2951 2952 2953 2954 2955 2956 2957 2958 2959 2960 2961 2962 2963 2964 2965 2966 2967 2968 2969 2970 2971 2972 2973 2974 2975 2976class ASRDataset(HuggingFaceDataset): \"\"\" Dataset class for processing a ASR dataset for transcription. Tested on the following set: +----------------+----------------------------------------+--------------------------+-----------------------------+ | Dataset | Domain | Speaking Style | hf-subset | +----------------+----------------------------------------+--------------------------+-----------------------------+ | TED-LIUM | TED talks | Oratory | release1, release2, release3| | | | | release3-speaker-adaptation | | VoxPopuli | European Parliament | Oratory | en, de, it, fr, ... | | LibriSpeech | Audiobook | Narrated | \"LIUM/tedlium\" | | GigaSpeech | Audiobook, podcast, YouTube | Narrated, spontaneous | xs, s, m, l, xl, dev, test | | SPGISpeech | Financial meetings | Oratory, spontaneous | S, M, L, dev, test | | AMI | Meetings | Spontaneous | ihm, sdm | +----------------+----------------------------------------+--------------------------+-----------------------------+ \"\"\" # noqa: E501 SUPPORTED_DATASET_PATHS = { \"openslr/librispeech_asr\", \"facebook/voxpopuli\", \"LIUM/tedlium\", \"edinburghcstr/ami\", \"speechcolab/gigaspeech\", \"kensho/spgispeech\", } DEFAULT_OUTPUT_LEN = 128 IS_MULTIMODAL = True # TODO Whisper-specific. Abstract interface when more models are supported. TRANSCRIPTION_PREAMBLE = \"<|startoftranscript|><|en|><|transcribe|><|notimestamps|>\" skip_long_audios: bool = True def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN prompt = ASRDataset.TRANSCRIPTION_PREAMBLE prompt_len = len(tokenizer(prompt).input_ids) sampled_requests = [] ind = 0 skipped = 0 for item in self.data: if len(sampled_requests) >= num_requests: break audio = item[\"audio\"] y, sr = audio[\"array\"], audio[\"sampling_rate\"] duration_s = librosa.get_duration(y=y, sr=sr) # Whisper max supported duration if self.skip_long_audios and duration_s > 30: skipped += 1 continue mm_content = {\"audio\": (y, sr)} sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_content, request_id=request_id_prefix + str(ind), ) ) ind += 1 if skipped: logger.warning( \"%d samples discarded from dataset due to\" \" their length being greater than\" \" what Whisper supports.\", skipped, ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶ DEFAULT_OUTPUT_LEN = 128 IS_MULTIMODAL class-attribute instance-attribute ¶ IS_MULTIMODAL = True SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = { \"openslr/librispeech_asr\", \"facebook/voxpopuli\", \"LIUM/tedlium\", \"edinburghcstr/ami\", \"speechcolab/gigaspeech\", \"kensho/spgispeech\", } TRANSCRIPTION_PREAMBLE class-attribute instance-attribute ¶ TRANSCRIPTION_PREAMBLE = \"<|startoftranscript|><|en|><|transcribe|><|notimestamps|>\" skip_long_audios class-attribute instance-attribute ¶ skip_long_audios: bool = True sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 2929 2930 2931 2932 2933 2934 2935 2936 2937 2938 2939 2940 2941 2942 2943 2944 2945 2946 2947 2948 2949 2950 2951 2952 2953 2954 2955 2956 2957 2958 2959 2960 2961 2962 2963 2964 2965 2966 2967 2968 2969 2970 2971 2972 2973 2974 2975 2976def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN prompt = ASRDataset.TRANSCRIPTION_PREAMBLE prompt_len = len(tokenizer(prompt).input_ids) sampled_requests = [] ind = 0 skipped = 0 for item in self.data: if len(sampled_requests) >= num_requests: break audio = item[\"audio\"] y, sr = audio[\"array\"], audio[\"sampling_rate\"] duration_s = librosa.get_duration(y=y, sr=sr) # Whisper max supported duration if self.skip_long_audios and duration_s > 30: skipped += 1 continue mm_content = {\"audio\": (y, sr)} sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_content, request_id=request_id_prefix + str(ind), ) ) ind += 1 if skipped: logger.warning( \"%d samples discarded from dataset due to\" \" their length being greater than\" \" what Whisper supports.\", skipped, ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests BenchmarkDataset ¶ Bases: ABC Source code in vllm/benchmarks/datasets.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254class BenchmarkDataset(ABC): DEFAULT_SEED = 0 IS_MULTIMODAL = False def __init__( self, dataset_path: str | None = None, random_seed: int = DEFAULT_SEED, disable_shuffle: bool = False, **kwargs, ) -> None: \"\"\" Initialize the BenchmarkDataset with an optional dataset path and random seed. Args: dataset_path (Optional[str]): Path to the dataset. If None, it indicates that a default or random dataset might be used. random_seed (int): Seed value for reproducible shuffling or sampling. Defaults to DEFAULT_SEED. \"\"\" self.dataset_path = dataset_path # Set the random seed, ensuring that a None value is replaced with the # default seed. self.random_seed = random_seed if random_seed is not None else self.DEFAULT_SEED self.disable_shuffle = disable_shuffle self.data = None def apply_multimodal_chat_transformation( self, prompt: str, mm_content: MultiModalDataDict | dict | list[dict] | None = None, ) -> list[dict]: \"\"\" Transform a prompt and optional multimodal content into a chat format. This method is used for chat models that expect a specific conversation format. \"\"\" content = [{\"text\": prompt, \"type\": \"text\"}] if mm_content is not None: if isinstance(mm_content, list): content.extend(cast(list[dict[str, Any]], mm_content)) elif isinstance(mm_content, dict): content.append(mm_content) else: raise TypeError( \"Could not process multimodal content of type: \" + f\"{type(mm_content)}\" ) return [{\"role\": \"user\", \"content\": content}] def load_data(self) -> None: \"\"\" Load data from the dataset path into self.data. This method must be overridden by subclasses since the method to load data will vary depending on the dataset format and source. Raises: NotImplementedError: If a subclass does not implement this method. \"\"\" # TODO (jenniferzhao): add support for downloading data raise NotImplementedError(\"load_data must be implemented in subclasses.\") def get_random_lora_request( self, max_loras: int | None = None, lora_path: str | None = None, ) -> LoRARequest | None: \"\"\" Optionally select a random LoRA request. This method is used when LoRA parameters are provided. It randomly selects a LoRA based on max_loras. Args: max_loras (Optional[int]): The maximum number of LoRAs available. If `None`, LoRA is not used. lora_path (Optional[str]): Path to the LoRA parameters on disk. If `None`, LoRA is not used. Returns: A new [`LoRARequest`][vllm.lora.request.LoRARequest] (or `None` if not applicable). \"\"\" if max_loras is None or lora_path is None: return None # Generate a random LoRA ID in the range [1, max_loras]. lora_id = random.randint(1, max_loras) lora_request = LoRARequest( lora_name=str(lora_id), lora_int_id=lora_id, lora_path=lora_path_on_disk(lora_path), ) return lora_request @abstractmethod def sample( self, tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, ) -> list[SampleRequest]: \"\"\" Abstract method to generate sample requests from the dataset. Subclasses must override this method to implement dataset-specific logic for generating a list of SampleRequest objects. Args: tokenizer (TokenizerLike): The tokenizer to be used for processing the dataset's text. num_requests (int): The number of sample requests to generate. request_id_prefix (str): The prefix of request_id. Returns: list[SampleRequest]: A list of sample requests generated from the dataset. \"\"\" raise NotImplementedError(\"sample must be implemented in subclasses.\") def maybe_oversample_requests( self, requests: list[SampleRequest], num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, ) -> None: \"\"\" Oversamples the list of requests if its size is less than the desired number. Args: requests (List[SampleRequest]): The current list of sampled requests. num_requests (int): The target number of requests. request_id_prefix (str): The prefix applied to generated request identifiers. \"\"\" if no_oversample: logger.info(\"Skipping oversampling. Total samples: %d.\", len(requests)) return if len(requests) < num_requests: random.seed(self.random_seed) needed = num_requests - len(requests) additional = [] for i in range(needed): req = deepcopy(random.choice(requests)) req.request_id = request_id_prefix + str(len(requests) + i) additional.append(req) requests.extend(additional) logger.info(\"Oversampled requests to reach %d total samples.\", num_requests) ids = [req.request_id for req in requests] if len(ids) != len(set(ids)): raise ValueError( \"Duplicate request_id found in the sampled \" \"requests. Please ensure that each request_id \" \"is unique.\" ) DEFAULT_SEED class-attribute instance-attribute ¶ DEFAULT_SEED = 0 IS_MULTIMODAL class-attribute instance-attribute ¶ IS_MULTIMODAL = False data instance-attribute ¶ data = None dataset_path instance-attribute ¶ dataset_path = dataset_path disable_shuffle instance-attribute ¶ disable_shuffle = disable_shuffle random_seed instance-attribute ¶ random_seed = ( random_seed if random_seed is not None else DEFAULT_SEED ) __init__ ¶ __init__( dataset_path: str | None = None, random_seed: int = DEFAULT_SEED, disable_shuffle: bool = False, **kwargs, ) -> None Initialize the BenchmarkDataset with an optional dataset path and random seed. Parameters: Name Type Description Default dataset_path Optional[str] Path to the dataset. If None, it indicates that a default or random dataset might be used. None random_seed int Seed value for reproducible shuffling or sampling. Defaults to DEFAULT_SEED. DEFAULT_SEED Source code in vllm/benchmarks/datasets.py 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117def __init__( self, dataset_path: str | None = None, random_seed: int = DEFAULT_SEED, disable_shuffle: bool = False, **kwargs, ) -> None: \"\"\" Initialize the BenchmarkDataset with an optional dataset path and random seed. Args: dataset_path (Optional[str]): Path to the dataset. If None, it indicates that a default or random dataset might be used. random_seed (int): Seed value for reproducible shuffling or sampling. Defaults to DEFAULT_SEED. \"\"\" self.dataset_path = dataset_path # Set the random seed, ensuring that a None value is replaced with the # default seed. self.random_seed = random_seed if random_seed is not None else self.DEFAULT_SEED self.disable_shuffle = disable_shuffle self.data = None apply_multimodal_chat_transformation ¶ apply_multimodal_chat_transformation( prompt: str, mm_content: MultiModalDataDict | dict | list[dict] | None = None, ) -> list[dict] Transform a prompt and optional multimodal content into a chat format. This method is used for chat models that expect a specific conversation format. Source code in vllm/benchmarks/datasets.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140def apply_multimodal_chat_transformation( self, prompt: str, mm_content: MultiModalDataDict | dict | list[dict] | None = None, ) -> list[dict]: \"\"\" Transform a prompt and optional multimodal content into a chat format. This method is used for chat models that expect a specific conversation format. \"\"\" content = [{\"text\": prompt, \"type\": \"text\"}] if mm_content is not None: if isinstance(mm_content, list): content.extend(cast(list[dict[str, Any]], mm_content)) elif isinstance(mm_content, dict): content.append(mm_content) else: raise TypeError( \"Could not process multimodal content of type: \" + f\"{type(mm_content)}\" ) return [{\"role\": \"user\", \"content\": content}] get_random_lora_request ¶ get_random_lora_request( max_loras: int | None = None, lora_path: str | None = None, ) -> LoRARequest | None Optionally select a random LoRA request. This method is used when LoRA parameters are provided. It randomly selects a LoRA based on max_loras. Parameters: Name Type Description Default max_loras Optional[int] The maximum number of LoRAs available. If None, LoRA is not used. None lora_path Optional[str] Path to the LoRA parameters on disk. If None, LoRA is not used. None Returns: Type Description LoRARequest | None A new LoRARequest LoRARequest | None (or None if not applicable). Source code in vllm/benchmarks/datasets.py 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186def get_random_lora_request( self, max_loras: int | None = None, lora_path: str | None = None, ) -> LoRARequest | None: \"\"\" Optionally select a random LoRA request. This method is used when LoRA parameters are provided. It randomly selects a LoRA based on max_loras. Args: max_loras (Optional[int]): The maximum number of LoRAs available. If `None`, LoRA is not used. lora_path (Optional[str]): Path to the LoRA parameters on disk. If `None`, LoRA is not used. Returns: A new [`LoRARequest`][vllm.lora.request.LoRARequest] (or `None` if not applicable). \"\"\" if max_loras is None or lora_path is None: return None # Generate a random LoRA ID in the range [1, max_loras]. lora_id = random.randint(1, max_loras) lora_request = LoRARequest( lora_name=str(lora_id), lora_int_id=lora_id, lora_path=lora_path_on_disk(lora_path), ) return lora_request load_data ¶ load_data() -> None Load data from the dataset path into self.data. This method must be overridden by subclasses since the method to load data will vary depending on the dataset format and source. Raises: Type Description NotImplementedError If a subclass does not implement this method. Source code in vllm/benchmarks/datasets.py 142 143 144 145 146 147 148 149 150 151 152 153def load_data(self) -> None: \"\"\" Load data from the dataset path into self.data. This method must be overridden by subclasses since the method to load data will vary depending on the dataset format and source. Raises: NotImplementedError: If a subclass does not implement this method. \"\"\" # TODO (jenniferzhao): add support for downloading data raise NotImplementedError(\"load_data must be implemented in subclasses.\") maybe_oversample_requests ¶ maybe_oversample_requests( requests: list[SampleRequest], num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, ) -> None Oversamples the list of requests if its size is less than the desired number. Parameters: Name Type Description Default requests List[SampleRequest] The current list of sampled requests. required num_requests int The target number of requests. required request_id_prefix str The prefix applied to generated request identifiers. '' Source code in vllm/benchmarks/datasets.py 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254def maybe_oversample_requests( self, requests: list[SampleRequest], num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, ) -> None: \"\"\" Oversamples the list of requests if its size is less than the desired number. Args: requests (List[SampleRequest]): The current list of sampled requests. num_requests (int): The target number of requests. request_id_prefix (str): The prefix applied to generated request identifiers. \"\"\" if no_oversample: logger.info(\"Skipping oversampling. Total samples: %d.\", len(requests)) return if len(requests) < num_requests: random.seed(self.random_seed) needed = num_requests - len(requests) additional = [] for i in range(needed): req = deepcopy(random.choice(requests)) req.request_id = request_id_prefix + str(len(requests) + i) additional.append(req) requests.extend(additional) logger.info(\"Oversampled requests to reach %d total samples.\", num_requests) ids = [req.request_id for req in requests] if len(ids) != len(set(ids)): raise ValueError( \"Duplicate request_id found in the sampled \" \"requests. Please ensure that each request_id \" \"is unique.\" ) sample abstractmethod ¶ sample( tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, ) -> list[SampleRequest] Abstract method to generate sample requests from the dataset. Subclasses must override this method to implement dataset-specific logic for generating a list of SampleRequest objects. Parameters: Name Type Description Default tokenizer TokenizerLike The tokenizer to be used for processing the dataset's text. required num_requests int The number of sample requests to generate. required request_id_prefix str The prefix of request_id. '' Returns: Type Description list[SampleRequest] list[SampleRequest]: A list of sample requests generated from the list[SampleRequest] dataset. Source code in vllm/benchmarks/datasets.py 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212@abstractmethod def sample( self, tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, ) -> list[SampleRequest]: \"\"\" Abstract method to generate sample requests from the dataset. Subclasses must override this method to implement dataset-specific logic for generating a list of SampleRequest objects. Args: tokenizer (TokenizerLike): The tokenizer to be used for processing the dataset's text. num_requests (int): The number of sample requests to generate. request_id_prefix (str): The prefix of request_id. Returns: list[SampleRequest]: A list of sample requests generated from the dataset. \"\"\" raise NotImplementedError(\"sample must be implemented in subclasses.\") BlazeditDataset ¶ Bases: HuggingFaceDataset Blazedit Dataset. https://github.com/ise-uiuc/blazedit 5k char version: vdaita/edit_5k_char 10k char version: vdaita/edit_10k_char Source code in vllm/benchmarks/datasets.py 2647 2648 2649 2650 2651 2652 2653 2654 2655 2656 2657 2658 2659 2660 2661 2662 2663 2664 2665 2666 2667 2668 2669 2670 2671 2672 2673 2674 2675 2676 2677 2678 2679 2680 2681 2682 2683 2684 2685 2686 2687 2688 2689 2690 2691 2692 2693 2694 2695 2696 2697 2698 2699 2700 2701 2702 2703 2704 2705 2706 2707 2708 2709 2710 2711 2712 2713 2714 2715 2716 2717 2718 2719 2720 2721 2722 2723 2724 2725 2726 2727 2728class BlazeditDataset(HuggingFaceDataset): \"\"\" Blazedit Dataset. https://github.com/ise-uiuc/blazedit 5k char version: vdaita/edit_5k_char 10k char version: vdaita/edit_10k_char \"\"\" # noqa: E501 # 5k char version will have output as ~5k chars # 10k char version will have output as ~10k chars # Assuming 3 char per token, 10k chars will be 3333 tokens # We set default to 4000 to be safe DEFAULT_OUTPUT_LEN = 4000 SUPPORTED_DATASET_PATHS = { \"vdaita/edit_5k_char\", \"vdaita/edit_10k_char\", } def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, min_distance: float = 0.0, max_distance: float = 1.0, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break code = item[\"code\"] change_request = item[\"change_request\"] norm_distance = item[\"norm_distance\"] # compare the levenshtein distance normalized by code length if norm_distance < min_distance or norm_distance > max_distance: continue # template copied from # https://github.com/ise-uiuc/blazedit/blob/7765137e656fd62de877422d2e4cf8de51228054/dataset/create_refined_dataset.py#L94-L105 # noqa: E501 prompt = f\"\"\"Given a code file, please apply the change requests and generate the new file. Original file: ```python {code} ``` Change request: {change_request} Please generate the new code file in the \"New file\" section below.\"\"\" # noqa: E501 # apply template if not skip_chat_template: prompt = tokenizer.apply_chat_template( [{\"role\": \"user\", \"content\": prompt}], add_generation_prompt=True, tokenize=False, ) prompt_len = len(tokenizer(prompt).input_ids) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶ DEFAULT_OUTPUT_LEN = 4000 SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = { \"vdaita/edit_5k_char\", \"vdaita/edit_10k_char\", } sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, min_distance: float = 0.0, max_distance: float = 1.0, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 2666 2667 2668 2669 2670 2671 2672 2673 2674 2675 2676 2677 2678 2679 2680 2681 2682 2683 2684 2685 2686 2687 2688 2689 2690 2691 2692 2693 2694 2695 2696 2697 2698 2699 2700 2701 2702 2703 2704 2705 2706 2707 2708 2709 2710 2711 2712 2713 2714 2715 2716 2717 2718 2719 2720 2721 2722 2723 2724 2725 2726 2727 2728 def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, min_distance: float = 0.0, max_distance: float = 1.0, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break code = item[\"code\"] change_request = item[\"change_request\"] norm_distance = item[\"norm_distance\"] # compare the levenshtein distance normalized by code length if norm_distance < min_distance or norm_distance > max_distance: continue # template copied from # https://github.com/ise-uiuc/blazedit/blob/7765137e656fd62de877422d2e4cf8de51228054/dataset/create_refined_dataset.py#L94-L105 # noqa: E501 prompt = f\"\"\"Given a code file, please apply the change requests and generate the new file. Original file: ```python {code} ``` Change request: {change_request} Please generate the new code file in the \"New file\" section below.\"\"\" # noqa: E501 # apply template if not skip_chat_template: prompt = tokenizer.apply_chat_template( [{\"role\": \"user\", \"content\": prompt}], add_generation_prompt=True, tokenize=False, ) prompt_len = len(tokenizer(prompt).input_ids) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests BurstGPTDataset ¶ Bases: BenchmarkDataset Implements the BurstGPT dataset. Loads data from a CSV file and generates sample requests based on synthetic prompt generation. Only rows with Model \"GPT-4\" and positive response tokens are used. Source code in vllm/benchmarks/datasets.py 2170 2171 2172 2173 2174 2175 2176 2177 2178 2179 2180 2181 2182 2183 2184 2185 2186 2187 2188 2189 2190 2191 2192 2193 2194 2195 2196 2197 2198 2199 2200 2201 2202 2203 2204 2205 2206 2207 2208 2209 2210 2211 2212 2213 2214 2215 2216 2217 2218 2219 2220 2221 2222 2223 2224 2225 2226 2227 2228 2229 2230 2231 2232 2233 2234 2235 2236 2237 2238 2239class BurstGPTDataset(BenchmarkDataset): \"\"\" Implements the BurstGPT dataset. Loads data from a CSV file and generates sample requests based on synthetic prompt generation. Only rows with Model \"GPT-4\" and positive response tokens are used. \"\"\" def __init__(self, **kwargs) -> None: super().__init__(**kwargs) self.load_data() def load_data( self, ): if self.dataset_path is None: raise ValueError(\"dataset_path must be provided for loading data.\") df = pd.read_csv(self.dataset_path) # Filter to keep only GPT-4 rows. gpt4_df = df[df[\"Model\"] == \"GPT-4\"] # Remove failed requests (where Response tokens is 0 or less). gpt4_df = gpt4_df[gpt4_df[\"Response tokens\"] > 0] # Sample the desired number of rows. self.data = gpt4_df def _sample_loaded_data(self, num_requests: int) -> list: if num_requests <= len(self.data): data = self.data.sample(n=num_requests, random_state=self.random_seed) else: data = self.data.sample( n=num_requests, random_state=self.random_seed, replace=True, ) # Convert the dataframe to a list of lists. return data.values.tolist() def sample( self, tokenizer: TokenizerLike, num_requests: int, max_loras: int | None = None, lora_path: str | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest]: samples = [] data = self._sample_loaded_data(num_requests=num_requests) for i in range(num_requests): input_len = int(data[i][2]) output_len = int(data[i][3]) lora_req = self.get_random_lora_request( max_loras=max_loras, lora_path=lora_path ) vocab_size = tokenizer.vocab_size # Generate a synthetic prompt: a list of token IDs computed as (i + # j) modulo vocab_size. token_ids = [(i + j) % vocab_size for j in range(input_len)] prompt = tokenizer.decode(token_ids) samples.append( SampleRequest( prompt=prompt, prompt_len=input_len, expected_output_len=output_len, lora_request=lora_req, request_id=request_id_prefix + str(i), ) ) return samples __init__ ¶ __init__(**kwargs) -> None Source code in vllm/benchmarks/datasets.py 2177 2178 2179def __init__(self, **kwargs) -> None: super().__init__(**kwargs) self.load_data() _sample_loaded_data ¶ _sample_loaded_data(num_requests: int) -> list Source code in vllm/benchmarks/datasets.py 2195 2196 2197 2198 2199 2200 2201 2202 2203 2204 2205def _sample_loaded_data(self, num_requests: int) -> list: if num_requests <= len(self.data): data = self.data.sample(n=num_requests, random_state=self.random_seed) else: data = self.data.sample( n=num_requests, random_state=self.random_seed, replace=True, ) # Convert the dataframe to a list of lists. return data.values.tolist() load_data ¶ load_data() Source code in vllm/benchmarks/datasets.py 2181 2182 2183 2184 2185 2186 2187 2188 2189 2190 2191 2192 2193def load_data( self, ): if self.dataset_path is None: raise ValueError(\"dataset_path must be provided for loading data.\") df = pd.read_csv(self.dataset_path) # Filter to keep only GPT-4 rows. gpt4_df = df[df[\"Model\"] == \"GPT-4\"] # Remove failed requests (where Response tokens is 0 or less). gpt4_df = gpt4_df[gpt4_df[\"Response tokens\"] > 0] # Sample the desired number of rows. self.data = gpt4_df sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, max_loras: int | None = None, lora_path: str | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest] Source code in vllm/benchmarks/datasets.py 2207 2208 2209 2210 2211 2212 2213 2214 2215 2216 2217 2218 2219 2220 2221 2222 2223 2224 2225 2226 2227 2228 2229 2230 2231 2232 2233 2234 2235 2236 2237 2238 2239def sample( self, tokenizer: TokenizerLike, num_requests: int, max_loras: int | None = None, lora_path: str | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest]: samples = [] data = self._sample_loaded_data(num_requests=num_requests) for i in range(num_requests): input_len = int(data[i][2]) output_len = int(data[i][3]) lora_req = self.get_random_lora_request( max_loras=max_loras, lora_path=lora_path ) vocab_size = tokenizer.vocab_size # Generate a synthetic prompt: a list of token IDs computed as (i + # j) modulo vocab_size. token_ids = [(i + j) % vocab_size for j in range(input_len)] prompt = tokenizer.decode(token_ids) samples.append( SampleRequest( prompt=prompt, prompt_len=input_len, expected_output_len=output_len, lora_request=lora_req, request_id=request_id_prefix + str(i), ) ) return samples ConversationDataset ¶ Bases: HuggingFaceDataset Dataset for text-only conversation data. Source code in vllm/benchmarks/datasets.py 2284 2285 2286 2287 2288 2289 2290 2291 2292 2293 2294 2295 2296 2297 2298 2299 2300 2301 2302 2303 2304 2305 2306 2307 2308 2309 2310 2311 2312 2313 2314 2315 2316 2317 2318 2319 2320 2321 2322 2323 2324 2325 2326 2327 2328 2329 2330 2331 2332 2333 2334 2335 2336 2337 2338 2339 2340 2341class ConversationDataset(HuggingFaceDataset): \"\"\"Dataset for text-only conversation data.\"\"\" SUPPORTED_DATASET_PATHS = { \"Aeala/ShareGPT_Vicuna_unfiltered\", } IS_MULTIMODAL = False def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: # Filter examples with at least 2 conversations filtered_data = self.data.filter(lambda x: len(x[\"conversations\"]) >= 2) sampled_requests = [] ind = 0 dynamic_output = output_len is None for item in filtered_data: if len(sampled_requests) >= num_requests: break conv = item[\"conversations\"] prompt, completion = conv[0][\"value\"], conv[1][\"value\"] prompt_ids = tokenizer(prompt).input_ids completion_ids = tokenizer(completion).input_ids prompt_len = len(prompt_ids) completion_len = len(completion_ids) output_len = completion_len if dynamic_output else output_len assert isinstance(output_len, int) and output_len > 0 if dynamic_output and not is_valid_sequence(prompt_len, completion_len): continue mm_content = process_image(item[\"image\"]) if \"image\" in item else None if enable_multimodal_chat: # Note: when chat is enabled the request prompt_len is no longer # accurate and we will be using request output to count the # actual prompt len and output len prompt = self.apply_multimodal_chat_transformation(prompt, mm_content) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_content, request_id=request_id_prefix + str(ind), ) ) ind += 1 self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests IS_MULTIMODAL class-attribute instance-attribute ¶ IS_MULTIMODAL = False SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = { \"Aeala/ShareGPT_Vicuna_unfiltered\" } sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 2292 2293 2294 2295 2296 2297 2298 2299 2300 2301 2302 2303 2304 2305 2306 2307 2308 2309 2310 2311 2312 2313 2314 2315 2316 2317 2318 2319 2320 2321 2322 2323 2324 2325 2326 2327 2328 2329 2330 2331 2332 2333 2334 2335 2336 2337 2338 2339 2340 2341def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: # Filter examples with at least 2 conversations filtered_data = self.data.filter(lambda x: len(x[\"conversations\"]) >= 2) sampled_requests = [] ind = 0 dynamic_output = output_len is None for item in filtered_data: if len(sampled_requests) >= num_requests: break conv = item[\"conversations\"] prompt, completion = conv[0][\"value\"], conv[1][\"value\"] prompt_ids = tokenizer(prompt).input_ids completion_ids = tokenizer(completion).input_ids prompt_len = len(prompt_ids) completion_len = len(completion_ids) output_len = completion_len if dynamic_output else output_len assert isinstance(output_len, int) and output_len > 0 if dynamic_output and not is_valid_sequence(prompt_len, completion_len): continue mm_content = process_image(item[\"image\"]) if \"image\" in item else None if enable_multimodal_chat: # Note: when chat is enabled the request prompt_len is no longer # accurate and we will be using request output to count the # actual prompt len and output len prompt = self.apply_multimodal_chat_transformation(prompt, mm_content) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_content, request_id=request_id_prefix + str(ind), ) ) ind += 1 self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests CustomDataset ¶ Bases: BenchmarkDataset Implements the Custom dataset. Loads data from a JSONL file and generates sample requests based on conversation turns. E.g., {\"prompt\": \"What is the capital of India?\"} {\"prompt\": \"What is the capital of Iran?\"} {\"prompt\": \"What is the capital of China?\"} Source code in vllm/benchmarks/datasets.py 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 1937 1938 1939 1940 1941 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025 2026class CustomDataset(BenchmarkDataset): \"\"\" Implements the Custom dataset. Loads data from a JSONL file and generates sample requests based on conversation turns. E.g., ``` {\"prompt\": \"What is the capital of India?\"} {\"prompt\": \"What is the capital of Iran?\"} {\"prompt\": \"What is the capital of China?\"} ``` \"\"\" def __init__(self, **kwargs) -> None: super().__init__(**kwargs) self.load_data() def load_data(self) -> None: if self.dataset_path is None: raise ValueError(\"dataset_path must be provided for loading data.\") # self.data will be a list of dictionaries # e.g., [{\"prompt\": \"What is the capital of India?\"}, ...] # This will be the standardized format which load_data() # has to convert into depending on the filetype of dataset_path. # sample() will assume this standardized format of self.data self.data = [] # Load the JSONL file if self.dataset_path.endswith(\".jsonl\"): jsonl_data = pd.read_json(path_or_buf=self.dataset_path, lines=True) # check if the JSONL file has a 'prompt' column if \"prompt\" not in jsonl_data.columns: raise ValueError(\"JSONL file must contain a 'prompt' column.\") # Convert each row to a dictionary and append to self.data # This will convert the DataFrame to a list of dictionaries # where each dictionary corresponds to a row in the DataFrame. # This is the standardized format we want for self.data for _, row in jsonl_data.iterrows(): self.data.append(row.to_dict()) else: raise NotImplementedError( \"Only JSONL format is supported for CustomDataset.\" ) random.seed(self.random_seed) if not getattr(self, \"disable_shuffle\", False): random.shuffle(self.data) def sample( self, tokenizer: TokenizerLike, num_requests: int, lora_path: str | None = None, max_loras: int | None = None, output_len: int | None = None, enable_multimodal_chat: bool = False, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: # load all data if needed self.num_available_samples = len(self.data) if num_requests <= 0: num_requests = self.num_available_samples logger.info( \"num_requests is set to 0 or negative, \" \"so using all available samples: %d\", num_requests, ) sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break prompt = item[\"prompt\"] # apply template if not skip_chat_template: prompt = tokenizer.apply_chat_template( [{\"role\": \"user\", \"content\": prompt}], add_generation_prompt=True, tokenize=False, ) prompt_len = len(tokenizer(prompt).input_ids) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests __init__ ¶ __init__(**kwargs) -> None Source code in vllm/benchmarks/datasets.py 1938 1939 1940def __init__(self, **kwargs) -> None: super().__init__(**kwargs) self.load_data() load_data ¶ load_data() -> None Source code in vllm/benchmarks/datasets.py 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974def load_data(self) -> None: if self.dataset_path is None: raise ValueError(\"dataset_path must be provided for loading data.\") # self.data will be a list of dictionaries # e.g., [{\"prompt\": \"What is the capital of India?\"}, ...] # This will be the standardized format which load_data() # has to convert into depending on the filetype of dataset_path. # sample() will assume this standardized format of self.data self.data = [] # Load the JSONL file if self.dataset_path.endswith(\".jsonl\"): jsonl_data = pd.read_json(path_or_buf=self.dataset_path, lines=True) # check if the JSONL file has a 'prompt' column if \"prompt\" not in jsonl_data.columns: raise ValueError(\"JSONL file must contain a 'prompt' column.\") # Convert each row to a dictionary and append to self.data # This will convert the DataFrame to a list of dictionaries # where each dictionary corresponds to a row in the DataFrame. # This is the standardized format we want for self.data for _, row in jsonl_data.iterrows(): self.data.append(row.to_dict()) else: raise NotImplementedError( \"Only JSONL format is supported for CustomDataset.\" ) random.seed(self.random_seed) if not getattr(self, \"disable_shuffle\", False): random.shuffle(self.data) sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, lora_path: str | None = None, max_loras: int | None = None, output_len: int | None = None, enable_multimodal_chat: bool = False, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 1976 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025 2026def sample( self, tokenizer: TokenizerLike, num_requests: int, lora_path: str | None = None, max_loras: int | None = None, output_len: int | None = None, enable_multimodal_chat: bool = False, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: # load all data if needed self.num_available_samples = len(self.data) if num_requests <= 0: num_requests = self.num_available_samples logger.info( \"num_requests is set to 0 or negative, \" \"so using all available samples: %d\", num_requests, ) sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break prompt = item[\"prompt\"] # apply template if not skip_chat_template: prompt = tokenizer.apply_chat_template( [{\"role\": \"user\", \"content\": prompt}], add_generation_prompt=True, tokenize=False, ) prompt_len = len(tokenizer(prompt).input_ids) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests HuggingFaceDataset ¶ Bases: BenchmarkDataset Base class for datasets hosted on HuggingFace. Source code in vllm/benchmarks/datasets.py 2245 2246 2247 2248 2249 2250 2251 2252 2253 2254 2255 2256 2257 2258 2259 2260 2261 2262 2263 2264 2265 2266 2267 2268 2269 2270 2271 2272 2273 2274 2275 2276class HuggingFaceDataset(BenchmarkDataset): \"\"\"Base class for datasets hosted on HuggingFace.\"\"\" SUPPORTED_DATASET_PATHS: set[str] | dict[str, Callable] = set() def __init__( self, dataset_path: str, dataset_split: str, no_stream: bool = False, dataset_subset: str | None = None, hf_name: str | None = None, **kwargs, ) -> None: super().__init__(dataset_path=dataset_path, **kwargs) self.dataset_split = dataset_split self.dataset_subset = dataset_subset self.load_stream = not no_stream self.hf_name = hf_name or dataset_path self.load_data() def load_data(self) -> None: \"\"\"Load data from HuggingFace datasets.\"\"\" self.data = load_dataset( self.dataset_path, name=self.dataset_subset, split=self.dataset_split, streaming=self.load_stream, ) if not getattr(self, \"disable_shuffle\", False): self.data = self.data.shuffle(seed=self.random_seed) SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS: set[str] | dict[str, Callable] = ( set() ) dataset_split instance-attribute ¶ dataset_split = dataset_split dataset_subset instance-attribute ¶ dataset_subset = dataset_subset hf_name instance-attribute ¶ hf_name = hf_name or dataset_path load_stream instance-attribute ¶ load_stream = not no_stream __init__ ¶ __init__( dataset_path: str, dataset_split: str, no_stream: bool = False, dataset_subset: str | None = None, hf_name: str | None = None, **kwargs, ) -> None Source code in vllm/benchmarks/datasets.py 2250 2251 2252 2253 2254 2255 2256 2257 2258 2259 2260 2261 2262 2263 2264 2265def __init__( self, dataset_path: str, dataset_split: str, no_stream: bool = False, dataset_subset: str | None = None, hf_name: str | None = None, **kwargs, ) -> None: super().__init__(dataset_path=dataset_path, **kwargs) self.dataset_split = dataset_split self.dataset_subset = dataset_subset self.load_stream = not no_stream self.hf_name = hf_name or dataset_path self.load_data() load_data ¶ load_data() -> None Load data from HuggingFace datasets. Source code in vllm/benchmarks/datasets.py 2267 2268 2269 2270 2271 2272 2273 2274 2275 2276def load_data(self) -> None: \"\"\"Load data from HuggingFace datasets.\"\"\" self.data = load_dataset( self.dataset_path, name=self.dataset_subset, split=self.dataset_split, streaming=self.load_stream, ) if not getattr(self, \"disable_shuffle\", False): self.data = self.data.shuffle(seed=self.random_seed) InstructCoderDataset ¶ Bases: HuggingFaceDataset InstructCoder Dataset. https://huggingface.co/datasets/likaixin/InstructCoder InstructCoder is the dataset designed for general code editing. It consists of 114,239 instruction-input-output triplets, and covers multiple distinct code editing scenario. Source code in vllm/benchmarks/datasets.py 2521 2522 2523 2524 2525 2526 2527 2528 2529 2530 2531 2532 2533 2534 2535 2536 2537 2538 2539 2540 2541 2542 2543 2544 2545 2546 2547 2548 2549 2550 2551 2552 2553 2554 2555 2556 2557 2558 2559 2560 2561 2562 2563 2564 2565 2566 2567 2568 2569 2570 2571 2572 2573 2574 2575 2576 2577class InstructCoderDataset(HuggingFaceDataset): \"\"\" InstructCoder Dataset. https://huggingface.co/datasets/likaixin/InstructCoder InstructCoder is the dataset designed for general code editing. It consists of 114,239 instruction-input-output triplets, and covers multiple distinct code editing scenario. \"\"\" DEFAULT_OUTPUT_LEN = 200 # this is the average default output length SUPPORTED_DATASET_PATHS = { \"likaixin/InstructCoder\", } def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break prompt = ( f\"{item['input']}\\n\\n{item['instruction']} Just output \" \"the code, do not include any explanation.\" ) # apply template if not skip_chat_template: prompt = tokenizer.apply_chat_template( [{\"role\": \"user\", \"content\": prompt}], add_generation_prompt=True, tokenize=False, ) prompt_len = len(tokenizer(prompt).input_ids) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶ DEFAULT_OUTPUT_LEN = 200 SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = {'likaixin/InstructCoder'} sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 2536 2537 2538 2539 2540 2541 2542 2543 2544 2545 2546 2547 2548 2549 2550 2551 2552 2553 2554 2555 2556 2557 2558 2559 2560 2561 2562 2563 2564 2565 2566 2567 2568 2569 2570 2571 2572 2573 2574 2575 2576 2577def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break prompt = ( f\"{item['input']}\\n\\n{item['instruction']} Just output \" \"the code, do not include any explanation.\" ) # apply template if not skip_chat_template: prompt = tokenizer.apply_chat_template( [{\"role\": \"user\", \"content\": prompt}], add_generation_prompt=True, tokenize=False, ) prompt_len = len(tokenizer(prompt).input_ids) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests MLPerfDataset ¶ Bases: HuggingFaceDataset MLPerf Inference Dataset. Dataset on HF: https://huggingface.co/datasets/mgoin/mlperf-inference-llama2-data https://huggingface.co/datasets/mgoin/mlperf-inference-llama3.1-data Each record contains \"system_prompt\": system role instruction. \"question\": user question. \"output\": reference answer. We combine the system prompt and question into a chat-formatted prompt (using the tokenizer's chat template) and set the expected output length to the tokenized length of the provided reference answer. Source code in vllm/benchmarks/datasets.py 2984 2985 2986 2987 2988 2989 2990 2991 2992 2993 2994 2995 2996 2997 2998 2999 3000 3001 3002 3003 3004 3005 3006 3007 3008 3009 3010 3011 3012 3013 3014 3015 3016 3017 3018 3019 3020 3021 3022 3023 3024 3025 3026 3027 3028 3029 3030 3031 3032 3033 3034 3035 3036 3037 3038 3039 3040 3041 3042 3043 3044 3045 3046 3047 3048 3049 3050 3051 3052 3053 3054 3055 3056 3057 3058 3059 3060 3061 3062class MLPerfDataset(HuggingFaceDataset): \"\"\" MLPerf Inference Dataset. Dataset on HF: https://huggingface.co/datasets/mgoin/mlperf-inference-llama2-data https://huggingface.co/datasets/mgoin/mlperf-inference-llama3.1-data Each record contains: - \"system_prompt\": system role instruction. - \"question\": user question. - \"output\": reference answer. We combine the system prompt and question into a chat-formatted prompt (using the tokenizer's chat template) and set the expected output length to the tokenized length of the provided reference answer. \"\"\" SUPPORTED_DATASET_PATHS = { \"mgoin/mlperf-inference-llama2-data\", \"mgoin/mlperf-inference-llama3.1-data\", } def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest]: # Force dynamic output length based on reference completion. dynamic_output = output_len is None sampled_requests: list[SampleRequest] = [] ind = 0 for item in self.data: if len(sampled_requests) >= num_requests: break system_prompt = item[\"system_prompt\"] question = item[\"question\"] reference_answer = item[\"output\"] # Build chat-style prompt using tokenizer template, if available. messages = [ {\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": question}, ] prompt_formatted = tokenizer.apply_chat_template( messages, add_generation_prompt=True, tokenize=False ) prompt_len = len(tokenizer(prompt_formatted).input_ids) # Determine output length from reference answer tokens. ref_out_len = len( tokenizer(reference_answer, add_special_tokens=False).input_ids ) expected_output_len = ref_out_len if dynamic_output else output_len # Validate sequence lengths. if not is_valid_sequence(prompt_len, expected_output_len): continue sampled_requests.append( SampleRequest( prompt=prompt_formatted, prompt_len=prompt_len, expected_output_len=expected_output_len, request_id=request_id_prefix + str(ind), ) ) ind += 1 self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = { \"mgoin/mlperf-inference-llama2-data\", \"mgoin/mlperf-inference-llama3.1-data\", } sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest] Source code in vllm/benchmarks/datasets.py 3007 3008 3009 3010 3011 3012 3013 3014 3015 3016 3017 3018 3019 3020 3021 3022 3023 3024 3025 3026 3027 3028 3029 3030 3031 3032 3033 3034 3035 3036 3037 3038 3039 3040 3041 3042 3043 3044 3045 3046 3047 3048 3049 3050 3051 3052 3053 3054 3055 3056 3057 3058 3059 3060 3061 3062def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest]: # Force dynamic output length based on reference completion. dynamic_output = output_len is None sampled_requests: list[SampleRequest] = [] ind = 0 for item in self.data: if len(sampled_requests) >= num_requests: break system_prompt = item[\"system_prompt\"] question = item[\"question\"] reference_answer = item[\"output\"] # Build chat-style prompt using tokenizer template, if available. messages = [ {\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": question}, ] prompt_formatted = tokenizer.apply_chat_template( messages, add_generation_prompt=True, tokenize=False ) prompt_len = len(tokenizer(prompt_formatted).input_ids) # Determine output length from reference answer tokens. ref_out_len = len( tokenizer(reference_answer, add_special_tokens=False).input_ids ) expected_output_len = ref_out_len if dynamic_output else output_len # Validate sequence lengths. if not is_valid_sequence(prompt_len, expected_output_len): continue sampled_requests.append( SampleRequest( prompt=prompt_formatted, prompt_len=prompt_len, expected_output_len=expected_output_len, request_id=request_id_prefix + str(ind), ) ) ind += 1 self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests MMStarDataset ¶ Bases: HuggingFaceDataset Lin-Chen/MMStar: https://huggingface.co/datasets/Lin-Chen/MMStar refer to: https://github.com/sgl-project/SpecForge/pull/106 Source code in vllm/benchmarks/datasets.py 3162 3163 3164 3165 3166 3167 3168 3169 3170 3171 3172 3173 3174 3175 3176 3177 3178 3179 3180 3181 3182 3183 3184 3185 3186 3187 3188 3189 3190 3191 3192 3193 3194 3195 3196 3197 3198 3199 3200 3201 3202 3203 3204 3205 3206 3207 3208 3209 3210 3211 3212 3213 3214 3215 3216 3217 3218 3219 3220 3221 3222 3223 3224 3225 3226 3227class MMStarDataset(HuggingFaceDataset): \"\"\" Lin-Chen/MMStar: https://huggingface.co/datasets/Lin-Chen/MMStar refer to: https://github.com/sgl-project/SpecForge/pull/106 \"\"\" DEFAULT_OUTPUT_LEN = 128 SUPPORTED_DATASET_PATHS = {\"Lin-Chen/MMStar\"} IS_MULTIMODAL = True def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest]: # If --hf-output-len is not set, use the default output length. output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests: list[SampleRequest] = [] for ind, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break # Split the question text from options # (keep only the part before \"Options:\"). full_q: str = item.get(\"question\", \"\") question_text = full_q.split(\"Options:\", 1)[0].strip() # Multimodal image content. mm_content = process_image(item[\"image\"]) # Compute prompt token length (note: this is plain text length # if enable_multimodal_chat is False). prompt_len = len(tokenizer(question_text).input_ids) if enable_multimodal_chat: # If multimodal content should be embedded in the chat message, # convert to [{\"role\":\"user\",\"content\":[...]}] prompt = self.apply_multimodal_chat_transformation( question_text, mm_content ) mm_for_request = None # Already embedded in chat content. else: # Default: prompt is plain text, # image is in mm_content for the bench to assemble. prompt = question_text mm_for_request = mm_content sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_for_request, request_id=request_id_prefix + str(ind), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶ DEFAULT_OUTPUT_LEN = 128 IS_MULTIMODAL class-attribute instance-attribute ¶ IS_MULTIMODAL = True SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = {'Lin-Chen/MMStar'} sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest] Source code in vllm/benchmarks/datasets.py 3172 3173 3174 3175 3176 3177 3178 3179 3180 3181 3182 3183 3184 3185 3186 3187 3188 3189 3190 3191 3192 3193 3194 3195 3196 3197 3198 3199 3200 3201 3202 3203 3204 3205 3206 3207 3208 3209 3210 3211 3212 3213 3214 3215 3216 3217 3218 3219 3220 3221 3222 3223 3224 3225 3226 3227def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest]: # If --hf-output-len is not set, use the default output length. output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests: list[SampleRequest] = [] for ind, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break # Split the question text from options # (keep only the part before \"Options:\"). full_q: str = item.get(\"question\", \"\") question_text = full_q.split(\"Options:\", 1)[0].strip() # Multimodal image content. mm_content = process_image(item[\"image\"]) # Compute prompt token length (note: this is plain text length # if enable_multimodal_chat is False). prompt_len = len(tokenizer(question_text).input_ids) if enable_multimodal_chat: # If multimodal content should be embedded in the chat message, # convert to [{\"role\":\"user\",\"content\":[...]}] prompt = self.apply_multimodal_chat_transformation( question_text, mm_content ) mm_for_request = None # Already embedded in chat content. else: # Default: prompt is plain text, # image is in mm_content for the bench to assemble. prompt = question_text mm_for_request = mm_content sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_for_request, request_id=request_id_prefix + str(ind), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests MMVUDataset ¶ Bases: HuggingFaceDataset MMVU Dataset. https://huggingface.co/datasets/yale-nlp/MMVU Source code in vllm/benchmarks/datasets.py 2462 2463 2464 2465 2466 2467 2468 2469 2470 2471 2472 2473 2474 2475 2476 2477 2478 2479 2480 2481 2482 2483 2484 2485 2486 2487 2488 2489 2490 2491 2492 2493 2494 2495 2496 2497 2498 2499 2500 2501 2502 2503 2504 2505 2506 2507 2508 2509 2510 2511 2512 2513class MMVUDataset(HuggingFaceDataset): \"\"\" MMVU Dataset. https://huggingface.co/datasets/yale-nlp/MMVU \"\"\" DEFAULT_OUTPUT_LEN = 128 SUPPORTED_DATASET_PATHS = { \"yale-nlp/MMVU\": lambda x: x[\"question\"] + \" \" + (\" \".join(f\"{k}.{v}\" for k, v in x[\"choices\"].items())), } def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break parser_fn = self.SUPPORTED_DATASET_PATHS.get(self.hf_name) if parser_fn is None: raise ValueError(f\"Unsupported dataset path: {self.hf_name}\") prompt = parser_fn(item) mm_content = process_video(item[\"video\"]) prompt_len = len(tokenizer(prompt).input_ids) if enable_multimodal_chat: # Note: when chat is enabled the request prompt_len is no longer # accurate and we will be using request output to count the # actual prompt len prompt = self.apply_multimodal_chat_transformation(prompt, mm_content) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_content, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶ DEFAULT_OUTPUT_LEN = 128 SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = { \"yale-nlp/MMVU\": lambda x: x[\"question\"] + \" \" + join(f\"{k}.{v}\" for k, v in (items())) } sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 2475 2476 2477 2478 2479 2480 2481 2482 2483 2484 2485 2486 2487 2488 2489 2490 2491 2492 2493 2494 2495 2496 2497 2498 2499 2500 2501 2502 2503 2504 2505 2506 2507 2508 2509 2510 2511 2512 2513def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break parser_fn = self.SUPPORTED_DATASET_PATHS.get(self.hf_name) if parser_fn is None: raise ValueError(f\"Unsupported dataset path: {self.hf_name}\") prompt = parser_fn(item) mm_content = process_video(item[\"video\"]) prompt_len = len(tokenizer(prompt).input_ids) if enable_multimodal_chat: # Note: when chat is enabled the request prompt_len is no longer # accurate and we will be using request output to count the # actual prompt len prompt = self.apply_multimodal_chat_transformation(prompt, mm_content) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_content, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests MTBenchDataset ¶ Bases: HuggingFaceDataset MT-Bench Dataset. https://huggingface.co/datasets/philschmid/mt-bench We create a single turn dataset for MT-Bench. This is similar to Spec decoding benchmark setup in vLLM https://github.com/vllm-project/vllm/blob/9d98ab5ec/examples/offline_inference/eagle.py#L14-L18 Source code in vllm/benchmarks/datasets.py 2585 2586 2587 2588 2589 2590 2591 2592 2593 2594 2595 2596 2597 2598 2599 2600 2601 2602 2603 2604 2605 2606 2607 2608 2609 2610 2611 2612 2613 2614 2615 2616 2617 2618 2619 2620 2621 2622 2623 2624 2625 2626 2627 2628 2629 2630 2631 2632 2633 2634 2635 2636 2637 2638 2639class MTBenchDataset(HuggingFaceDataset): \"\"\" MT-Bench Dataset. https://huggingface.co/datasets/philschmid/mt-bench We create a single turn dataset for MT-Bench. This is similar to Spec decoding benchmark setup in vLLM https://github.com/vllm-project/vllm/blob/9d98ab5ec/examples/offline_inference/eagle.py#L14-L18 \"\"\" # noqa: E501 DEFAULT_OUTPUT_LEN = 256 # avg len used in SD bench in vLLM SUPPORTED_DATASET_PATHS = { \"philschmid/mt-bench\", } def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break prompt = item[\"turns\"][0] # apply template if not skip_chat_template: prompt = tokenizer.apply_chat_template( [{\"role\": \"user\", \"content\": prompt}], add_generation_prompt=True, tokenize=False, ) prompt_len = len(tokenizer(prompt).input_ids) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶ DEFAULT_OUTPUT_LEN = 256 SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = {'philschmid/mt-bench'} sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 2600 2601 2602 2603 2604 2605 2606 2607 2608 2609 2610 2611 2612 2613 2614 2615 2616 2617 2618 2619 2620 2621 2622 2623 2624 2625 2626 2627 2628 2629 2630 2631 2632 2633 2634 2635 2636 2637 2638 2639def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break prompt = item[\"turns\"][0] # apply template if not skip_chat_template: prompt = tokenizer.apply_chat_template( [{\"role\": \"user\", \"content\": prompt}], add_generation_prompt=True, tokenize=False, ) prompt_len = len(tokenizer(prompt).input_ids) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests MultiModalConversationDataset ¶ Bases: HuggingFaceDataset Dataset for multimodal conversation data. Source code in vllm/benchmarks/datasets.py 2344 2345 2346 2347 2348 2349 2350 2351 2352 2353 2354 2355 2356 2357 2358 2359 2360 2361 2362 2363 2364 2365 2366 2367 2368 2369 2370 2371 2372 2373 2374 2375 2376 2377 2378 2379 2380 2381 2382 2383 2384 2385 2386 2387 2388 2389 2390 2391 2392 2393 2394 2395 2396 2397 2398 2399 2400 2401class MultiModalConversationDataset(HuggingFaceDataset): \"\"\"Dataset for multimodal conversation data.\"\"\" SUPPORTED_DATASET_PATHS = { \"lmms-lab/LLaVA-OneVision-Data\", } IS_MULTIMODAL = True def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: # Filter examples with at least 2 conversations filtered_data = self.data.filter(lambda x: len(x[\"conversations\"]) >= 2) sampled_requests = [] ind = 0 dynamic_output = output_len is None for item in filtered_data: if len(sampled_requests) >= num_requests: break conv = item[\"conversations\"] prompt, completion = conv[0][\"value\"], conv[1][\"value\"] prompt_ids = tokenizer(prompt).input_ids completion_ids = tokenizer(completion).input_ids prompt_len = len(prompt_ids) completion_len = len(completion_ids) output_len = completion_len if dynamic_output else output_len assert isinstance(output_len, int) and output_len > 0 if dynamic_output and not is_valid_sequence(prompt_len, completion_len): continue mm_content = process_image(item[\"image\"]) if \"image\" in item else None if enable_multimodal_chat: # Note: when chat is enabled the request prompt_len is no longer # accurate and we will be using request output to count the # actual prompt len and output len prompt = self.apply_multimodal_chat_transformation(prompt, mm_content) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_content, request_id=request_id_prefix + str(ind), ) ) ind += 1 self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests IS_MULTIMODAL class-attribute instance-attribute ¶ IS_MULTIMODAL = True SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = {'lmms-lab/LLaVA-OneVision-Data'} sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 2352 2353 2354 2355 2356 2357 2358 2359 2360 2361 2362 2363 2364 2365 2366 2367 2368 2369 2370 2371 2372 2373 2374 2375 2376 2377 2378 2379 2380 2381 2382 2383 2384 2385 2386 2387 2388 2389 2390 2391 2392 2393 2394 2395 2396 2397 2398 2399 2400 2401def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: # Filter examples with at least 2 conversations filtered_data = self.data.filter(lambda x: len(x[\"conversations\"]) >= 2) sampled_requests = [] ind = 0 dynamic_output = output_len is None for item in filtered_data: if len(sampled_requests) >= num_requests: break conv = item[\"conversations\"] prompt, completion = conv[0][\"value\"], conv[1][\"value\"] prompt_ids = tokenizer(prompt).input_ids completion_ids = tokenizer(completion).input_ids prompt_len = len(prompt_ids) completion_len = len(completion_ids) output_len = completion_len if dynamic_output else output_len assert isinstance(output_len, int) and output_len > 0 if dynamic_output and not is_valid_sequence(prompt_len, completion_len): continue mm_content = process_image(item[\"image\"]) if \"image\" in item else None if enable_multimodal_chat: # Note: when chat is enabled the request prompt_len is no longer # accurate and we will be using request output to count the # actual prompt len and output len prompt = self.apply_multimodal_chat_transformation(prompt, mm_content) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_content, request_id=request_id_prefix + str(ind), ) ) ind += 1 self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests NextEditPredictionDataset ¶ Bases: HuggingFaceDataset Dataset class for processing a Next Edit Prediction dataset. Source code in vllm/benchmarks/datasets.py 2845 2846 2847 2848 2849 2850 2851 2852 2853 2854 2855 2856 2857 2858 2859 2860 2861 2862 2863 2864 2865 2866 2867 2868 2869 2870 2871 2872 2873 2874 2875 2876 2877 2878 2879 2880 2881 2882 2883 2884 2885 2886class NextEditPredictionDataset(HuggingFaceDataset): \"\"\" Dataset class for processing a Next Edit Prediction dataset. \"\"\" SUPPORTED_DATASET_PATHS = { \"zed-industries/zeta\", } MAPPING_PROMPT_FUNCS = { \"zed-industries/zeta\": _format_zeta_prompt, } def sample( self, tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ): formatting_prompt_func = self.MAPPING_PROMPT_FUNCS.get(self.hf_name) if formatting_prompt_func is None: raise ValueError(f\"Unsupported dataset path: {self.hf_name}\") samples = [] for i, sample in enumerate(self.data): sample = formatting_prompt_func(sample) samples.append( SampleRequest( prompt=sample[\"prompt\"], prompt_len=len(tokenizer(sample[\"prompt\"]).input_ids), expected_output_len=len( tokenizer(sample[\"expected_output\"]).input_ids ), request_id=request_id_prefix + str(i), ) ) if len(samples) >= num_requests: break self.maybe_oversample_requests( samples, num_requests, request_id_prefix, no_oversample ) return samples MAPPING_PROMPT_FUNCS class-attribute instance-attribute ¶ MAPPING_PROMPT_FUNCS = { \"zed-industries/zeta\": _format_zeta_prompt } SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = {'zed-industries/zeta'} sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) Source code in vllm/benchmarks/datasets.py 2857 2858 2859 2860 2861 2862 2863 2864 2865 2866 2867 2868 2869 2870 2871 2872 2873 2874 2875 2876 2877 2878 2879 2880 2881 2882 2883 2884 2885 2886def sample( self, tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ): formatting_prompt_func = self.MAPPING_PROMPT_FUNCS.get(self.hf_name) if formatting_prompt_func is None: raise ValueError(f\"Unsupported dataset path: {self.hf_name}\") samples = [] for i, sample in enumerate(self.data): sample = formatting_prompt_func(sample) samples.append( SampleRequest( prompt=sample[\"prompt\"], prompt_len=len(tokenizer(sample[\"prompt\"]).input_ids), expected_output_len=len( tokenizer(sample[\"expected_output\"]).input_ids ), request_id=request_id_prefix + str(i), ) ) if len(samples) >= num_requests: break self.maybe_oversample_requests( samples, num_requests, request_id_prefix, no_oversample ) return samples PrefixRepetitionRandomDataset ¶ Bases: BenchmarkDataset Source code in vllm/benchmarks/datasets.py 3070 3071 3072 3073 3074 3075 3076 3077 3078 3079 3080 3081 3082 3083 3084 3085 3086 3087 3088 3089 3090 3091 3092 3093 3094 3095 3096 3097 3098 3099 3100 3101 3102 3103 3104 3105 3106 3107 3108 3109 3110 3111 3112 3113 3114 3115 3116 3117 3118 3119 3120 3121 3122 3123 3124 3125 3126 3127 3128 3129 3130 3131 3132 3133 3134 3135 3136 3137 3138 3139 3140 3141 3142 3143 3144 3145 3146 3147 3148 3149 3150 3151 3152 3153 3154class PrefixRepetitionRandomDataset(BenchmarkDataset): # Default values copied from benchmark_serving.py for the repeated prefix # dataset. DEFAULT_PREFIX_LEN = 256 DEFAULT_SUFFIX_LEN = 256 DEFAULT_NUM_PREFIXES = 10 DEFAULT_OUTPUT_LEN = 128 def __init__( self, **kwargs, ) -> None: super().__init__(**kwargs) random.seed(self.random_seed) np.random.seed(self.random_seed) def sample( self, tokenizer: TokenizerLike, num_requests: int, prefix_len: int = DEFAULT_PREFIX_LEN, suffix_len: int = DEFAULT_SUFFIX_LEN, num_prefixes: int = DEFAULT_NUM_PREFIXES, output_len: int = DEFAULT_OUTPUT_LEN, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest]: vocab_size = tokenizer.vocab_size prompts_per_prefix = num_requests // num_prefixes if prompts_per_prefix == 0: raise ValueError( f\"num_requests ({num_requests}) must be greater than or equal \" f\"to num_prefixes ({num_prefixes})\" ) def _generate_exact_length_tokens(target_length: int) -> list[int]: \"\"\"Generate tokens that decode and re-encode to exactly target_length.\"\"\" # Generate random tokens tokens = np.random.randint(0, vocab_size, size=target_length).tolist() _, adjusted_tokens, token_mismatch = gen_prompt_decode_to_target_len( # noqa: E501 tokenizer=tokenizer, token_sequence=tokens, target_token_len=target_length, add_special_tokens=False, ) return adjusted_tokens, token_mismatch requests = [] token_mismatch_total = 0 for _ in range(num_prefixes): prefix_tokens, prefix_mismatch = _generate_exact_length_tokens(prefix_len) token_mismatch_total += prefix_mismatch for _ in range(prompts_per_prefix): suffix_tokens, suffix_mismatch = _generate_exact_length_tokens( suffix_len ) token_mismatch_total += suffix_mismatch combined_tokens = prefix_tokens + suffix_tokens prompt = tokenizer.decode(combined_tokens) prompt_len = len(combined_tokens) requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, ) ) if token_mismatch_total != 0: sign = \"more\" if token_mismatch_total > 0 else \"fewer\" logger.warning( \"Across all generated prompts, there were %d %s tokens \" \"than expected after decoding and re-encoding. This is \" \"expected due to the imperfect nature of the sampling \" \"procedure.\", abs(token_mismatch_total), sign, ) if not getattr(self, \"disable_shuffle\", False): random.shuffle(requests) return requests DEFAULT_NUM_PREFIXES class-attribute instance-attribute ¶ DEFAULT_NUM_PREFIXES = 10 DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶ DEFAULT_OUTPUT_LEN = 128 DEFAULT_PREFIX_LEN class-attribute instance-attribute ¶ DEFAULT_PREFIX_LEN = 256 DEFAULT_SUFFIX_LEN class-attribute instance-attribute ¶ DEFAULT_SUFFIX_LEN = 256 __init__ ¶ __init__(**kwargs) -> None Source code in vllm/benchmarks/datasets.py 3078 3079 3080 3081 3082 3083 3084def __init__( self, **kwargs, ) -> None: super().__init__(**kwargs) random.seed(self.random_seed) np.random.seed(self.random_seed) sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, prefix_len: int = DEFAULT_PREFIX_LEN, suffix_len: int = DEFAULT_SUFFIX_LEN, num_prefixes: int = DEFAULT_NUM_PREFIXES, output_len: int = DEFAULT_OUTPUT_LEN, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest] Source code in vllm/benchmarks/datasets.py 3086 3087 3088 3089 3090 3091 3092 3093 3094 3095 3096 3097 3098 3099 3100 3101 3102 3103 3104 3105 3106 3107 3108 3109 3110 3111 3112 3113 3114 3115 3116 3117 3118 3119 3120 3121 3122 3123 3124 3125 3126 3127 3128 3129 3130 3131 3132 3133 3134 3135 3136 3137 3138 3139 3140 3141 3142 3143 3144 3145 3146 3147 3148 3149 3150 3151 3152 3153 3154def sample( self, tokenizer: TokenizerLike, num_requests: int, prefix_len: int = DEFAULT_PREFIX_LEN, suffix_len: int = DEFAULT_SUFFIX_LEN, num_prefixes: int = DEFAULT_NUM_PREFIXES, output_len: int = DEFAULT_OUTPUT_LEN, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest]: vocab_size = tokenizer.vocab_size prompts_per_prefix = num_requests // num_prefixes if prompts_per_prefix == 0: raise ValueError( f\"num_requests ({num_requests}) must be greater than or equal \" f\"to num_prefixes ({num_prefixes})\" ) def _generate_exact_length_tokens(target_length: int) -> list[int]: \"\"\"Generate tokens that decode and re-encode to exactly target_length.\"\"\" # Generate random tokens tokens = np.random.randint(0, vocab_size, size=target_length).tolist() _, adjusted_tokens, token_mismatch = gen_prompt_decode_to_target_len( # noqa: E501 tokenizer=tokenizer, token_sequence=tokens, target_token_len=target_length, add_special_tokens=False, ) return adjusted_tokens, token_mismatch requests = [] token_mismatch_total = 0 for _ in range(num_prefixes): prefix_tokens, prefix_mismatch = _generate_exact_length_tokens(prefix_len) token_mismatch_total += prefix_mismatch for _ in range(prompts_per_prefix): suffix_tokens, suffix_mismatch = _generate_exact_length_tokens( suffix_len ) token_mismatch_total += suffix_mismatch combined_tokens = prefix_tokens + suffix_tokens prompt = tokenizer.decode(combined_tokens) prompt_len = len(combined_tokens) requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, ) ) if token_mismatch_total != 0: sign = \"more\" if token_mismatch_total > 0 else \"fewer\" logger.warning( \"Across all generated prompts, there were %d %s tokens \" \"than expected after decoding and re-encoding. This is \" \"expected due to the imperfect nature of the sampling \" \"procedure.\", abs(token_mismatch_total), sign, ) if not getattr(self, \"disable_shuffle\", False): random.shuffle(requests) return requests RandomDataset ¶ Bases: BenchmarkDataset Synthetic text-only dataset for serving/throughput benchmarks. Strategy: - Sample input/output token lengths per request from integer-uniform ranges around configured means (controlled by range_ratio). - Prepend a fixed random prefix of length prefix_len. - Generate the remaining tokens as a reproducible sequence: (offset + index + arange(input_len)) % vocab_size. - Decode then re-encode/truncate to ensure prompt token counts match. - Uses numpy.default_rng seeded with random_seed for reproducible sampling. Source code in vllm/benchmarks/datasets.py 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668class RandomDataset(BenchmarkDataset): \"\"\" Synthetic text-only dataset for serving/throughput benchmarks. Strategy: - Sample input/output token lengths per request from integer-uniform ranges around configured means (controlled by range_ratio). - Prepend a fixed random prefix of length prefix_len. - Generate the remaining tokens as a reproducible sequence: (offset + index + arange(input_len)) % vocab_size. - Decode then re-encode/truncate to ensure prompt token counts match. - Uses numpy.default_rng seeded with random_seed for reproducible sampling. \"\"\" # Default values copied from benchmark_serving.py for the random dataset. DEFAULT_PREFIX_LEN = 0 DEFAULT_RANGE_RATIO = 0.0 DEFAULT_INPUT_LEN = 1024 DEFAULT_OUTPUT_LEN = 128 def __init__(self, **kwargs) -> None: super().__init__(**kwargs) # Use numpy's default_rng for deterministic sampling # Do not use random.seed() or np.random.seed() elsewhere in this class. # This ensures that the RNG is isolated from global RNG state. self._rng = np.random.default_rng(self.random_seed) def sample( self, tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, prefix_len: int = DEFAULT_PREFIX_LEN, range_ratio: float = DEFAULT_RANGE_RATIO, input_len: int = DEFAULT_INPUT_LEN, output_len: int = DEFAULT_OUTPUT_LEN, batchsize: int = 1, **kwargs, ) -> list[SampleRequest]: # validate total input tokens (prefix + sampled) is at least 1. num_special = int(tokenizer.num_special_tokens_to_add()) real_input_len = max(0, int(input_len) - num_special) min_sampled_input = math.floor(real_input_len * (1.0 - float(range_ratio))) min_total_input = int(prefix_len) + min_sampled_input if min_total_input < 1: raise ValueError( \"--random-input-len is too small: with tokenizer special \" f\"tokens {num_special} and --random-range-ratio {range_ratio}, \" \"the minimum possible total input tokens (prefix + sampled) is \" f\"{min_total_input}. Increase --random-input-len and/or \" \"--random-prefix-len, or decrease --random-range-ratio so that \" \"prefix_len + floor(max(0, random_input_len - num_special)) \" \"* (1 - range_ratio) >= 1.\" ) input_lens, output_lens, offsets = self.get_sampling_params( num_requests, range_ratio, input_len, output_len, tokenizer ) vocab_size = tokenizer.vocab_size prohibited_tokens = tokenizer.all_special_ids all_tokens = np.arange(vocab_size) allowed_tokens = np.array(list(set(all_tokens) - set(prohibited_tokens))) # Generate prefix once prefix_token_ids = self.get_prefix(allowed_tokens, prefix_len) requests = [] token_mismatch_total = 0 for i in range(num_requests): prompt, total_input_len, token_mismatch = self.generate_token_sequence( # noqa: E501 tokenizer=tokenizer, prefix_token_ids=prefix_token_ids, prefix_len=prefix_len, vocab_size=vocab_size, input_len=int(input_lens[i]), offset=int(offsets[i]), index=i, allowed_tokens=allowed_tokens, ) token_mismatch_total += token_mismatch requests.append( SampleRequest( prompt=prompt, prompt_len=total_input_len, expected_output_len=int(output_lens[i]), request_id=request_id_prefix + str(i), ) ) # only used for embeddings benchmark. if batchsize > 1: batch_requests = [] # Create batched requests for i in range(0, num_requests, batchsize): batch = requests[i : i + batchsize] batch_requests.append( SampleRequest( prompt=[req.prompt for req in batch], prompt_len=sum(req.prompt_len for req in batch), expected_output_len=0, request_id=request_id_prefix + str(i // batchsize), ) ) requests = batch_requests if token_mismatch_total != 0: sign = \"more\" if token_mismatch_total > 0 else \"fewer\" logger.warning( \"Across all generated prompts, there were %d %s tokens \" \"than expected after decoding and re-encoding. This is \" \"expected due to the imperfect nature of the sampling \" \"procedure.\", abs(token_mismatch_total), sign, ) return requests def get_prefix( self, allowed_tokens: np.ndarray, prefix_len: int, ) -> list[int]: \"\"\" Get the prefix for the dataset. \"\"\" return ( allowed_tokens[ self._rng.integers(0, len(allowed_tokens), size=prefix_len) ].tolist() if prefix_len > 0 else [] ) def get_sampling_params( self, num_requests: int, range_ratio: float, input_len: int, output_len: int, tokenizer: TokenizerLike, ) -> tuple[np.ndarray, np.ndarray, np.ndarray]: \"\"\" Get the sampling parameters for the dataset. \"\"\" # Enforce range_ratio < 1 if not (0.0 <= range_ratio < 1.0): raise ValueError(\"range_ratio must be in [0, 1).\") num_special_tokens = int(tokenizer.num_special_tokens_to_add()) real_input_len = max(0, int(input_len) - num_special_tokens) # Bounds use floor for low and ceil for high input_low = math.floor(real_input_len * (1 - range_ratio)) input_high = math.ceil(real_input_len * (1 + range_ratio)) output_low = math.floor(output_len * (1 - range_ratio)) output_high = math.ceil(output_len * (1 + range_ratio)) # Ensure the lower bound for output length is at least 1 to # prevent sampling 0 tokens. output_low = max(output_low, 1) output_high = max(output_high, 1) if input_low > input_high: raise ValueError( f\"Invalid input sampling interval: low={input_low} > high={input_high}\" ) if output_low > output_high: raise ValueError( \"Invalid output sampling interval: \" f\"low={output_low} > high={output_high}\" ) logger.info( \"Sampling input_len from [%s, %s] and output_len from [%s, %s]\", input_low, input_high, output_low, output_high, ) input_lens = self._rng.integers(input_low, input_high + 1, size=num_requests) output_lens = self._rng.integers(output_low, output_high + 1, size=num_requests) offsets = self._rng.integers(0, tokenizer.vocab_size, size=num_requests) return input_lens, output_lens, offsets def generate_token_sequence( self, *, tokenizer: TokenizerLike, prefix_token_ids: list[int], prefix_len: int, vocab_size: int, input_len: int, offset: int, index: int, allowed_tokens: np.ndarray, ) -> tuple[str, int, int]: \"\"\" Returns (prompt, total_input_len). NOTE: After decoding the prompt we have to encode and decode it again. This is done because in some cases N consecutive tokens give a string tokenized into != N number of tokens. For example for GPT2Tokenizer: [6880, 6881] -> ['Ġcalls', 'here'] -> [1650, 939, 486] -> ['Ġcall', 'sh', 'ere'] To avoid uncontrolled change of the prompt length, the encoded sequence is truncated before being decoded again. \"\"\" # Build the inner sequence by sampling # sequentially from the allowed tokens inner_seq = allowed_tokens[ (offset + index + np.arange(input_len)) % len(allowed_tokens) ].tolist() token_sequence = prefix_token_ids + inner_seq # Decode, then re-encode and truncate to preserve token count invariants total_input_len = prefix_len + int(input_len) prompt, adjusted_token_sequence, token_mismatch = ( gen_prompt_decode_to_target_len( tokenizer=tokenizer, token_sequence=token_sequence, target_token_len=total_input_len, add_special_tokens=False, rng=self._rng, ) ) total_input_len = len(adjusted_token_sequence) return prompt, total_input_len, token_mismatch DEFAULT_INPUT_LEN class-attribute instance-attribute ¶ DEFAULT_INPUT_LEN = 1024 DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶ DEFAULT_OUTPUT_LEN = 128 DEFAULT_PREFIX_LEN class-attribute instance-attribute ¶ DEFAULT_PREFIX_LEN = 0 DEFAULT_RANGE_RATIO class-attribute instance-attribute ¶ DEFAULT_RANGE_RATIO = 0.0 _rng instance-attribute ¶ _rng = default_rng(random_seed) __init__ ¶ __init__(**kwargs) -> None Source code in vllm/benchmarks/datasets.py 461 462 463 464 465 466def __init__(self, **kwargs) -> None: super().__init__(**kwargs) # Use numpy's default_rng for deterministic sampling # Do not use random.seed() or np.random.seed() elsewhere in this class. # This ensures that the RNG is isolated from global RNG state. self._rng = np.random.default_rng(self.random_seed) generate_token_sequence ¶ generate_token_sequence( *, tokenizer: TokenizerLike, prefix_token_ids: list[int], prefix_len: int, vocab_size: int, input_len: int, offset: int, index: int, allowed_tokens: ndarray, ) -> tuple[str, int, int] Returns (prompt, total_input_len). NOTE: After decoding the prompt we have to encode and decode it again. This is done because in some cases N consecutive tokens give a string tokenized into != N number of tokens. For example for GPT2Tokenizer: [6880, 6881] -> ['Ġcalls', 'here'] -> [1650, 939, 486] -> ['Ġcall', 'sh', 'ere'] To avoid uncontrolled change of the prompt length, the encoded sequence is truncated before being decoded again. Source code in vllm/benchmarks/datasets.py 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668def generate_token_sequence( self, *, tokenizer: TokenizerLike, prefix_token_ids: list[int], prefix_len: int, vocab_size: int, input_len: int, offset: int, index: int, allowed_tokens: np.ndarray, ) -> tuple[str, int, int]: \"\"\" Returns (prompt, total_input_len). NOTE: After decoding the prompt we have to encode and decode it again. This is done because in some cases N consecutive tokens give a string tokenized into != N number of tokens. For example for GPT2Tokenizer: [6880, 6881] -> ['Ġcalls', 'here'] -> [1650, 939, 486] -> ['Ġcall', 'sh', 'ere'] To avoid uncontrolled change of the prompt length, the encoded sequence is truncated before being decoded again. \"\"\" # Build the inner sequence by sampling # sequentially from the allowed tokens inner_seq = allowed_tokens[ (offset + index + np.arange(input_len)) % len(allowed_tokens) ].tolist() token_sequence = prefix_token_ids + inner_seq # Decode, then re-encode and truncate to preserve token count invariants total_input_len = prefix_len + int(input_len) prompt, adjusted_token_sequence, token_mismatch = ( gen_prompt_decode_to_target_len( tokenizer=tokenizer, token_sequence=token_sequence, target_token_len=total_input_len, add_special_tokens=False, rng=self._rng, ) ) total_input_len = len(adjusted_token_sequence) return prompt, total_input_len, token_mismatch get_prefix ¶ get_prefix( allowed_tokens: ndarray, prefix_len: int ) -> list[int] Get the prefix for the dataset. Source code in vllm/benchmarks/datasets.py 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574def get_prefix( self, allowed_tokens: np.ndarray, prefix_len: int, ) -> list[int]: \"\"\" Get the prefix for the dataset. \"\"\" return ( allowed_tokens[ self._rng.integers(0, len(allowed_tokens), size=prefix_len) ].tolist() if prefix_len > 0 else [] ) get_sampling_params ¶ get_sampling_params( num_requests: int, range_ratio: float, input_len: int, output_len: int, tokenizer: TokenizerLike, ) -> tuple[ndarray, ndarray, ndarray] Get the sampling parameters for the dataset. Source code in vllm/benchmarks/datasets.py 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623def get_sampling_params( self, num_requests: int, range_ratio: float, input_len: int, output_len: int, tokenizer: TokenizerLike, ) -> tuple[np.ndarray, np.ndarray, np.ndarray]: \"\"\" Get the sampling parameters for the dataset. \"\"\" # Enforce range_ratio < 1 if not (0.0 <= range_ratio < 1.0): raise ValueError(\"range_ratio must be in [0, 1).\") num_special_tokens = int(tokenizer.num_special_tokens_to_add()) real_input_len = max(0, int(input_len) - num_special_tokens) # Bounds use floor for low and ceil for high input_low = math.floor(real_input_len * (1 - range_ratio)) input_high = math.ceil(real_input_len * (1 + range_ratio)) output_low = math.floor(output_len * (1 - range_ratio)) output_high = math.ceil(output_len * (1 + range_ratio)) # Ensure the lower bound for output length is at least 1 to # prevent sampling 0 tokens. output_low = max(output_low, 1) output_high = max(output_high, 1) if input_low > input_high: raise ValueError( f\"Invalid input sampling interval: low={input_low} > high={input_high}\" ) if output_low > output_high: raise ValueError( \"Invalid output sampling interval: \" f\"low={output_low} > high={output_high}\" ) logger.info( \"Sampling input_len from [%s, %s] and output_len from [%s, %s]\", input_low, input_high, output_low, output_high, ) input_lens = self._rng.integers(input_low, input_high + 1, size=num_requests) output_lens = self._rng.integers(output_low, output_high + 1, size=num_requests) offsets = self._rng.integers(0, tokenizer.vocab_size, size=num_requests) return input_lens, output_lens, offsets sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, prefix_len: int = DEFAULT_PREFIX_LEN, range_ratio: float = DEFAULT_RANGE_RATIO, input_len: int = DEFAULT_INPUT_LEN, output_len: int = DEFAULT_OUTPUT_LEN, batchsize: int = 1, **kwargs, ) -> list[SampleRequest] Source code in vllm/benchmarks/datasets.py 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558def sample( self, tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, prefix_len: int = DEFAULT_PREFIX_LEN, range_ratio: float = DEFAULT_RANGE_RATIO, input_len: int = DEFAULT_INPUT_LEN, output_len: int = DEFAULT_OUTPUT_LEN, batchsize: int = 1, **kwargs, ) -> list[SampleRequest]: # validate total input tokens (prefix + sampled) is at least 1. num_special = int(tokenizer.num_special_tokens_to_add()) real_input_len = max(0, int(input_len) - num_special) min_sampled_input = math.floor(real_input_len * (1.0 - float(range_ratio))) min_total_input = int(prefix_len) + min_sampled_input if min_total_input < 1: raise ValueError( \"--random-input-len is too small: with tokenizer special \" f\"tokens {num_special} and --random-range-ratio {range_ratio}, \" \"the minimum possible total input tokens (prefix + sampled) is \" f\"{min_total_input}. Increase --random-input-len and/or \" \"--random-prefix-len, or decrease --random-range-ratio so that \" \"prefix_len + floor(max(0, random_input_len - num_special)) \" \"* (1 - range_ratio) >= 1.\" ) input_lens, output_lens, offsets = self.get_sampling_params( num_requests, range_ratio, input_len, output_len, tokenizer ) vocab_size = tokenizer.vocab_size prohibited_tokens = tokenizer.all_special_ids all_tokens = np.arange(vocab_size) allowed_tokens = np.array(list(set(all_tokens) - set(prohibited_tokens))) # Generate prefix once prefix_token_ids = self.get_prefix(allowed_tokens, prefix_len) requests = [] token_mismatch_total = 0 for i in range(num_requests): prompt, total_input_len, token_mismatch = self.generate_token_sequence( # noqa: E501 tokenizer=tokenizer, prefix_token_ids=prefix_token_ids, prefix_len=prefix_len, vocab_size=vocab_size, input_len=int(input_lens[i]), offset=int(offsets[i]), index=i, allowed_tokens=allowed_tokens, ) token_mismatch_total += token_mismatch requests.append( SampleRequest( prompt=prompt, prompt_len=total_input_len, expected_output_len=int(output_lens[i]), request_id=request_id_prefix + str(i), ) ) # only used for embeddings benchmark. if batchsize > 1: batch_requests = [] # Create batched requests for i in range(0, num_requests, batchsize): batch = requests[i : i + batchsize] batch_requests.append( SampleRequest( prompt=[req.prompt for req in batch], prompt_len=sum(req.prompt_len for req in batch), expected_output_len=0, request_id=request_id_prefix + str(i // batchsize), ) ) requests = batch_requests if token_mismatch_total != 0: sign = \"more\" if token_mismatch_total > 0 else \"fewer\" logger.warning( \"Across all generated prompts, there were %d %s tokens \" \"than expected after decoding and re-encoding. This is \" \"expected due to the imperfect nature of the sampling \" \"procedure.\", abs(token_mismatch_total), sign, ) return requests RandomDatasetForReranking ¶ Bases: RandomDataset Random dataset specialized for the needs of scoring: - Batches of inputs - Inputs composed of pairs Source code in vllm/benchmarks/datasets.py 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780class RandomDatasetForReranking(RandomDataset): \"\"\" Random dataset specialized for the needs of scoring: - Batches of inputs - Inputs composed of pairs \"\"\" def __init__(self, **kwargs) -> None: super().__init__(**kwargs) def sample( self, tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", range_ratio: float = RandomDataset.DEFAULT_RANGE_RATIO, input_len: int = RandomDataset.DEFAULT_INPUT_LEN, batchsize: int = 1, is_reranker: bool = True, **kwargs, ) -> list[SampleRequest]: n_sep_tokens = int(is_reranker) query_len_param = (input_len // 2) - n_sep_tokens if is_reranker else input_len query_lens, _, query_offsets = self.get_sampling_params( 1, range_ratio, query_len_param, 0, tokenizer ) query_len = int(query_lens[0]) if not is_reranker: assert num_requests > 1 and batchsize > 1 num_requests -= 1 batchsize -= 1 doc_len_param = input_len else: doc_len_param = input_len - query_len - n_sep_tokens doc_lens, _, doc_offsets = self.get_sampling_params( num_requests, range_ratio, doc_len_param, 0, tokenizer ) vocab_size = tokenizer.vocab_size prohibited_tokens = tokenizer.all_special_ids all_tokens = np.arange(vocab_size) allowed_tokens = np.array(list(set(all_tokens) - set(prohibited_tokens))) query_prompt, query_input_len, token_mismatch_total = ( self.generate_token_sequence( tokenizer=tokenizer, prefix_token_ids=[], prefix_len=0, vocab_size=vocab_size, input_len=query_len, offset=int(query_offsets[0]), index=0, allowed_tokens=allowed_tokens, ) ) requests = [] for i in range(num_requests): prompt, total_input_len, token_mismatch = self.generate_token_sequence( # noqa: E501 tokenizer=tokenizer, prefix_token_ids=[], prefix_len=0, vocab_size=vocab_size, input_len=int(doc_lens[i]), offset=int(doc_offsets[i]), index=i + 1, allowed_tokens=allowed_tokens, ) token_mismatch_total += token_mismatch requests.append((prompt, total_input_len)) batch_requests = [] # Create batched requests for i in range(0, num_requests, batchsize): batch = requests[i : i + batchsize] query_contrib = ( (query_input_len + n_sep_tokens) * len(batch) if is_reranker else query_input_len ) batch_requests.append( SampleRequest( prompt=[query_prompt] + [req[0] for req in batch], prompt_len=query_contrib + sum(req[1] for req in batch), expected_output_len=0, request_id=request_id_prefix + str(i // batchsize), ) ) if token_mismatch_total != 0: logger.warning( \"Across all generated prompts, there were %d %s tokens \" \"than expected after decoding and re-encoding. This is \" \"expected due to the imperfect nature of the sampling \" \"procedure.\", abs(token_mismatch_total), \"more\" if token_mismatch_total > 0 else \"fewer\", ) return batch_requests __init__ ¶ __init__(**kwargs) -> None Source code in vllm/benchmarks/datasets.py 683 684def __init__(self, **kwargs) -> None: super().__init__(**kwargs) sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", range_ratio: float = DEFAULT_RANGE_RATIO, input_len: int = DEFAULT_INPUT_LEN, batchsize: int = 1, is_reranker: bool = True, **kwargs, ) -> list[SampleRequest] Source code in vllm/benchmarks/datasets.py 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780def sample( self, tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", range_ratio: float = RandomDataset.DEFAULT_RANGE_RATIO, input_len: int = RandomDataset.DEFAULT_INPUT_LEN, batchsize: int = 1, is_reranker: bool = True, **kwargs, ) -> list[SampleRequest]: n_sep_tokens = int(is_reranker) query_len_param = (input_len // 2) - n_sep_tokens if is_reranker else input_len query_lens, _, query_offsets = self.get_sampling_params( 1, range_ratio, query_len_param, 0, tokenizer ) query_len = int(query_lens[0]) if not is_reranker: assert num_requests > 1 and batchsize > 1 num_requests -= 1 batchsize -= 1 doc_len_param = input_len else: doc_len_param = input_len - query_len - n_sep_tokens doc_lens, _, doc_offsets = self.get_sampling_params( num_requests, range_ratio, doc_len_param, 0, tokenizer ) vocab_size = tokenizer.vocab_size prohibited_tokens = tokenizer.all_special_ids all_tokens = np.arange(vocab_size) allowed_tokens = np.array(list(set(all_tokens) - set(prohibited_tokens))) query_prompt, query_input_len, token_mismatch_total = ( self.generate_token_sequence( tokenizer=tokenizer, prefix_token_ids=[], prefix_len=0, vocab_size=vocab_size, input_len=query_len, offset=int(query_offsets[0]), index=0, allowed_tokens=allowed_tokens, ) ) requests = [] for i in range(num_requests): prompt, total_input_len, token_mismatch = self.generate_token_sequence( # noqa: E501 tokenizer=tokenizer, prefix_token_ids=[], prefix_len=0, vocab_size=vocab_size, input_len=int(doc_lens[i]), offset=int(doc_offsets[i]), index=i + 1, allowed_tokens=allowed_tokens, ) token_mismatch_total += token_mismatch requests.append((prompt, total_input_len)) batch_requests = [] # Create batched requests for i in range(0, num_requests, batchsize): batch = requests[i : i + batchsize] query_contrib = ( (query_input_len + n_sep_tokens) * len(batch) if is_reranker else query_input_len ) batch_requests.append( SampleRequest( prompt=[query_prompt] + [req[0] for req in batch], prompt_len=query_contrib + sum(req[1] for req in batch), expected_output_len=0, request_id=request_id_prefix + str(i // batchsize), ) ) if token_mismatch_total != 0: logger.warning( \"Across all generated prompts, there were %d %s tokens \" \"than expected after decoding and re-encoding. This is \" \"expected due to the imperfect nature of the sampling \" \"procedure.\", abs(token_mismatch_total), \"more\" if token_mismatch_total > 0 else \"fewer\", ) return batch_requests RandomMultiModalDataset ¶ Bases: RandomDataset Synthetic multimodal dataset (text + images) that extends RandomDataset. Status: - Images: supported via synthetic RGB data. - Video: supported via synthetic RGB data. - Audio: not yet supported. Sampling overview: 1) Number of items per request is sampled uniformly from the integer range [floor(n·(1−r)), ceil(n·(1+r))], where n is the base count and r is num_mm_items_range_ratio in [0, 1]. r=0 keeps it fixed; r=1 allows 0. The maximum is further clamped to the sum of per-modality limits. 2) Each item’s modality and shape is sampled from bucket_config, a dict mapping (height, width, num_frames) → probability. We treat num_frames=1 as image and num_frames > 1 as video. Entries with zero probability are removed and the rest are renormalized to sum to 1. 3) Per-modality hard caps are enforced via limit_mm_per_prompt. When a modality reaches its cap, all of its buckets are excluded and the remaining probabilities are renormalized. Example bucket configuration: {(256, 256, 1): 0.5, (720, 1280, 1): 0.4, (720, 1280, 16): 0.1} - Two image buckets (num_frames=1) and one video bucket (num_frames=16). OBS.: Only image sampling is supported for now. Source code in vllm/benchmarks/datasets.py 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203class RandomMultiModalDataset(RandomDataset): \"\"\" Synthetic multimodal dataset (text + images) that extends RandomDataset. Status: - Images: supported via synthetic RGB data. - Video: supported via synthetic RGB data. - Audio: not yet supported. Sampling overview: 1) Number of items per request is sampled uniformly from the integer range [floor(n·(1−r)), ceil(n·(1+r))], where n is the base count and r is `num_mm_items_range_ratio` in [0, 1]. r=0 keeps it fixed; r=1 allows 0. The maximum is further clamped to the sum of per-modality limits. 2) Each item’s modality and shape is sampled from `bucket_config`, a dict mapping (height, width, num_frames) → probability. We treat `num_frames`=1 as image and `num_frames` > 1 as video. Entries with zero probability are removed and the rest are renormalized to sum to 1. 3) Per-modality hard caps are enforced via `limit_mm_per_prompt`. When a modality reaches its cap, all of its buckets are excluded and the remaining probabilities are renormalized. Example bucket configuration: {(256, 256, 1): 0.5, (720, 1280, 1): 0.4, (720, 1280, 16): 0.1} - Two image buckets (`num_frames`=1) and one video bucket (`num_frames`=16). OBS.: Only image sampling is supported for now. \"\"\" IS_MULTIMODAL = True DEFAULT_LIMIT_MM_PER_PROMPT = {\"image\": 255, \"video\": 1} DEFAULT_BASE_ITEMS_PER_REQUEST = 1 DEFAULT_NUM_MM_ITEMS_RANGE_RATIO = 0.0 DEFAULT_MM_ITEM_BUCKET_CONFIG = { (256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0, } DEFAULT_ENABLE_MULTIMODAL_CHAT = False def __init__(self, **kwargs) -> None: super().__init__(**kwargs) def generate_synthetic_image(self, width: int, height: int) -> Image.Image: \"\"\"Generate synthetic PIL image with random RGB values. NOTE: iid pixel sampling results in worst-case compression (good for stressing I/O), but very unlike real photos. We could consider a “low-freq” mode (e.g., noise blur) to emulate network realism instead of max stress. \"\"\" random_pixels = self._rng.integers( 0, 256, (height, width, 3), dtype=np.uint8, ) return Image.fromarray(random_pixels) def generate_synthetic_video( self, width: int, height: int, num_frames: int ) -> dict: \"\"\"Generate synthetic video with random values. Creates a video with random pixel values, encodes it to MP4 format, and returns the content as bytes. \"\"\" import cv2 random_pixels = self._rng.integers( 0, 256, (num_frames, height, width, 3), dtype=np.uint8, ) # Create a temporary video file in memory fourcc = cv2.VideoWriter_fourcc(*\"mp4v\") fps = 30 # frames per second with NamedTemporaryFile(suffix=\".mp4\", delete_on_close=False) as temp_file: temp_path = temp_file.name # Create video writer video_writer = cv2.VideoWriter( temp_path, fourcc=fourcc, fps=fps, frameSize=(width, height) ) if not video_writer.isOpened(): raise RuntimeError(\"Failed to create video writer\") for frame in random_pixels: video_writer.write(frame) video_writer.release() temp_file.close() # Read the video file content with open(temp_path, \"rb\") as f: video_content = f.read() return {\"bytes\": video_content} def map_config_to_modality(self, config: tuple[int, int, int]) -> str: \"\"\"Map the configuration to the modality.\"\"\" if config[-1] == 1: return \"image\" elif config[-1] > 1: return \"video\" else: raise ValueError(f\"Invalid multimodal item configuration: {config}\") def normalize_bucket_config( self, bucket_config: dict[tuple[int, int, int], float] ) -> dict[tuple[int, int, int], float]: \"\"\" Remove zero probability entries and normalize the bucket config to sum to 1. \"\"\" # Raise error if value is negative if any(v < 0 for v in bucket_config.values()): raise ValueError(\"Bucket config values must be non-negative.\") # Remove zero probability entries bucket_config = {k: v for k, v in bucket_config.items() if v > 0} # if bucket config is empty, raise error if not bucket_config: raise ValueError( \"Got invalid bucket config. Bucket config values must be non-zero.\" ) # Normalize the remaining bucket config to sum to 1 total = sum(bucket_config.values()) return {k: v / total for k, v in bucket_config.items()} def generate_mm_item( self, mm_item_config: tuple[int, int, int], ) -> Mapping[str, Any]: \"\"\" Create synthetic images and videos and apply process_image/process_video respectively. This follows the OpenAI API chat completions https://github.com/openai/openai-python \"\"\" if self.map_config_to_modality(mm_item_config) == \"image\": return process_image( self.generate_synthetic_image(mm_item_config[1], mm_item_config[0]) ) elif self.map_config_to_modality(mm_item_config) == \"video\": return process_video( self.generate_synthetic_video( mm_item_config[1], mm_item_config[0], mm_item_config[2] ) ) else: raise ValueError(f\"Invalid multimodal item configuration: {mm_item_config}\") def get_mm_item_sampling_params( self, base_items_per_request: int, num_mm_items_range_ratio: float, limit_mm_per_prompt: dict[str, int], bucket_config: dict[tuple[int, int, int], float], ) -> tuple[int, int, dict[str, int], dict[tuple[int, int, int], float]]: \"\"\" Get the sampling parameters for the multimodal items. \"\"\" # Enforce num_mm_items_range_ratio <= 1 if not (0.0 <= num_mm_items_range_ratio <= 1.0): raise ValueError(\"num_mm_items_range_ratio must be in [0, 1].\") # Ensure modalities to sample are in limit_mm_per_prompt for k, v in bucket_config.items(): # get modality from bucket config modality = self.map_config_to_modality(k) if modality not in limit_mm_per_prompt: raise ValueError( f\"Modality {modality} is not in \" f\"limit_mm_per_prompt: \" f\"{limit_mm_per_prompt.keys()}\" ) # Remove zero probability entries # and normalize bucket config to sum to 1 bucket_config = self.normalize_bucket_config(bucket_config) logger.info( \"Normalized bucket config: %s\", bucket_config, ) # Only consider limit per prompt for modalities in bucket config allowed_modalities = {self.map_config_to_modality(cfg) for cfg in bucket_config} limit_mm_per_prompt = { k: v for k, v in limit_mm_per_prompt.items() if k in allowed_modalities } if not limit_mm_per_prompt: raise ValueError(\"No valid limits for modalities present in bucket_config.\") logger.info( \"Updated mm-limit-per-prompt: %s\", limit_mm_per_prompt, ) # Get max and min num mm items and ensure # it is at most the sum of limit_mm_per_prompt for all modalities max_num_mm_items = min( sum(limit_mm_per_prompt.values()), math.ceil(base_items_per_request * (1 + num_mm_items_range_ratio)), ) # Ensure min num mm items is at least 0 min_num_mm_items = max( 0, math.floor(base_items_per_request * (1 - num_mm_items_range_ratio)) ) # Raise error if min num mm items is greater than max num mm items if min_num_mm_items > max_num_mm_items: raise ValueError( f\"Min num mm items is greater than max mm items: \" f\"{min_num_mm_items} > {max_num_mm_items}\" ) logger.info( \"Sampling number of multimodal items from [%s, %s]\", min_num_mm_items, max_num_mm_items, ) return ( min_num_mm_items, max_num_mm_items, limit_mm_per_prompt, bucket_config, ) def get_mm_item_iterator( self, min_num_mm_items: int, max_num_mm_items: int, bucket_config: dict[tuple[int, int, int], float], limit_mm_per_prompt: dict[str, int], ) -> Iterator[tuple[int, int, int]]: \"\"\" Iterator over the multimodal items for each request whose size is between min_num_mm_items and max_num_mm_items. Loop over the bucket config and sample a multimodal item. Loop until the number of multimodal items sampled is equal to request_num_mm_items or limit of multimodal items per prompt for all modalities is reached. Note: - This function operates on a per-request shallow copy of `bucket_config` (tuple->float). The original dict passed to `sample` is not mutated. If this ever changes, a test is implemented and will fail. \"\"\" # Get the number of multimodal items to sample request_num_mm_items = int( self._rng.integers(min_num_mm_items, max_num_mm_items + 1) ) # If request_num_mm_items is 0, yield an empty iterator if request_num_mm_items == 0: return # Initialize modality counters modality_counter = {self.map_config_to_modality(k): 0 for k in bucket_config} # Copy the bucket config to avoid modifying the original bucket_config_copy = bucket_config.copy() # Loop over the number of multimodal items to sample while sum(modality_counter.values()) < request_num_mm_items: # Sample a multimodal item config mm_item_config = self._rng.choice( list(bucket_config_copy.keys()), p=list(bucket_config_copy.values()) ) modality = self.map_config_to_modality(mm_item_config) # Check that modality count is less than limit per prompt if modality_counter[modality] < limit_mm_per_prompt[modality]: modality_counter[modality] += 1 yield (mm_item_config) else: # If the counter is greater than the limit per prompt # set all multimodal items of this modality to 0 for k, v in bucket_config_copy.items(): if self.map_config_to_modality(k) == modality: bucket_config_copy[k] = 0 # If all configs are 0, break the loop # This should not happen as request_num_mm_items is at most # the sum of limit_mm_per_prompt for all modalities if all(v == 0 for v in bucket_config_copy.values()): logger.warning( \"Exhausted all multimodal items of modality %s\", modality ) break # Renormalize the bucket config bucket_config_copy = self.normalize_bucket_config(bucket_config_copy) def sample( self, tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, prefix_len: int = RandomDataset.DEFAULT_PREFIX_LEN, range_ratio: float = RandomDataset.DEFAULT_RANGE_RATIO, input_len: int = RandomDataset.DEFAULT_INPUT_LEN, output_len: int = RandomDataset.DEFAULT_OUTPUT_LEN, limit_mm_per_prompt: dict[str, int] = DEFAULT_LIMIT_MM_PER_PROMPT, base_items_per_request: int = DEFAULT_BASE_ITEMS_PER_REQUEST, num_mm_items_range_ratio: float = DEFAULT_NUM_MM_ITEMS_RANGE_RATIO, bucket_config: dict[ tuple[int, int, int], float ] = DEFAULT_MM_ITEM_BUCKET_CONFIG, enable_multimodal_chat: bool = DEFAULT_ENABLE_MULTIMODAL_CHAT, **kwargs, ) -> list[SampleRequest]: # Get the sampling parameters for the dataset input_lens, output_lens, offsets = self.get_sampling_params( num_requests, range_ratio, input_len, output_len, tokenizer ) ( min_num_mm_items, max_num_mm_items, limit_mm_per_prompt, bucket_config, ) = self.get_mm_item_sampling_params( base_items_per_request, num_mm_items_range_ratio, limit_mm_per_prompt, bucket_config, ) vocab_size = tokenizer.vocab_size # Can't use tokenizer.all_special_ids since # it returns ONLY ids from special_tokens_map.json # We want to exclude placeholder tokens and all # tokens that indicate start/end of image as it # may break prompt replacement logic. prohibited_tokens = list( tok_id for tok_id, token in tokenizer.added_tokens_decoder.items() if token.special ) all_tokens = np.arange(vocab_size) allowed_tokens = np.array(list(set(all_tokens) - set(prohibited_tokens))) logger.debug( \"Sampling from %d out of %d (vocab size)\", len(allowed_tokens), vocab_size ) # Generate prefix once prefix_token_ids = self.get_prefix(allowed_tokens, prefix_len) # Add synthetic multimodal items to each request mm_requests = [] token_mismatch_total = 0 for i in range(num_requests): prompt, total_input_len, token_mismatch = self.generate_token_sequence( # noqa: E501 tokenizer=tokenizer, prefix_token_ids=prefix_token_ids, prefix_len=prefix_len, vocab_size=vocab_size, input_len=int(input_lens[i]), offset=int(offsets[i]), index=i, allowed_tokens=allowed_tokens, ) token_mismatch_total += token_mismatch # Get multimodal item iterator for a given request mm_item_iterator = self.get_mm_item_iterator( min_num_mm_items, max_num_mm_items, bucket_config, limit_mm_per_prompt, ) mm_content = cast( list[dict[str, Any]], [ self.generate_mm_item(mm_item_config) for mm_item_config in mm_item_iterator ], ) if enable_multimodal_chat: # NOTE: For now this option is only provided for completeness # given that the serve.py benchmark currently does not use it. mm_chat_prompt: Any = prompt mm_chat_prompt = self.apply_multimodal_chat_transformation( prompt, mm_content ) sample_request = SampleRequest( prompt=mm_chat_prompt, prompt_len=total_input_len, expected_output_len=int(output_lens[i]), multi_modal_data=None, request_id=request_id_prefix + str(i), ) else: sample_request = SampleRequest( prompt=prompt, prompt_len=total_input_len, expected_output_len=int(output_lens[i]), multi_modal_data=mm_content, request_id=request_id_prefix + str(i), ) mm_requests.append(sample_request) if token_mismatch_total != 0: sign = \"more\" if token_mismatch_total > 0 else \"fewer\" logger.warning( \"Across all generated prompts, there were %d %s tokens \" \"than expected after decoding and re-encoding. This is \" \"expected due to the imperfect nature of the sampling \" \"procedure.\", abs(token_mismatch_total), sign, ) return mm_requests DEFAULT_BASE_ITEMS_PER_REQUEST class-attribute instance-attribute ¶ DEFAULT_BASE_ITEMS_PER_REQUEST = 1 DEFAULT_ENABLE_MULTIMODAL_CHAT class-attribute instance-attribute ¶ DEFAULT_ENABLE_MULTIMODAL_CHAT = False DEFAULT_LIMIT_MM_PER_PROMPT class-attribute instance-attribute ¶ DEFAULT_LIMIT_MM_PER_PROMPT = {'image': 255, 'video': 1} DEFAULT_MM_ITEM_BUCKET_CONFIG class-attribute instance-attribute ¶ DEFAULT_MM_ITEM_BUCKET_CONFIG = { (256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0, } DEFAULT_NUM_MM_ITEMS_RANGE_RATIO class-attribute instance-attribute ¶ DEFAULT_NUM_MM_ITEMS_RANGE_RATIO = 0.0 IS_MULTIMODAL class-attribute instance-attribute ¶ IS_MULTIMODAL = True __init__ ¶ __init__(**kwargs) -> None Source code in vllm/benchmarks/datasets.py 830 831def __init__(self, **kwargs) -> None: super().__init__(**kwargs) generate_mm_item ¶ generate_mm_item( mm_item_config: tuple[int, int, int], ) -> Mapping[str, Any] Create synthetic images and videos and apply process_image/process_video respectively. This follows the OpenAI API chat completions https://github.com/openai/openai-python Source code in vllm/benchmarks/datasets.py 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945def generate_mm_item( self, mm_item_config: tuple[int, int, int], ) -> Mapping[str, Any]: \"\"\" Create synthetic images and videos and apply process_image/process_video respectively. This follows the OpenAI API chat completions https://github.com/openai/openai-python \"\"\" if self.map_config_to_modality(mm_item_config) == \"image\": return process_image( self.generate_synthetic_image(mm_item_config[1], mm_item_config[0]) ) elif self.map_config_to_modality(mm_item_config) == \"video\": return process_video( self.generate_synthetic_video( mm_item_config[1], mm_item_config[0], mm_item_config[2] ) ) else: raise ValueError(f\"Invalid multimodal item configuration: {mm_item_config}\") generate_synthetic_image ¶ generate_synthetic_image(width: int, height: int) -> Image Generate synthetic PIL image with random RGB values. NOTE: iid pixel sampling results in worst-case compression (good for stressing I/O), but very unlike real photos. We could consider a “low-freq” mode (e.g., noise blur) to emulate network realism instead of max stress. Source code in vllm/benchmarks/datasets.py 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847def generate_synthetic_image(self, width: int, height: int) -> Image.Image: \"\"\"Generate synthetic PIL image with random RGB values. NOTE: iid pixel sampling results in worst-case compression (good for stressing I/O), but very unlike real photos. We could consider a “low-freq” mode (e.g., noise blur) to emulate network realism instead of max stress. \"\"\" random_pixels = self._rng.integers( 0, 256, (height, width, 3), dtype=np.uint8, ) return Image.fromarray(random_pixels) generate_synthetic_video ¶ generate_synthetic_video( width: int, height: int, num_frames: int ) -> dict Generate synthetic video with random values. Creates a video with random pixel values, encodes it to MP4 format, and returns the content as bytes. Source code in vllm/benchmarks/datasets.py 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891def generate_synthetic_video( self, width: int, height: int, num_frames: int ) -> dict: \"\"\"Generate synthetic video with random values. Creates a video with random pixel values, encodes it to MP4 format, and returns the content as bytes. \"\"\" import cv2 random_pixels = self._rng.integers( 0, 256, (num_frames, height, width, 3), dtype=np.uint8, ) # Create a temporary video file in memory fourcc = cv2.VideoWriter_fourcc(*\"mp4v\") fps = 30 # frames per second with NamedTemporaryFile(suffix=\".mp4\", delete_on_close=False) as temp_file: temp_path = temp_file.name # Create video writer video_writer = cv2.VideoWriter( temp_path, fourcc=fourcc, fps=fps, frameSize=(width, height) ) if not video_writer.isOpened(): raise RuntimeError(\"Failed to create video writer\") for frame in random_pixels: video_writer.write(frame) video_writer.release() temp_file.close() # Read the video file content with open(temp_path, \"rb\") as f: video_content = f.read() return {\"bytes\": video_content} get_mm_item_iterator ¶ get_mm_item_iterator( min_num_mm_items: int, max_num_mm_items: int, bucket_config: dict[tuple[int, int, int], float], limit_mm_per_prompt: dict[str, int], ) -> Iterator[tuple[int, int, int]] Iterator over the multimodal items for each request whose size is between min_num_mm_items and max_num_mm_items. Loop over the bucket config and sample a multimodal item. Loop until the number of multimodal items sampled is equal to request_num_mm_items or limit of multimodal items per prompt for all modalities is reached. Note: - This function operates on a per-request shallow copy of bucket_config (tuple->float). The original dict passed to sample is not mutated. If this ever changes, a test is implemented and will fail. Source code in vllm/benchmarks/datasets.py 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081def get_mm_item_iterator( self, min_num_mm_items: int, max_num_mm_items: int, bucket_config: dict[tuple[int, int, int], float], limit_mm_per_prompt: dict[str, int], ) -> Iterator[tuple[int, int, int]]: \"\"\" Iterator over the multimodal items for each request whose size is between min_num_mm_items and max_num_mm_items. Loop over the bucket config and sample a multimodal item. Loop until the number of multimodal items sampled is equal to request_num_mm_items or limit of multimodal items per prompt for all modalities is reached. Note: - This function operates on a per-request shallow copy of `bucket_config` (tuple->float). The original dict passed to `sample` is not mutated. If this ever changes, a test is implemented and will fail. \"\"\" # Get the number of multimodal items to sample request_num_mm_items = int( self._rng.integers(min_num_mm_items, max_num_mm_items + 1) ) # If request_num_mm_items is 0, yield an empty iterator if request_num_mm_items == 0: return # Initialize modality counters modality_counter = {self.map_config_to_modality(k): 0 for k in bucket_config} # Copy the bucket config to avoid modifying the original bucket_config_copy = bucket_config.copy() # Loop over the number of multimodal items to sample while sum(modality_counter.values()) < request_num_mm_items: # Sample a multimodal item config mm_item_config = self._rng.choice( list(bucket_config_copy.keys()), p=list(bucket_config_copy.values()) ) modality = self.map_config_to_modality(mm_item_config) # Check that modality count is less than limit per prompt if modality_counter[modality] < limit_mm_per_prompt[modality]: modality_counter[modality] += 1 yield (mm_item_config) else: # If the counter is greater than the limit per prompt # set all multimodal items of this modality to 0 for k, v in bucket_config_copy.items(): if self.map_config_to_modality(k) == modality: bucket_config_copy[k] = 0 # If all configs are 0, break the loop # This should not happen as request_num_mm_items is at most # the sum of limit_mm_per_prompt for all modalities if all(v == 0 for v in bucket_config_copy.values()): logger.warning( \"Exhausted all multimodal items of modality %s\", modality ) break # Renormalize the bucket config bucket_config_copy = self.normalize_bucket_config(bucket_config_copy) get_mm_item_sampling_params ¶ get_mm_item_sampling_params( base_items_per_request: int, num_mm_items_range_ratio: float, limit_mm_per_prompt: dict[str, int], bucket_config: dict[tuple[int, int, int], float], ) -> tuple[ int, int, dict[str, int], dict[tuple[int, int, int], float], ] Get the sampling parameters for the multimodal items. Source code in vllm/benchmarks/datasets.py 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020def get_mm_item_sampling_params( self, base_items_per_request: int, num_mm_items_range_ratio: float, limit_mm_per_prompt: dict[str, int], bucket_config: dict[tuple[int, int, int], float], ) -> tuple[int, int, dict[str, int], dict[tuple[int, int, int], float]]: \"\"\" Get the sampling parameters for the multimodal items. \"\"\" # Enforce num_mm_items_range_ratio <= 1 if not (0.0 <= num_mm_items_range_ratio <= 1.0): raise ValueError(\"num_mm_items_range_ratio must be in [0, 1].\") # Ensure modalities to sample are in limit_mm_per_prompt for k, v in bucket_config.items(): # get modality from bucket config modality = self.map_config_to_modality(k) if modality not in limit_mm_per_prompt: raise ValueError( f\"Modality {modality} is not in \" f\"limit_mm_per_prompt: \" f\"{limit_mm_per_prompt.keys()}\" ) # Remove zero probability entries # and normalize bucket config to sum to 1 bucket_config = self.normalize_bucket_config(bucket_config) logger.info( \"Normalized bucket config: %s\", bucket_config, ) # Only consider limit per prompt for modalities in bucket config allowed_modalities = {self.map_config_to_modality(cfg) for cfg in bucket_config} limit_mm_per_prompt = { k: v for k, v in limit_mm_per_prompt.items() if k in allowed_modalities } if not limit_mm_per_prompt: raise ValueError(\"No valid limits for modalities present in bucket_config.\") logger.info( \"Updated mm-limit-per-prompt: %s\", limit_mm_per_prompt, ) # Get max and min num mm items and ensure # it is at most the sum of limit_mm_per_prompt for all modalities max_num_mm_items = min( sum(limit_mm_per_prompt.values()), math.ceil(base_items_per_request * (1 + num_mm_items_range_ratio)), ) # Ensure min num mm items is at least 0 min_num_mm_items = max( 0, math.floor(base_items_per_request * (1 - num_mm_items_range_ratio)) ) # Raise error if min num mm items is greater than max num mm items if min_num_mm_items > max_num_mm_items: raise ValueError( f\"Min num mm items is greater than max mm items: \" f\"{min_num_mm_items} > {max_num_mm_items}\" ) logger.info( \"Sampling number of multimodal items from [%s, %s]\", min_num_mm_items, max_num_mm_items, ) return ( min_num_mm_items, max_num_mm_items, limit_mm_per_prompt, bucket_config, ) map_config_to_modality ¶ map_config_to_modality(config: tuple[int, int, int]) -> str Map the configuration to the modality. Source code in vllm/benchmarks/datasets.py 893 894 895 896 897 898 899 900def map_config_to_modality(self, config: tuple[int, int, int]) -> str: \"\"\"Map the configuration to the modality.\"\"\" if config[-1] == 1: return \"image\" elif config[-1] > 1: return \"video\" else: raise ValueError(f\"Invalid multimodal item configuration: {config}\") normalize_bucket_config ¶ normalize_bucket_config( bucket_config: dict[tuple[int, int, int], float], ) -> dict[tuple[int, int, int], float] Remove zero probability entries and normalize the bucket config to sum to 1. Source code in vllm/benchmarks/datasets.py 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921def normalize_bucket_config( self, bucket_config: dict[tuple[int, int, int], float] ) -> dict[tuple[int, int, int], float]: \"\"\" Remove zero probability entries and normalize the bucket config to sum to 1. \"\"\" # Raise error if value is negative if any(v < 0 for v in bucket_config.values()): raise ValueError(\"Bucket config values must be non-negative.\") # Remove zero probability entries bucket_config = {k: v for k, v in bucket_config.items() if v > 0} # if bucket config is empty, raise error if not bucket_config: raise ValueError( \"Got invalid bucket config. Bucket config values must be non-zero.\" ) # Normalize the remaining bucket config to sum to 1 total = sum(bucket_config.values()) return {k: v / total for k, v in bucket_config.items()} sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, prefix_len: int = DEFAULT_PREFIX_LEN, range_ratio: float = DEFAULT_RANGE_RATIO, input_len: int = DEFAULT_INPUT_LEN, output_len: int = DEFAULT_OUTPUT_LEN, limit_mm_per_prompt: dict[ str, int ] = DEFAULT_LIMIT_MM_PER_PROMPT, base_items_per_request: int = DEFAULT_BASE_ITEMS_PER_REQUEST, num_mm_items_range_ratio: float = DEFAULT_NUM_MM_ITEMS_RANGE_RATIO, bucket_config: dict[ tuple[int, int, int], float ] = DEFAULT_MM_ITEM_BUCKET_CONFIG, enable_multimodal_chat: bool = DEFAULT_ENABLE_MULTIMODAL_CHAT, **kwargs, ) -> list[SampleRequest] Source code in vllm/benchmarks/datasets.py 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203def sample( self, tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, prefix_len: int = RandomDataset.DEFAULT_PREFIX_LEN, range_ratio: float = RandomDataset.DEFAULT_RANGE_RATIO, input_len: int = RandomDataset.DEFAULT_INPUT_LEN, output_len: int = RandomDataset.DEFAULT_OUTPUT_LEN, limit_mm_per_prompt: dict[str, int] = DEFAULT_LIMIT_MM_PER_PROMPT, base_items_per_request: int = DEFAULT_BASE_ITEMS_PER_REQUEST, num_mm_items_range_ratio: float = DEFAULT_NUM_MM_ITEMS_RANGE_RATIO, bucket_config: dict[ tuple[int, int, int], float ] = DEFAULT_MM_ITEM_BUCKET_CONFIG, enable_multimodal_chat: bool = DEFAULT_ENABLE_MULTIMODAL_CHAT, **kwargs, ) -> list[SampleRequest]: # Get the sampling parameters for the dataset input_lens, output_lens, offsets = self.get_sampling_params( num_requests, range_ratio, input_len, output_len, tokenizer ) ( min_num_mm_items, max_num_mm_items, limit_mm_per_prompt, bucket_config, ) = self.get_mm_item_sampling_params( base_items_per_request, num_mm_items_range_ratio, limit_mm_per_prompt, bucket_config, ) vocab_size = tokenizer.vocab_size # Can't use tokenizer.all_special_ids since # it returns ONLY ids from special_tokens_map.json # We want to exclude placeholder tokens and all # tokens that indicate start/end of image as it # may break prompt replacement logic. prohibited_tokens = list( tok_id for tok_id, token in tokenizer.added_tokens_decoder.items() if token.special ) all_tokens = np.arange(vocab_size) allowed_tokens = np.array(list(set(all_tokens) - set(prohibited_tokens))) logger.debug( \"Sampling from %d out of %d (vocab size)\", len(allowed_tokens), vocab_size ) # Generate prefix once prefix_token_ids = self.get_prefix(allowed_tokens, prefix_len) # Add synthetic multimodal items to each request mm_requests = [] token_mismatch_total = 0 for i in range(num_requests): prompt, total_input_len, token_mismatch = self.generate_token_sequence( # noqa: E501 tokenizer=tokenizer, prefix_token_ids=prefix_token_ids, prefix_len=prefix_len, vocab_size=vocab_size, input_len=int(input_lens[i]), offset=int(offsets[i]), index=i, allowed_tokens=allowed_tokens, ) token_mismatch_total += token_mismatch # Get multimodal item iterator for a given request mm_item_iterator = self.get_mm_item_iterator( min_num_mm_items, max_num_mm_items, bucket_config, limit_mm_per_prompt, ) mm_content = cast( list[dict[str, Any]], [ self.generate_mm_item(mm_item_config) for mm_item_config in mm_item_iterator ], ) if enable_multimodal_chat: # NOTE: For now this option is only provided for completeness # given that the serve.py benchmark currently does not use it. mm_chat_prompt: Any = prompt mm_chat_prompt = self.apply_multimodal_chat_transformation( prompt, mm_content ) sample_request = SampleRequest( prompt=mm_chat_prompt, prompt_len=total_input_len, expected_output_len=int(output_lens[i]), multi_modal_data=None, request_id=request_id_prefix + str(i), ) else: sample_request = SampleRequest( prompt=prompt, prompt_len=total_input_len, expected_output_len=int(output_lens[i]), multi_modal_data=mm_content, request_id=request_id_prefix + str(i), ) mm_requests.append(sample_request) if token_mismatch_total != 0: sign = \"more\" if token_mismatch_total > 0 else \"fewer\" logger.warning( \"Across all generated prompts, there were %d %s tokens \" \"than expected after decoding and re-encoding. This is \" \"expected due to the imperfect nature of the sampling \" \"procedure.\", abs(token_mismatch_total), sign, ) return mm_requests SampleRequest dataclass ¶ Represents a single inference request for benchmarking. Source code in vllm/benchmarks/datasets.py 72 73 74 75 76 77 78 79 80 81 82 83@dataclass class SampleRequest: \"\"\" Represents a single inference request for benchmarking. \"\"\" prompt: str | list[str] prompt_len: int expected_output_len: int multi_modal_data: MultiModalDataDict | dict | list[dict] | None = None lora_request: LoRARequest | None = None request_id: str | None = None expected_output_len instance-attribute ¶ expected_output_len: int lora_request class-attribute instance-attribute ¶ lora_request: LoRARequest | None = None multi_modal_data class-attribute instance-attribute ¶ multi_modal_data: ( MultiModalDataDict | dict | list[dict] | None ) = None prompt instance-attribute ¶ prompt: str | list[str] prompt_len instance-attribute ¶ prompt_len: int request_id class-attribute instance-attribute ¶ request_id: str | None = None __init__ ¶ __init__( prompt: str | list[str], prompt_len: int, expected_output_len: int, multi_modal_data: MultiModalDataDict | dict | list[dict] | None = None, lora_request: LoRARequest | None = None, request_id: str | None = None, ) -> None ShareGPTDataset ¶ Bases: BenchmarkDataset Implements the ShareGPT dataset. Loads data from a JSON file and generates sample requests based on conversation turns. Source code in vllm/benchmarks/datasets.py 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294class ShareGPTDataset(BenchmarkDataset): \"\"\" Implements the ShareGPT dataset. Loads data from a JSON file and generates sample requests based on conversation turns. \"\"\" def __init__(self, **kwargs) -> None: super().__init__(**kwargs) self.load_data() def load_data(self) -> None: if self.dataset_path is None: raise ValueError(\"dataset_path must be provided for loading data.\") with open(self.dataset_path, encoding=\"utf-8\") as f: self.data = json.load(f) # Filter entries with at least two conversation turns. self.data = [ entry for entry in self.data if \"conversations\" in entry and len(entry[\"conversations\"]) >= 2 ] random.seed(self.random_seed) if not getattr(self, \"disable_shuffle\", False): random.shuffle(self.data) def sample( self, tokenizer: TokenizerLike, num_requests: int, lora_path: str | None = None, max_loras: int | None = None, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: samples: list = [] ind = 0 for entry in self.data: if len(samples) >= num_requests: break prompt, completion = ( entry[\"conversations\"][0][\"value\"], entry[\"conversations\"][1][\"value\"], ) lora_request = self.get_random_lora_request( max_loras=max_loras, lora_path=lora_path ) prompt_ids = tokenizer(prompt).input_ids completion_ids = tokenizer(completion).input_ids prompt_len = len(prompt_ids) new_output_len = len(completion_ids) if output_len is None else output_len if not is_valid_sequence( prompt_len, new_output_len, skip_min_output_len_check=output_len is not None, ): continue if image_path := entry.get(\"image\"): mm_content = process_image(image_path) elif video_path := entry.get(\"video\"): mm_content = process_video(video_path) else: mm_content = None if enable_multimodal_chat: prompt = self.apply_multimodal_chat_transformation(prompt, mm_content) samples.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=new_output_len, lora_request=lora_request, multi_modal_data=mm_content, request_id=request_id_prefix + str(ind), ) ) ind += 1 self.maybe_oversample_requests( samples, num_requests, request_id_prefix, no_oversample ) return samples __init__ ¶ __init__(**kwargs) -> None Source code in vllm/benchmarks/datasets.py 1217 1218 1219def __init__(self, **kwargs) -> None: super().__init__(**kwargs) self.load_data() load_data ¶ load_data() -> None Source code in vllm/benchmarks/datasets.py 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235def load_data(self) -> None: if self.dataset_path is None: raise ValueError(\"dataset_path must be provided for loading data.\") with open(self.dataset_path, encoding=\"utf-8\") as f: self.data = json.load(f) # Filter entries with at least two conversation turns. self.data = [ entry for entry in self.data if \"conversations\" in entry and len(entry[\"conversations\"]) >= 2 ] random.seed(self.random_seed) if not getattr(self, \"disable_shuffle\", False): random.shuffle(self.data) sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, lora_path: str | None = None, max_loras: int | None = None, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294def sample( self, tokenizer: TokenizerLike, num_requests: int, lora_path: str | None = None, max_loras: int | None = None, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: samples: list = [] ind = 0 for entry in self.data: if len(samples) >= num_requests: break prompt, completion = ( entry[\"conversations\"][0][\"value\"], entry[\"conversations\"][1][\"value\"], ) lora_request = self.get_random_lora_request( max_loras=max_loras, lora_path=lora_path ) prompt_ids = tokenizer(prompt).input_ids completion_ids = tokenizer(completion).input_ids prompt_len = len(prompt_ids) new_output_len = len(completion_ids) if output_len is None else output_len if not is_valid_sequence( prompt_len, new_output_len, skip_min_output_len_check=output_len is not None, ): continue if image_path := entry.get(\"image\"): mm_content = process_image(image_path) elif video_path := entry.get(\"video\"): mm_content = process_video(video_path) else: mm_content = None if enable_multimodal_chat: prompt = self.apply_multimodal_chat_transformation(prompt, mm_content) samples.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=new_output_len, lora_request=lora_request, multi_modal_data=mm_content, request_id=request_id_prefix + str(ind), ) ) ind += 1 self.maybe_oversample_requests( samples, num_requests, request_id_prefix, no_oversample ) return samples SonnetDataset ¶ Bases: BenchmarkDataset Simplified implementation of the Sonnet dataset. Loads poem lines from a text file and generates sample requests. Default values here copied from benchmark_serving.py for the sonnet dataset. Source code in vllm/benchmarks/datasets.py 2079 2080 2081 2082 2083 2084 2085 2086 2087 2088 2089 2090 2091 2092 2093 2094 2095 2096 2097 2098 2099 2100 2101 2102 2103 2104 2105 2106 2107 2108 2109 2110 2111 2112 2113 2114 2115 2116 2117 2118 2119 2120 2121 2122 2123 2124 2125 2126 2127 2128 2129 2130 2131 2132 2133 2134 2135 2136 2137 2138 2139 2140 2141 2142 2143 2144 2145 2146 2147 2148 2149 2150 2151 2152 2153 2154 2155 2156 2157 2158 2159 2160 2161 2162@deprecated( \"SonnetDataset is deprecated and will be removed in a future version.\", ) class SonnetDataset(BenchmarkDataset): \"\"\" Simplified implementation of the Sonnet dataset. Loads poem lines from a text file and generates sample requests. Default values here copied from `benchmark_serving.py` for the sonnet dataset. \"\"\" DEFAULT_PREFIX_LEN = 200 DEFAULT_INPUT_LEN = 550 DEFAULT_OUTPUT_LEN = 150 def __init__( self, **kwargs, ) -> None: super().__init__(**kwargs) self.load_data() def load_data(self) -> None: if not self.dataset_path: raise ValueError(\"dataset_path must be provided.\") with open(self.dataset_path, encoding=\"utf-8\") as f: self.data = f.readlines() def sample( self, tokenizer: TokenizerLike, num_requests: int, prefix_len: int = DEFAULT_PREFIX_LEN, input_len: int = DEFAULT_INPUT_LEN, output_len: int = DEFAULT_OUTPUT_LEN, return_prompt_formatted: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: # Calculate average token length for a poem line. tokenized_lines = [tokenizer(line).input_ids for line in self.data] avg_len = sum(len(tokens) for tokens in tokenized_lines) / len(tokenized_lines) # Build the base prompt. base_prompt = \"Pick as many lines as you can from these poem lines:\\n\" base_msg = [{\"role\": \"user\", \"content\": base_prompt}] base_fmt = tokenizer.apply_chat_template( base_msg, add_generation_prompt=True, tokenize=False ) base_offset = len(tokenizer(base_fmt).input_ids) if input_len <= base_offset: raise ValueError( f\"'input_len' must be higher than the base prompt length \" f\"({base_offset}).\" ) # Determine how many poem lines to use. num_input_lines = round((input_len - base_offset) / avg_len) num_prefix_lines = max(round((prefix_len - base_offset) / avg_len), 0) prefix_lines = self.data[:num_prefix_lines] samples = [] ind = 0 while len(samples) < num_requests: extra_lines = random.choices( self.data, k=num_input_lines - num_prefix_lines ) prompt = f\"{base_prompt}{''.join(prefix_lines + extra_lines)}\" msg = [{\"role\": \"user\", \"content\": prompt}] prompt_formatted = tokenizer.apply_chat_template( msg, add_generation_prompt=True, tokenize=False ) prompt_len = len(tokenizer(prompt_formatted).input_ids) if prompt_len <= input_len: samples.append( SampleRequest( prompt=prompt_formatted if return_prompt_formatted else prompt, prompt_len=prompt_len, expected_output_len=output_len, request_id=request_id_prefix + str(ind), ) ) ind += 1 return samples DEFAULT_INPUT_LEN class-attribute instance-attribute ¶ DEFAULT_INPUT_LEN = 550 DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶ DEFAULT_OUTPUT_LEN = 150 DEFAULT_PREFIX_LEN class-attribute instance-attribute ¶ DEFAULT_PREFIX_LEN = 200 __init__ ¶ __init__(**kwargs) -> None Source code in vllm/benchmarks/datasets.py 2093 2094 2095 2096 2097 2098def __init__( self, **kwargs, ) -> None: super().__init__(**kwargs) self.load_data() load_data ¶ load_data() -> None Source code in vllm/benchmarks/datasets.py 2100 2101 2102 2103 2104def load_data(self) -> None: if not self.dataset_path: raise ValueError(\"dataset_path must be provided.\") with open(self.dataset_path, encoding=\"utf-8\") as f: self.data = f.readlines() sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, prefix_len: int = DEFAULT_PREFIX_LEN, input_len: int = DEFAULT_INPUT_LEN, output_len: int = DEFAULT_OUTPUT_LEN, return_prompt_formatted: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 2106 2107 2108 2109 2110 2111 2112 2113 2114 2115 2116 2117 2118 2119 2120 2121 2122 2123 2124 2125 2126 2127 2128 2129 2130 2131 2132 2133 2134 2135 2136 2137 2138 2139 2140 2141 2142 2143 2144 2145 2146 2147 2148 2149 2150 2151 2152 2153 2154 2155 2156 2157 2158 2159 2160 2161 2162def sample( self, tokenizer: TokenizerLike, num_requests: int, prefix_len: int = DEFAULT_PREFIX_LEN, input_len: int = DEFAULT_INPUT_LEN, output_len: int = DEFAULT_OUTPUT_LEN, return_prompt_formatted: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: # Calculate average token length for a poem line. tokenized_lines = [tokenizer(line).input_ids for line in self.data] avg_len = sum(len(tokens) for tokens in tokenized_lines) / len(tokenized_lines) # Build the base prompt. base_prompt = \"Pick as many lines as you can from these poem lines:\\n\" base_msg = [{\"role\": \"user\", \"content\": base_prompt}] base_fmt = tokenizer.apply_chat_template( base_msg, add_generation_prompt=True, tokenize=False ) base_offset = len(tokenizer(base_fmt).input_ids) if input_len <= base_offset: raise ValueError( f\"'input_len' must be higher than the base prompt length \" f\"({base_offset}).\" ) # Determine how many poem lines to use. num_input_lines = round((input_len - base_offset) / avg_len) num_prefix_lines = max(round((prefix_len - base_offset) / avg_len), 0) prefix_lines = self.data[:num_prefix_lines] samples = [] ind = 0 while len(samples) < num_requests: extra_lines = random.choices( self.data, k=num_input_lines - num_prefix_lines ) prompt = f\"{base_prompt}{''.join(prefix_lines + extra_lines)}\" msg = [{\"role\": \"user\", \"content\": prompt}] prompt_formatted = tokenizer.apply_chat_template( msg, add_generation_prompt=True, tokenize=False ) prompt_len = len(tokenizer(prompt_formatted).input_ids) if prompt_len <= input_len: samples.append( SampleRequest( prompt=prompt_formatted if return_prompt_formatted else prompt, prompt_len=prompt_len, expected_output_len=output_len, request_id=request_id_prefix + str(ind), ) ) ind += 1 return samples SpecBench ¶ Bases: CustomDataset Implements the SpecBench dataset: https://github.com/hemingkx/Spec-Bench Download the dataset using: wget https://raw.githubusercontent.com/hemingkx/Spec-Bench/refs/heads/main/data/spec_bench/question.jsonl Source code in vllm/benchmarks/datasets.py 2034 2035 2036 2037 2038 2039 2040 2041 2042 2043 2044 2045 2046 2047 2048 2049 2050 2051 2052 2053 2054 2055 2056 2057 2058 2059 2060 2061 2062 2063 2064 2065 2066 2067 2068 2069 2070 2071class SpecBench(CustomDataset): \"\"\" Implements the SpecBench dataset: https://github.com/hemingkx/Spec-Bench Download the dataset using: wget https://raw.githubusercontent.com/hemingkx/Spec-Bench/refs/heads/main/data/spec_bench/question.jsonl \"\"\" # noqa: E501 def __init__(self, **kwargs) -> None: self.category = kwargs.pop(\"category\", None) super().__init__(**kwargs) self.load_data() def load_data(self) -> None: if self.dataset_path is None: raise ValueError(\"dataset_path must be provided for loading data.\") self.data = [] # Load the JSONL file jsonl_data = pd.read_json(path_or_buf=self.dataset_path, lines=True) # check if the JSONL file has a 'turns' column if \"turns\" not in jsonl_data.columns: raise ValueError(\"JSONL file must contain a 'turns' column.\") for _, row in jsonl_data.iterrows(): # sample only from a specific category if specified if (not self.category) or (self.category == row[\"category\"]): prompt = row[\"turns\"][0] self.data.append({\"prompt\": prompt}) random.seed(self.random_seed) if not getattr(self, \"disable_shuffle\", False): random.shuffle(self.data) def sample(self, **kwargs) -> list: # leverage CustomDataset sample return super().sample(**kwargs) category instance-attribute ¶ category = pop('category', None) __init__ ¶ __init__(**kwargs) -> None Source code in vllm/benchmarks/datasets.py 2041 2042 2043 2044def __init__(self, **kwargs) -> None: self.category = kwargs.pop(\"category\", None) super().__init__(**kwargs) self.load_data() load_data ¶ load_data() -> None Source code in vllm/benchmarks/datasets.py 2046 2047 2048 2049 2050 2051 2052 2053 2054 2055 2056 2057 2058 2059 2060 2061 2062 2063 2064 2065 2066 2067def load_data(self) -> None: if self.dataset_path is None: raise ValueError(\"dataset_path must be provided for loading data.\") self.data = [] # Load the JSONL file jsonl_data = pd.read_json(path_or_buf=self.dataset_path, lines=True) # check if the JSONL file has a 'turns' column if \"turns\" not in jsonl_data.columns: raise ValueError(\"JSONL file must contain a 'turns' column.\") for _, row in jsonl_data.iterrows(): # sample only from a specific category if specified if (not self.category) or (self.category == row[\"category\"]): prompt = row[\"turns\"][0] self.data.append({\"prompt\": prompt}) random.seed(self.random_seed) if not getattr(self, \"disable_shuffle\", False): random.shuffle(self.data) sample ¶ sample(**kwargs) -> list Source code in vllm/benchmarks/datasets.py 2069 2070 2071def sample(self, **kwargs) -> list: # leverage CustomDataset sample return super().sample(**kwargs) VisionArenaDataset ¶ Bases: HuggingFaceDataset Vision Arena Dataset. Source code in vllm/benchmarks/datasets.py 2409 2410 2411 2412 2413 2414 2415 2416 2417 2418 2419 2420 2421 2422 2423 2424 2425 2426 2427 2428 2429 2430 2431 2432 2433 2434 2435 2436 2437 2438 2439 2440 2441 2442 2443 2444 2445 2446 2447 2448 2449 2450 2451 2452 2453 2454 2455 2456 2457 2458 2459class VisionArenaDataset(HuggingFaceDataset): \"\"\" Vision Arena Dataset. \"\"\" DEFAULT_OUTPUT_LEN = 128 SUPPORTED_DATASET_PATHS = { \"lmarena-ai/VisionArena-Chat\": lambda x: x[\"conversation\"][0][0][\"content\"], \"lmarena-ai/vision-arena-bench-v0.1\": lambda x: x[\"turns\"][0][0][\"content\"], } IS_MULTIMODAL = True def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break parser_fn = self.SUPPORTED_DATASET_PATHS.get(self.hf_name) if parser_fn is None: raise ValueError(f\"Unsupported dataset path: {self.hf_name}\") prompt = parser_fn(item) mm_content = process_image(item[\"images\"][0]) prompt_len = len(tokenizer(prompt).input_ids) if enable_multimodal_chat: # Note: when chat is enabled the request prompt_len is no longer # accurate and we will be using request output to count the # actual prompt len prompt = self.apply_multimodal_chat_transformation(prompt, mm_content) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_content, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶ DEFAULT_OUTPUT_LEN = 128 IS_MULTIMODAL class-attribute instance-attribute ¶ IS_MULTIMODAL = True SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = { \"lmarena-ai/VisionArena-Chat\": lambda x: x[ \"conversation\" ][0][0][\"content\"], \"lmarena-ai/vision-arena-bench-v0.1\": lambda x: x[ \"turns\" ][0][0][\"content\"], } sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 2421 2422 2423 2424 2425 2426 2427 2428 2429 2430 2431 2432 2433 2434 2435 2436 2437 2438 2439 2440 2441 2442 2443 2444 2445 2446 2447 2448 2449 2450 2451 2452 2453 2454 2455 2456 2457 2458 2459def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break parser_fn = self.SUPPORTED_DATASET_PATHS.get(self.hf_name) if parser_fn is None: raise ValueError(f\"Unsupported dataset path: {self.hf_name}\") prompt = parser_fn(item) mm_content = process_image(item[\"images\"][0]) prompt_len = len(tokenizer(prompt).input_ids) if enable_multimodal_chat: # Note: when chat is enabled the request prompt_len is no longer # accurate and we will be using request output to count the # actual prompt len prompt = self.apply_multimodal_chat_transformation(prompt, mm_content) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_content, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests _ValidateDatasetArgs ¶ Bases: Action Argparse action to validate dataset name and path compatibility. Source code in vllm/benchmarks/datasets.py 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314class _ValidateDatasetArgs(argparse.Action): \"\"\"Argparse action to validate dataset name and path compatibility.\"\"\" def __call__(self, parser, namespace, values, option_string=None): setattr(namespace, self.dest, values) # Get current values of both dataset_name and dataset_path dataset_name = getattr(namespace, \"dataset_name\", \"random\") dataset_path = getattr(namespace, \"dataset_path\", None) # Validate the combination if dataset_name == \"random\" and dataset_path is not None: parser.error( \"Cannot use 'random' dataset with --dataset-path. \" \"Please specify the appropriate --dataset-name (e.g., \" \"'sharegpt', 'custom', 'sonnet') for your dataset file: \" f\"{dataset_path}\" ) __call__ ¶ __call__(parser, namespace, values, option_string=None) Source code in vllm/benchmarks/datasets.py 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314def __call__(self, parser, namespace, values, option_string=None): setattr(namespace, self.dest, values) # Get current values of both dataset_name and dataset_path dataset_name = getattr(namespace, \"dataset_name\", \"random\") dataset_path = getattr(namespace, \"dataset_path\", None) # Validate the combination if dataset_name == \"random\" and dataset_path is not None: parser.error( \"Cannot use 'random' dataset with --dataset-path. \" \"Please specify the appropriate --dataset-name (e.g., \" \"'sharegpt', 'custom', 'sonnet') for your dataset file: \" f\"{dataset_path}\" ) _format_zeta_prompt ¶ _format_zeta_prompt( sample: dict, original_start_marker: str = \"<|editable_region_start|>\", ) -> dict Format the zeta prompt for the Next Edit Prediction (NEP) dataset. This function formats examples from the NEP dataset into prompts and expected outputs. It could be further extended to support more NEP datasets. Parameters: Name Type Description Default sample dict The dataset sample containing events, inputs, and outputs. required original_start_marker str The marker indicating the start of the editable region. Defaults to \"<|editable_region_start|>\". '<|editable_region_start|>' Returns: Type Description dict A dictionary with the formatted prompts and expected outputs. Source code in vllm/benchmarks/datasets.py 2812 2813 2814 2815 2816 2817 2818 2819 2820 2821 2822 2823 2824 2825 2826 2827 2828 2829 2830 2831 2832 2833 2834 2835 2836 2837 2838 2839 2840 2841 2842def _format_zeta_prompt( sample: dict, original_start_marker: str = \"<|editable_region_start|>\" ) -> dict: \"\"\"Format the zeta prompt for the Next Edit Prediction (NEP) dataset. This function formats examples from the NEP dataset into prompts and expected outputs. It could be further extended to support more NEP datasets. Args: sample: The dataset sample containing events, inputs, and outputs. original_start_marker: The marker indicating the start of the editable region. Defaults to \"<|editable_region_start|>\". Returns: A dictionary with the formatted prompts and expected outputs. \"\"\" events = sample[\"events\"] input = sample[\"input\"] output = sample[\"output\"] prompt = zeta_prompt.format(events, input) # following the original implementation, extract the focused region # from the raw output output_start_index = output.find(original_start_marker) output_focused_region = output[output_start_index:] expected_output = output_focused_region return {\"prompt\": prompt, \"expected_output\": expected_output} add_dataset_parser ¶ add_dataset_parser(parser: ArgumentParser) Source code in vllm/benchmarks/datasets.py 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 1633 1634 1635 1636 1637 1638def add_dataset_parser(parser: FlexibleArgumentParser): parser.add_argument(\"--seed\", type=int, default=0) parser.add_argument( \"--num-prompts\", type=int, default=1000, help=\"Number of prompts to process.\", ) parser.add_argument( \"--dataset-name\", type=str, default=\"random\", action=_ValidateDatasetArgs, choices=[ \"sharegpt\", \"burstgpt\", \"sonnet\", \"random\", \"random-mm\", \"random-rerank\", \"hf\", \"custom\", \"prefix_repetition\", \"spec_bench\", ], help=\"Name of the dataset to benchmark on.\", ) parser.add_argument( \"--no-stream\", action=\"store_true\", help=\"Do not load the dataset in streaming mode.\", ) parser.add_argument( \"--dataset-path\", type=str, default=None, action=_ValidateDatasetArgs, help=\"Path to the sharegpt/sonnet dataset. \" \"Or the huggingface dataset ID if using HF dataset.\", ) parser.add_argument( \"--no-oversample\", action=\"store_true\", help=\"Do not oversample if the dataset has fewer samples than num-prompts.\", ) parser.add_argument( \"--skip-chat-template\", action=\"store_true\", help=\"Skip applying chat template to prompt for datasets that support it.\", ) parser.add_argument( \"--disable-shuffle\", action=\"store_true\", help=\"Disable shuffling of dataset samples for deterministic ordering.\", ) # group for dataset specific arguments custom_group = parser.add_argument_group(\"custom dataset options\") custom_group.add_argument( \"--custom-output-len\", type=int, default=256, help=\"Number of output tokens per request, used only for custom dataset.\", ) spec_bench_group = parser.add_argument_group(\"spec bench dataset options\") spec_bench_group.add_argument( \"--spec-bench-output-len\", type=int, default=256, help=\"Num of output tokens per request, used only for spec bench dataset.\", ) spec_bench_group.add_argument( \"--spec-bench-category\", type=str, default=None, help=\"Category for spec bench dataset. If None, use all categories.\", ) sonnet_group = parser.add_argument_group(\"sonnet dataset options\") sonnet_group.add_argument( \"--sonnet-input-len\", type=int, default=550, help=\"Number of input tokens per request, used only for sonnet dataset.\", ) sonnet_group.add_argument( \"--sonnet-output-len\", type=int, default=150, help=\"Number of output tokens per request, used only for sonnet dataset.\", ) sonnet_group.add_argument( \"--sonnet-prefix-len\", type=int, default=200, help=\"Number of prefix tokens per request, used only for sonnet dataset.\", ) sharegpt_group = parser.add_argument_group(\"sharegpt dataset options\") sharegpt_group.add_argument( \"--sharegpt-output-len\", type=int, default=None, help=\"Output length for each request. Overrides the output length \" \"from the ShareGPT dataset.\", ) blazedit_group = parser.add_argument_group(\"blazedit dataset options\") blazedit_group.add_argument( \"--blazedit-min-distance\", type=float, default=0.0, help=\"Minimum distance for blazedit dataset. Min: 0, Max: 1.0\", ) blazedit_group.add_argument( \"--blazedit-max-distance\", type=float, default=1.0, help=\"Maximum distance for blazedit dataset. Min: 0, Max: 1.0\", ) random_group = parser.add_argument_group(\"random dataset options\") random_group.add_argument( \"--random-input-len\", type=int, default=1024, help=\"Number of input tokens per request, used only for random sampling.\", ) random_group.add_argument( \"--random-output-len\", type=int, default=128, help=\"Number of output tokens per request, used only for random sampling.\", ) random_group.add_argument( \"--random-range-ratio\", type=float, default=0.0, help=\"Range ratio for sampling input/output length, \" \"used only for random sampling. Must be in the range [0, 1) to define \" \"a symmetric sampling range\" \"[length * (1 - range_ratio), length * (1 + range_ratio)].\", ) random_group.add_argument( \"--random-prefix-len\", type=int, default=0, help=( \"Number of fixed prefix tokens before the random context \" \"in a request. \" \"The total input length is the sum of `random-prefix-len` and \" \"a random \" \"context length sampled from [input_len * (1 - range_ratio), \" \"input_len * (1 + range_ratio)].\" ), ) random_group.add_argument( \"--random-batch-size\", type=int, default=1, help=(\"Batch size for random sampling. Only used for embeddings benchmark.\"), ) random_group.add_argument( \"--no-reranker\", action=\"store_true\", help=( \"Whether the model supports reranking natively.\" \" Only used for reranker benchmark.\" ), ) # random multimodal dataset options random_mm_group = parser.add_argument_group( \"random multimodal dataset options extended from random dataset\" ) random_mm_group.add_argument( \"--random-mm-base-items-per-request\", type=int, default=RandomMultiModalDataset.DEFAULT_BASE_ITEMS_PER_REQUEST, help=( \"Base number of multimodal items per request for random-mm. \" \"Actual per-request count is sampled around this base using \" \"--random-mm-num-mm-items-range-ratio.\" ), ) random_mm_group.add_argument( \"--random-mm-num-mm-items-range-ratio\", type=float, default=RandomMultiModalDataset.DEFAULT_NUM_MM_ITEMS_RANGE_RATIO, help=( \"Range ratio r in [0, 1] for sampling items per request. \" \"We sample uniformly from the closed integer range \" \"[floor(n*(1-r)), ceil(n*(1+r))] \" \"where n is the base items per request. \" \"r=0 keeps it fixed; r=1 allows 0 items. The maximum is clamped \" \"to the sum of per-modality limits from \" \"--random-mm-limit-mm-per-prompt. \" \"An error is raised if the computed min exceeds the max.\" ), ) random_mm_group.add_argument( \"--random-mm-limit-mm-per-prompt\", type=json.loads, default=RandomMultiModalDataset.DEFAULT_LIMIT_MM_PER_PROMPT, help=( \"Per-modality hard caps for items attached per request, e.g. \" '\\'{\"image\": 3, \"video\": 0}\\'. The sampled per-request item ' \"count is clamped to the sum of these limits. When a modality \" \"reaches its cap, its buckets are excluded and probabilities are \" \"renormalized.\" \"OBS.: Only image sampling is supported for now.\" ), ) def _parse_mm_bucket_config(v: object) -> dict[tuple[int, int, int], float]: # If already a dict (e.g., programmatic call), normalize keys def normalize(d: dict) -> dict[tuple[int, int, int], float]: out: dict[tuple[int, int, int], float] = {} for k, val in d.items(): key = k if isinstance(key, str): with suppress(Exception): key = ast.literal_eval(key) if not ( isinstance(key, tuple) and len(key) == 3 and all(isinstance(x, int) for x in key) ): raise ValueError( f\"Invalid bucket key {k!r}. Expected tuple (H, W, T).\" ) out[(int(key[0]), int(key[1]), int(key[2]))] = float(val) return out if isinstance(v, dict): return normalize(v) if isinstance(v, str): # Python literal (supports tuple keys) parsed = ast.literal_eval(v) if not isinstance(parsed, dict): raise ValueError(\"Bucket config must parse to a dict.\") return normalize(parsed) raise ValueError(\"Unsupported value for --random-mm-bucket-config.\") random_mm_group.add_argument( \"--random-mm-bucket-config\", type=_parse_mm_bucket_config, default=RandomMultiModalDataset.DEFAULT_MM_ITEM_BUCKET_CONFIG, help=( \"The bucket config is a dictionary mapping a multimodal item\" \"sampling configuration to a probability.\" \"Currently allows for 2 modalities: images and videos. \" \"An bucket key is a tuple of (height, width, num_frames)\" \"The value is the probability of sampling that specific item. \" \"Example: \" \"--random-mm-bucket-config \" \"{(256, 256, 1): 0.5, (720, 1280, 1): 0.4, (720, 1280, 16): 0.10} \" \"First item: images with resolution 256x256 w.p. 0.5\" \"Second item: images with resolution 720x1280 w.p. 0.4 \" \"Third item: videos with resolution 720x1280 and 16 frames w.p. 0.1\" \"OBS.: If the probabilities do not sum to 1, they are normalized.\" \"OBS bis.: Only image sampling is supported for now.\" ), ) hf_group = parser.add_argument_group(\"hf dataset options\") hf_group.add_argument( \"--hf-subset\", type=str, default=None, help=\"Subset of the HF dataset.\" ) hf_group.add_argument( \"--hf-split\", type=str, default=None, help=\"Split of the HF dataset.\" ) hf_group.add_argument( \"--hf-name\", type=str, default=None, help=( \"Name of the dataset on HuggingFace \" \"(e.g., 'lmarena-ai/VisionArena-Chat'). \" \"Specify this if your dataset-path is a local path.\" ), ) hf_group.add_argument( \"--hf-output-len\", type=int, default=None, help=\"Output length for each request. Overrides the output lengths \" \"from the sampled HF dataset.\", ) prefix_repetition_group = parser.add_argument_group( \"prefix repetition dataset options\" ) prefix_repetition_group.add_argument( \"--prefix-repetition-prefix-len\", type=int, default=256, help=\"Number of prefix tokens per request, used only for prefix \" \"repetition dataset.\", ) prefix_repetition_group.add_argument( \"--prefix-repetition-suffix-len\", type=int, default=256, help=\"Number of suffix tokens per request, used only for prefix \" \"repetition dataset. Total input length is prefix_len + suffix_len.\", ) prefix_repetition_group.add_argument( \"--prefix-repetition-num-prefixes\", type=int, default=10, help=\"Number of prefixes to generate, used only for prefix repetition \" \"dataset. Prompts per prefix is num_requests // num_prefixes.\", ) prefix_repetition_group.add_argument( \"--prefix-repetition-output-len\", type=int, default=128, help=\"Number of output tokens per request, used only for prefix \" \"repetition dataset.\", ) gen_prompt_decode_to_target_len ¶ gen_prompt_decode_to_target_len( tokenizer: TokenizerLike, token_sequence: list[int], target_token_len: int, max_retry: int = 10, add_special_tokens: bool = False, rng: Generator | None = None, ) -> tuple[str, list[int]] Ensure decoded-then-encoded prompt length matches the target token length. This function decodes an initial token sequence to text and re-encodes it , iteratively adjusting the token sequence length to match a target. This is necessary because some tokenizers do not guarantee a 1:1 mapping between consecutive tokens and the decoded-then-encoded sequence length. For example, for GPT2Tokenizer: [6880, 6881] -> ['Ġcalls', 'here'] -> [1650, 939, 486] -> ['Ġcall', 'sh', 'ere'] Returns a tuple of the final prompt string and the adjusted token sequence. Source code in vllm/benchmarks/datasets.py 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433def gen_prompt_decode_to_target_len( tokenizer: TokenizerLike, token_sequence: list[int], target_token_len: int, max_retry: int = 10, add_special_tokens: bool = False, rng: np.random.Generator | None = None, ) -> tuple[str, list[int]]: \"\"\" Ensure decoded-then-encoded prompt length matches the target token length. This function decodes an initial token sequence to text and re-encodes it , iteratively adjusting the token sequence length to match a target. This is necessary because some tokenizers do not guarantee a 1:1 mapping between consecutive tokens and the decoded-then-encoded sequence length. For example, for GPT2Tokenizer: [6880, 6881] -> ['Ġcalls', 'here'] -> [1650, 939, 486] -> ['Ġcall', 'sh', 'ere'] Returns a tuple of the final prompt string and the adjusted token sequence. \"\"\" remain_num_try = max_retry token_mismatch = 0 while True: prompt = tokenizer.decode(token_sequence) token_sequence = tokenizer.encode(prompt, add_special_tokens=add_special_tokens) if remain_num_try <= 0: if len(token_sequence) != target_token_len: token_mismatch = len(token_sequence) - target_token_len break if len(token_sequence) == target_token_len: break elif len(token_sequence) < target_token_len: if rng is not None: extra_tokens = rng.integers( 0, tokenizer.vocab_size, size=target_token_len - len(token_sequence), ).tolist() else: extra_tokens = np.random.randint( 0, tokenizer.vocab_size, size=target_token_len - len(token_sequence), ).tolist() token_sequence.extend(extra_tokens) elif len(token_sequence) > target_token_len: token_sequence = token_sequence[:target_token_len] remain_num_try -= 1 return prompt, token_sequence, token_mismatch get_samples ¶ get_samples( args, tokenizer: TokenizerLike ) -> list[SampleRequest] Source code in vllm/benchmarks/datasets.py 1641 1642 1643 1644 1645 1646 1647 1648 1649 1650 1651 1652 1653 1654 1655 1656 1657 1658 1659 1660 1661 1662 1663 1664 1665 1666 1667 1668 1669 1670 1671 1672 1673 1674 1675 1676 1677 1678 1679 1680 1681 1682 1683 1684 1685 1686 1687 1688 1689 1690 1691 1692 1693 1694 1695 1696 1697 1698 1699 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728 1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 1742 1743 1744 1745 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 1793 1794 1795 1796 1797 1798 1799 1800 1801 1802 1803 1804 1805 1806 1807 1808 1809 1810 1811 1812 1813 1814 1815 1816 1817 1818 1819 1820 1821 1822 1823 1824 1825 1826 1827 1828 1829 1830 1831 1832 1833 1834 1835 1836 1837 1838 1839 1840 1841 1842 1843 1844 1845 1846 1847 1848 1849 1850 1851 1852 1853 1854 1855 1856 1857 1858 1859 1860 1861 1862 1863 1864 1865 1866 1867 1868 1869 1870 1871 1872 1873 1874 1875 1876 1877 1878 1879 1880 1881 1882 1883 1884 1885 1886 1887 1888 1889 1890 1891 1892 1893 1894 1895 1896 1897 1898 1899 1900 1901 1902 1903 1904 1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919def get_samples(args, tokenizer: TokenizerLike) -> list[SampleRequest]: if not hasattr(args, \"request_id_prefix\"): args.request_id_prefix = \"\" if args.dataset_name == \"custom\": dataset = CustomDataset( dataset_path=args.dataset_path, disable_shuffle=args.disable_shuffle ) input_requests = dataset.sample( num_requests=args.num_prompts, tokenizer=tokenizer, output_len=args.custom_output_len, skip_chat_template=args.skip_chat_template, request_id_prefix=args.request_id_prefix, no_oversample=args.no_oversample, ) elif args.dataset_name == \"sonnet\": dataset = SonnetDataset( dataset_path=args.dataset_path, disable_shuffle=args.disable_shuffle ) # For the \"sonnet\" dataset, formatting depends on the backend. if args.backend == \"openai-chat\": input_requests = dataset.sample( num_requests=args.num_prompts, input_len=args.sonnet_input_len, output_len=args.sonnet_output_len, prefix_len=args.sonnet_prefix_len, tokenizer=tokenizer, return_prompt_formatted=False, request_id_prefix=args.request_id_prefix, no_oversample=args.no_oversample, ) else: assert tokenizer.chat_template or tokenizer.default_chat_template, ( \"Tokenizer/model must have chat template for sonnet dataset.\" ) input_requests = dataset.sample( num_requests=args.num_prompts, input_len=args.sonnet_input_len, output_len=args.sonnet_output_len, prefix_len=args.sonnet_prefix_len, tokenizer=tokenizer, return_prompt_formatted=True, request_id_prefix=args.request_id_prefix, no_oversample=args.no_oversample, ) elif args.dataset_name == \"hf\": # all following datasets are implemented from the # HuggingFaceDataset base class hf_kwargs = {} if ( args.dataset_path in VisionArenaDataset.SUPPORTED_DATASET_PATHS or args.hf_name in VisionArenaDataset.SUPPORTED_DATASET_PATHS ): dataset_class = VisionArenaDataset args.hf_split = \"train\" args.hf_subset = None elif ( args.dataset_path in MMVUDataset.SUPPORTED_DATASET_PATHS or args.hf_name in MMVUDataset.SUPPORTED_DATASET_PATHS ): dataset_class = MMVUDataset args.hf_split = \"validation\" args.hf_subset = None elif ( args.dataset_path in InstructCoderDataset.SUPPORTED_DATASET_PATHS or args.hf_name in InstructCoderDataset.SUPPORTED_DATASET_PATHS ): dataset_class = InstructCoderDataset args.hf_split = \"train\" elif ( args.dataset_path in MTBenchDataset.SUPPORTED_DATASET_PATHS or args.hf_name in MTBenchDataset.SUPPORTED_DATASET_PATHS ): dataset_class = MTBenchDataset args.hf_split = \"train\" elif ( args.dataset_path in MultiModalConversationDataset.SUPPORTED_DATASET_PATHS or args.hf_name in MultiModalConversationDataset.SUPPORTED_DATASET_PATHS ): dataset_class = MultiModalConversationDataset elif ( args.dataset_path in ConversationDataset.SUPPORTED_DATASET_PATHS or args.hf_name in ConversationDataset.SUPPORTED_DATASET_PATHS ): dataset_class = ConversationDataset elif ( args.dataset_path in AIMODataset.SUPPORTED_DATASET_PATHS or args.hf_name in AIMODataset.SUPPORTED_DATASET_PATHS ): dataset_class = AIMODataset args.hf_split = \"train\" elif ( args.dataset_path in NextEditPredictionDataset.SUPPORTED_DATASET_PATHS # noqa: E501 or args.hf_name in NextEditPredictionDataset.SUPPORTED_DATASET_PATHS ): dataset_class = NextEditPredictionDataset args.hf_split = \"train\" elif ( args.dataset_path in ASRDataset.SUPPORTED_DATASET_PATHS or args.hf_name in ASRDataset.SUPPORTED_DATASET_PATHS ): dataset_class = ASRDataset args.hf_split = \"train\" elif args.dataset_path in BlazeditDataset.SUPPORTED_DATASET_PATHS: dataset_class = BlazeditDataset args.hf_split = \"train\" hf_kwargs = { \"min_distance\": args.blazedit_min_distance, \"max_distance\": args.blazedit_max_distance, } elif ( args.dataset_path in MLPerfDataset.SUPPORTED_DATASET_PATHS or args.hf_name in MLPerfDataset.SUPPORTED_DATASET_PATHS ): dataset_class = MLPerfDataset args.hf_split = \"train\" elif ( args.dataset_path in MMStarDataset.SUPPORTED_DATASET_PATHS or args.hf_name in MMStarDataset.SUPPORTED_DATASET_PATHS ): dataset_class = MMStarDataset args.hf_split = \"val\" args.hf_subset = None else: supported_datasets = set( [ dataset_name for cls in HuggingFaceDataset.__subclasses__() for dataset_name in cls.SUPPORTED_DATASET_PATHS ] ) raise ValueError( f\"Unsupported dataset path: {args.dataset_path}. \" \"Huggingface dataset only supports dataset_path\" f\" from one of following: {supported_datasets}. \" \"Please consider contributing if you would \" \"like to add support for additional dataset formats.\" ) if dataset_class.IS_MULTIMODAL and not ( args.backend in (\"openai-chat\", \"openai-audio\") or \"embeddings-\" in args.backend ): # multi-modal benchmark is only available on OpenAI Chat # endpoint-type. raise ValueError( \"Multi-modal content is only supported on 'openai-chat' and \" \"'openai-audio' backends.\" ) input_requests = dataset_class( dataset_path=args.dataset_path, dataset_subset=args.hf_subset, dataset_split=args.hf_split, random_seed=args.seed, no_stream=args.no_stream, hf_name=args.hf_name, disable_shuffle=args.disable_shuffle, ).sample( num_requests=args.num_prompts, tokenizer=tokenizer, output_len=args.hf_output_len, request_id_prefix=args.request_id_prefix, no_oversample=args.no_oversample, skip_chat_template=args.skip_chat_template, **hf_kwargs, ) else: # For datasets that follow a similar structure, use a mapping. dataset_mapping = { \"spec_bench\": lambda: SpecBench( dataset_path=args.dataset_path, category=args.spec_bench_category, disable_shuffle=args.disable_shuffle, ).sample( num_requests=args.num_prompts, tokenizer=tokenizer, output_len=args.spec_bench_output_len, request_id_prefix=args.request_id_prefix, no_oversample=args.no_oversample, ), \"sharegpt\": lambda: ShareGPTDataset( random_seed=args.seed, dataset_path=args.dataset_path, disable_shuffle=args.disable_shuffle, ).sample( tokenizer=tokenizer, num_requests=args.num_prompts, output_len=args.sharegpt_output_len, request_id_prefix=args.request_id_prefix, no_oversample=args.no_oversample, ), \"burstgpt\": lambda: BurstGPTDataset( random_seed=args.seed, dataset_path=args.dataset_path, disable_shuffle=args.disable_shuffle, ).sample( tokenizer=tokenizer, num_requests=args.num_prompts, request_id_prefix=args.request_id_prefix, no_oversample=args.no_oversample, ), \"random\": lambda: RandomDataset( random_seed=args.seed, dataset_path=args.dataset_path, disable_shuffle=args.disable_shuffle, ).sample( tokenizer=tokenizer, num_requests=args.num_prompts, prefix_len=args.random_prefix_len, input_len=args.random_input_len, output_len=args.random_output_len, range_ratio=args.random_range_ratio, request_id_prefix=args.request_id_prefix, batchsize=args.random_batch_size, no_oversample=args.no_oversample, ), \"random-mm\": lambda: RandomMultiModalDataset( random_seed=args.seed, dataset_path=args.dataset_path, disable_shuffle=args.disable_shuffle, ).sample( tokenizer=tokenizer, num_requests=args.num_prompts, prefix_len=args.random_prefix_len, range_ratio=args.random_range_ratio, input_len=args.random_input_len, output_len=args.random_output_len, base_items_per_request=args.random_mm_base_items_per_request, limit_mm_per_prompt=args.random_mm_limit_mm_per_prompt, num_mm_items_range_ratio=args.random_mm_num_mm_items_range_ratio, bucket_config=args.random_mm_bucket_config, request_id_prefix=args.request_id_prefix, no_oversample=args.no_oversample, ), \"random-rerank\": lambda: RandomDatasetForReranking( random_seed=args.seed, dataset_path=args.dataset_path, disable_shuffle=args.disable_shuffle, ).sample( tokenizer=tokenizer, num_requests=args.num_prompts, input_len=args.random_input_len, range_ratio=args.random_range_ratio, request_id_prefix=args.request_id_prefix, batchsize=args.random_batch_size, is_reranker=not args.no_reranker, ), \"prefix_repetition\": lambda: PrefixRepetitionRandomDataset( random_seed=args.seed, dataset_path=args.dataset_path, disable_shuffle=args.disable_shuffle, ).sample( tokenizer=tokenizer, num_requests=args.num_prompts, prefix_len=args.prefix_repetition_prefix_len, suffix_len=args.prefix_repetition_suffix_len, num_prefixes=args.prefix_repetition_num_prefixes, output_len=args.prefix_repetition_output_len, request_id_prefix=args.request_id_prefix, no_oversample=args.no_oversample, ), } try: # Enforce endpoint compatibility for multimodal datasets. if args.dataset_name == \"random-mm\" and args.backend not in [\"openai-chat\"]: raise ValueError( \"Multi-modal content (images) is only supported on \" \"'openai-chat' backend.\" ) input_requests = dataset_mapping[args.dataset_name]() except KeyError as err: raise ValueError(f\"Unknown dataset: {args.dataset_name}\") from err return input_requests is_valid_sequence ¶ is_valid_sequence( prompt_len: int, output_len: int, min_len: int = 4, max_prompt_len: int = 1024, max_total_len: int = 2048, skip_min_output_len_check: bool = False, ) -> bool Validate a sequence based on prompt and output lengths. Default pruning criteria are copied from the original sample_hf_requests and sample_sharegpt_requests functions in benchmark_serving.py, as well as from sample_requests in benchmark_throughput.py. Source code in vllm/benchmarks/datasets.py 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286def is_valid_sequence( prompt_len: int, output_len: int, min_len: int = 4, max_prompt_len: int = 1024, max_total_len: int = 2048, skip_min_output_len_check: bool = False, ) -> bool: \"\"\" Validate a sequence based on prompt and output lengths. Default pruning criteria are copied from the original `sample_hf_requests` and `sample_sharegpt_requests` functions in benchmark_serving.py, as well as from `sample_requests` in benchmark_throughput.py. \"\"\" # Check for invalid conditions prompt_too_short = prompt_len < min_len output_too_short = (not skip_min_output_len_check) and (output_len < min_len) prompt_too_long = prompt_len > max_prompt_len combined_too_long = (prompt_len + output_len) > max_total_len # Return True if none of the invalid conditions are met return not ( prompt_too_short or output_too_short or prompt_too_long or combined_too_long ) lora_path_on_disk cached ¶ lora_path_on_disk(lora_path: str) -> str Source code in vllm/benchmarks/datasets.py 289 290 291@cache def lora_path_on_disk(lora_path: str) -> str: return get_adapter_absolute_path(lora_path) process_image ¶ process_image(image: Any) -> Mapping[str, Any] Process a single image input and return a multimedia content dictionary. Supports the following input types: Dictionary with raw image bytes: - Expects a dict with a 'bytes' key containing raw image data. - Loads the bytes as a PIL.Image.Image. PIL.Image.Image input: - Converts the image to RGB. - Saves the image as a JPEG in memory. - Encodes the JPEG data as a base64 string. - Returns a dictionary with the image as a base64 data URL. String input: - Treats the string as a URL or local file path. - Prepends \"file://\" if the string doesn't start with \"http://\" or \"file://\". - Returns a dictionary with the image URL. Raises: Type Description ValueError If the input is not a supported type. Source code in vllm/benchmarks/datasets.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341def process_image(image: Any) -> Mapping[str, Any]: \"\"\" Process a single image input and return a multimedia content dictionary. Supports the following input types: 1. Dictionary with raw image bytes: - Expects a dict with a 'bytes' key containing raw image data. - Loads the bytes as a PIL.Image.Image. 2. PIL.Image.Image input: - Converts the image to RGB. - Saves the image as a JPEG in memory. - Encodes the JPEG data as a base64 string. - Returns a dictionary with the image as a base64 data URL. 3. String input: - Treats the string as a URL or local file path. - Prepends \"file://\" if the string doesn't start with \"http://\" or \"file://\". - Returns a dictionary with the image URL. Raises: ValueError: If the input is not a supported type. \"\"\" if isinstance(image, dict) and \"bytes\" in image: image = Image.open(BytesIO(image[\"bytes\"])) if isinstance(image, Image.Image): image = convert_image_mode(image, \"RGB\") with io.BytesIO() as image_data: image.save(image_data, format=\"JPEG\") image_base64 = base64.b64encode(image_data.getvalue()).decode(\"utf-8\") return { \"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_base64}\"}, } if isinstance(image, str): image_url = ( image if image.startswith((\"http://\", \"https://\", \"file://\")) else f\"file://{image}\" ) return {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}} raise ValueError( f\"Invalid image input {image}. Must be a PIL.Image.Image\" \" or str or dictionary with raw image bytes.\" ) process_video ¶ process_video(video: Any) -> Mapping[str, Any] Process a single video input and return a multimedia content dictionary. Supports the following input types: Dictionary with raw video bytes: - Expects a dict with a 'bytes' key containing raw video data. String input: - Treats the string as a URL or local file path. - Prepends \"file://\" if the string doesn't start with \"http://\" or \"file://\". - Returns a dictionary with the image URL. Raises: Type Description ValueError If the input is not a supported type. Source code in vllm/benchmarks/datasets.py 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378def process_video(video: Any) -> Mapping[str, Any]: \"\"\" Process a single video input and return a multimedia content dictionary. Supports the following input types: 1. Dictionary with raw video bytes: - Expects a dict with a 'bytes' key containing raw video data. 2. String input: - Treats the string as a URL or local file path. - Prepends \"file://\" if the string doesn't start with \"http://\" or \"file://\". - Returns a dictionary with the image URL. Raises: ValueError: If the input is not a supported type. \"\"\" if isinstance(video, dict) and \"bytes\" in video: video_bytes = video[\"bytes\"] video_base64 = base64.b64encode(video_bytes).decode(\"utf-8\") return { \"type\": \"video_url\", \"video_url\": {\"url\": f\"data:video/mp4;base64,{video_base64}\"}, } if isinstance(video, str): video_url = ( video if video.startswith((\"http://\", \"https://\", \"file://\")) else f\"file://{video}\" ) return {\"type\": \"video_url\", \"video_url\": {\"url\": video_url}} raise ValueError( f\"Invalid video input {video}. Must be a string of local path/remote url, or a dictionary with raw video bytes in the form of `{{'bytes': raw_video_bytes}}`.\" # noqa: E501 )",
      "code": ""
    },
    {
      "description": "datasets module-attribute ¶ datasets = PlaceholderModule('datasets') logger module-attribute ¶ logger = getLogger(__name__) lora_tokenizer_cache module-attribute ¶ lora_tokenizer_cache: dict[int, TokenizerLike] = {} zeta_prompt module-attribute ¶ zeta_prompt = \"### Instruction:\\nYou are a code completion assistant and your task is to analyze user edits and then rewrite an excerpt that the user provides, suggesting the appropriate edits within the excerpt, taking into account the cursor location.\\n\\n### User Edits:\\n\\n{}\\n\\n### User Excerpt:\\n\\n{}\\n\\n### Response:\\n\\n\" AIMODataset ¶ Bases: HuggingFaceDataset Dataset class for processing a AIMO dataset with reasoning questions. Source code in vllm/benchmarks/datasets.py 2736 2737 2738 2739 2740 2741 2742 2743 2744 2745 2746 2747 2748 2749 2750 2751 2752 2753 2754 2755 2756 2757 2758 2759 2760 2761 2762 2763 2764 2765 2766 2767 2768 2769 2770 2771 2772 2773 2774 2775 2776 2777 2778 2779 2780 2781 2782 2783 2784 2785 2786 2787 2788class AIMODataset(HuggingFaceDataset): \"\"\" Dataset class for processing a AIMO dataset with reasoning questions. \"\"\" SUPPORTED_DATASET_PATHS = { \"AI-MO/aimo-validation-aime\", \"AI-MO/NuminaMath-1.5\", \"AI-MO/NuminaMath-CoT\", } def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: sampled_requests = [] ind = 0 dynamic_output = output_len is None for item in self.data: if len(sampled_requests) >= num_requests: break prompt, completion = item[\"problem\"], item[\"solution\"] prompt_ids = tokenizer(prompt).input_ids completion_ids = tokenizer(completion).input_ids prompt_len = len(prompt_ids) completion_len = len(completion_ids) output_len = completion_len if dynamic_output else output_len assert isinstance(output_len, int) and output_len > 0 if dynamic_output and not is_valid_sequence( prompt_len, completion_len, max_prompt_len=2048, max_total_len=32000 ): continue sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=None, request_id=request_id_prefix + str(ind), ) ) ind += 1 self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = { \"AI-MO/aimo-validation-aime\", \"AI-MO/NuminaMath-1.5\", \"AI-MO/NuminaMath-CoT\", } sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 2747 2748 2749 2750 2751 2752 2753 2754 2755 2756 2757 2758 2759 2760 2761 2762 2763 2764 2765 2766 2767 2768 2769 2770 2771 2772 2773 2774 2775 2776 2777 2778 2779 2780 2781 2782 2783 2784 2785 2786 2787 2788def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: sampled_requests = [] ind = 0 dynamic_output = output_len is None for item in self.data: if len(sampled_requests) >= num_requests: break prompt, completion = item[\"problem\"], item[\"solution\"] prompt_ids = tokenizer(prompt).input_ids completion_ids = tokenizer(completion).input_ids prompt_len = len(prompt_ids) completion_len = len(completion_ids) output_len = completion_len if dynamic_output else output_len assert isinstance(output_len, int) and output_len > 0 if dynamic_output and not is_valid_sequence( prompt_len, completion_len, max_prompt_len=2048, max_total_len=32000 ): continue sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=None, request_id=request_id_prefix + str(ind), ) ) ind += 1 self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests ASRDataset ¶ Bases: HuggingFaceDataset Dataset class for processing a ASR dataset for transcription. Tested on the following set: +----------------+----------------------------------------+--------------------------+-----------------------------+ | Dataset | Domain | Speaking Style | hf-subset | +----------------+----------------------------------------+--------------------------+-----------------------------+ | TED-LIUM | TED talks | Oratory | release1, release2, release3| | | | | release3-speaker-adaptation | | VoxPopuli | European Parliament | Oratory | en, de, it, fr, ... | | LibriSpeech | Audiobook | Narrated | \"LIUM/tedlium\" | | GigaSpeech | Audiobook, podcast, YouTube | Narrated, spontaneous | xs, s, m, l, xl, dev, test | | SPGISpeech | Financial meetings | Oratory, spontaneous | S, M, L, dev, test | | AMI | Meetings | Spontaneous | ihm, sdm | +----------------+----------------------------------------+--------------------------+-----------------------------+ Source code in vllm/benchmarks/datasets.py 2894 2895 2896 2897 2898 2899 2900 2901 2902 2903 2904 2905 2906 2907 2908 2909 2910 2911 2912 2913 2914 2915 2916 2917 2918 2919 2920 2921 2922 2923 2924 2925 2926 2927 2928 2929 2930 2931 2932 2933 2934 2935 2936 2937 2938 2939 2940 2941 2942 2943 2944 2945 2946 2947 2948 2949 2950 2951 2952 2953 2954 2955 2956 2957 2958 2959 2960 2961 2962 2963 2964 2965 2966 2967 2968 2969 2970 2971 2972 2973 2974 2975 2976class ASRDataset(HuggingFaceDataset): \"\"\" Dataset class for processing a ASR dataset for transcription. Tested on the following set: +----------------+----------------------------------------+--------------------------+-----------------------------+ | Dataset | Domain | Speaking Style | hf-subset | +----------------+----------------------------------------+--------------------------+-----------------------------+ | TED-LIUM | TED talks | Oratory | release1, release2, release3| | | | | release3-speaker-adaptation | | VoxPopuli | European Parliament | Oratory | en, de, it, fr, ... | | LibriSpeech | Audiobook | Narrated | \"LIUM/tedlium\" | | GigaSpeech | Audiobook, podcast, YouTube | Narrated, spontaneous | xs, s, m, l, xl, dev, test | | SPGISpeech | Financial meetings | Oratory, spontaneous | S, M, L, dev, test | | AMI | Meetings | Spontaneous | ihm, sdm | +----------------+----------------------------------------+--------------------------+-----------------------------+ \"\"\" # noqa: E501 SUPPORTED_DATASET_PATHS = { \"openslr/librispeech_asr\", \"facebook/voxpopuli\", \"LIUM/tedlium\", \"edinburghcstr/ami\", \"speechcolab/gigaspeech\", \"kensho/spgispeech\", } DEFAULT_OUTPUT_LEN = 128 IS_MULTIMODAL = True # TODO Whisper-specific. Abstract interface when more models are supported. TRANSCRIPTION_PREAMBLE = \"<|startoftranscript|><|en|><|transcribe|><|notimestamps|>\" skip_long_audios: bool = True def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN prompt = ASRDataset.TRANSCRIPTION_PREAMBLE prompt_len = len(tokenizer(prompt).input_ids) sampled_requests = [] ind = 0 skipped = 0 for item in self.data: if len(sampled_requests) >= num_requests: break audio = item[\"audio\"] y, sr = audio[\"array\"], audio[\"sampling_rate\"] duration_s = librosa.get_duration(y=y, sr=sr) # Whisper max supported duration if self.skip_long_audios and duration_s > 30: skipped += 1 continue mm_content = {\"audio\": (y, sr)} sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_content, request_id=request_id_prefix + str(ind), ) ) ind += 1 if skipped: logger.warning( \"%d samples discarded from dataset due to\" \" their length being greater than\" \" what Whisper supports.\", skipped, ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶ DEFAULT_OUTPUT_LEN = 128 IS_MULTIMODAL class-attribute instance-attribute ¶ IS_MULTIMODAL = True SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = { \"openslr/librispeech_asr\", \"facebook/voxpopuli\", \"LIUM/tedlium\", \"edinburghcstr/ami\", \"speechcolab/gigaspeech\", \"kensho/spgispeech\", } TRANSCRIPTION_PREAMBLE class-attribute instance-attribute ¶ TRANSCRIPTION_PREAMBLE = \"<|startoftranscript|><|en|><|transcribe|><|notimestamps|>\" skip_long_audios class-attribute instance-attribute ¶ skip_long_audios: bool = True sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 2929 2930 2931 2932 2933 2934 2935 2936 2937 2938 2939 2940 2941 2942 2943 2944 2945 2946 2947 2948 2949 2950 2951 2952 2953 2954 2955 2956 2957 2958 2959 2960 2961 2962 2963 2964 2965 2966 2967 2968 2969 2970 2971 2972 2973 2974 2975 2976def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN prompt = ASRDataset.TRANSCRIPTION_PREAMBLE prompt_len = len(tokenizer(prompt).input_ids) sampled_requests = [] ind = 0 skipped = 0 for item in self.data: if len(sampled_requests) >= num_requests: break audio = item[\"audio\"] y, sr = audio[\"array\"], audio[\"sampling_rate\"] duration_s = librosa.get_duration(y=y, sr=sr) # Whisper max supported duration if self.skip_long_audios and duration_s > 30: skipped += 1 continue mm_content = {\"audio\": (y, sr)} sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_content, request_id=request_id_prefix + str(ind), ) ) ind += 1 if skipped: logger.warning( \"%d samples discarded from dataset due to\" \" their length being greater than\" \" what Whisper supports.\", skipped, ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests BenchmarkDataset ¶ Bases: ABC Source code in vllm/benchmarks/datasets.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254class BenchmarkDataset(ABC): DEFAULT_SEED = 0 IS_MULTIMODAL = False def __init__( self, dataset_path: str | None = None, random_seed: int = DEFAULT_SEED, disable_shuffle: bool = False, **kwargs, ) -> None: \"\"\" Initialize the BenchmarkDataset with an optional dataset path and random seed. Args: dataset_path (Optional[str]): Path to the dataset. If None, it indicates that a default or random dataset might be used. random_seed (int): Seed value for reproducible shuffling or sampling. Defaults to DEFAULT_SEED. \"\"\" self.dataset_path = dataset_path # Set the random seed, ensuring that a None value is replaced with the # default seed. self.random_seed = random_seed if random_seed is not None else self.DEFAULT_SEED self.disable_shuffle = disable_shuffle self.data = None def apply_multimodal_chat_transformation( self, prompt: str, mm_content: MultiModalDataDict | dict | list[dict] | None = None, ) -> list[dict]: \"\"\" Transform a prompt and optional multimodal content into a chat format. This method is used for chat models that expect a specific conversation format. \"\"\" content = [{\"text\": prompt, \"type\": \"text\"}] if mm_content is not None: if isinstance(mm_content, list): content.extend(cast(list[dict[str, Any]], mm_content)) elif isinstance(mm_content, dict): content.append(mm_content) else: raise TypeError( \"Could not process multimodal content of type: \" + f\"{type(mm_content)}\" ) return [{\"role\": \"user\", \"content\": content}] def load_data(self) -> None: \"\"\" Load data from the dataset path into self.data. This method must be overridden by subclasses since the method to load data will vary depending on the dataset format and source. Raises: NotImplementedError: If a subclass does not implement this method. \"\"\" # TODO (jenniferzhao): add support for downloading data raise NotImplementedError(\"load_data must be implemented in subclasses.\") def get_random_lora_request( self, max_loras: int | None = None, lora_path: str | None = None, ) -> LoRARequest | None: \"\"\" Optionally select a random LoRA request. This method is used when LoRA parameters are provided. It randomly selects a LoRA based on max_loras. Args: max_loras (Optional[int]): The maximum number of LoRAs available. If `None`, LoRA is not used. lora_path (Optional[str]): Path to the LoRA parameters on disk. If `None`, LoRA is not used. Returns: A new [`LoRARequest`][vllm.lora.request.LoRARequest] (or `None` if not applicable). \"\"\" if max_loras is None or lora_path is None: return None # Generate a random LoRA ID in the range [1, max_loras]. lora_id = random.randint(1, max_loras) lora_request = LoRARequest( lora_name=str(lora_id), lora_int_id=lora_id, lora_path=lora_path_on_disk(lora_path), ) return lora_request @abstractmethod def sample( self, tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, ) -> list[SampleRequest]: \"\"\" Abstract method to generate sample requests from the dataset. Subclasses must override this method to implement dataset-specific logic for generating a list of SampleRequest objects. Args: tokenizer (TokenizerLike): The tokenizer to be used for processing the dataset's text. num_requests (int): The number of sample requests to generate. request_id_prefix (str): The prefix of request_id. Returns: list[SampleRequest]: A list of sample requests generated from the dataset. \"\"\" raise NotImplementedError(\"sample must be implemented in subclasses.\") def maybe_oversample_requests( self, requests: list[SampleRequest], num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, ) -> None: \"\"\" Oversamples the list of requests if its size is less than the desired number. Args: requests (List[SampleRequest]): The current list of sampled requests. num_requests (int): The target number of requests. request_id_prefix (str): The prefix applied to generated request identifiers. \"\"\" if no_oversample: logger.info(\"Skipping oversampling. Total samples: %d.\", len(requests)) return if len(requests) < num_requests: random.seed(self.random_seed) needed = num_requests - len(requests) additional = [] for i in range(needed): req = deepcopy(random.choice(requests)) req.request_id = request_id_prefix + str(len(requests) + i) additional.append(req) requests.extend(additional) logger.info(\"Oversampled requests to reach %d total samples.\", num_requests) ids = [req.request_id for req in requests] if len(ids) != len(set(ids)): raise ValueError( \"Duplicate request_id found in the sampled \" \"requests. Please ensure that each request_id \" \"is unique.\" ) DEFAULT_SEED class-attribute instance-attribute ¶ DEFAULT_SEED = 0 IS_MULTIMODAL class-attribute instance-attribute ¶ IS_MULTIMODAL = False data instance-attribute ¶ data = None dataset_path instance-attribute ¶ dataset_path = dataset_path disable_shuffle instance-attribute ¶ disable_shuffle = disable_shuffle random_seed instance-attribute ¶ random_seed = ( random_seed if random_seed is not None else DEFAULT_SEED ) __init__ ¶ __init__( dataset_path: str | None = None, random_seed: int = DEFAULT_SEED, disable_shuffle: bool = False, **kwargs, ) -> None Initialize the BenchmarkDataset with an optional dataset path and random seed. Parameters: Name Type Description Default dataset_path Optional[str] Path to the dataset. If None, it indicates that a default or random dataset might be used. None random_seed int Seed value for reproducible shuffling or sampling. Defaults to DEFAULT_SEED. DEFAULT_SEED Source code in vllm/benchmarks/datasets.py 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117def __init__( self, dataset_path: str | None = None, random_seed: int = DEFAULT_SEED, disable_shuffle: bool = False, **kwargs, ) -> None: \"\"\" Initialize the BenchmarkDataset with an optional dataset path and random seed. Args: dataset_path (Optional[str]): Path to the dataset. If None, it indicates that a default or random dataset might be used. random_seed (int): Seed value for reproducible shuffling or sampling. Defaults to DEFAULT_SEED. \"\"\" self.dataset_path = dataset_path # Set the random seed, ensuring that a None value is replaced with the # default seed. self.random_seed = random_seed if random_seed is not None else self.DEFAULT_SEED self.disable_shuffle = disable_shuffle self.data = None apply_multimodal_chat_transformation ¶ apply_multimodal_chat_transformation( prompt: str, mm_content: MultiModalDataDict | dict | list[dict] | None = None, ) -> list[dict] Transform a prompt and optional multimodal content into a chat format. This method is used for chat models that expect a specific conversation format. Source code in vllm/benchmarks/datasets.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140def apply_multimodal_chat_transformation( self, prompt: str, mm_content: MultiModalDataDict | dict | list[dict] | None = None, ) -> list[dict]: \"\"\" Transform a prompt and optional multimodal content into a chat format. This method is used for chat models that expect a specific conversation format. \"\"\" content = [{\"text\": prompt, \"type\": \"text\"}] if mm_content is not None: if isinstance(mm_content, list): content.extend(cast(list[dict[str, Any]], mm_content)) elif isinstance(mm_content, dict): content.append(mm_content) else: raise TypeError( \"Could not process multimodal content of type: \" + f\"{type(mm_content)}\" ) return [{\"role\": \"user\", \"content\": content}] get_random_lora_request ¶ get_random_lora_request( max_loras: int | None = None, lora_path: str | None = None, ) -> LoRARequest | None Optionally select a random LoRA request. This method is used when LoRA parameters are provided. It randomly selects a LoRA based on max_loras. Parameters: Name Type Description Default max_loras Optional[int] The maximum number of LoRAs available. If None, LoRA is not used. None lora_path Optional[str] Path to the LoRA parameters on disk. If None, LoRA is not used. None Returns: Type Description LoRARequest | None A new LoRARequest LoRARequest | None (or None if not applicable). Source code in vllm/benchmarks/datasets.py 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186def get_random_lora_request( self, max_loras: int | None = None, lora_path: str | None = None, ) -> LoRARequest | None: \"\"\" Optionally select a random LoRA request. This method is used when LoRA parameters are provided. It randomly selects a LoRA based on max_loras. Args: max_loras (Optional[int]): The maximum number of LoRAs available. If `None`, LoRA is not used. lora_path (Optional[str]): Path to the LoRA parameters on disk. If `None`, LoRA is not used. Returns: A new [`LoRARequest`][vllm.lora.request.LoRARequest] (or `None` if not applicable). \"\"\" if max_loras is None or lora_path is None: return None # Generate a random LoRA ID in the range [1, max_loras]. lora_id = random.randint(1, max_loras) lora_request = LoRARequest( lora_name=str(lora_id), lora_int_id=lora_id, lora_path=lora_path_on_disk(lora_path), ) return lora_request load_data ¶ load_data() -> None Load data from the dataset path into self.data. This method must be overridden by subclasses since the method to load data will vary depending on the dataset format and source. Raises: Type Description NotImplementedError If a subclass does not implement this method. Source code in vllm/benchmarks/datasets.py 142 143 144 145 146 147 148 149 150 151 152 153def load_data(self) -> None: \"\"\" Load data from the dataset path into self.data. This method must be overridden by subclasses since the method to load data will vary depending on the dataset format and source. Raises: NotImplementedError: If a subclass does not implement this method. \"\"\" # TODO (jenniferzhao): add support for downloading data raise NotImplementedError(\"load_data must be implemented in subclasses.\") maybe_oversample_requests ¶ maybe_oversample_requests( requests: list[SampleRequest], num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, ) -> None Oversamples the list of requests if its size is less than the desired number. Parameters: Name Type Description Default requests List[SampleRequest] The current list of sampled requests. required num_requests int The target number of requests. required request_id_prefix str The prefix applied to generated request identifiers. '' Source code in vllm/benchmarks/datasets.py 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254def maybe_oversample_requests( self, requests: list[SampleRequest], num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, ) -> None: \"\"\" Oversamples the list of requests if its size is less than the desired number. Args: requests (List[SampleRequest]): The current list of sampled requests. num_requests (int): The target number of requests. request_id_prefix (str): The prefix applied to generated request identifiers. \"\"\" if no_oversample: logger.info(\"Skipping oversampling. Total samples: %d.\", len(requests)) return if len(requests) < num_requests: random.seed(self.random_seed) needed = num_requests - len(requests) additional = [] for i in range(needed): req = deepcopy(random.choice(requests)) req.request_id = request_id_prefix + str(len(requests) + i) additional.append(req) requests.extend(additional) logger.info(\"Oversampled requests to reach %d total samples.\", num_requests) ids = [req.request_id for req in requests] if len(ids) != len(set(ids)): raise ValueError( \"Duplicate request_id found in the sampled \" \"requests. Please ensure that each request_id \" \"is unique.\" ) sample abstractmethod ¶ sample( tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, ) -> list[SampleRequest] Abstract method to generate sample requests from the dataset. Subclasses must override this method to implement dataset-specific logic for generating a list of SampleRequest objects. Parameters: Name Type Description Default tokenizer TokenizerLike The tokenizer to be used for processing the dataset's text. required num_requests int The number of sample requests to generate. required request_id_prefix str The prefix of request_id. '' Returns: Type Description list[SampleRequest] list[SampleRequest]: A list of sample requests generated from the list[SampleRequest] dataset. Source code in vllm/benchmarks/datasets.py 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212@abstractmethod def sample( self, tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, ) -> list[SampleRequest]: \"\"\" Abstract method to generate sample requests from the dataset. Subclasses must override this method to implement dataset-specific logic for generating a list of SampleRequest objects. Args: tokenizer (TokenizerLike): The tokenizer to be used for processing the dataset's text. num_requests (int): The number of sample requests to generate. request_id_prefix (str): The prefix of request_id. Returns: list[SampleRequest]: A list of sample requests generated from the dataset. \"\"\" raise NotImplementedError(\"sample must be implemented in subclasses.\") BlazeditDataset ¶ Bases: HuggingFaceDataset Blazedit Dataset. https://github.com/ise-uiuc/blazedit 5k char version: vdaita/edit_5k_char 10k char version: vdaita/edit_10k_char Source code in vllm/benchmarks/datasets.py 2647 2648 2649 2650 2651 2652 2653 2654 2655 2656 2657 2658 2659 2660 2661 2662 2663 2664 2665 2666 2667 2668 2669 2670 2671 2672 2673 2674 2675 2676 2677 2678 2679 2680 2681 2682 2683 2684 2685 2686 2687 2688 2689 2690 2691 2692 2693 2694 2695 2696 2697 2698 2699 2700 2701 2702 2703 2704 2705 2706 2707 2708 2709 2710 2711 2712 2713 2714 2715 2716 2717 2718 2719 2720 2721 2722 2723 2724 2725 2726 2727 2728class BlazeditDataset(HuggingFaceDataset): \"\"\" Blazedit Dataset. https://github.com/ise-uiuc/blazedit 5k char version: vdaita/edit_5k_char 10k char version: vdaita/edit_10k_char \"\"\" # noqa: E501 # 5k char version will have output as ~5k chars # 10k char version will have output as ~10k chars # Assuming 3 char per token, 10k chars will be 3333 tokens # We set default to 4000 to be safe DEFAULT_OUTPUT_LEN = 4000 SUPPORTED_DATASET_PATHS = { \"vdaita/edit_5k_char\", \"vdaita/edit_10k_char\", } def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, min_distance: float = 0.0, max_distance: float = 1.0, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break code = item[\"code\"] change_request = item[\"change_request\"] norm_distance = item[\"norm_distance\"] # compare the levenshtein distance normalized by code length if norm_distance < min_distance or norm_distance > max_distance: continue # template copied from # https://github.com/ise-uiuc/blazedit/blob/7765137e656fd62de877422d2e4cf8de51228054/dataset/create_refined_dataset.py#L94-L105 # noqa: E501 prompt = f\"\"\"Given a code file, please apply the change requests and generate the new file. Original file: ```python {code} ``` Change request: {change_request} Please generate the new code file in the \"New file\" section below.\"\"\" # noqa: E501 # apply template if not skip_chat_template: prompt = tokenizer.apply_chat_template( [{\"role\": \"user\", \"content\": prompt}], add_generation_prompt=True, tokenize=False, ) prompt_len = len(tokenizer(prompt).input_ids) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶ DEFAULT_OUTPUT_LEN = 4000 SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = { \"vdaita/edit_5k_char\", \"vdaita/edit_10k_char\", } sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, min_distance: float = 0.0, max_distance: float = 1.0, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 2666 2667 2668 2669 2670 2671 2672 2673 2674 2675 2676 2677 2678 2679 2680 2681 2682 2683 2684 2685 2686 2687 2688 2689 2690 2691 2692 2693 2694 2695 2696 2697 2698 2699 2700 2701 2702 2703 2704 2705 2706 2707 2708 2709 2710 2711 2712 2713 2714 2715 2716 2717 2718 2719 2720 2721 2722 2723 2724 2725 2726 2727 2728 def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, min_distance: float = 0.0, max_distance: float = 1.0, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break code = item[\"code\"] change_request = item[\"change_request\"] norm_distance = item[\"norm_distance\"] # compare the levenshtein distance normalized by code length if norm_distance < min_distance or norm_distance > max_distance: continue # template copied from # https://github.com/ise-uiuc/blazedit/blob/7765137e656fd62de877422d2e4cf8de51228054/dataset/create_refined_dataset.py#L94-L105 # noqa: E501 prompt = f\"\"\"Given a code file, please apply the change requests and generate the new file. Original file: ```python {code} ``` Change request: {change_request} Please generate the new code file in the \"New file\" section below.\"\"\" # noqa: E501 # apply template if not skip_chat_template: prompt = tokenizer.apply_chat_template( [{\"role\": \"user\", \"content\": prompt}], add_generation_prompt=True, tokenize=False, ) prompt_len = len(tokenizer(prompt).input_ids) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests BurstGPTDataset ¶ Bases: BenchmarkDataset Implements the BurstGPT dataset. Loads data from a CSV file and generates sample requests based on synthetic prompt generation. Only rows with Model \"GPT-4\" and positive response tokens are used. Source code in vllm/benchmarks/datasets.py 2170 2171 2172 2173 2174 2175 2176 2177 2178 2179 2180 2181 2182 2183 2184 2185 2186 2187 2188 2189 2190 2191 2192 2193 2194 2195 2196 2197 2198 2199 2200 2201 2202 2203 2204 2205 2206 2207 2208 2209 2210 2211 2212 2213 2214 2215 2216 2217 2218 2219 2220 2221 2222 2223 2224 2225 2226 2227 2228 2229 2230 2231 2232 2233 2234 2235 2236 2237 2238 2239class BurstGPTDataset(BenchmarkDataset): \"\"\" Implements the BurstGPT dataset. Loads data from a CSV file and generates sample requests based on synthetic prompt generation. Only rows with Model \"GPT-4\" and positive response tokens are used. \"\"\" def __init__(self, **kwargs) -> None: super().__init__(**kwargs) self.load_data() def load_data( self, ): if self.dataset_path is None: raise ValueError(\"dataset_path must be provided for loading data.\") df = pd.read_csv(self.dataset_path) # Filter to keep only GPT-4 rows. gpt4_df = df[df[\"Model\"] == \"GPT-4\"] # Remove failed requests (where Response tokens is 0 or less). gpt4_df = gpt4_df[gpt4_df[\"Response tokens\"] > 0] # Sample the desired number of rows. self.data = gpt4_df def _sample_loaded_data(self, num_requests: int) -> list: if num_requests <= len(self.data): data = self.data.sample(n=num_requests, random_state=self.random_seed) else: data = self.data.sample( n=num_requests, random_state=self.random_seed, replace=True, ) # Convert the dataframe to a list of lists. return data.values.tolist() def sample( self, tokenizer: TokenizerLike, num_requests: int, max_loras: int | None = None, lora_path: str | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest]: samples = [] data = self._sample_loaded_data(num_requests=num_requests) for i in range(num_requests): input_len = int(data[i][2]) output_len = int(data[i][3]) lora_req = self.get_random_lora_request( max_loras=max_loras, lora_path=lora_path ) vocab_size = tokenizer.vocab_size # Generate a synthetic prompt: a list of token IDs computed as (i + # j) modulo vocab_size. token_ids = [(i + j) % vocab_size for j in range(input_len)] prompt = tokenizer.decode(token_ids) samples.append( SampleRequest( prompt=prompt, prompt_len=input_len, expected_output_len=output_len, lora_request=lora_req, request_id=request_id_prefix + str(i), ) ) return samples __init__ ¶ __init__(**kwargs) -> None Source code in vllm/benchmarks/datasets.py 2177 2178 2179def __init__(self, **kwargs) -> None: super().__init__(**kwargs) self.load_data() _sample_loaded_data ¶ _sample_loaded_data(num_requests: int) -> list Source code in vllm/benchmarks/datasets.py 2195 2196 2197 2198 2199 2200 2201 2202 2203 2204 2205def _sample_loaded_data(self, num_requests: int) -> list: if num_requests <= len(self.data): data = self.data.sample(n=num_requests, random_state=self.random_seed) else: data = self.data.sample( n=num_requests, random_state=self.random_seed, replace=True, ) # Convert the dataframe to a list of lists. return data.values.tolist() load_data ¶ load_data() Source code in vllm/benchmarks/datasets.py 2181 2182 2183 2184 2185 2186 2187 2188 2189 2190 2191 2192 2193def load_data( self, ): if self.dataset_path is None: raise ValueError(\"dataset_path must be provided for loading data.\") df = pd.read_csv(self.dataset_path) # Filter to keep only GPT-4 rows. gpt4_df = df[df[\"Model\"] == \"GPT-4\"] # Remove failed requests (where Response tokens is 0 or less). gpt4_df = gpt4_df[gpt4_df[\"Response tokens\"] > 0] # Sample the desired number of rows. self.data = gpt4_df sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, max_loras: int | None = None, lora_path: str | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest] Source code in vllm/benchmarks/datasets.py 2207 2208 2209 2210 2211 2212 2213 2214 2215 2216 2217 2218 2219 2220 2221 2222 2223 2224 2225 2226 2227 2228 2229 2230 2231 2232 2233 2234 2235 2236 2237 2238 2239def sample( self, tokenizer: TokenizerLike, num_requests: int, max_loras: int | None = None, lora_path: str | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest]: samples = [] data = self._sample_loaded_data(num_requests=num_requests) for i in range(num_requests): input_len = int(data[i][2]) output_len = int(data[i][3]) lora_req = self.get_random_lora_request( max_loras=max_loras, lora_path=lora_path ) vocab_size = tokenizer.vocab_size # Generate a synthetic prompt: a list of token IDs computed as (i + # j) modulo vocab_size. token_ids = [(i + j) % vocab_size for j in range(input_len)] prompt = tokenizer.decode(token_ids) samples.append( SampleRequest( prompt=prompt, prompt_len=input_len, expected_output_len=output_len, lora_request=lora_req, request_id=request_id_prefix + str(i), ) ) return samples ConversationDataset ¶ Bases: HuggingFaceDataset Dataset for text-only conversation data. Source code in vllm/benchmarks/datasets.py 2284 2285 2286 2287 2288 2289 2290 2291 2292 2293 2294 2295 2296 2297 2298 2299 2300 2301 2302 2303 2304 2305 2306 2307 2308 2309 2310 2311 2312 2313 2314 2315 2316 2317 2318 2319 2320 2321 2322 2323 2324 2325 2326 2327 2328 2329 2330 2331 2332 2333 2334 2335 2336 2337 2338 2339 2340 2341class ConversationDataset(HuggingFaceDataset): \"\"\"Dataset for text-only conversation data.\"\"\" SUPPORTED_DATASET_PATHS = { \"Aeala/ShareGPT_Vicuna_unfiltered\", } IS_MULTIMODAL = False def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: # Filter examples with at least 2 conversations filtered_data = self.data.filter(lambda x: len(x[\"conversations\"]) >= 2) sampled_requests = [] ind = 0 dynamic_output = output_len is None for item in filtered_data: if len(sampled_requests) >= num_requests: break conv = item[\"conversations\"] prompt, completion = conv[0][\"value\"], conv[1][\"value\"] prompt_ids = tokenizer(prompt).input_ids completion_ids = tokenizer(completion).input_ids prompt_len = len(prompt_ids) completion_len = len(completion_ids) output_len = completion_len if dynamic_output else output_len assert isinstance(output_len, int) and output_len > 0 if dynamic_output and not is_valid_sequence(prompt_len, completion_len): continue mm_content = process_image(item[\"image\"]) if \"image\" in item else None if enable_multimodal_chat: # Note: when chat is enabled the request prompt_len is no longer # accurate and we will be using request output to count the # actual prompt len and output len prompt = self.apply_multimodal_chat_transformation(prompt, mm_content) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_content, request_id=request_id_prefix + str(ind), ) ) ind += 1 self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests IS_MULTIMODAL class-attribute instance-attribute ¶ IS_MULTIMODAL = False SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = { \"Aeala/ShareGPT_Vicuna_unfiltered\" } sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 2292 2293 2294 2295 2296 2297 2298 2299 2300 2301 2302 2303 2304 2305 2306 2307 2308 2309 2310 2311 2312 2313 2314 2315 2316 2317 2318 2319 2320 2321 2322 2323 2324 2325 2326 2327 2328 2329 2330 2331 2332 2333 2334 2335 2336 2337 2338 2339 2340 2341def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: # Filter examples with at least 2 conversations filtered_data = self.data.filter(lambda x: len(x[\"conversations\"]) >= 2) sampled_requests = [] ind = 0 dynamic_output = output_len is None for item in filtered_data: if len(sampled_requests) >= num_requests: break conv = item[\"conversations\"] prompt, completion = conv[0][\"value\"], conv[1][\"value\"] prompt_ids = tokenizer(prompt).input_ids completion_ids = tokenizer(completion).input_ids prompt_len = len(prompt_ids) completion_len = len(completion_ids) output_len = completion_len if dynamic_output else output_len assert isinstance(output_len, int) and output_len > 0 if dynamic_output and not is_valid_sequence(prompt_len, completion_len): continue mm_content = process_image(item[\"image\"]) if \"image\" in item else None if enable_multimodal_chat: # Note: when chat is enabled the request prompt_len is no longer # accurate and we will be using request output to count the # actual prompt len and output len prompt = self.apply_multimodal_chat_transformation(prompt, mm_content) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_content, request_id=request_id_prefix + str(ind), ) ) ind += 1 self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests CustomDataset ¶ Bases: BenchmarkDataset Implements the Custom dataset. Loads data from a JSONL file and generates sample requests based on conversation turns. E.g., {\"prompt\": \"What is the capital of India?\"} {\"prompt\": \"What is the capital of Iran?\"} {\"prompt\": \"What is the capital of China?\"} Source code in vllm/benchmarks/datasets.py 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 1937 1938 1939 1940 1941 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025 2026class CustomDataset(BenchmarkDataset): \"\"\" Implements the Custom dataset. Loads data from a JSONL file and generates sample requests based on conversation turns. E.g., ``` {\"prompt\": \"What is the capital of India?\"} {\"prompt\": \"What is the capital of Iran?\"} {\"prompt\": \"What is the capital of China?\"} ``` \"\"\" def __init__(self, **kwargs) -> None: super().__init__(**kwargs) self.load_data() def load_data(self) -> None: if self.dataset_path is None: raise ValueError(\"dataset_path must be provided for loading data.\") # self.data will be a list of dictionaries # e.g., [{\"prompt\": \"What is the capital of India?\"}, ...] # This will be the standardized format which load_data() # has to convert into depending on the filetype of dataset_path. # sample() will assume this standardized format of self.data self.data = [] # Load the JSONL file if self.dataset_path.endswith(\".jsonl\"): jsonl_data = pd.read_json(path_or_buf=self.dataset_path, lines=True) # check if the JSONL file has a 'prompt' column if \"prompt\" not in jsonl_data.columns: raise ValueError(\"JSONL file must contain a 'prompt' column.\") # Convert each row to a dictionary and append to self.data # This will convert the DataFrame to a list of dictionaries # where each dictionary corresponds to a row in the DataFrame. # This is the standardized format we want for self.data for _, row in jsonl_data.iterrows(): self.data.append(row.to_dict()) else: raise NotImplementedError( \"Only JSONL format is supported for CustomDataset.\" ) random.seed(self.random_seed) if not getattr(self, \"disable_shuffle\", False): random.shuffle(self.data) def sample( self, tokenizer: TokenizerLike, num_requests: int, lora_path: str | None = None, max_loras: int | None = None, output_len: int | None = None, enable_multimodal_chat: bool = False, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: # load all data if needed self.num_available_samples = len(self.data) if num_requests <= 0: num_requests = self.num_available_samples logger.info( \"num_requests is set to 0 or negative, \" \"so using all available samples: %d\", num_requests, ) sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break prompt = item[\"prompt\"] # apply template if not skip_chat_template: prompt = tokenizer.apply_chat_template( [{\"role\": \"user\", \"content\": prompt}], add_generation_prompt=True, tokenize=False, ) prompt_len = len(tokenizer(prompt).input_ids) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests __init__ ¶ __init__(**kwargs) -> None Source code in vllm/benchmarks/datasets.py 1938 1939 1940def __init__(self, **kwargs) -> None: super().__init__(**kwargs) self.load_data() load_data ¶ load_data() -> None Source code in vllm/benchmarks/datasets.py 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974def load_data(self) -> None: if self.dataset_path is None: raise ValueError(\"dataset_path must be provided for loading data.\") # self.data will be a list of dictionaries # e.g., [{\"prompt\": \"What is the capital of India?\"}, ...] # This will be the standardized format which load_data() # has to convert into depending on the filetype of dataset_path. # sample() will assume this standardized format of self.data self.data = [] # Load the JSONL file if self.dataset_path.endswith(\".jsonl\"): jsonl_data = pd.read_json(path_or_buf=self.dataset_path, lines=True) # check if the JSONL file has a 'prompt' column if \"prompt\" not in jsonl_data.columns: raise ValueError(\"JSONL file must contain a 'prompt' column.\") # Convert each row to a dictionary and append to self.data # This will convert the DataFrame to a list of dictionaries # where each dictionary corresponds to a row in the DataFrame. # This is the standardized format we want for self.data for _, row in jsonl_data.iterrows(): self.data.append(row.to_dict()) else: raise NotImplementedError( \"Only JSONL format is supported for CustomDataset.\" ) random.seed(self.random_seed) if not getattr(self, \"disable_shuffle\", False): random.shuffle(self.data) sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, lora_path: str | None = None, max_loras: int | None = None, output_len: int | None = None, enable_multimodal_chat: bool = False, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 1976 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025 2026def sample( self, tokenizer: TokenizerLike, num_requests: int, lora_path: str | None = None, max_loras: int | None = None, output_len: int | None = None, enable_multimodal_chat: bool = False, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: # load all data if needed self.num_available_samples = len(self.data) if num_requests <= 0: num_requests = self.num_available_samples logger.info( \"num_requests is set to 0 or negative, \" \"so using all available samples: %d\", num_requests, ) sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break prompt = item[\"prompt\"] # apply template if not skip_chat_template: prompt = tokenizer.apply_chat_template( [{\"role\": \"user\", \"content\": prompt}], add_generation_prompt=True, tokenize=False, ) prompt_len = len(tokenizer(prompt).input_ids) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests HuggingFaceDataset ¶ Bases: BenchmarkDataset Base class for datasets hosted on HuggingFace. Source code in vllm/benchmarks/datasets.py 2245 2246 2247 2248 2249 2250 2251 2252 2253 2254 2255 2256 2257 2258 2259 2260 2261 2262 2263 2264 2265 2266 2267 2268 2269 2270 2271 2272 2273 2274 2275 2276class HuggingFaceDataset(BenchmarkDataset): \"\"\"Base class for datasets hosted on HuggingFace.\"\"\" SUPPORTED_DATASET_PATHS: set[str] | dict[str, Callable] = set() def __init__( self, dataset_path: str, dataset_split: str, no_stream: bool = False, dataset_subset: str | None = None, hf_name: str | None = None, **kwargs, ) -> None: super().__init__(dataset_path=dataset_path, **kwargs) self.dataset_split = dataset_split self.dataset_subset = dataset_subset self.load_stream = not no_stream self.hf_name = hf_name or dataset_path self.load_data() def load_data(self) -> None: \"\"\"Load data from HuggingFace datasets.\"\"\" self.data = load_dataset( self.dataset_path, name=self.dataset_subset, split=self.dataset_split, streaming=self.load_stream, ) if not getattr(self, \"disable_shuffle\", False): self.data = self.data.shuffle(seed=self.random_seed) SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS: set[str] | dict[str, Callable] = ( set() ) dataset_split instance-attribute ¶ dataset_split = dataset_split dataset_subset instance-attribute ¶ dataset_subset = dataset_subset hf_name instance-attribute ¶ hf_name = hf_name or dataset_path load_stream instance-attribute ¶ load_stream = not no_stream __init__ ¶ __init__( dataset_path: str, dataset_split: str, no_stream: bool = False, dataset_subset: str | None = None, hf_name: str | None = None, **kwargs, ) -> None Source code in vllm/benchmarks/datasets.py 2250 2251 2252 2253 2254 2255 2256 2257 2258 2259 2260 2261 2262 2263 2264 2265def __init__( self, dataset_path: str, dataset_split: str, no_stream: bool = False, dataset_subset: str | None = None, hf_name: str | None = None, **kwargs, ) -> None: super().__init__(dataset_path=dataset_path, **kwargs) self.dataset_split = dataset_split self.dataset_subset = dataset_subset self.load_stream = not no_stream self.hf_name = hf_name or dataset_path self.load_data() load_data ¶ load_data() -> None Load data from HuggingFace datasets. Source code in vllm/benchmarks/datasets.py 2267 2268 2269 2270 2271 2272 2273 2274 2275 2276def load_data(self) -> None: \"\"\"Load data from HuggingFace datasets.\"\"\" self.data = load_dataset( self.dataset_path, name=self.dataset_subset, split=self.dataset_split, streaming=self.load_stream, ) if not getattr(self, \"disable_shuffle\", False): self.data = self.data.shuffle(seed=self.random_seed) InstructCoderDataset ¶ Bases: HuggingFaceDataset InstructCoder Dataset. https://huggingface.co/datasets/likaixin/InstructCoder InstructCoder is the dataset designed for general code editing. It consists of 114,239 instruction-input-output triplets, and covers multiple distinct code editing scenario. Source code in vllm/benchmarks/datasets.py 2521 2522 2523 2524 2525 2526 2527 2528 2529 2530 2531 2532 2533 2534 2535 2536 2537 2538 2539 2540 2541 2542 2543 2544 2545 2546 2547 2548 2549 2550 2551 2552 2553 2554 2555 2556 2557 2558 2559 2560 2561 2562 2563 2564 2565 2566 2567 2568 2569 2570 2571 2572 2573 2574 2575 2576 2577class InstructCoderDataset(HuggingFaceDataset): \"\"\" InstructCoder Dataset. https://huggingface.co/datasets/likaixin/InstructCoder InstructCoder is the dataset designed for general code editing. It consists of 114,239 instruction-input-output triplets, and covers multiple distinct code editing scenario. \"\"\" DEFAULT_OUTPUT_LEN = 200 # this is the average default output length SUPPORTED_DATASET_PATHS = { \"likaixin/InstructCoder\", } def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break prompt = ( f\"{item['input']}\\n\\n{item['instruction']} Just output \" \"the code, do not include any explanation.\" ) # apply template if not skip_chat_template: prompt = tokenizer.apply_chat_template( [{\"role\": \"user\", \"content\": prompt}], add_generation_prompt=True, tokenize=False, ) prompt_len = len(tokenizer(prompt).input_ids) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶ DEFAULT_OUTPUT_LEN = 200 SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = {'likaixin/InstructCoder'} sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 2536 2537 2538 2539 2540 2541 2542 2543 2544 2545 2546 2547 2548 2549 2550 2551 2552 2553 2554 2555 2556 2557 2558 2559 2560 2561 2562 2563 2564 2565 2566 2567 2568 2569 2570 2571 2572 2573 2574 2575 2576 2577def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break prompt = ( f\"{item['input']}\\n\\n{item['instruction']} Just output \" \"the code, do not include any explanation.\" ) # apply template if not skip_chat_template: prompt = tokenizer.apply_chat_template( [{\"role\": \"user\", \"content\": prompt}], add_generation_prompt=True, tokenize=False, ) prompt_len = len(tokenizer(prompt).input_ids) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests MLPerfDataset ¶ Bases: HuggingFaceDataset MLPerf Inference Dataset. Dataset on HF: https://huggingface.co/datasets/mgoin/mlperf-inference-llama2-data https://huggingface.co/datasets/mgoin/mlperf-inference-llama3.1-data Each record contains \"system_prompt\": system role instruction. \"question\": user question. \"output\": reference answer. We combine the system prompt and question into a chat-formatted prompt (using the tokenizer's chat template) and set the expected output length to the tokenized length of the provided reference answer. Source code in vllm/benchmarks/datasets.py 2984 2985 2986 2987 2988 2989 2990 2991 2992 2993 2994 2995 2996 2997 2998 2999 3000 3001 3002 3003 3004 3005 3006 3007 3008 3009 3010 3011 3012 3013 3014 3015 3016 3017 3018 3019 3020 3021 3022 3023 3024 3025 3026 3027 3028 3029 3030 3031 3032 3033 3034 3035 3036 3037 3038 3039 3040 3041 3042 3043 3044 3045 3046 3047 3048 3049 3050 3051 3052 3053 3054 3055 3056 3057 3058 3059 3060 3061 3062class MLPerfDataset(HuggingFaceDataset): \"\"\" MLPerf Inference Dataset. Dataset on HF: https://huggingface.co/datasets/mgoin/mlperf-inference-llama2-data https://huggingface.co/datasets/mgoin/mlperf-inference-llama3.1-data Each record contains: - \"system_prompt\": system role instruction. - \"question\": user question. - \"output\": reference answer. We combine the system prompt and question into a chat-formatted prompt (using the tokenizer's chat template) and set the expected output length to the tokenized length of the provided reference answer. \"\"\" SUPPORTED_DATASET_PATHS = { \"mgoin/mlperf-inference-llama2-data\", \"mgoin/mlperf-inference-llama3.1-data\", } def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest]: # Force dynamic output length based on reference completion. dynamic_output = output_len is None sampled_requests: list[SampleRequest] = [] ind = 0 for item in self.data: if len(sampled_requests) >= num_requests: break system_prompt = item[\"system_prompt\"] question = item[\"question\"] reference_answer = item[\"output\"] # Build chat-style prompt using tokenizer template, if available. messages = [ {\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": question}, ] prompt_formatted = tokenizer.apply_chat_template( messages, add_generation_prompt=True, tokenize=False ) prompt_len = len(tokenizer(prompt_formatted).input_ids) # Determine output length from reference answer tokens. ref_out_len = len( tokenizer(reference_answer, add_special_tokens=False).input_ids ) expected_output_len = ref_out_len if dynamic_output else output_len # Validate sequence lengths. if not is_valid_sequence(prompt_len, expected_output_len): continue sampled_requests.append( SampleRequest( prompt=prompt_formatted, prompt_len=prompt_len, expected_output_len=expected_output_len, request_id=request_id_prefix + str(ind), ) ) ind += 1 self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = { \"mgoin/mlperf-inference-llama2-data\", \"mgoin/mlperf-inference-llama3.1-data\", } sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest] Source code in vllm/benchmarks/datasets.py 3007 3008 3009 3010 3011 3012 3013 3014 3015 3016 3017 3018 3019 3020 3021 3022 3023 3024 3025 3026 3027 3028 3029 3030 3031 3032 3033 3034 3035 3036 3037 3038 3039 3040 3041 3042 3043 3044 3045 3046 3047 3048 3049 3050 3051 3052 3053 3054 3055 3056 3057 3058 3059 3060 3061 3062def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest]: # Force dynamic output length based on reference completion. dynamic_output = output_len is None sampled_requests: list[SampleRequest] = [] ind = 0 for item in self.data: if len(sampled_requests) >= num_requests: break system_prompt = item[\"system_prompt\"] question = item[\"question\"] reference_answer = item[\"output\"] # Build chat-style prompt using tokenizer template, if available. messages = [ {\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": question}, ] prompt_formatted = tokenizer.apply_chat_template( messages, add_generation_prompt=True, tokenize=False ) prompt_len = len(tokenizer(prompt_formatted).input_ids) # Determine output length from reference answer tokens. ref_out_len = len( tokenizer(reference_answer, add_special_tokens=False).input_ids ) expected_output_len = ref_out_len if dynamic_output else output_len # Validate sequence lengths. if not is_valid_sequence(prompt_len, expected_output_len): continue sampled_requests.append( SampleRequest( prompt=prompt_formatted, prompt_len=prompt_len, expected_output_len=expected_output_len, request_id=request_id_prefix + str(ind), ) ) ind += 1 self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests MMStarDataset ¶ Bases: HuggingFaceDataset Lin-Chen/MMStar: https://huggingface.co/datasets/Lin-Chen/MMStar refer to: https://github.com/sgl-project/SpecForge/pull/106 Source code in vllm/benchmarks/datasets.py 3162 3163 3164 3165 3166 3167 3168 3169 3170 3171 3172 3173 3174 3175 3176 3177 3178 3179 3180 3181 3182 3183 3184 3185 3186 3187 3188 3189 3190 3191 3192 3193 3194 3195 3196 3197 3198 3199 3200 3201 3202 3203 3204 3205 3206 3207 3208 3209 3210 3211 3212 3213 3214 3215 3216 3217 3218 3219 3220 3221 3222 3223 3224 3225 3226 3227class MMStarDataset(HuggingFaceDataset): \"\"\" Lin-Chen/MMStar: https://huggingface.co/datasets/Lin-Chen/MMStar refer to: https://github.com/sgl-project/SpecForge/pull/106 \"\"\" DEFAULT_OUTPUT_LEN = 128 SUPPORTED_DATASET_PATHS = {\"Lin-Chen/MMStar\"} IS_MULTIMODAL = True def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest]: # If --hf-output-len is not set, use the default output length. output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests: list[SampleRequest] = [] for ind, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break # Split the question text from options # (keep only the part before \"Options:\"). full_q: str = item.get(\"question\", \"\") question_text = full_q.split(\"Options:\", 1)[0].strip() # Multimodal image content. mm_content = process_image(item[\"image\"]) # Compute prompt token length (note: this is plain text length # if enable_multimodal_chat is False). prompt_len = len(tokenizer(question_text).input_ids) if enable_multimodal_chat: # If multimodal content should be embedded in the chat message, # convert to [{\"role\":\"user\",\"content\":[...]}] prompt = self.apply_multimodal_chat_transformation( question_text, mm_content ) mm_for_request = None # Already embedded in chat content. else: # Default: prompt is plain text, # image is in mm_content for the bench to assemble. prompt = question_text mm_for_request = mm_content sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_for_request, request_id=request_id_prefix + str(ind), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶ DEFAULT_OUTPUT_LEN = 128 IS_MULTIMODAL class-attribute instance-attribute ¶ IS_MULTIMODAL = True SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = {'Lin-Chen/MMStar'} sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest] Source code in vllm/benchmarks/datasets.py 3172 3173 3174 3175 3176 3177 3178 3179 3180 3181 3182 3183 3184 3185 3186 3187 3188 3189 3190 3191 3192 3193 3194 3195 3196 3197 3198 3199 3200 3201 3202 3203 3204 3205 3206 3207 3208 3209 3210 3211 3212 3213 3214 3215 3216 3217 3218 3219 3220 3221 3222 3223 3224 3225 3226 3227def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest]: # If --hf-output-len is not set, use the default output length. output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests: list[SampleRequest] = [] for ind, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break # Split the question text from options # (keep only the part before \"Options:\"). full_q: str = item.get(\"question\", \"\") question_text = full_q.split(\"Options:\", 1)[0].strip() # Multimodal image content. mm_content = process_image(item[\"image\"]) # Compute prompt token length (note: this is plain text length # if enable_multimodal_chat is False). prompt_len = len(tokenizer(question_text).input_ids) if enable_multimodal_chat: # If multimodal content should be embedded in the chat message, # convert to [{\"role\":\"user\",\"content\":[...]}] prompt = self.apply_multimodal_chat_transformation( question_text, mm_content ) mm_for_request = None # Already embedded in chat content. else: # Default: prompt is plain text, # image is in mm_content for the bench to assemble. prompt = question_text mm_for_request = mm_content sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_for_request, request_id=request_id_prefix + str(ind), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests MMVUDataset ¶ Bases: HuggingFaceDataset MMVU Dataset. https://huggingface.co/datasets/yale-nlp/MMVU Source code in vllm/benchmarks/datasets.py 2462 2463 2464 2465 2466 2467 2468 2469 2470 2471 2472 2473 2474 2475 2476 2477 2478 2479 2480 2481 2482 2483 2484 2485 2486 2487 2488 2489 2490 2491 2492 2493 2494 2495 2496 2497 2498 2499 2500 2501 2502 2503 2504 2505 2506 2507 2508 2509 2510 2511 2512 2513class MMVUDataset(HuggingFaceDataset): \"\"\" MMVU Dataset. https://huggingface.co/datasets/yale-nlp/MMVU \"\"\" DEFAULT_OUTPUT_LEN = 128 SUPPORTED_DATASET_PATHS = { \"yale-nlp/MMVU\": lambda x: x[\"question\"] + \" \" + (\" \".join(f\"{k}.{v}\" for k, v in x[\"choices\"].items())), } def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break parser_fn = self.SUPPORTED_DATASET_PATHS.get(self.hf_name) if parser_fn is None: raise ValueError(f\"Unsupported dataset path: {self.hf_name}\") prompt = parser_fn(item) mm_content = process_video(item[\"video\"]) prompt_len = len(tokenizer(prompt).input_ids) if enable_multimodal_chat: # Note: when chat is enabled the request prompt_len is no longer # accurate and we will be using request output to count the # actual prompt len prompt = self.apply_multimodal_chat_transformation(prompt, mm_content) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_content, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶ DEFAULT_OUTPUT_LEN = 128 SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = { \"yale-nlp/MMVU\": lambda x: x[\"question\"] + \" \" + join(f\"{k}.{v}\" for k, v in (items())) } sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 2475 2476 2477 2478 2479 2480 2481 2482 2483 2484 2485 2486 2487 2488 2489 2490 2491 2492 2493 2494 2495 2496 2497 2498 2499 2500 2501 2502 2503 2504 2505 2506 2507 2508 2509 2510 2511 2512 2513def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break parser_fn = self.SUPPORTED_DATASET_PATHS.get(self.hf_name) if parser_fn is None: raise ValueError(f\"Unsupported dataset path: {self.hf_name}\") prompt = parser_fn(item) mm_content = process_video(item[\"video\"]) prompt_len = len(tokenizer(prompt).input_ids) if enable_multimodal_chat: # Note: when chat is enabled the request prompt_len is no longer # accurate and we will be using request output to count the # actual prompt len prompt = self.apply_multimodal_chat_transformation(prompt, mm_content) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_content, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests MTBenchDataset ¶ Bases: HuggingFaceDataset MT-Bench Dataset. https://huggingface.co/datasets/philschmid/mt-bench We create a single turn dataset for MT-Bench. This is similar to Spec decoding benchmark setup in vLLM https://github.com/vllm-project/vllm/blob/9d98ab5ec/examples/offline_inference/eagle.py#L14-L18 Source code in vllm/benchmarks/datasets.py 2585 2586 2587 2588 2589 2590 2591 2592 2593 2594 2595 2596 2597 2598 2599 2600 2601 2602 2603 2604 2605 2606 2607 2608 2609 2610 2611 2612 2613 2614 2615 2616 2617 2618 2619 2620 2621 2622 2623 2624 2625 2626 2627 2628 2629 2630 2631 2632 2633 2634 2635 2636 2637 2638 2639class MTBenchDataset(HuggingFaceDataset): \"\"\" MT-Bench Dataset. https://huggingface.co/datasets/philschmid/mt-bench We create a single turn dataset for MT-Bench. This is similar to Spec decoding benchmark setup in vLLM https://github.com/vllm-project/vllm/blob/9d98ab5ec/examples/offline_inference/eagle.py#L14-L18 \"\"\" # noqa: E501 DEFAULT_OUTPUT_LEN = 256 # avg len used in SD bench in vLLM SUPPORTED_DATASET_PATHS = { \"philschmid/mt-bench\", } def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break prompt = item[\"turns\"][0] # apply template if not skip_chat_template: prompt = tokenizer.apply_chat_template( [{\"role\": \"user\", \"content\": prompt}], add_generation_prompt=True, tokenize=False, ) prompt_len = len(tokenizer(prompt).input_ids) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶ DEFAULT_OUTPUT_LEN = 256 SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = {'philschmid/mt-bench'} sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 2600 2601 2602 2603 2604 2605 2606 2607 2608 2609 2610 2611 2612 2613 2614 2615 2616 2617 2618 2619 2620 2621 2622 2623 2624 2625 2626 2627 2628 2629 2630 2631 2632 2633 2634 2635 2636 2637 2638 2639def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, skip_chat_template: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break prompt = item[\"turns\"][0] # apply template if not skip_chat_template: prompt = tokenizer.apply_chat_template( [{\"role\": \"user\", \"content\": prompt}], add_generation_prompt=True, tokenize=False, ) prompt_len = len(tokenizer(prompt).input_ids) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests MultiModalConversationDataset ¶ Bases: HuggingFaceDataset Dataset for multimodal conversation data. Source code in vllm/benchmarks/datasets.py 2344 2345 2346 2347 2348 2349 2350 2351 2352 2353 2354 2355 2356 2357 2358 2359 2360 2361 2362 2363 2364 2365 2366 2367 2368 2369 2370 2371 2372 2373 2374 2375 2376 2377 2378 2379 2380 2381 2382 2383 2384 2385 2386 2387 2388 2389 2390 2391 2392 2393 2394 2395 2396 2397 2398 2399 2400 2401class MultiModalConversationDataset(HuggingFaceDataset): \"\"\"Dataset for multimodal conversation data.\"\"\" SUPPORTED_DATASET_PATHS = { \"lmms-lab/LLaVA-OneVision-Data\", } IS_MULTIMODAL = True def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: # Filter examples with at least 2 conversations filtered_data = self.data.filter(lambda x: len(x[\"conversations\"]) >= 2) sampled_requests = [] ind = 0 dynamic_output = output_len is None for item in filtered_data: if len(sampled_requests) >= num_requests: break conv = item[\"conversations\"] prompt, completion = conv[0][\"value\"], conv[1][\"value\"] prompt_ids = tokenizer(prompt).input_ids completion_ids = tokenizer(completion).input_ids prompt_len = len(prompt_ids) completion_len = len(completion_ids) output_len = completion_len if dynamic_output else output_len assert isinstance(output_len, int) and output_len > 0 if dynamic_output and not is_valid_sequence(prompt_len, completion_len): continue mm_content = process_image(item[\"image\"]) if \"image\" in item else None if enable_multimodal_chat: # Note: when chat is enabled the request prompt_len is no longer # accurate and we will be using request output to count the # actual prompt len and output len prompt = self.apply_multimodal_chat_transformation(prompt, mm_content) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_content, request_id=request_id_prefix + str(ind), ) ) ind += 1 self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests IS_MULTIMODAL class-attribute instance-attribute ¶ IS_MULTIMODAL = True SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = {'lmms-lab/LLaVA-OneVision-Data'} sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 2352 2353 2354 2355 2356 2357 2358 2359 2360 2361 2362 2363 2364 2365 2366 2367 2368 2369 2370 2371 2372 2373 2374 2375 2376 2377 2378 2379 2380 2381 2382 2383 2384 2385 2386 2387 2388 2389 2390 2391 2392 2393 2394 2395 2396 2397 2398 2399 2400 2401def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: # Filter examples with at least 2 conversations filtered_data = self.data.filter(lambda x: len(x[\"conversations\"]) >= 2) sampled_requests = [] ind = 0 dynamic_output = output_len is None for item in filtered_data: if len(sampled_requests) >= num_requests: break conv = item[\"conversations\"] prompt, completion = conv[0][\"value\"], conv[1][\"value\"] prompt_ids = tokenizer(prompt).input_ids completion_ids = tokenizer(completion).input_ids prompt_len = len(prompt_ids) completion_len = len(completion_ids) output_len = completion_len if dynamic_output else output_len assert isinstance(output_len, int) and output_len > 0 if dynamic_output and not is_valid_sequence(prompt_len, completion_len): continue mm_content = process_image(item[\"image\"]) if \"image\" in item else None if enable_multimodal_chat: # Note: when chat is enabled the request prompt_len is no longer # accurate and we will be using request output to count the # actual prompt len and output len prompt = self.apply_multimodal_chat_transformation(prompt, mm_content) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_content, request_id=request_id_prefix + str(ind), ) ) ind += 1 self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests NextEditPredictionDataset ¶ Bases: HuggingFaceDataset Dataset class for processing a Next Edit Prediction dataset. Source code in vllm/benchmarks/datasets.py 2845 2846 2847 2848 2849 2850 2851 2852 2853 2854 2855 2856 2857 2858 2859 2860 2861 2862 2863 2864 2865 2866 2867 2868 2869 2870 2871 2872 2873 2874 2875 2876 2877 2878 2879 2880 2881 2882 2883 2884 2885 2886class NextEditPredictionDataset(HuggingFaceDataset): \"\"\" Dataset class for processing a Next Edit Prediction dataset. \"\"\" SUPPORTED_DATASET_PATHS = { \"zed-industries/zeta\", } MAPPING_PROMPT_FUNCS = { \"zed-industries/zeta\": _format_zeta_prompt, } def sample( self, tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ): formatting_prompt_func = self.MAPPING_PROMPT_FUNCS.get(self.hf_name) if formatting_prompt_func is None: raise ValueError(f\"Unsupported dataset path: {self.hf_name}\") samples = [] for i, sample in enumerate(self.data): sample = formatting_prompt_func(sample) samples.append( SampleRequest( prompt=sample[\"prompt\"], prompt_len=len(tokenizer(sample[\"prompt\"]).input_ids), expected_output_len=len( tokenizer(sample[\"expected_output\"]).input_ids ), request_id=request_id_prefix + str(i), ) ) if len(samples) >= num_requests: break self.maybe_oversample_requests( samples, num_requests, request_id_prefix, no_oversample ) return samples MAPPING_PROMPT_FUNCS class-attribute instance-attribute ¶ MAPPING_PROMPT_FUNCS = { \"zed-industries/zeta\": _format_zeta_prompt } SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = {'zed-industries/zeta'} sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) Source code in vllm/benchmarks/datasets.py 2857 2858 2859 2860 2861 2862 2863 2864 2865 2866 2867 2868 2869 2870 2871 2872 2873 2874 2875 2876 2877 2878 2879 2880 2881 2882 2883 2884 2885 2886def sample( self, tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ): formatting_prompt_func = self.MAPPING_PROMPT_FUNCS.get(self.hf_name) if formatting_prompt_func is None: raise ValueError(f\"Unsupported dataset path: {self.hf_name}\") samples = [] for i, sample in enumerate(self.data): sample = formatting_prompt_func(sample) samples.append( SampleRequest( prompt=sample[\"prompt\"], prompt_len=len(tokenizer(sample[\"prompt\"]).input_ids), expected_output_len=len( tokenizer(sample[\"expected_output\"]).input_ids ), request_id=request_id_prefix + str(i), ) ) if len(samples) >= num_requests: break self.maybe_oversample_requests( samples, num_requests, request_id_prefix, no_oversample ) return samples PrefixRepetitionRandomDataset ¶ Bases: BenchmarkDataset Source code in vllm/benchmarks/datasets.py 3070 3071 3072 3073 3074 3075 3076 3077 3078 3079 3080 3081 3082 3083 3084 3085 3086 3087 3088 3089 3090 3091 3092 3093 3094 3095 3096 3097 3098 3099 3100 3101 3102 3103 3104 3105 3106 3107 3108 3109 3110 3111 3112 3113 3114 3115 3116 3117 3118 3119 3120 3121 3122 3123 3124 3125 3126 3127 3128 3129 3130 3131 3132 3133 3134 3135 3136 3137 3138 3139 3140 3141 3142 3143 3144 3145 3146 3147 3148 3149 3150 3151 3152 3153 3154class PrefixRepetitionRandomDataset(BenchmarkDataset): # Default values copied from benchmark_serving.py for the repeated prefix # dataset. DEFAULT_PREFIX_LEN = 256 DEFAULT_SUFFIX_LEN = 256 DEFAULT_NUM_PREFIXES = 10 DEFAULT_OUTPUT_LEN = 128 def __init__( self, **kwargs, ) -> None: super().__init__(**kwargs) random.seed(self.random_seed) np.random.seed(self.random_seed) def sample( self, tokenizer: TokenizerLike, num_requests: int, prefix_len: int = DEFAULT_PREFIX_LEN, suffix_len: int = DEFAULT_SUFFIX_LEN, num_prefixes: int = DEFAULT_NUM_PREFIXES, output_len: int = DEFAULT_OUTPUT_LEN, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest]: vocab_size = tokenizer.vocab_size prompts_per_prefix = num_requests // num_prefixes if prompts_per_prefix == 0: raise ValueError( f\"num_requests ({num_requests}) must be greater than or equal \" f\"to num_prefixes ({num_prefixes})\" ) def _generate_exact_length_tokens(target_length: int) -> list[int]: \"\"\"Generate tokens that decode and re-encode to exactly target_length.\"\"\" # Generate random tokens tokens = np.random.randint(0, vocab_size, size=target_length).tolist() _, adjusted_tokens, token_mismatch = gen_prompt_decode_to_target_len( # noqa: E501 tokenizer=tokenizer, token_sequence=tokens, target_token_len=target_length, add_special_tokens=False, ) return adjusted_tokens, token_mismatch requests = [] token_mismatch_total = 0 for _ in range(num_prefixes): prefix_tokens, prefix_mismatch = _generate_exact_length_tokens(prefix_len) token_mismatch_total += prefix_mismatch for _ in range(prompts_per_prefix): suffix_tokens, suffix_mismatch = _generate_exact_length_tokens( suffix_len ) token_mismatch_total += suffix_mismatch combined_tokens = prefix_tokens + suffix_tokens prompt = tokenizer.decode(combined_tokens) prompt_len = len(combined_tokens) requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, ) ) if token_mismatch_total != 0: sign = \"more\" if token_mismatch_total > 0 else \"fewer\" logger.warning( \"Across all generated prompts, there were %d %s tokens \" \"than expected after decoding and re-encoding. This is \" \"expected due to the imperfect nature of the sampling \" \"procedure.\", abs(token_mismatch_total), sign, ) if not getattr(self, \"disable_shuffle\", False): random.shuffle(requests) return requests DEFAULT_NUM_PREFIXES class-attribute instance-attribute ¶ DEFAULT_NUM_PREFIXES = 10 DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶ DEFAULT_OUTPUT_LEN = 128 DEFAULT_PREFIX_LEN class-attribute instance-attribute ¶ DEFAULT_PREFIX_LEN = 256 DEFAULT_SUFFIX_LEN class-attribute instance-attribute ¶ DEFAULT_SUFFIX_LEN = 256 __init__ ¶ __init__(**kwargs) -> None Source code in vllm/benchmarks/datasets.py 3078 3079 3080 3081 3082 3083 3084def __init__( self, **kwargs, ) -> None: super().__init__(**kwargs) random.seed(self.random_seed) np.random.seed(self.random_seed) sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, prefix_len: int = DEFAULT_PREFIX_LEN, suffix_len: int = DEFAULT_SUFFIX_LEN, num_prefixes: int = DEFAULT_NUM_PREFIXES, output_len: int = DEFAULT_OUTPUT_LEN, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest] Source code in vllm/benchmarks/datasets.py 3086 3087 3088 3089 3090 3091 3092 3093 3094 3095 3096 3097 3098 3099 3100 3101 3102 3103 3104 3105 3106 3107 3108 3109 3110 3111 3112 3113 3114 3115 3116 3117 3118 3119 3120 3121 3122 3123 3124 3125 3126 3127 3128 3129 3130 3131 3132 3133 3134 3135 3136 3137 3138 3139 3140 3141 3142 3143 3144 3145 3146 3147 3148 3149 3150 3151 3152 3153 3154def sample( self, tokenizer: TokenizerLike, num_requests: int, prefix_len: int = DEFAULT_PREFIX_LEN, suffix_len: int = DEFAULT_SUFFIX_LEN, num_prefixes: int = DEFAULT_NUM_PREFIXES, output_len: int = DEFAULT_OUTPUT_LEN, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list[SampleRequest]: vocab_size = tokenizer.vocab_size prompts_per_prefix = num_requests // num_prefixes if prompts_per_prefix == 0: raise ValueError( f\"num_requests ({num_requests}) must be greater than or equal \" f\"to num_prefixes ({num_prefixes})\" ) def _generate_exact_length_tokens(target_length: int) -> list[int]: \"\"\"Generate tokens that decode and re-encode to exactly target_length.\"\"\" # Generate random tokens tokens = np.random.randint(0, vocab_size, size=target_length).tolist() _, adjusted_tokens, token_mismatch = gen_prompt_decode_to_target_len( # noqa: E501 tokenizer=tokenizer, token_sequence=tokens, target_token_len=target_length, add_special_tokens=False, ) return adjusted_tokens, token_mismatch requests = [] token_mismatch_total = 0 for _ in range(num_prefixes): prefix_tokens, prefix_mismatch = _generate_exact_length_tokens(prefix_len) token_mismatch_total += prefix_mismatch for _ in range(prompts_per_prefix): suffix_tokens, suffix_mismatch = _generate_exact_length_tokens( suffix_len ) token_mismatch_total += suffix_mismatch combined_tokens = prefix_tokens + suffix_tokens prompt = tokenizer.decode(combined_tokens) prompt_len = len(combined_tokens) requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, ) ) if token_mismatch_total != 0: sign = \"more\" if token_mismatch_total > 0 else \"fewer\" logger.warning( \"Across all generated prompts, there were %d %s tokens \" \"than expected after decoding and re-encoding. This is \" \"expected due to the imperfect nature of the sampling \" \"procedure.\", abs(token_mismatch_total), sign, ) if not getattr(self, \"disable_shuffle\", False): random.shuffle(requests) return requests RandomDataset ¶ Bases: BenchmarkDataset Synthetic text-only dataset for serving/throughput benchmarks. Strategy: - Sample input/output token lengths per request from integer-uniform ranges around configured means (controlled by range_ratio). - Prepend a fixed random prefix of length prefix_len. - Generate the remaining tokens as a reproducible sequence: (offset + index + arange(input_len)) % vocab_size. - Decode then re-encode/truncate to ensure prompt token counts match. - Uses numpy.default_rng seeded with random_seed for reproducible sampling. Source code in vllm/benchmarks/datasets.py 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668class RandomDataset(BenchmarkDataset): \"\"\" Synthetic text-only dataset for serving/throughput benchmarks. Strategy: - Sample input/output token lengths per request from integer-uniform ranges around configured means (controlled by range_ratio). - Prepend a fixed random prefix of length prefix_len. - Generate the remaining tokens as a reproducible sequence: (offset + index + arange(input_len)) % vocab_size. - Decode then re-encode/truncate to ensure prompt token counts match. - Uses numpy.default_rng seeded with random_seed for reproducible sampling. \"\"\" # Default values copied from benchmark_serving.py for the random dataset. DEFAULT_PREFIX_LEN = 0 DEFAULT_RANGE_RATIO = 0.0 DEFAULT_INPUT_LEN = 1024 DEFAULT_OUTPUT_LEN = 128 def __init__(self, **kwargs) -> None: super().__init__(**kwargs) # Use numpy's default_rng for deterministic sampling # Do not use random.seed() or np.random.seed() elsewhere in this class. # This ensures that the RNG is isolated from global RNG state. self._rng = np.random.default_rng(self.random_seed) def sample( self, tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, prefix_len: int = DEFAULT_PREFIX_LEN, range_ratio: float = DEFAULT_RANGE_RATIO, input_len: int = DEFAULT_INPUT_LEN, output_len: int = DEFAULT_OUTPUT_LEN, batchsize: int = 1, **kwargs, ) -> list[SampleRequest]: # validate total input tokens (prefix + sampled) is at least 1. num_special = int(tokenizer.num_special_tokens_to_add()) real_input_len = max(0, int(input_len) - num_special) min_sampled_input = math.floor(real_input_len * (1.0 - float(range_ratio))) min_total_input = int(prefix_len) + min_sampled_input if min_total_input < 1: raise ValueError( \"--random-input-len is too small: with tokenizer special \" f\"tokens {num_special} and --random-range-ratio {range_ratio}, \" \"the minimum possible total input tokens (prefix + sampled) is \" f\"{min_total_input}. Increase --random-input-len and/or \" \"--random-prefix-len, or decrease --random-range-ratio so that \" \"prefix_len + floor(max(0, random_input_len - num_special)) \" \"* (1 - range_ratio) >= 1.\" ) input_lens, output_lens, offsets = self.get_sampling_params( num_requests, range_ratio, input_len, output_len, tokenizer ) vocab_size = tokenizer.vocab_size prohibited_tokens = tokenizer.all_special_ids all_tokens = np.arange(vocab_size) allowed_tokens = np.array(list(set(all_tokens) - set(prohibited_tokens))) # Generate prefix once prefix_token_ids = self.get_prefix(allowed_tokens, prefix_len) requests = [] token_mismatch_total = 0 for i in range(num_requests): prompt, total_input_len, token_mismatch = self.generate_token_sequence( # noqa: E501 tokenizer=tokenizer, prefix_token_ids=prefix_token_ids, prefix_len=prefix_len, vocab_size=vocab_size, input_len=int(input_lens[i]), offset=int(offsets[i]), index=i, allowed_tokens=allowed_tokens, ) token_mismatch_total += token_mismatch requests.append( SampleRequest( prompt=prompt, prompt_len=total_input_len, expected_output_len=int(output_lens[i]), request_id=request_id_prefix + str(i), ) ) # only used for embeddings benchmark. if batchsize > 1: batch_requests = [] # Create batched requests for i in range(0, num_requests, batchsize): batch = requests[i : i + batchsize] batch_requests.append( SampleRequest( prompt=[req.prompt for req in batch], prompt_len=sum(req.prompt_len for req in batch), expected_output_len=0, request_id=request_id_prefix + str(i // batchsize), ) ) requests = batch_requests if token_mismatch_total != 0: sign = \"more\" if token_mismatch_total > 0 else \"fewer\" logger.warning( \"Across all generated prompts, there were %d %s tokens \" \"than expected after decoding and re-encoding. This is \" \"expected due to the imperfect nature of the sampling \" \"procedure.\", abs(token_mismatch_total), sign, ) return requests def get_prefix( self, allowed_tokens: np.ndarray, prefix_len: int, ) -> list[int]: \"\"\" Get the prefix for the dataset. \"\"\" return ( allowed_tokens[ self._rng.integers(0, len(allowed_tokens), size=prefix_len) ].tolist() if prefix_len > 0 else [] ) def get_sampling_params( self, num_requests: int, range_ratio: float, input_len: int, output_len: int, tokenizer: TokenizerLike, ) -> tuple[np.ndarray, np.ndarray, np.ndarray]: \"\"\" Get the sampling parameters for the dataset. \"\"\" # Enforce range_ratio < 1 if not (0.0 <= range_ratio < 1.0): raise ValueError(\"range_ratio must be in [0, 1).\") num_special_tokens = int(tokenizer.num_special_tokens_to_add()) real_input_len = max(0, int(input_len) - num_special_tokens) # Bounds use floor for low and ceil for high input_low = math.floor(real_input_len * (1 - range_ratio)) input_high = math.ceil(real_input_len * (1 + range_ratio)) output_low = math.floor(output_len * (1 - range_ratio)) output_high = math.ceil(output_len * (1 + range_ratio)) # Ensure the lower bound for output length is at least 1 to # prevent sampling 0 tokens. output_low = max(output_low, 1) output_high = max(output_high, 1) if input_low > input_high: raise ValueError( f\"Invalid input sampling interval: low={input_low} > high={input_high}\" ) if output_low > output_high: raise ValueError( \"Invalid output sampling interval: \" f\"low={output_low} > high={output_high}\" ) logger.info( \"Sampling input_len from [%s, %s] and output_len from [%s, %s]\", input_low, input_high, output_low, output_high, ) input_lens = self._rng.integers(input_low, input_high + 1, size=num_requests) output_lens = self._rng.integers(output_low, output_high + 1, size=num_requests) offsets = self._rng.integers(0, tokenizer.vocab_size, size=num_requests) return input_lens, output_lens, offsets def generate_token_sequence( self, *, tokenizer: TokenizerLike, prefix_token_ids: list[int], prefix_len: int, vocab_size: int, input_len: int, offset: int, index: int, allowed_tokens: np.ndarray, ) -> tuple[str, int, int]: \"\"\" Returns (prompt, total_input_len). NOTE: After decoding the prompt we have to encode and decode it again. This is done because in some cases N consecutive tokens give a string tokenized into != N number of tokens. For example for GPT2Tokenizer: [6880, 6881] -> ['Ġcalls', 'here'] -> [1650, 939, 486] -> ['Ġcall', 'sh', 'ere'] To avoid uncontrolled change of the prompt length, the encoded sequence is truncated before being decoded again. \"\"\" # Build the inner sequence by sampling # sequentially from the allowed tokens inner_seq = allowed_tokens[ (offset + index + np.arange(input_len)) % len(allowed_tokens) ].tolist() token_sequence = prefix_token_ids + inner_seq # Decode, then re-encode and truncate to preserve token count invariants total_input_len = prefix_len + int(input_len) prompt, adjusted_token_sequence, token_mismatch = ( gen_prompt_decode_to_target_len( tokenizer=tokenizer, token_sequence=token_sequence, target_token_len=total_input_len, add_special_tokens=False, rng=self._rng, ) ) total_input_len = len(adjusted_token_sequence) return prompt, total_input_len, token_mismatch DEFAULT_INPUT_LEN class-attribute instance-attribute ¶ DEFAULT_INPUT_LEN = 1024 DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶ DEFAULT_OUTPUT_LEN = 128 DEFAULT_PREFIX_LEN class-attribute instance-attribute ¶ DEFAULT_PREFIX_LEN = 0 DEFAULT_RANGE_RATIO class-attribute instance-attribute ¶ DEFAULT_RANGE_RATIO = 0.0 _rng instance-attribute ¶ _rng = default_rng(random_seed) __init__ ¶ __init__(**kwargs) -> None Source code in vllm/benchmarks/datasets.py 461 462 463 464 465 466def __init__(self, **kwargs) -> None: super().__init__(**kwargs) # Use numpy's default_rng for deterministic sampling # Do not use random.seed() or np.random.seed() elsewhere in this class. # This ensures that the RNG is isolated from global RNG state. self._rng = np.random.default_rng(self.random_seed) generate_token_sequence ¶ generate_token_sequence( *, tokenizer: TokenizerLike, prefix_token_ids: list[int], prefix_len: int, vocab_size: int, input_len: int, offset: int, index: int, allowed_tokens: ndarray, ) -> tuple[str, int, int] Returns (prompt, total_input_len). NOTE: After decoding the prompt we have to encode and decode it again. This is done because in some cases N consecutive tokens give a string tokenized into != N number of tokens. For example for GPT2Tokenizer: [6880, 6881] -> ['Ġcalls', 'here'] -> [1650, 939, 486] -> ['Ġcall', 'sh', 'ere'] To avoid uncontrolled change of the prompt length, the encoded sequence is truncated before being decoded again. Source code in vllm/benchmarks/datasets.py 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668def generate_token_sequence( self, *, tokenizer: TokenizerLike, prefix_token_ids: list[int], prefix_len: int, vocab_size: int, input_len: int, offset: int, index: int, allowed_tokens: np.ndarray, ) -> tuple[str, int, int]: \"\"\" Returns (prompt, total_input_len). NOTE: After decoding the prompt we have to encode and decode it again. This is done because in some cases N consecutive tokens give a string tokenized into != N number of tokens. For example for GPT2Tokenizer: [6880, 6881] -> ['Ġcalls', 'here'] -> [1650, 939, 486] -> ['Ġcall', 'sh', 'ere'] To avoid uncontrolled change of the prompt length, the encoded sequence is truncated before being decoded again. \"\"\" # Build the inner sequence by sampling # sequentially from the allowed tokens inner_seq = allowed_tokens[ (offset + index + np.arange(input_len)) % len(allowed_tokens) ].tolist() token_sequence = prefix_token_ids + inner_seq # Decode, then re-encode and truncate to preserve token count invariants total_input_len = prefix_len + int(input_len) prompt, adjusted_token_sequence, token_mismatch = ( gen_prompt_decode_to_target_len( tokenizer=tokenizer, token_sequence=token_sequence, target_token_len=total_input_len, add_special_tokens=False, rng=self._rng, ) ) total_input_len = len(adjusted_token_sequence) return prompt, total_input_len, token_mismatch get_prefix ¶ get_prefix( allowed_tokens: ndarray, prefix_len: int ) -> list[int] Get the prefix for the dataset. Source code in vllm/benchmarks/datasets.py 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574def get_prefix( self, allowed_tokens: np.ndarray, prefix_len: int, ) -> list[int]: \"\"\" Get the prefix for the dataset. \"\"\" return ( allowed_tokens[ self._rng.integers(0, len(allowed_tokens), size=prefix_len) ].tolist() if prefix_len > 0 else [] ) get_sampling_params ¶ get_sampling_params( num_requests: int, range_ratio: float, input_len: int, output_len: int, tokenizer: TokenizerLike, ) -> tuple[ndarray, ndarray, ndarray] Get the sampling parameters for the dataset. Source code in vllm/benchmarks/datasets.py 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623def get_sampling_params( self, num_requests: int, range_ratio: float, input_len: int, output_len: int, tokenizer: TokenizerLike, ) -> tuple[np.ndarray, np.ndarray, np.ndarray]: \"\"\" Get the sampling parameters for the dataset. \"\"\" # Enforce range_ratio < 1 if not (0.0 <= range_ratio < 1.0): raise ValueError(\"range_ratio must be in [0, 1).\") num_special_tokens = int(tokenizer.num_special_tokens_to_add()) real_input_len = max(0, int(input_len) - num_special_tokens) # Bounds use floor for low and ceil for high input_low = math.floor(real_input_len * (1 - range_ratio)) input_high = math.ceil(real_input_len * (1 + range_ratio)) output_low = math.floor(output_len * (1 - range_ratio)) output_high = math.ceil(output_len * (1 + range_ratio)) # Ensure the lower bound for output length is at least 1 to # prevent sampling 0 tokens. output_low = max(output_low, 1) output_high = max(output_high, 1) if input_low > input_high: raise ValueError( f\"Invalid input sampling interval: low={input_low} > high={input_high}\" ) if output_low > output_high: raise ValueError( \"Invalid output sampling interval: \" f\"low={output_low} > high={output_high}\" ) logger.info( \"Sampling input_len from [%s, %s] and output_len from [%s, %s]\", input_low, input_high, output_low, output_high, ) input_lens = self._rng.integers(input_low, input_high + 1, size=num_requests) output_lens = self._rng.integers(output_low, output_high + 1, size=num_requests) offsets = self._rng.integers(0, tokenizer.vocab_size, size=num_requests) return input_lens, output_lens, offsets sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, prefix_len: int = DEFAULT_PREFIX_LEN, range_ratio: float = DEFAULT_RANGE_RATIO, input_len: int = DEFAULT_INPUT_LEN, output_len: int = DEFAULT_OUTPUT_LEN, batchsize: int = 1, **kwargs, ) -> list[SampleRequest] Source code in vllm/benchmarks/datasets.py 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558def sample( self, tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, prefix_len: int = DEFAULT_PREFIX_LEN, range_ratio: float = DEFAULT_RANGE_RATIO, input_len: int = DEFAULT_INPUT_LEN, output_len: int = DEFAULT_OUTPUT_LEN, batchsize: int = 1, **kwargs, ) -> list[SampleRequest]: # validate total input tokens (prefix + sampled) is at least 1. num_special = int(tokenizer.num_special_tokens_to_add()) real_input_len = max(0, int(input_len) - num_special) min_sampled_input = math.floor(real_input_len * (1.0 - float(range_ratio))) min_total_input = int(prefix_len) + min_sampled_input if min_total_input < 1: raise ValueError( \"--random-input-len is too small: with tokenizer special \" f\"tokens {num_special} and --random-range-ratio {range_ratio}, \" \"the minimum possible total input tokens (prefix + sampled) is \" f\"{min_total_input}. Increase --random-input-len and/or \" \"--random-prefix-len, or decrease --random-range-ratio so that \" \"prefix_len + floor(max(0, random_input_len - num_special)) \" \"* (1 - range_ratio) >= 1.\" ) input_lens, output_lens, offsets = self.get_sampling_params( num_requests, range_ratio, input_len, output_len, tokenizer ) vocab_size = tokenizer.vocab_size prohibited_tokens = tokenizer.all_special_ids all_tokens = np.arange(vocab_size) allowed_tokens = np.array(list(set(all_tokens) - set(prohibited_tokens))) # Generate prefix once prefix_token_ids = self.get_prefix(allowed_tokens, prefix_len) requests = [] token_mismatch_total = 0 for i in range(num_requests): prompt, total_input_len, token_mismatch = self.generate_token_sequence( # noqa: E501 tokenizer=tokenizer, prefix_token_ids=prefix_token_ids, prefix_len=prefix_len, vocab_size=vocab_size, input_len=int(input_lens[i]), offset=int(offsets[i]), index=i, allowed_tokens=allowed_tokens, ) token_mismatch_total += token_mismatch requests.append( SampleRequest( prompt=prompt, prompt_len=total_input_len, expected_output_len=int(output_lens[i]), request_id=request_id_prefix + str(i), ) ) # only used for embeddings benchmark. if batchsize > 1: batch_requests = [] # Create batched requests for i in range(0, num_requests, batchsize): batch = requests[i : i + batchsize] batch_requests.append( SampleRequest( prompt=[req.prompt for req in batch], prompt_len=sum(req.prompt_len for req in batch), expected_output_len=0, request_id=request_id_prefix + str(i // batchsize), ) ) requests = batch_requests if token_mismatch_total != 0: sign = \"more\" if token_mismatch_total > 0 else \"fewer\" logger.warning( \"Across all generated prompts, there were %d %s tokens \" \"than expected after decoding and re-encoding. This is \" \"expected due to the imperfect nature of the sampling \" \"procedure.\", abs(token_mismatch_total), sign, ) return requests RandomDatasetForReranking ¶ Bases: RandomDataset Random dataset specialized for the needs of scoring: - Batches of inputs - Inputs composed of pairs Source code in vllm/benchmarks/datasets.py 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780class RandomDatasetForReranking(RandomDataset): \"\"\" Random dataset specialized for the needs of scoring: - Batches of inputs - Inputs composed of pairs \"\"\" def __init__(self, **kwargs) -> None: super().__init__(**kwargs) def sample( self, tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", range_ratio: float = RandomDataset.DEFAULT_RANGE_RATIO, input_len: int = RandomDataset.DEFAULT_INPUT_LEN, batchsize: int = 1, is_reranker: bool = True, **kwargs, ) -> list[SampleRequest]: n_sep_tokens = int(is_reranker) query_len_param = (input_len // 2) - n_sep_tokens if is_reranker else input_len query_lens, _, query_offsets = self.get_sampling_params( 1, range_ratio, query_len_param, 0, tokenizer ) query_len = int(query_lens[0]) if not is_reranker: assert num_requests > 1 and batchsize > 1 num_requests -= 1 batchsize -= 1 doc_len_param = input_len else: doc_len_param = input_len - query_len - n_sep_tokens doc_lens, _, doc_offsets = self.get_sampling_params( num_requests, range_ratio, doc_len_param, 0, tokenizer ) vocab_size = tokenizer.vocab_size prohibited_tokens = tokenizer.all_special_ids all_tokens = np.arange(vocab_size) allowed_tokens = np.array(list(set(all_tokens) - set(prohibited_tokens))) query_prompt, query_input_len, token_mismatch_total = ( self.generate_token_sequence( tokenizer=tokenizer, prefix_token_ids=[], prefix_len=0, vocab_size=vocab_size, input_len=query_len, offset=int(query_offsets[0]), index=0, allowed_tokens=allowed_tokens, ) ) requests = [] for i in range(num_requests): prompt, total_input_len, token_mismatch = self.generate_token_sequence( # noqa: E501 tokenizer=tokenizer, prefix_token_ids=[], prefix_len=0, vocab_size=vocab_size, input_len=int(doc_lens[i]), offset=int(doc_offsets[i]), index=i + 1, allowed_tokens=allowed_tokens, ) token_mismatch_total += token_mismatch requests.append((prompt, total_input_len)) batch_requests = [] # Create batched requests for i in range(0, num_requests, batchsize): batch = requests[i : i + batchsize] query_contrib = ( (query_input_len + n_sep_tokens) * len(batch) if is_reranker else query_input_len ) batch_requests.append( SampleRequest( prompt=[query_prompt] + [req[0] for req in batch], prompt_len=query_contrib + sum(req[1] for req in batch), expected_output_len=0, request_id=request_id_prefix + str(i // batchsize), ) ) if token_mismatch_total != 0: logger.warning( \"Across all generated prompts, there were %d %s tokens \" \"than expected after decoding and re-encoding. This is \" \"expected due to the imperfect nature of the sampling \" \"procedure.\", abs(token_mismatch_total), \"more\" if token_mismatch_total > 0 else \"fewer\", ) return batch_requests __init__ ¶ __init__(**kwargs) -> None Source code in vllm/benchmarks/datasets.py 683 684def __init__(self, **kwargs) -> None: super().__init__(**kwargs) sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", range_ratio: float = DEFAULT_RANGE_RATIO, input_len: int = DEFAULT_INPUT_LEN, batchsize: int = 1, is_reranker: bool = True, **kwargs, ) -> list[SampleRequest] Source code in vllm/benchmarks/datasets.py 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780def sample( self, tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", range_ratio: float = RandomDataset.DEFAULT_RANGE_RATIO, input_len: int = RandomDataset.DEFAULT_INPUT_LEN, batchsize: int = 1, is_reranker: bool = True, **kwargs, ) -> list[SampleRequest]: n_sep_tokens = int(is_reranker) query_len_param = (input_len // 2) - n_sep_tokens if is_reranker else input_len query_lens, _, query_offsets = self.get_sampling_params( 1, range_ratio, query_len_param, 0, tokenizer ) query_len = int(query_lens[0]) if not is_reranker: assert num_requests > 1 and batchsize > 1 num_requests -= 1 batchsize -= 1 doc_len_param = input_len else: doc_len_param = input_len - query_len - n_sep_tokens doc_lens, _, doc_offsets = self.get_sampling_params( num_requests, range_ratio, doc_len_param, 0, tokenizer ) vocab_size = tokenizer.vocab_size prohibited_tokens = tokenizer.all_special_ids all_tokens = np.arange(vocab_size) allowed_tokens = np.array(list(set(all_tokens) - set(prohibited_tokens))) query_prompt, query_input_len, token_mismatch_total = ( self.generate_token_sequence( tokenizer=tokenizer, prefix_token_ids=[], prefix_len=0, vocab_size=vocab_size, input_len=query_len, offset=int(query_offsets[0]), index=0, allowed_tokens=allowed_tokens, ) ) requests = [] for i in range(num_requests): prompt, total_input_len, token_mismatch = self.generate_token_sequence( # noqa: E501 tokenizer=tokenizer, prefix_token_ids=[], prefix_len=0, vocab_size=vocab_size, input_len=int(doc_lens[i]), offset=int(doc_offsets[i]), index=i + 1, allowed_tokens=allowed_tokens, ) token_mismatch_total += token_mismatch requests.append((prompt, total_input_len)) batch_requests = [] # Create batched requests for i in range(0, num_requests, batchsize): batch = requests[i : i + batchsize] query_contrib = ( (query_input_len + n_sep_tokens) * len(batch) if is_reranker else query_input_len ) batch_requests.append( SampleRequest( prompt=[query_prompt] + [req[0] for req in batch], prompt_len=query_contrib + sum(req[1] for req in batch), expected_output_len=0, request_id=request_id_prefix + str(i // batchsize), ) ) if token_mismatch_total != 0: logger.warning( \"Across all generated prompts, there were %d %s tokens \" \"than expected after decoding and re-encoding. This is \" \"expected due to the imperfect nature of the sampling \" \"procedure.\", abs(token_mismatch_total), \"more\" if token_mismatch_total > 0 else \"fewer\", ) return batch_requests RandomMultiModalDataset ¶ Bases: RandomDataset Synthetic multimodal dataset (text + images) that extends RandomDataset. Status: - Images: supported via synthetic RGB data. - Video: supported via synthetic RGB data. - Audio: not yet supported. Sampling overview: 1) Number of items per request is sampled uniformly from the integer range [floor(n·(1−r)), ceil(n·(1+r))], where n is the base count and r is num_mm_items_range_ratio in [0, 1]. r=0 keeps it fixed; r=1 allows 0. The maximum is further clamped to the sum of per-modality limits. 2) Each item’s modality and shape is sampled from bucket_config, a dict mapping (height, width, num_frames) → probability. We treat num_frames=1 as image and num_frames > 1 as video. Entries with zero probability are removed and the rest are renormalized to sum to 1. 3) Per-modality hard caps are enforced via limit_mm_per_prompt. When a modality reaches its cap, all of its buckets are excluded and the remaining probabilities are renormalized. Example bucket configuration: {(256, 256, 1): 0.5, (720, 1280, 1): 0.4, (720, 1280, 16): 0.1} - Two image buckets (num_frames=1) and one video bucket (num_frames=16). OBS.: Only image sampling is supported for now. Source code in vllm/benchmarks/datasets.py 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203class RandomMultiModalDataset(RandomDataset): \"\"\" Synthetic multimodal dataset (text + images) that extends RandomDataset. Status: - Images: supported via synthetic RGB data. - Video: supported via synthetic RGB data. - Audio: not yet supported. Sampling overview: 1) Number of items per request is sampled uniformly from the integer range [floor(n·(1−r)), ceil(n·(1+r))], where n is the base count and r is `num_mm_items_range_ratio` in [0, 1]. r=0 keeps it fixed; r=1 allows 0. The maximum is further clamped to the sum of per-modality limits. 2) Each item’s modality and shape is sampled from `bucket_config`, a dict mapping (height, width, num_frames) → probability. We treat `num_frames`=1 as image and `num_frames` > 1 as video. Entries with zero probability are removed and the rest are renormalized to sum to 1. 3) Per-modality hard caps are enforced via `limit_mm_per_prompt`. When a modality reaches its cap, all of its buckets are excluded and the remaining probabilities are renormalized. Example bucket configuration: {(256, 256, 1): 0.5, (720, 1280, 1): 0.4, (720, 1280, 16): 0.1} - Two image buckets (`num_frames`=1) and one video bucket (`num_frames`=16). OBS.: Only image sampling is supported for now. \"\"\" IS_MULTIMODAL = True DEFAULT_LIMIT_MM_PER_PROMPT = {\"image\": 255, \"video\": 1} DEFAULT_BASE_ITEMS_PER_REQUEST = 1 DEFAULT_NUM_MM_ITEMS_RANGE_RATIO = 0.0 DEFAULT_MM_ITEM_BUCKET_CONFIG = { (256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0, } DEFAULT_ENABLE_MULTIMODAL_CHAT = False def __init__(self, **kwargs) -> None: super().__init__(**kwargs) def generate_synthetic_image(self, width: int, height: int) -> Image.Image: \"\"\"Generate synthetic PIL image with random RGB values. NOTE: iid pixel sampling results in worst-case compression (good for stressing I/O), but very unlike real photos. We could consider a “low-freq” mode (e.g., noise blur) to emulate network realism instead of max stress. \"\"\" random_pixels = self._rng.integers( 0, 256, (height, width, 3), dtype=np.uint8, ) return Image.fromarray(random_pixels) def generate_synthetic_video( self, width: int, height: int, num_frames: int ) -> dict: \"\"\"Generate synthetic video with random values. Creates a video with random pixel values, encodes it to MP4 format, and returns the content as bytes. \"\"\" import cv2 random_pixels = self._rng.integers( 0, 256, (num_frames, height, width, 3), dtype=np.uint8, ) # Create a temporary video file in memory fourcc = cv2.VideoWriter_fourcc(*\"mp4v\") fps = 30 # frames per second with NamedTemporaryFile(suffix=\".mp4\", delete_on_close=False) as temp_file: temp_path = temp_file.name # Create video writer video_writer = cv2.VideoWriter( temp_path, fourcc=fourcc, fps=fps, frameSize=(width, height) ) if not video_writer.isOpened(): raise RuntimeError(\"Failed to create video writer\") for frame in random_pixels: video_writer.write(frame) video_writer.release() temp_file.close() # Read the video file content with open(temp_path, \"rb\") as f: video_content = f.read() return {\"bytes\": video_content} def map_config_to_modality(self, config: tuple[int, int, int]) -> str: \"\"\"Map the configuration to the modality.\"\"\" if config[-1] == 1: return \"image\" elif config[-1] > 1: return \"video\" else: raise ValueError(f\"Invalid multimodal item configuration: {config}\") def normalize_bucket_config( self, bucket_config: dict[tuple[int, int, int], float] ) -> dict[tuple[int, int, int], float]: \"\"\" Remove zero probability entries and normalize the bucket config to sum to 1. \"\"\" # Raise error if value is negative if any(v < 0 for v in bucket_config.values()): raise ValueError(\"Bucket config values must be non-negative.\") # Remove zero probability entries bucket_config = {k: v for k, v in bucket_config.items() if v > 0} # if bucket config is empty, raise error if not bucket_config: raise ValueError( \"Got invalid bucket config. Bucket config values must be non-zero.\" ) # Normalize the remaining bucket config to sum to 1 total = sum(bucket_config.values()) return {k: v / total for k, v in bucket_config.items()} def generate_mm_item( self, mm_item_config: tuple[int, int, int], ) -> Mapping[str, Any]: \"\"\" Create synthetic images and videos and apply process_image/process_video respectively. This follows the OpenAI API chat completions https://github.com/openai/openai-python \"\"\" if self.map_config_to_modality(mm_item_config) == \"image\": return process_image( self.generate_synthetic_image(mm_item_config[1], mm_item_config[0]) ) elif self.map_config_to_modality(mm_item_config) == \"video\": return process_video( self.generate_synthetic_video( mm_item_config[1], mm_item_config[0], mm_item_config[2] ) ) else: raise ValueError(f\"Invalid multimodal item configuration: {mm_item_config}\") def get_mm_item_sampling_params( self, base_items_per_request: int, num_mm_items_range_ratio: float, limit_mm_per_prompt: dict[str, int], bucket_config: dict[tuple[int, int, int], float], ) -> tuple[int, int, dict[str, int], dict[tuple[int, int, int], float]]: \"\"\" Get the sampling parameters for the multimodal items. \"\"\" # Enforce num_mm_items_range_ratio <= 1 if not (0.0 <= num_mm_items_range_ratio <= 1.0): raise ValueError(\"num_mm_items_range_ratio must be in [0, 1].\") # Ensure modalities to sample are in limit_mm_per_prompt for k, v in bucket_config.items(): # get modality from bucket config modality = self.map_config_to_modality(k) if modality not in limit_mm_per_prompt: raise ValueError( f\"Modality {modality} is not in \" f\"limit_mm_per_prompt: \" f\"{limit_mm_per_prompt.keys()}\" ) # Remove zero probability entries # and normalize bucket config to sum to 1 bucket_config = self.normalize_bucket_config(bucket_config) logger.info( \"Normalized bucket config: %s\", bucket_config, ) # Only consider limit per prompt for modalities in bucket config allowed_modalities = {self.map_config_to_modality(cfg) for cfg in bucket_config} limit_mm_per_prompt = { k: v for k, v in limit_mm_per_prompt.items() if k in allowed_modalities } if not limit_mm_per_prompt: raise ValueError(\"No valid limits for modalities present in bucket_config.\") logger.info( \"Updated mm-limit-per-prompt: %s\", limit_mm_per_prompt, ) # Get max and min num mm items and ensure # it is at most the sum of limit_mm_per_prompt for all modalities max_num_mm_items = min( sum(limit_mm_per_prompt.values()), math.ceil(base_items_per_request * (1 + num_mm_items_range_ratio)), ) # Ensure min num mm items is at least 0 min_num_mm_items = max( 0, math.floor(base_items_per_request * (1 - num_mm_items_range_ratio)) ) # Raise error if min num mm items is greater than max num mm items if min_num_mm_items > max_num_mm_items: raise ValueError( f\"Min num mm items is greater than max mm items: \" f\"{min_num_mm_items} > {max_num_mm_items}\" ) logger.info( \"Sampling number of multimodal items from [%s, %s]\", min_num_mm_items, max_num_mm_items, ) return ( min_num_mm_items, max_num_mm_items, limit_mm_per_prompt, bucket_config, ) def get_mm_item_iterator( self, min_num_mm_items: int, max_num_mm_items: int, bucket_config: dict[tuple[int, int, int], float], limit_mm_per_prompt: dict[str, int], ) -> Iterator[tuple[int, int, int]]: \"\"\" Iterator over the multimodal items for each request whose size is between min_num_mm_items and max_num_mm_items. Loop over the bucket config and sample a multimodal item. Loop until the number of multimodal items sampled is equal to request_num_mm_items or limit of multimodal items per prompt for all modalities is reached. Note: - This function operates on a per-request shallow copy of `bucket_config` (tuple->float). The original dict passed to `sample` is not mutated. If this ever changes, a test is implemented and will fail. \"\"\" # Get the number of multimodal items to sample request_num_mm_items = int( self._rng.integers(min_num_mm_items, max_num_mm_items + 1) ) # If request_num_mm_items is 0, yield an empty iterator if request_num_mm_items == 0: return # Initialize modality counters modality_counter = {self.map_config_to_modality(k): 0 for k in bucket_config} # Copy the bucket config to avoid modifying the original bucket_config_copy = bucket_config.copy() # Loop over the number of multimodal items to sample while sum(modality_counter.values()) < request_num_mm_items: # Sample a multimodal item config mm_item_config = self._rng.choice( list(bucket_config_copy.keys()), p=list(bucket_config_copy.values()) ) modality = self.map_config_to_modality(mm_item_config) # Check that modality count is less than limit per prompt if modality_counter[modality] < limit_mm_per_prompt[modality]: modality_counter[modality] += 1 yield (mm_item_config) else: # If the counter is greater than the limit per prompt # set all multimodal items of this modality to 0 for k, v in bucket_config_copy.items(): if self.map_config_to_modality(k) == modality: bucket_config_copy[k] = 0 # If all configs are 0, break the loop # This should not happen as request_num_mm_items is at most # the sum of limit_mm_per_prompt for all modalities if all(v == 0 for v in bucket_config_copy.values()): logger.warning( \"Exhausted all multimodal items of modality %s\", modality ) break # Renormalize the bucket config bucket_config_copy = self.normalize_bucket_config(bucket_config_copy) def sample( self, tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, prefix_len: int = RandomDataset.DEFAULT_PREFIX_LEN, range_ratio: float = RandomDataset.DEFAULT_RANGE_RATIO, input_len: int = RandomDataset.DEFAULT_INPUT_LEN, output_len: int = RandomDataset.DEFAULT_OUTPUT_LEN, limit_mm_per_prompt: dict[str, int] = DEFAULT_LIMIT_MM_PER_PROMPT, base_items_per_request: int = DEFAULT_BASE_ITEMS_PER_REQUEST, num_mm_items_range_ratio: float = DEFAULT_NUM_MM_ITEMS_RANGE_RATIO, bucket_config: dict[ tuple[int, int, int], float ] = DEFAULT_MM_ITEM_BUCKET_CONFIG, enable_multimodal_chat: bool = DEFAULT_ENABLE_MULTIMODAL_CHAT, **kwargs, ) -> list[SampleRequest]: # Get the sampling parameters for the dataset input_lens, output_lens, offsets = self.get_sampling_params( num_requests, range_ratio, input_len, output_len, tokenizer ) ( min_num_mm_items, max_num_mm_items, limit_mm_per_prompt, bucket_config, ) = self.get_mm_item_sampling_params( base_items_per_request, num_mm_items_range_ratio, limit_mm_per_prompt, bucket_config, ) vocab_size = tokenizer.vocab_size # Can't use tokenizer.all_special_ids since # it returns ONLY ids from special_tokens_map.json # We want to exclude placeholder tokens and all # tokens that indicate start/end of image as it # may break prompt replacement logic. prohibited_tokens = list( tok_id for tok_id, token in tokenizer.added_tokens_decoder.items() if token.special ) all_tokens = np.arange(vocab_size) allowed_tokens = np.array(list(set(all_tokens) - set(prohibited_tokens))) logger.debug( \"Sampling from %d out of %d (vocab size)\", len(allowed_tokens), vocab_size ) # Generate prefix once prefix_token_ids = self.get_prefix(allowed_tokens, prefix_len) # Add synthetic multimodal items to each request mm_requests = [] token_mismatch_total = 0 for i in range(num_requests): prompt, total_input_len, token_mismatch = self.generate_token_sequence( # noqa: E501 tokenizer=tokenizer, prefix_token_ids=prefix_token_ids, prefix_len=prefix_len, vocab_size=vocab_size, input_len=int(input_lens[i]), offset=int(offsets[i]), index=i, allowed_tokens=allowed_tokens, ) token_mismatch_total += token_mismatch # Get multimodal item iterator for a given request mm_item_iterator = self.get_mm_item_iterator( min_num_mm_items, max_num_mm_items, bucket_config, limit_mm_per_prompt, ) mm_content = cast( list[dict[str, Any]], [ self.generate_mm_item(mm_item_config) for mm_item_config in mm_item_iterator ], ) if enable_multimodal_chat: # NOTE: For now this option is only provided for completeness # given that the serve.py benchmark currently does not use it. mm_chat_prompt: Any = prompt mm_chat_prompt = self.apply_multimodal_chat_transformation( prompt, mm_content ) sample_request = SampleRequest( prompt=mm_chat_prompt, prompt_len=total_input_len, expected_output_len=int(output_lens[i]), multi_modal_data=None, request_id=request_id_prefix + str(i), ) else: sample_request = SampleRequest( prompt=prompt, prompt_len=total_input_len, expected_output_len=int(output_lens[i]), multi_modal_data=mm_content, request_id=request_id_prefix + str(i), ) mm_requests.append(sample_request) if token_mismatch_total != 0: sign = \"more\" if token_mismatch_total > 0 else \"fewer\" logger.warning( \"Across all generated prompts, there were %d %s tokens \" \"than expected after decoding and re-encoding. This is \" \"expected due to the imperfect nature of the sampling \" \"procedure.\", abs(token_mismatch_total), sign, ) return mm_requests DEFAULT_BASE_ITEMS_PER_REQUEST class-attribute instance-attribute ¶ DEFAULT_BASE_ITEMS_PER_REQUEST = 1 DEFAULT_ENABLE_MULTIMODAL_CHAT class-attribute instance-attribute ¶ DEFAULT_ENABLE_MULTIMODAL_CHAT = False DEFAULT_LIMIT_MM_PER_PROMPT class-attribute instance-attribute ¶ DEFAULT_LIMIT_MM_PER_PROMPT = {'image': 255, 'video': 1} DEFAULT_MM_ITEM_BUCKET_CONFIG class-attribute instance-attribute ¶ DEFAULT_MM_ITEM_BUCKET_CONFIG = { (256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0, } DEFAULT_NUM_MM_ITEMS_RANGE_RATIO class-attribute instance-attribute ¶ DEFAULT_NUM_MM_ITEMS_RANGE_RATIO = 0.0 IS_MULTIMODAL class-attribute instance-attribute ¶ IS_MULTIMODAL = True __init__ ¶ __init__(**kwargs) -> None Source code in vllm/benchmarks/datasets.py 830 831def __init__(self, **kwargs) -> None: super().__init__(**kwargs) generate_mm_item ¶ generate_mm_item( mm_item_config: tuple[int, int, int], ) -> Mapping[str, Any] Create synthetic images and videos and apply process_image/process_video respectively. This follows the OpenAI API chat completions https://github.com/openai/openai-python Source code in vllm/benchmarks/datasets.py 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945def generate_mm_item( self, mm_item_config: tuple[int, int, int], ) -> Mapping[str, Any]: \"\"\" Create synthetic images and videos and apply process_image/process_video respectively. This follows the OpenAI API chat completions https://github.com/openai/openai-python \"\"\" if self.map_config_to_modality(mm_item_config) == \"image\": return process_image( self.generate_synthetic_image(mm_item_config[1], mm_item_config[0]) ) elif self.map_config_to_modality(mm_item_config) == \"video\": return process_video( self.generate_synthetic_video( mm_item_config[1], mm_item_config[0], mm_item_config[2] ) ) else: raise ValueError(f\"Invalid multimodal item configuration: {mm_item_config}\") generate_synthetic_image ¶ generate_synthetic_image(width: int, height: int) -> Image Generate synthetic PIL image with random RGB values. NOTE: iid pixel sampling results in worst-case compression (good for stressing I/O), but very unlike real photos. We could consider a “low-freq” mode (e.g., noise blur) to emulate network realism instead of max stress. Source code in vllm/benchmarks/datasets.py 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847def generate_synthetic_image(self, width: int, height: int) -> Image.Image: \"\"\"Generate synthetic PIL image with random RGB values. NOTE: iid pixel sampling results in worst-case compression (good for stressing I/O), but very unlike real photos. We could consider a “low-freq” mode (e.g., noise blur) to emulate network realism instead of max stress. \"\"\" random_pixels = self._rng.integers( 0, 256, (height, width, 3), dtype=np.uint8, ) return Image.fromarray(random_pixels) generate_synthetic_video ¶ generate_synthetic_video( width: int, height: int, num_frames: int ) -> dict Generate synthetic video with random values. Creates a video with random pixel values, encodes it to MP4 format, and returns the content as bytes. Source code in vllm/benchmarks/datasets.py 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891def generate_synthetic_video( self, width: int, height: int, num_frames: int ) -> dict: \"\"\"Generate synthetic video with random values. Creates a video with random pixel values, encodes it to MP4 format, and returns the content as bytes. \"\"\" import cv2 random_pixels = self._rng.integers( 0, 256, (num_frames, height, width, 3), dtype=np.uint8, ) # Create a temporary video file in memory fourcc = cv2.VideoWriter_fourcc(*\"mp4v\") fps = 30 # frames per second with NamedTemporaryFile(suffix=\".mp4\", delete_on_close=False) as temp_file: temp_path = temp_file.name # Create video writer video_writer = cv2.VideoWriter( temp_path, fourcc=fourcc, fps=fps, frameSize=(width, height) ) if not video_writer.isOpened(): raise RuntimeError(\"Failed to create video writer\") for frame in random_pixels: video_writer.write(frame) video_writer.release() temp_file.close() # Read the video file content with open(temp_path, \"rb\") as f: video_content = f.read() return {\"bytes\": video_content} get_mm_item_iterator ¶ get_mm_item_iterator( min_num_mm_items: int, max_num_mm_items: int, bucket_config: dict[tuple[int, int, int], float], limit_mm_per_prompt: dict[str, int], ) -> Iterator[tuple[int, int, int]] Iterator over the multimodal items for each request whose size is between min_num_mm_items and max_num_mm_items. Loop over the bucket config and sample a multimodal item. Loop until the number of multimodal items sampled is equal to request_num_mm_items or limit of multimodal items per prompt for all modalities is reached. Note: - This function operates on a per-request shallow copy of bucket_config (tuple->float). The original dict passed to sample is not mutated. If this ever changes, a test is implemented and will fail. Source code in vllm/benchmarks/datasets.py 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081def get_mm_item_iterator( self, min_num_mm_items: int, max_num_mm_items: int, bucket_config: dict[tuple[int, int, int], float], limit_mm_per_prompt: dict[str, int], ) -> Iterator[tuple[int, int, int]]: \"\"\" Iterator over the multimodal items for each request whose size is between min_num_mm_items and max_num_mm_items. Loop over the bucket config and sample a multimodal item. Loop until the number of multimodal items sampled is equal to request_num_mm_items or limit of multimodal items per prompt for all modalities is reached. Note: - This function operates on a per-request shallow copy of `bucket_config` (tuple->float). The original dict passed to `sample` is not mutated. If this ever changes, a test is implemented and will fail. \"\"\" # Get the number of multimodal items to sample request_num_mm_items = int( self._rng.integers(min_num_mm_items, max_num_mm_items + 1) ) # If request_num_mm_items is 0, yield an empty iterator if request_num_mm_items == 0: return # Initialize modality counters modality_counter = {self.map_config_to_modality(k): 0 for k in bucket_config} # Copy the bucket config to avoid modifying the original bucket_config_copy = bucket_config.copy() # Loop over the number of multimodal items to sample while sum(modality_counter.values()) < request_num_mm_items: # Sample a multimodal item config mm_item_config = self._rng.choice( list(bucket_config_copy.keys()), p=list(bucket_config_copy.values()) ) modality = self.map_config_to_modality(mm_item_config) # Check that modality count is less than limit per prompt if modality_counter[modality] < limit_mm_per_prompt[modality]: modality_counter[modality] += 1 yield (mm_item_config) else: # If the counter is greater than the limit per prompt # set all multimodal items of this modality to 0 for k, v in bucket_config_copy.items(): if self.map_config_to_modality(k) == modality: bucket_config_copy[k] = 0 # If all configs are 0, break the loop # This should not happen as request_num_mm_items is at most # the sum of limit_mm_per_prompt for all modalities if all(v == 0 for v in bucket_config_copy.values()): logger.warning( \"Exhausted all multimodal items of modality %s\", modality ) break # Renormalize the bucket config bucket_config_copy = self.normalize_bucket_config(bucket_config_copy) get_mm_item_sampling_params ¶ get_mm_item_sampling_params( base_items_per_request: int, num_mm_items_range_ratio: float, limit_mm_per_prompt: dict[str, int], bucket_config: dict[tuple[int, int, int], float], ) -> tuple[ int, int, dict[str, int], dict[tuple[int, int, int], float], ] Get the sampling parameters for the multimodal items. Source code in vllm/benchmarks/datasets.py 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020def get_mm_item_sampling_params( self, base_items_per_request: int, num_mm_items_range_ratio: float, limit_mm_per_prompt: dict[str, int], bucket_config: dict[tuple[int, int, int], float], ) -> tuple[int, int, dict[str, int], dict[tuple[int, int, int], float]]: \"\"\" Get the sampling parameters for the multimodal items. \"\"\" # Enforce num_mm_items_range_ratio <= 1 if not (0.0 <= num_mm_items_range_ratio <= 1.0): raise ValueError(\"num_mm_items_range_ratio must be in [0, 1].\") # Ensure modalities to sample are in limit_mm_per_prompt for k, v in bucket_config.items(): # get modality from bucket config modality = self.map_config_to_modality(k) if modality not in limit_mm_per_prompt: raise ValueError( f\"Modality {modality} is not in \" f\"limit_mm_per_prompt: \" f\"{limit_mm_per_prompt.keys()}\" ) # Remove zero probability entries # and normalize bucket config to sum to 1 bucket_config = self.normalize_bucket_config(bucket_config) logger.info( \"Normalized bucket config: %s\", bucket_config, ) # Only consider limit per prompt for modalities in bucket config allowed_modalities = {self.map_config_to_modality(cfg) for cfg in bucket_config} limit_mm_per_prompt = { k: v for k, v in limit_mm_per_prompt.items() if k in allowed_modalities } if not limit_mm_per_prompt: raise ValueError(\"No valid limits for modalities present in bucket_config.\") logger.info( \"Updated mm-limit-per-prompt: %s\", limit_mm_per_prompt, ) # Get max and min num mm items and ensure # it is at most the sum of limit_mm_per_prompt for all modalities max_num_mm_items = min( sum(limit_mm_per_prompt.values()), math.ceil(base_items_per_request * (1 + num_mm_items_range_ratio)), ) # Ensure min num mm items is at least 0 min_num_mm_items = max( 0, math.floor(base_items_per_request * (1 - num_mm_items_range_ratio)) ) # Raise error if min num mm items is greater than max num mm items if min_num_mm_items > max_num_mm_items: raise ValueError( f\"Min num mm items is greater than max mm items: \" f\"{min_num_mm_items} > {max_num_mm_items}\" ) logger.info( \"Sampling number of multimodal items from [%s, %s]\", min_num_mm_items, max_num_mm_items, ) return ( min_num_mm_items, max_num_mm_items, limit_mm_per_prompt, bucket_config, ) map_config_to_modality ¶ map_config_to_modality(config: tuple[int, int, int]) -> str Map the configuration to the modality. Source code in vllm/benchmarks/datasets.py 893 894 895 896 897 898 899 900def map_config_to_modality(self, config: tuple[int, int, int]) -> str: \"\"\"Map the configuration to the modality.\"\"\" if config[-1] == 1: return \"image\" elif config[-1] > 1: return \"video\" else: raise ValueError(f\"Invalid multimodal item configuration: {config}\") normalize_bucket_config ¶ normalize_bucket_config( bucket_config: dict[tuple[int, int, int], float], ) -> dict[tuple[int, int, int], float] Remove zero probability entries and normalize the bucket config to sum to 1. Source code in vllm/benchmarks/datasets.py 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921def normalize_bucket_config( self, bucket_config: dict[tuple[int, int, int], float] ) -> dict[tuple[int, int, int], float]: \"\"\" Remove zero probability entries and normalize the bucket config to sum to 1. \"\"\" # Raise error if value is negative if any(v < 0 for v in bucket_config.values()): raise ValueError(\"Bucket config values must be non-negative.\") # Remove zero probability entries bucket_config = {k: v for k, v in bucket_config.items() if v > 0} # if bucket config is empty, raise error if not bucket_config: raise ValueError( \"Got invalid bucket config. Bucket config values must be non-zero.\" ) # Normalize the remaining bucket config to sum to 1 total = sum(bucket_config.values()) return {k: v / total for k, v in bucket_config.items()} sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, prefix_len: int = DEFAULT_PREFIX_LEN, range_ratio: float = DEFAULT_RANGE_RATIO, input_len: int = DEFAULT_INPUT_LEN, output_len: int = DEFAULT_OUTPUT_LEN, limit_mm_per_prompt: dict[ str, int ] = DEFAULT_LIMIT_MM_PER_PROMPT, base_items_per_request: int = DEFAULT_BASE_ITEMS_PER_REQUEST, num_mm_items_range_ratio: float = DEFAULT_NUM_MM_ITEMS_RANGE_RATIO, bucket_config: dict[ tuple[int, int, int], float ] = DEFAULT_MM_ITEM_BUCKET_CONFIG, enable_multimodal_chat: bool = DEFAULT_ENABLE_MULTIMODAL_CHAT, **kwargs, ) -> list[SampleRequest] Source code in vllm/benchmarks/datasets.py 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203def sample( self, tokenizer: TokenizerLike, num_requests: int, request_id_prefix: str = \"\", no_oversample: bool = False, prefix_len: int = RandomDataset.DEFAULT_PREFIX_LEN, range_ratio: float = RandomDataset.DEFAULT_RANGE_RATIO, input_len: int = RandomDataset.DEFAULT_INPUT_LEN, output_len: int = RandomDataset.DEFAULT_OUTPUT_LEN, limit_mm_per_prompt: dict[str, int] = DEFAULT_LIMIT_MM_PER_PROMPT, base_items_per_request: int = DEFAULT_BASE_ITEMS_PER_REQUEST, num_mm_items_range_ratio: float = DEFAULT_NUM_MM_ITEMS_RANGE_RATIO, bucket_config: dict[ tuple[int, int, int], float ] = DEFAULT_MM_ITEM_BUCKET_CONFIG, enable_multimodal_chat: bool = DEFAULT_ENABLE_MULTIMODAL_CHAT, **kwargs, ) -> list[SampleRequest]: # Get the sampling parameters for the dataset input_lens, output_lens, offsets = self.get_sampling_params( num_requests, range_ratio, input_len, output_len, tokenizer ) ( min_num_mm_items, max_num_mm_items, limit_mm_per_prompt, bucket_config, ) = self.get_mm_item_sampling_params( base_items_per_request, num_mm_items_range_ratio, limit_mm_per_prompt, bucket_config, ) vocab_size = tokenizer.vocab_size # Can't use tokenizer.all_special_ids since # it returns ONLY ids from special_tokens_map.json # We want to exclude placeholder tokens and all # tokens that indicate start/end of image as it # may break prompt replacement logic. prohibited_tokens = list( tok_id for tok_id, token in tokenizer.added_tokens_decoder.items() if token.special ) all_tokens = np.arange(vocab_size) allowed_tokens = np.array(list(set(all_tokens) - set(prohibited_tokens))) logger.debug( \"Sampling from %d out of %d (vocab size)\", len(allowed_tokens), vocab_size ) # Generate prefix once prefix_token_ids = self.get_prefix(allowed_tokens, prefix_len) # Add synthetic multimodal items to each request mm_requests = [] token_mismatch_total = 0 for i in range(num_requests): prompt, total_input_len, token_mismatch = self.generate_token_sequence( # noqa: E501 tokenizer=tokenizer, prefix_token_ids=prefix_token_ids, prefix_len=prefix_len, vocab_size=vocab_size, input_len=int(input_lens[i]), offset=int(offsets[i]), index=i, allowed_tokens=allowed_tokens, ) token_mismatch_total += token_mismatch # Get multimodal item iterator for a given request mm_item_iterator = self.get_mm_item_iterator( min_num_mm_items, max_num_mm_items, bucket_config, limit_mm_per_prompt, ) mm_content = cast( list[dict[str, Any]], [ self.generate_mm_item(mm_item_config) for mm_item_config in mm_item_iterator ], ) if enable_multimodal_chat: # NOTE: For now this option is only provided for completeness # given that the serve.py benchmark currently does not use it. mm_chat_prompt: Any = prompt mm_chat_prompt = self.apply_multimodal_chat_transformation( prompt, mm_content ) sample_request = SampleRequest( prompt=mm_chat_prompt, prompt_len=total_input_len, expected_output_len=int(output_lens[i]), multi_modal_data=None, request_id=request_id_prefix + str(i), ) else: sample_request = SampleRequest( prompt=prompt, prompt_len=total_input_len, expected_output_len=int(output_lens[i]), multi_modal_data=mm_content, request_id=request_id_prefix + str(i), ) mm_requests.append(sample_request) if token_mismatch_total != 0: sign = \"more\" if token_mismatch_total > 0 else \"fewer\" logger.warning( \"Across all generated prompts, there were %d %s tokens \" \"than expected after decoding and re-encoding. This is \" \"expected due to the imperfect nature of the sampling \" \"procedure.\", abs(token_mismatch_total), sign, ) return mm_requests SampleRequest dataclass ¶ Represents a single inference request for benchmarking. Source code in vllm/benchmarks/datasets.py 72 73 74 75 76 77 78 79 80 81 82 83@dataclass class SampleRequest: \"\"\" Represents a single inference request for benchmarking. \"\"\" prompt: str | list[str] prompt_len: int expected_output_len: int multi_modal_data: MultiModalDataDict | dict | list[dict] | None = None lora_request: LoRARequest | None = None request_id: str | None = None expected_output_len instance-attribute ¶ expected_output_len: int lora_request class-attribute instance-attribute ¶ lora_request: LoRARequest | None = None multi_modal_data class-attribute instance-attribute ¶ multi_modal_data: ( MultiModalDataDict | dict | list[dict] | None ) = None prompt instance-attribute ¶ prompt: str | list[str] prompt_len instance-attribute ¶ prompt_len: int request_id class-attribute instance-attribute ¶ request_id: str | None = None __init__ ¶ __init__( prompt: str | list[str], prompt_len: int, expected_output_len: int, multi_modal_data: MultiModalDataDict | dict | list[dict] | None = None, lora_request: LoRARequest | None = None, request_id: str | None = None, ) -> None ShareGPTDataset ¶ Bases: BenchmarkDataset Implements the ShareGPT dataset. Loads data from a JSON file and generates sample requests based on conversation turns. Source code in vllm/benchmarks/datasets.py 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294class ShareGPTDataset(BenchmarkDataset): \"\"\" Implements the ShareGPT dataset. Loads data from a JSON file and generates sample requests based on conversation turns. \"\"\" def __init__(self, **kwargs) -> None: super().__init__(**kwargs) self.load_data() def load_data(self) -> None: if self.dataset_path is None: raise ValueError(\"dataset_path must be provided for loading data.\") with open(self.dataset_path, encoding=\"utf-8\") as f: self.data = json.load(f) # Filter entries with at least two conversation turns. self.data = [ entry for entry in self.data if \"conversations\" in entry and len(entry[\"conversations\"]) >= 2 ] random.seed(self.random_seed) if not getattr(self, \"disable_shuffle\", False): random.shuffle(self.data) def sample( self, tokenizer: TokenizerLike, num_requests: int, lora_path: str | None = None, max_loras: int | None = None, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: samples: list = [] ind = 0 for entry in self.data: if len(samples) >= num_requests: break prompt, completion = ( entry[\"conversations\"][0][\"value\"], entry[\"conversations\"][1][\"value\"], ) lora_request = self.get_random_lora_request( max_loras=max_loras, lora_path=lora_path ) prompt_ids = tokenizer(prompt).input_ids completion_ids = tokenizer(completion).input_ids prompt_len = len(prompt_ids) new_output_len = len(completion_ids) if output_len is None else output_len if not is_valid_sequence( prompt_len, new_output_len, skip_min_output_len_check=output_len is not None, ): continue if image_path := entry.get(\"image\"): mm_content = process_image(image_path) elif video_path := entry.get(\"video\"): mm_content = process_video(video_path) else: mm_content = None if enable_multimodal_chat: prompt = self.apply_multimodal_chat_transformation(prompt, mm_content) samples.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=new_output_len, lora_request=lora_request, multi_modal_data=mm_content, request_id=request_id_prefix + str(ind), ) ) ind += 1 self.maybe_oversample_requests( samples, num_requests, request_id_prefix, no_oversample ) return samples __init__ ¶ __init__(**kwargs) -> None Source code in vllm/benchmarks/datasets.py 1217 1218 1219def __init__(self, **kwargs) -> None: super().__init__(**kwargs) self.load_data() load_data ¶ load_data() -> None Source code in vllm/benchmarks/datasets.py 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235def load_data(self) -> None: if self.dataset_path is None: raise ValueError(\"dataset_path must be provided for loading data.\") with open(self.dataset_path, encoding=\"utf-8\") as f: self.data = json.load(f) # Filter entries with at least two conversation turns. self.data = [ entry for entry in self.data if \"conversations\" in entry and len(entry[\"conversations\"]) >= 2 ] random.seed(self.random_seed) if not getattr(self, \"disable_shuffle\", False): random.shuffle(self.data) sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, lora_path: str | None = None, max_loras: int | None = None, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294def sample( self, tokenizer: TokenizerLike, num_requests: int, lora_path: str | None = None, max_loras: int | None = None, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: samples: list = [] ind = 0 for entry in self.data: if len(samples) >= num_requests: break prompt, completion = ( entry[\"conversations\"][0][\"value\"], entry[\"conversations\"][1][\"value\"], ) lora_request = self.get_random_lora_request( max_loras=max_loras, lora_path=lora_path ) prompt_ids = tokenizer(prompt).input_ids completion_ids = tokenizer(completion).input_ids prompt_len = len(prompt_ids) new_output_len = len(completion_ids) if output_len is None else output_len if not is_valid_sequence( prompt_len, new_output_len, skip_min_output_len_check=output_len is not None, ): continue if image_path := entry.get(\"image\"): mm_content = process_image(image_path) elif video_path := entry.get(\"video\"): mm_content = process_video(video_path) else: mm_content = None if enable_multimodal_chat: prompt = self.apply_multimodal_chat_transformation(prompt, mm_content) samples.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=new_output_len, lora_request=lora_request, multi_modal_data=mm_content, request_id=request_id_prefix + str(ind), ) ) ind += 1 self.maybe_oversample_requests( samples, num_requests, request_id_prefix, no_oversample ) return samples SonnetDataset ¶ Bases: BenchmarkDataset Simplified implementation of the Sonnet dataset. Loads poem lines from a text file and generates sample requests. Default values here copied from benchmark_serving.py for the sonnet dataset. Source code in vllm/benchmarks/datasets.py 2079 2080 2081 2082 2083 2084 2085 2086 2087 2088 2089 2090 2091 2092 2093 2094 2095 2096 2097 2098 2099 2100 2101 2102 2103 2104 2105 2106 2107 2108 2109 2110 2111 2112 2113 2114 2115 2116 2117 2118 2119 2120 2121 2122 2123 2124 2125 2126 2127 2128 2129 2130 2131 2132 2133 2134 2135 2136 2137 2138 2139 2140 2141 2142 2143 2144 2145 2146 2147 2148 2149 2150 2151 2152 2153 2154 2155 2156 2157 2158 2159 2160 2161 2162@deprecated( \"SonnetDataset is deprecated and will be removed in a future version.\", ) class SonnetDataset(BenchmarkDataset): \"\"\" Simplified implementation of the Sonnet dataset. Loads poem lines from a text file and generates sample requests. Default values here copied from `benchmark_serving.py` for the sonnet dataset. \"\"\" DEFAULT_PREFIX_LEN = 200 DEFAULT_INPUT_LEN = 550 DEFAULT_OUTPUT_LEN = 150 def __init__( self, **kwargs, ) -> None: super().__init__(**kwargs) self.load_data() def load_data(self) -> None: if not self.dataset_path: raise ValueError(\"dataset_path must be provided.\") with open(self.dataset_path, encoding=\"utf-8\") as f: self.data = f.readlines() def sample( self, tokenizer: TokenizerLike, num_requests: int, prefix_len: int = DEFAULT_PREFIX_LEN, input_len: int = DEFAULT_INPUT_LEN, output_len: int = DEFAULT_OUTPUT_LEN, return_prompt_formatted: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: # Calculate average token length for a poem line. tokenized_lines = [tokenizer(line).input_ids for line in self.data] avg_len = sum(len(tokens) for tokens in tokenized_lines) / len(tokenized_lines) # Build the base prompt. base_prompt = \"Pick as many lines as you can from these poem lines:\\n\" base_msg = [{\"role\": \"user\", \"content\": base_prompt}] base_fmt = tokenizer.apply_chat_template( base_msg, add_generation_prompt=True, tokenize=False ) base_offset = len(tokenizer(base_fmt).input_ids) if input_len <= base_offset: raise ValueError( f\"'input_len' must be higher than the base prompt length \" f\"({base_offset}).\" ) # Determine how many poem lines to use. num_input_lines = round((input_len - base_offset) / avg_len) num_prefix_lines = max(round((prefix_len - base_offset) / avg_len), 0) prefix_lines = self.data[:num_prefix_lines] samples = [] ind = 0 while len(samples) < num_requests: extra_lines = random.choices( self.data, k=num_input_lines - num_prefix_lines ) prompt = f\"{base_prompt}{''.join(prefix_lines + extra_lines)}\" msg = [{\"role\": \"user\", \"content\": prompt}] prompt_formatted = tokenizer.apply_chat_template( msg, add_generation_prompt=True, tokenize=False ) prompt_len = len(tokenizer(prompt_formatted).input_ids) if prompt_len <= input_len: samples.append( SampleRequest( prompt=prompt_formatted if return_prompt_formatted else prompt, prompt_len=prompt_len, expected_output_len=output_len, request_id=request_id_prefix + str(ind), ) ) ind += 1 return samples DEFAULT_INPUT_LEN class-attribute instance-attribute ¶ DEFAULT_INPUT_LEN = 550 DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶ DEFAULT_OUTPUT_LEN = 150 DEFAULT_PREFIX_LEN class-attribute instance-attribute ¶ DEFAULT_PREFIX_LEN = 200 __init__ ¶ __init__(**kwargs) -> None Source code in vllm/benchmarks/datasets.py 2093 2094 2095 2096 2097 2098def __init__( self, **kwargs, ) -> None: super().__init__(**kwargs) self.load_data() load_data ¶ load_data() -> None Source code in vllm/benchmarks/datasets.py 2100 2101 2102 2103 2104def load_data(self) -> None: if not self.dataset_path: raise ValueError(\"dataset_path must be provided.\") with open(self.dataset_path, encoding=\"utf-8\") as f: self.data = f.readlines() sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, prefix_len: int = DEFAULT_PREFIX_LEN, input_len: int = DEFAULT_INPUT_LEN, output_len: int = DEFAULT_OUTPUT_LEN, return_prompt_formatted: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 2106 2107 2108 2109 2110 2111 2112 2113 2114 2115 2116 2117 2118 2119 2120 2121 2122 2123 2124 2125 2126 2127 2128 2129 2130 2131 2132 2133 2134 2135 2136 2137 2138 2139 2140 2141 2142 2143 2144 2145 2146 2147 2148 2149 2150 2151 2152 2153 2154 2155 2156 2157 2158 2159 2160 2161 2162def sample( self, tokenizer: TokenizerLike, num_requests: int, prefix_len: int = DEFAULT_PREFIX_LEN, input_len: int = DEFAULT_INPUT_LEN, output_len: int = DEFAULT_OUTPUT_LEN, return_prompt_formatted: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: # Calculate average token length for a poem line. tokenized_lines = [tokenizer(line).input_ids for line in self.data] avg_len = sum(len(tokens) for tokens in tokenized_lines) / len(tokenized_lines) # Build the base prompt. base_prompt = \"Pick as many lines as you can from these poem lines:\\n\" base_msg = [{\"role\": \"user\", \"content\": base_prompt}] base_fmt = tokenizer.apply_chat_template( base_msg, add_generation_prompt=True, tokenize=False ) base_offset = len(tokenizer(base_fmt).input_ids) if input_len <= base_offset: raise ValueError( f\"'input_len' must be higher than the base prompt length \" f\"({base_offset}).\" ) # Determine how many poem lines to use. num_input_lines = round((input_len - base_offset) / avg_len) num_prefix_lines = max(round((prefix_len - base_offset) / avg_len), 0) prefix_lines = self.data[:num_prefix_lines] samples = [] ind = 0 while len(samples) < num_requests: extra_lines = random.choices( self.data, k=num_input_lines - num_prefix_lines ) prompt = f\"{base_prompt}{''.join(prefix_lines + extra_lines)}\" msg = [{\"role\": \"user\", \"content\": prompt}] prompt_formatted = tokenizer.apply_chat_template( msg, add_generation_prompt=True, tokenize=False ) prompt_len = len(tokenizer(prompt_formatted).input_ids) if prompt_len <= input_len: samples.append( SampleRequest( prompt=prompt_formatted if return_prompt_formatted else prompt, prompt_len=prompt_len, expected_output_len=output_len, request_id=request_id_prefix + str(ind), ) ) ind += 1 return samples SpecBench ¶ Bases: CustomDataset Implements the SpecBench dataset: https://github.com/hemingkx/Spec-Bench Download the dataset using: wget https://raw.githubusercontent.com/hemingkx/Spec-Bench/refs/heads/main/data/spec_bench/question.jsonl Source code in vllm/benchmarks/datasets.py 2034 2035 2036 2037 2038 2039 2040 2041 2042 2043 2044 2045 2046 2047 2048 2049 2050 2051 2052 2053 2054 2055 2056 2057 2058 2059 2060 2061 2062 2063 2064 2065 2066 2067 2068 2069 2070 2071class SpecBench(CustomDataset): \"\"\" Implements the SpecBench dataset: https://github.com/hemingkx/Spec-Bench Download the dataset using: wget https://raw.githubusercontent.com/hemingkx/Spec-Bench/refs/heads/main/data/spec_bench/question.jsonl \"\"\" # noqa: E501 def __init__(self, **kwargs) -> None: self.category = kwargs.pop(\"category\", None) super().__init__(**kwargs) self.load_data() def load_data(self) -> None: if self.dataset_path is None: raise ValueError(\"dataset_path must be provided for loading data.\") self.data = [] # Load the JSONL file jsonl_data = pd.read_json(path_or_buf=self.dataset_path, lines=True) # check if the JSONL file has a 'turns' column if \"turns\" not in jsonl_data.columns: raise ValueError(\"JSONL file must contain a 'turns' column.\") for _, row in jsonl_data.iterrows(): # sample only from a specific category if specified if (not self.category) or (self.category == row[\"category\"]): prompt = row[\"turns\"][0] self.data.append({\"prompt\": prompt}) random.seed(self.random_seed) if not getattr(self, \"disable_shuffle\", False): random.shuffle(self.data) def sample(self, **kwargs) -> list: # leverage CustomDataset sample return super().sample(**kwargs) category instance-attribute ¶ category = pop('category', None) __init__ ¶ __init__(**kwargs) -> None Source code in vllm/benchmarks/datasets.py 2041 2042 2043 2044def __init__(self, **kwargs) -> None: self.category = kwargs.pop(\"category\", None) super().__init__(**kwargs) self.load_data() load_data ¶ load_data() -> None Source code in vllm/benchmarks/datasets.py 2046 2047 2048 2049 2050 2051 2052 2053 2054 2055 2056 2057 2058 2059 2060 2061 2062 2063 2064 2065 2066 2067def load_data(self) -> None: if self.dataset_path is None: raise ValueError(\"dataset_path must be provided for loading data.\") self.data = [] # Load the JSONL file jsonl_data = pd.read_json(path_or_buf=self.dataset_path, lines=True) # check if the JSONL file has a 'turns' column if \"turns\" not in jsonl_data.columns: raise ValueError(\"JSONL file must contain a 'turns' column.\") for _, row in jsonl_data.iterrows(): # sample only from a specific category if specified if (not self.category) or (self.category == row[\"category\"]): prompt = row[\"turns\"][0] self.data.append({\"prompt\": prompt}) random.seed(self.random_seed) if not getattr(self, \"disable_shuffle\", False): random.shuffle(self.data) sample ¶ sample(**kwargs) -> list Source code in vllm/benchmarks/datasets.py 2069 2070 2071def sample(self, **kwargs) -> list: # leverage CustomDataset sample return super().sample(**kwargs) VisionArenaDataset ¶ Bases: HuggingFaceDataset Vision Arena Dataset. Source code in vllm/benchmarks/datasets.py 2409 2410 2411 2412 2413 2414 2415 2416 2417 2418 2419 2420 2421 2422 2423 2424 2425 2426 2427 2428 2429 2430 2431 2432 2433 2434 2435 2436 2437 2438 2439 2440 2441 2442 2443 2444 2445 2446 2447 2448 2449 2450 2451 2452 2453 2454 2455 2456 2457 2458 2459class VisionArenaDataset(HuggingFaceDataset): \"\"\" Vision Arena Dataset. \"\"\" DEFAULT_OUTPUT_LEN = 128 SUPPORTED_DATASET_PATHS = { \"lmarena-ai/VisionArena-Chat\": lambda x: x[\"conversation\"][0][0][\"content\"], \"lmarena-ai/vision-arena-bench-v0.1\": lambda x: x[\"turns\"][0][0][\"content\"], } IS_MULTIMODAL = True def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break parser_fn = self.SUPPORTED_DATASET_PATHS.get(self.hf_name) if parser_fn is None: raise ValueError(f\"Unsupported dataset path: {self.hf_name}\") prompt = parser_fn(item) mm_content = process_image(item[\"images\"][0]) prompt_len = len(tokenizer(prompt).input_ids) if enable_multimodal_chat: # Note: when chat is enabled the request prompt_len is no longer # accurate and we will be using request output to count the # actual prompt len prompt = self.apply_multimodal_chat_transformation(prompt, mm_content) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_content, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests DEFAULT_OUTPUT_LEN class-attribute instance-attribute ¶ DEFAULT_OUTPUT_LEN = 128 IS_MULTIMODAL class-attribute instance-attribute ¶ IS_MULTIMODAL = True SUPPORTED_DATASET_PATHS class-attribute instance-attribute ¶ SUPPORTED_DATASET_PATHS = { \"lmarena-ai/VisionArena-Chat\": lambda x: x[ \"conversation\" ][0][0][\"content\"], \"lmarena-ai/vision-arena-bench-v0.1\": lambda x: x[ \"turns\" ][0][0][\"content\"], } sample ¶ sample( tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list Source code in vllm/benchmarks/datasets.py 2421 2422 2423 2424 2425 2426 2427 2428 2429 2430 2431 2432 2433 2434 2435 2436 2437 2438 2439 2440 2441 2442 2443 2444 2445 2446 2447 2448 2449 2450 2451 2452 2453 2454 2455 2456 2457 2458 2459def sample( self, tokenizer: TokenizerLike, num_requests: int, output_len: int | None = None, enable_multimodal_chat: bool = False, request_id_prefix: str = \"\", no_oversample: bool = False, **kwargs, ) -> list: output_len = output_len if output_len is not None else self.DEFAULT_OUTPUT_LEN sampled_requests = [] for i, item in enumerate(self.data): if len(sampled_requests) >= num_requests: break parser_fn = self.SUPPORTED_DATASET_PATHS.get(self.hf_name) if parser_fn is None: raise ValueError(f\"Unsupported dataset path: {self.hf_name}\") prompt = parser_fn(item) mm_content = process_image(item[\"images\"][0]) prompt_len = len(tokenizer(prompt).input_ids) if enable_multimodal_chat: # Note: when chat is enabled the request prompt_len is no longer # accurate and we will be using request output to count the # actual prompt len prompt = self.apply_multimodal_chat_transformation(prompt, mm_content) sampled_requests.append( SampleRequest( prompt=prompt, prompt_len=prompt_len, expected_output_len=output_len, multi_modal_data=mm_content, request_id=request_id_prefix + str(i), ) ) self.maybe_oversample_requests( sampled_requests, num_requests, request_id_prefix, no_oversample ) return sampled_requests _ValidateDatasetArgs ¶ Bases: Action Argparse action to validate dataset name and path compatibility. Source code in vllm/benchmarks/datasets.py 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314class _ValidateDatasetArgs(argparse.Action): \"\"\"Argparse action to validate dataset name and path compatibility.\"\"\" def __call__(self, parser, namespace, values, option_string=None): setattr(namespace, self.dest, values) # Get current values of both dataset_name and dataset_path dataset_name = getattr(namespace, \"dataset_name\", \"random\") dataset_path = getattr(namespace, \"dataset_path\", None) # Validate the combination if dataset_name == \"random\" and dataset_path is not None: parser.error( \"Cannot use 'random' dataset with --dataset-path. \" \"Please specify the appropriate --dataset-name (e.g., \" \"'sharegpt', 'custom', 'sonnet') for your dataset file: \" f\"{dataset_path}\" ) __call__ ¶ __call__(parser, namespace, values, option_string=None) Source code in vllm/benchmarks/datasets.py 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314def __call__(self, parser, namespace, values, option_string=None): setattr(namespace, self.dest, values) # Get current values of both dataset_name and dataset_path dataset_name = getattr(namespace, \"dataset_name\", \"random\") dataset_path = getattr(namespace, \"dataset_path\", None) # Validate the combination if dataset_name == \"random\" and dataset_path is not None: parser.error( \"Cannot use 'random' dataset with --dataset-path. \" \"Please specify the appropriate --dataset-name (e.g., \" \"'sharegpt', 'custom', 'sonnet') for your dataset file: \" f\"{dataset_path}\" ) _format_zeta_prompt ¶ _format_zeta_prompt( sample: dict, original_start_marker: str = \"<|editable_region_start|>\", ) -> dict Format the zeta prompt for the Next Edit Prediction (NEP) dataset. This function formats examples from the NEP dataset into prompts and expected outputs. It could be further extended to support more NEP datasets. Parameters: Name Type Description Default sample dict The dataset sample containing events, inputs, and outputs. required original_start_marker str The marker indicating the start of the editable region. Defaults to \"<|editable_region_start|>\". '<|editable_region_start|>' Returns: Type Description dict A dictionary with the formatted prompts and expected outputs. Source code in vllm/benchmarks/datasets.py 2812 2813 2814 2815 2816 2817 2818 2819 2820 2821 2822 2823 2824 2825 2826 2827 2828 2829 2830 2831 2832 2833 2834 2835 2836 2837 2838 2839 2840 2841 2842def _format_zeta_prompt( sample: dict, original_start_marker: str = \"<|editable_region_start|>\" ) -> dict: \"\"\"Format the zeta prompt for the Next Edit Prediction (NEP) dataset. This function formats examples from the NEP dataset into prompts and expected outputs. It could be further extended to support more NEP datasets. Args: sample: The dataset sample containing events, inputs, and outputs. original_start_marker: The marker indicating the start of the editable region. Defaults to \"<|editable_region_start|>\". Returns: A dictionary with the formatted prompts and expected outputs. \"\"\" events = sample[\"events\"] input = sample[\"input\"] output = sample[\"output\"] prompt = zeta_prompt.format(events, input) # following the original implementation, extract the focused region # from the raw output output_start_index = output.find(original_start_marker) output_focused_region = output[output_start_index:] expected_output = output_focused_region return {\"prompt\": prompt, \"expected_output\": expected_output} add_dataset_parser ¶ add_dataset_parser(parser: ArgumentParser) Source code in vllm/benchmarks/datasets.py 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 1633 1634 1635 1636 1637 1638def add_dataset_parser(parser: FlexibleArgumentParser): parser.add_argument(\"--seed\", type=int, default=0) parser.add_argument( \"--num-prompts\", type=int, default=1000, help=\"Number of prompts to process.\", ) parser.add_argument( \"--dataset-name\", type=str, default=\"random\", action=_ValidateDatasetArgs, choices=[ \"sharegpt\", \"burstgpt\", \"sonnet\", \"random\", \"random-mm\", \"random-rerank\", \"hf\", \"custom\", \"prefix_repetition\", \"spec_bench\", ], help=\"Name of the dataset to benchmark on.\", ) parser.add_argument( \"--no-stream\", action=\"store_true\", help=\"Do not load the dataset in streaming mode.\", ) parser.add_argument( \"--dataset-path\", type=str, default=None, action=_ValidateDatasetArgs, help=\"Path to the sharegpt/sonnet dataset. \" \"Or the huggingface dataset ID if using HF dataset.\", ) parser.add_argument( \"--no-oversample\", action=\"store_true\", help=\"Do not oversample if the dataset has fewer samples than num-prompts.\", ) parser.add_argument( \"--skip-chat-template\", action=\"store_true\", help=\"Skip applying chat template to prompt for datasets that support it.\", ) parser.add_argument( \"--disable-shuffle\", action=\"store_true\", help=\"Disable shuffling of dataset samples for deterministic ordering.\", ) # group for dataset specific arguments custom_group = parser.add_argument_group(\"custom dataset options\") custom_group.add_argument( \"--custom-output-len\", type=int, default=256, help=\"Number of output tokens per request, used only for custom dataset.\", ) spec_bench_group = parser.add_argument_group(\"spec bench dataset options\") spec_bench_group.add_argument( \"--spec-bench-output-len\", type=int, default=256, help=\"Num of output tokens per request, used only for spec bench dataset.\", ) spec_bench_group.add_argument( \"--spec-bench-category\", type=str, default=None, help=\"Category for spec bench dataset. If None, use all categories.\", ) sonnet_group = parser.add_argument_group(\"sonnet dataset options\") sonnet_group.add_argument( \"--sonnet-input-len\", type=int, default=550, help=\"Number of input tokens per request, used only for sonnet dataset.\", ) sonnet_group.add_argument( \"--sonnet-output-len\", type=int, default=150, help=\"Number of output tokens per request, used only for sonnet dataset.\", ) sonnet_group.add_argument( \"--sonnet-prefix-len\", type=int, default=200, help=\"Number of prefix tokens per request, used only for sonnet dataset.\", ) sharegpt_group = parser.add_argument_group(\"sharegpt dataset options\") sharegpt_group.add_argument( \"--sharegpt-output-len\", type=int, default=None, help=\"Output length for each request. Overrides the output length \" \"from the ShareGPT dataset.\", ) blazedit_group = parser.add_argument_group(\"blazedit dataset options\") blazedit_group.add_argument( \"--blazedit-min-distance\", type=float, default=0.0, help=\"Minimum distance for blazedit dataset. Min: 0, Max: 1.0\", ) blazedit_group.add_argument( \"--blazedit-max-distance\", type=float, default=1.0, help=\"Maximum distance for blazedit dataset. Min: 0, Max: 1.0\", ) random_group = parser.add_argument_group(\"random dataset options\") random_group.add_argument( \"--random-input-len\", type=int, default=1024, help=\"Number of input tokens per request, used only for random sampling.\", ) random_group.add_argument( \"--random-output-len\", type=int, default=128, help=\"Number of output tokens per request, used only for random sampling.\", ) random_group.add_argument( \"--random-range-ratio\", type=float, default=0.0, help=\"Range ratio for sampling input/output length, \" \"used only for random sampling. Must be in the range [0, 1) to define \" \"a symmetric sampling range\" \"[length * (1 - range_ratio), length * (1 + range_ratio)].\", ) random_group.add_argument( \"--random-prefix-len\", type=int, default=0, help=( \"Number of fixed prefix tokens before the random context \" \"in a request. \" \"The total input length is the sum of `random-prefix-len` and \" \"a random \" \"context length sampled from [input_len * (1 - range_ratio), \" \"input_len * (1 + range_ratio)].\" ), ) random_group.add_argument( \"--random-batch-size\", type=int, default=1, help=(\"Batch size for random sampling. Only used for embeddings benchmark.\"), ) random_group.add_argument( \"--no-reranker\", action=\"store_true\", help=( \"Whether the model supports reranking natively.\" \" Only used for reranker benchmark.\" ), ) # random multimodal dataset options random_mm_group = parser.add_argument_group( \"random multimodal dataset options extended from random dataset\" ) random_mm_group.add_argument( \"--random-mm-base-items-per-request\", type=int, default=RandomMultiModalDataset.DEFAULT_BASE_ITEMS_PER_REQUEST, help=( \"Base number of multimodal items per request for random-mm. \" \"Actual per-request count is sampled around this base using \" \"--random-mm-num-mm-items-range-ratio.\" ), ) random_mm_group.add_argument( \"--random-mm-num-mm-items-range-ratio\", type=float, default=RandomMultiModalDataset.DEFAULT_NUM_MM_ITEMS_RANGE_RATIO, help=( \"Range ratio r in [0, 1] for sampling items per request. \" \"We sample uniformly from the closed integer range \" \"[floor(n*(1-r)), ceil(n*(1+r))] \" \"where n is the base items per request. \" \"r=0 keeps it fixed; r=1 allows 0 items. The maximum is clamped \" \"to the sum of per-modality limits from \" \"--random-mm-limit-mm-per-prompt. \" \"An error is raised if the computed min exceeds the max.\" ), ) random_mm_group.add_argument( \"--random-mm-limit-mm-per-prompt\", type=json.loads, default=RandomMultiModalDataset.DEFAULT_LIMIT_MM_PER_PROMPT, help=( \"Per-modality hard caps for items attached per request, e.g. \" '\\'{\"image\": 3, \"video\": 0}\\'. The sampled per-request item ' \"count is clamped to the sum of these limits. When a modality \" \"reaches its cap, its buckets are excluded and probabilities are \" \"renormalized.\" \"OBS.: Only image sampling is supported for now.\" ), ) def _parse_mm_bucket_config(v: object) -> dict[tuple[int, int, int], float]: # If already a dict (e.g., programmatic call), normalize keys def normalize(d: dict) -> dict[tuple[int, int, int], float]: out: dict[tuple[int, int, int], float] = {} for k, val in d.items(): key = k if isinstance(key, str): with suppress(Exception): key = ast.literal_eval(key) if not ( isinstance(key, tuple) and len(key) == 3 and all(isinstance(x, int) for x in key) ): raise ValueError( f\"Invalid bucket key {k!r}. Expected tuple (H, W, T).\" ) out[(int(key[0]), int(key[1]), int(key[2]))] = float(val) return out if isinstance(v, dict): return normalize(v) if isinstance(v, str): # Python literal (supports tuple keys) parsed = ast.literal_eval(v) if not isinstance(parsed, dict): raise ValueError(\"Bucket config must parse to a dict.\") return normalize(parsed) raise ValueError(\"Unsupported value for --random-mm-bucket-config.\") random_mm_group.add_argument( \"--random-mm-bucket-config\", type=_parse_mm_bucket_config, default=RandomMultiModalDataset.DEFAULT_MM_ITEM_BUCKET_CONFIG, help=( \"The bucket config is a dictionary mapping a multimodal item\" \"sampling configuration to a probability.\" \"Currently allows for 2 modalities: images and videos. \" \"An bucket key is a tuple of (height, width, num_frames)\" \"The value is the probability of sampling that specific item. \" \"Example: \" \"--random-mm-bucket-config \" \"{(256, 256, 1): 0.5, (720, 1280, 1): 0.4, (720, 1280, 16): 0.10} \" \"First item: images with resolution 256x256 w.p. 0.5\" \"Second item: images with resolution 720x1280 w.p. 0.4 \" \"Third item: videos with resolution 720x1280 and 16 frames w.p. 0.1\" \"OBS.: If the probabilities do not sum to 1, they are normalized.\" \"OBS bis.: Only image sampling is supported for now.\" ), ) hf_group = parser.add_argument_group(\"hf dataset options\") hf_group.add_argument( \"--hf-subset\", type=str, default=None, help=\"Subset of the HF dataset.\" ) hf_group.add_argument( \"--hf-split\", type=str, default=None, help=\"Split of the HF dataset.\" ) hf_group.add_argument( \"--hf-name\", type=str, default=None, help=( \"Name of the dataset on HuggingFace \" \"(e.g., 'lmarena-ai/VisionArena-Chat'). \" \"Specify this if your dataset-path is a local path.\" ), ) hf_group.add_argument( \"--hf-output-len\", type=int, default=None, help=\"Output length for each request. Overrides the output lengths \" \"from the sampled HF dataset.\", ) prefix_repetition_group = parser.add_argument_group( \"prefix repetition dataset options\" ) prefix_repetition_group.add_argument( \"--prefix-repetition-prefix-len\", type=int, default=256, help=\"Number of prefix tokens per request, used only for prefix \" \"repetition dataset.\", ) prefix_repetition_group.add_argument( \"--prefix-repetition-suffix-len\", type=int, default=256, help=\"Number of suffix tokens per request, used only for prefix \" \"repetition dataset. Total input length is prefix_len + suffix_len.\", ) prefix_repetition_group.add_argument( \"--prefix-repetition-num-prefixes\", type=int, default=10, help=\"Number of prefixes to generate, used only for prefix repetition \" \"dataset. Prompts per prefix is num_requests // num_prefixes.\", ) prefix_repetition_group.add_argument( \"--prefix-repetition-output-len\", type=int, default=128, help=\"Number of output tokens per request, used only for prefix \" \"repetition dataset.\", ) gen_prompt_decode_to_target_len ¶ gen_prompt_decode_to_target_len( tokenizer: TokenizerLike, token_sequence: list[int], target_token_len: int, max_retry: int = 10, add_special_tokens: bool = False, rng: Generator | None = None, ) -> tuple[str, list[int]] Ensure decoded-then-encoded prompt length matches the target token length. This function decodes an initial token sequence to text and re-encodes it , iteratively adjusting the token sequence length to match a target. This is necessary because some tokenizers do not guarantee a 1:1 mapping between consecutive tokens and the decoded-then-encoded sequence length. For example, for GPT2Tokenizer: [6880, 6881] -> ['Ġcalls', 'here'] -> [1650, 939, 486] -> ['Ġcall', 'sh', 'ere'] Returns a tuple of the final prompt string and the adjusted token sequence. Source code in vllm/benchmarks/datasets.py 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433def gen_prompt_decode_to_target_len( tokenizer: TokenizerLike, token_sequence: list[int], target_token_len: int, max_retry: int = 10, add_special_tokens: bool = False, rng: np.random.Generator | None = None, ) -> tuple[str, list[int]]: \"\"\" Ensure decoded-then-encoded prompt length matches the target token length. This function decodes an initial token sequence to text and re-encodes it , iteratively adjusting the token sequence length to match a target. This is necessary because some tokenizers do not guarantee a 1:1 mapping between consecutive tokens and the decoded-then-encoded sequence length. For example, for GPT2Tokenizer: [6880, 6881] -> ['Ġcalls', 'here'] -> [1650, 939, 486] -> ['Ġcall', 'sh', 'ere'] Returns a tuple of the final prompt string and the adjusted token sequence. \"\"\" remain_num_try = max_retry token_mismatch = 0 while True: prompt = tokenizer.decode(token_sequence) token_sequence = tokenizer.encode(prompt, add_special_tokens=add_special_tokens) if remain_num_try <= 0: if len(token_sequence) != target_token_len: token_mismatch = len(token_sequence) - target_token_len break if len(token_sequence) == target_token_len: break elif len(token_sequence) < target_token_len: if rng is not None: extra_tokens = rng.integers( 0, tokenizer.vocab_size, size=target_token_len - len(token_sequence), ).tolist() else: extra_tokens = np.random.randint( 0, tokenizer.vocab_size, size=target_token_len - len(token_sequence), ).tolist() token_sequence.extend(extra_tokens) elif len(token_sequence) > target_token_len: token_sequence = token_sequence[:target_token_len] remain_num_try -= 1 return prompt, token_sequence, token_mismatch get_samples ¶ get_samples( args, tokenizer: TokenizerLike ) -> list[SampleRequest] Source code in vllm/benchmarks/datasets.py 1641 1642 1643 1644 1645 1646 1647 1648 1649 1650 1651 1652 1653 1654 1655 1656 1657 1658 1659 1660 1661 1662 1663 1664 1665 1666 1667 1668 1669 1670 1671 1672 1673 1674 1675 1676 1677 1678 1679 1680 1681 1682 1683 1684 1685 1686 1687 1688 1689 1690 1691 1692 1693 1694 1695 1696 1697 1698 1699 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728 1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 1742 1743 1744 1745 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 1793 1794 1795 1796 1797 1798 1799 1800 1801 1802 1803 1804 1805 1806 1807 1808 1809 1810 1811 1812 1813 1814 1815 1816 1817 1818 1819 1820 1821 1822 1823 1824 1825 1826 1827 1828 1829 1830 1831 1832 1833 1834 1835 1836 1837 1838 1839 1840 1841 1842 1843 1844 1845 1846 1847 1848 1849 1850 1851 1852 1853 1854 1855 1856 1857 1858 1859 1860 1861 1862 1863 1864 1865 1866 1867 1868 1869 1870 1871 1872 1873 1874 1875 1876 1877 1878 1879 1880 1881 1882 1883 1884 1885 1886 1887 1888 1889 1890 1891 1892 1893 1894 1895 1896 1897 1898 1899 1900 1901 1902 1903 1904 1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919def get_samples(args, tokenizer: TokenizerLike) -> list[SampleRequest]: if not hasattr(args, \"request_id_prefix\"): args.request_id_prefix = \"\" if args.dataset_name == \"custom\": dataset = CustomDataset( dataset_path=args.dataset_path, disable_shuffle=args.disable_shuffle ) input_requests = dataset.sample( num_requests=args.num_prompts, tokenizer=tokenizer, output_len=args.custom_output_len, skip_chat_template=args.skip_chat_template, request_id_prefix=args.request_id_prefix, no_oversample=args.no_oversample, ) elif args.dataset_name == \"sonnet\": dataset = SonnetDataset( dataset_path=args.dataset_path, disable_shuffle=args.disable_shuffle ) # For the \"sonnet\" dataset, formatting depends on the backend. if args.backend == \"openai-chat\": input_requests = dataset.sample( num_requests=args.num_prompts, input_len=args.sonnet_input_len, output_len=args.sonnet_output_len, prefix_len=args.sonnet_prefix_len, tokenizer=tokenizer, return_prompt_formatted=False, request_id_prefix=args.request_id_prefix, no_oversample=args.no_oversample, ) else: assert tokenizer.chat_template or tokenizer.default_chat_template, ( \"Tokenizer/model must have chat template for sonnet dataset.\" ) input_requests = dataset.sample( num_requests=args.num_prompts, input_len=args.sonnet_input_len, output_len=args.sonnet_output_len, prefix_len=args.sonnet_prefix_len, tokenizer=tokenizer, return_prompt_formatted=True, request_id_prefix=args.request_id_prefix, no_oversample=args.no_oversample, ) elif args.dataset_name == \"hf\": # all following datasets are implemented from the # HuggingFaceDataset base class hf_kwargs = {} if ( args.dataset_path in VisionArenaDataset.SUPPORTED_DATASET_PATHS or args.hf_name in VisionArenaDataset.SUPPORTED_DATASET_PATHS ): dataset_class = VisionArenaDataset args.hf_split = \"train\" args.hf_subset = None elif ( args.dataset_path in MMVUDataset.SUPPORTED_DATASET_PATHS or args.hf_name in MMVUDataset.SUPPORTED_DATASET_PATHS ): dataset_class = MMVUDataset args.hf_split = \"validation\" args.hf_subset = None elif ( args.dataset_path in InstructCoderDataset.SUPPORTED_DATASET_PATHS or args.hf_name in InstructCoderDataset.SUPPORTED_DATASET_PATHS ): dataset_class = InstructCoderDataset args.hf_split = \"train\" elif ( args.dataset_path in MTBenchDataset.SUPPORTED_DATASET_PATHS or args.hf_name in MTBenchDataset.SUPPORTED_DATASET_PATHS ): dataset_class = MTBenchDataset args.hf_split = \"train\" elif ( args.dataset_path in MultiModalConversationDataset.SUPPORTED_DATASET_PATHS or args.hf_name in MultiModalConversationDataset.SUPPORTED_DATASET_PATHS ): dataset_class = MultiModalConversationDataset elif ( args.dataset_path in ConversationDataset.SUPPORTED_DATASET_PATHS or args.hf_name in ConversationDataset.SUPPORTED_DATASET_PATHS ): dataset_class = ConversationDataset elif ( args.dataset_path in AIMODataset.SUPPORTED_DATASET_PATHS or args.hf_name in AIMODataset.SUPPORTED_DATASET_PATHS ): dataset_class = AIMODataset args.hf_split = \"train\" elif ( args.dataset_path in NextEditPredictionDataset.SUPPORTED_DATASET_PATHS # noqa: E501 or args.hf_name in NextEditPredictionDataset.SUPPORTED_DATASET_PATHS ): dataset_class = NextEditPredictionDataset args.hf_split = \"train\" elif ( args.dataset_path in ASRDataset.SUPPORTED_DATASET_PATHS or args.hf_name in ASRDataset.SUPPORTED_DATASET_PATHS ): dataset_class = ASRDataset args.hf_split = \"train\" elif args.dataset_path in BlazeditDataset.SUPPORTED_DATASET_PATHS: dataset_class = BlazeditDataset args.hf_split = \"train\" hf_kwargs = { \"min_distance\": args.blazedit_min_distance, \"max_distance\": args.blazedit_max_distance, } elif ( args.dataset_path in MLPerfDataset.SUPPORTED_DATASET_PATHS or args.hf_name in MLPerfDataset.SUPPORTED_DATASET_PATHS ): dataset_class = MLPerfDataset args.hf_split = \"train\" elif ( args.dataset_path in MMStarDataset.SUPPORTED_DATASET_PATHS or args.hf_name in MMStarDataset.SUPPORTED_DATASET_PATHS ): dataset_class = MMStarDataset args.hf_split = \"val\" args.hf_subset = None else: supported_datasets = set( [ dataset_name for cls in HuggingFaceDataset.__subclasses__() for dataset_name in cls.SUPPORTED_DATASET_PATHS ] ) raise ValueError( f\"Unsupported dataset path: {args.dataset_path}. \" \"Huggingface dataset only supports dataset_path\" f\" from one of following: {supported_datasets}. \" \"Please consider contributing if you would \" \"like to add support for additional dataset formats.\" ) if dataset_class.IS_MULTIMODAL and not ( args.backend in (\"openai-chat\", \"openai-audio\") or \"embeddings-\" in args.backend ): # multi-modal benchmark is only available on OpenAI Chat # endpoint-type. raise ValueError( \"Multi-modal content is only supported on 'openai-chat' and \" \"'openai-audio' backends.\" ) input_requests = dataset_class( dataset_path=args.dataset_path, dataset_subset=args.hf_subset, dataset_split=args.hf_split, random_seed=args.seed, no_stream=args.no_stream, hf_name=args.hf_name, disable_shuffle=args.disable_shuffle, ).sample( num_requests=args.num_prompts, tokenizer=tokenizer, output_len=args.hf_output_len, request_id_prefix=args.request_id_prefix, no_oversample=args.no_oversample, skip_chat_template=args.skip_chat_template, **hf_kwargs, ) else: # For datasets that follow a similar structure, use a mapping. dataset_mapping = { \"spec_bench\": lambda: SpecBench( dataset_path=args.dataset_path, category=args.spec_bench_category, disable_shuffle=args.disable_shuffle, ).sample( num_requests=args.num_prompts, tokenizer=tokenizer, output_len=args.spec_bench_output_len, request_id_prefix=args.request_id_prefix, no_oversample=args.no_oversample, ), \"sharegpt\": lambda: ShareGPTDataset( random_seed=args.seed, dataset_path=args.dataset_path, disable_shuffle=args.disable_shuffle, ).sample( tokenizer=tokenizer, num_requests=args.num_prompts, output_len=args.sharegpt_output_len, request_id_prefix=args.request_id_prefix, no_oversample=args.no_oversample, ), \"burstgpt\": lambda: BurstGPTDataset( random_seed=args.seed, dataset_path=args.dataset_path, disable_shuffle=args.disable_shuffle, ).sample( tokenizer=tokenizer, num_requests=args.num_prompts, request_id_prefix=args.request_id_prefix, no_oversample=args.no_oversample, ), \"random\": lambda: RandomDataset( random_seed=args.seed, dataset_path=args.dataset_path, disable_shuffle=args.disable_shuffle, ).sample( tokenizer=tokenizer, num_requests=args.num_prompts, prefix_len=args.random_prefix_len, input_len=args.random_input_len, output_len=args.random_output_len, range_ratio=args.random_range_ratio, request_id_prefix=args.request_id_prefix, batchsize=args.random_batch_size, no_oversample=args.no_oversample, ), \"random-mm\": lambda: RandomMultiModalDataset( random_seed=args.seed, dataset_path=args.dataset_path, disable_shuffle=args.disable_shuffle, ).sample( tokenizer=tokenizer, num_requests=args.num_prompts, prefix_len=args.random_prefix_len, range_ratio=args.random_range_ratio, input_len=args.random_input_len, output_len=args.random_output_len, base_items_per_request=args.random_mm_base_items_per_request, limit_mm_per_prompt=args.random_mm_limit_mm_per_prompt, num_mm_items_range_ratio=args.random_mm_num_mm_items_range_ratio, bucket_config=args.random_mm_bucket_config, request_id_prefix=args.request_id_prefix, no_oversample=args.no_oversample, ), \"random-rerank\": lambda: RandomDatasetForReranking( random_seed=args.seed, dataset_path=args.dataset_path, disable_shuffle=args.disable_shuffle, ).sample( tokenizer=tokenizer, num_requests=args.num_prompts, input_len=args.random_input_len, range_ratio=args.random_range_ratio, request_id_prefix=args.request_id_prefix, batchsize=args.random_batch_size, is_reranker=not args.no_reranker, ), \"prefix_repetition\": lambda: PrefixRepetitionRandomDataset( random_seed=args.seed, dataset_path=args.dataset_path, disable_shuffle=args.disable_shuffle, ).sample( tokenizer=tokenizer, num_requests=args.num_prompts, prefix_len=args.prefix_repetition_prefix_len, suffix_len=args.prefix_repetition_suffix_len, num_prefixes=args.prefix_repetition_num_prefixes, output_len=args.prefix_repetition_output_len, request_id_prefix=args.request_id_prefix, no_oversample=args.no_oversample, ), } try: # Enforce endpoint compatibility for multimodal datasets. if args.dataset_name == \"random-mm\" and args.backend not in [\"openai-chat\"]: raise ValueError( \"Multi-modal content (images) is only supported on \" \"'openai-chat' backend.\" ) input_requests = dataset_mapping[args.dataset_name]() except KeyError as err: raise ValueError(f\"Unknown dataset: {args.dataset_name}\") from err return input_requests is_valid_sequence ¶ is_valid_sequence( prompt_len: int, output_len: int, min_len: int = 4, max_prompt_len: int = 1024, max_total_len: int = 2048, skip_min_output_len_check: bool = False, ) -> bool Validate a sequence based on prompt and output lengths. Default pruning criteria are copied from the original sample_hf_requests and sample_sharegpt_requests functions in benchmark_serving.py, as well as from sample_requests in benchmark_throughput.py. Source code in vllm/benchmarks/datasets.py 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286def is_valid_sequence( prompt_len: int, output_len: int, min_len: int = 4, max_prompt_len: int = 1024, max_total_len: int = 2048, skip_min_output_len_check: bool = False, ) -> bool: \"\"\" Validate a sequence based on prompt and output lengths. Default pruning criteria are copied from the original `sample_hf_requests` and `sample_sharegpt_requests` functions in benchmark_serving.py, as well as from `sample_requests` in benchmark_throughput.py. \"\"\" # Check for invalid conditions prompt_too_short = prompt_len < min_len output_too_short = (not skip_min_output_len_check) and (output_len < min_len) prompt_too_long = prompt_len > max_prompt_len combined_too_long = (prompt_len + output_len) > max_total_len # Return True if none of the invalid conditions are met return not ( prompt_too_short or output_too_short or prompt_too_long or combined_too_long ) lora_path_on_disk cached ¶ lora_path_on_disk(lora_path: str) -> str Source code in vllm/benchmarks/datasets.py 289 290 291@cache def lora_path_on_disk(lora_path: str) -> str: return get_adapter_absolute_path(lora_path) process_image ¶ process_image(image: Any) -> Mapping[str, Any] Process a single image input and return a multimedia content dictionary. Supports the following input types: Dictionary with raw image bytes: - Expects a dict with a 'bytes' key containing raw image data. - Loads the bytes as a PIL.Image.Image. PIL.Image.Image input: - Converts the image to RGB. - Saves the image as a JPEG in memory. - Encodes the JPEG data as a base64 string. - Returns a dictionary with the image as a base64 data URL. String input: - Treats the string as a URL or local file path. - Prepends \"file://\" if the string doesn't start with \"http://\" or \"file://\". - Returns a dictionary with the image URL. Raises: Type Description ValueError If the input is not a supported type. Source code in vllm/benchmarks/datasets.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341def process_image(image: Any) -> Mapping[str, Any]: \"\"\" Process a single image input and return a multimedia content dictionary. Supports the following input types: 1. Dictionary with raw image bytes: - Expects a dict with a 'bytes' key containing raw image data. - Loads the bytes as a PIL.Image.Image. 2. PIL.Image.Image input: - Converts the image to RGB. - Saves the image as a JPEG in memory. - Encodes the JPEG data as a base64 string. - Returns a dictionary with the image as a base64 data URL. 3. String input: - Treats the string as a URL or local file path. - Prepends \"file://\" if the string doesn't start with \"http://\" or \"file://\". - Returns a dictionary with the image URL. Raises: ValueError: If the input is not a supported type. \"\"\" if isinstance(image, dict) and \"bytes\" in image: image = Image.open(BytesIO(image[\"bytes\"])) if isinstance(image, Image.Image): image = convert_image_mode(image, \"RGB\") with io.BytesIO() as image_data: image.save(image_data, format=\"JPEG\") image_base64 = base64.b64encode(image_data.getvalue()).decode(\"utf-8\") return { \"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_base64}\"}, } if isinstance(image, str): image_url = ( image if image.startswith((\"http://\", \"https://\", \"file://\")) else f\"file://{image}\" ) return {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}} raise ValueError( f\"Invalid image input {image}. Must be a PIL.Image.Image\" \" or str or dictionary with raw image bytes.\" ) process_video ¶ process_video(video: Any) -> Mapping[str, Any] Process a single video input and return a multimedia content dictionary. Supports the following input types: Dictionary with raw video bytes: - Expects a dict with a 'bytes' key containing raw video data. String input: - Treats the string as a URL or local file path. - Prepends \"file://\" if the string doesn't start with \"http://\" or \"file://\". - Returns a dictionary with the image URL. Raises: Type Description ValueError If the input is not a supported type. Source code in vllm/benchmarks/datasets.py 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378def process_video(video: Any) -> Mapping[str, Any]: \"\"\" Process a single video input and return a multimedia content dictionary. Supports the following input types: 1. Dictionary with raw video bytes: - Expects a dict with a 'bytes' key containing raw video data. 2. String input: - Treats the string as a URL or local file path. - Prepends \"file://\" if the string doesn't start with \"http://\" or \"file://\". - Returns a dictionary with the image URL. Raises: ValueError: If the input is not a supported type. \"\"\" if isinstance(video, dict) and \"bytes\" in video: video_bytes = video[\"bytes\"] video_base64 = base64.b64encode(video_bytes).decode(\"utf-8\") return { \"type\": \"video_url\", \"video_url\": {\"url\": f\"data:video/mp4;base64,{video_base64}\"}, } if isinstance(video, str): video_url = ( video if video.startswith((\"http://\", \"https://\", \"file://\")) else f\"file://{video}\" ) return {\"type\": \"video_url\", \"video_url\": {\"url\": video_url}} raise ValueError( f\"Invalid video input {video}. Must be a string of local path/remote url, or a dictionary with raw video bytes in the form of `{{'bytes': raw_video_bytes}}`.\" # noqa: E501 )",
      "code": ""
    }
  ],
  "links": [
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/datasets/",
    "https://docs.vllm.ai/en/latest/",
    "https://docs.vllm.ai/en/latest/usage/",
    "https://docs.vllm.ai/en/latest/contributing/",
    "https://docs.vllm.ai/en/latest/benchmarking/",
    "https://docs.vllm.ai/en/latest/api/",
    "https://docs.vllm.ai/en/latest/cli/",
    "https://docs.vllm.ai/en/latest/community/contact_us/",
    "https://docs.vllm.ai/en/latest/getting_started/quickstart/",
    "https://docs.vllm.ai/en/latest/getting_started/installation/",
    "https://docs.vllm.ai/en/latest/getting_started/installation/gpu/",
    "https://docs.vllm.ai/en/latest/getting_started/installation/cpu/",
    "https://docs.vllm.ai/en/latest/examples/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/async_llm_streaming/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/audio_language/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/automatic_prefix_caching/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/basic/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/batch_llm_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/chat_with_tools/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/context_extension/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/data_parallel/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/disaggregated-prefill-v1/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/disaggregated_prefill/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/encoder_decoder_multimodal/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/kv_load_failure_recovery/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/llm_engine_example/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/llm_engine_reset_kv/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/load_sharded_state/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/logits_processor/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/lora_with_quantization_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/metrics/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/mistral-small/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/mlpspeculator/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/multilora_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/openai_batch/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/prefix_caching/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/prompt_embed_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/qwen2_5_omni/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/qwen3_omni/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/qwen_1m/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/reproducibility/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf_colocate/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf_online_quant/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf_utils/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/save_sharded_state/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/simple_profiling/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/skip_loading_weights_in_engine_init/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/spec_decode/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/structured_outputs/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/torchrun_dp_example/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/torchrun_example/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/vision_language/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/vision_language_multi_image/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/api_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/chart-helm/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/dashboards/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_encoder/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_prefill/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_serving/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/elastic_ep/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/gradio_openai_chatbot_webserver/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/gradio_webserver/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/kv_events_subscriber/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/multi-node-serving/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/multi_instance_data_parallel/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_for_multimodal/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools_required/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools_xlam/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools_xlam_streaming/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_tool_calls_with_reasoning/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_with_reasoning/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_with_reasoning_streaming/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_completion_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_responses_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_responses_client_with_mcp_tools/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_responses_client_with_tools/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_transcription_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_translation_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/opentelemetry/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/prometheus_grafana/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/prompt_embed_inference_with_openai_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/ray_serve_deepseek/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/retrieval_augmented_generation_with_langchain/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/retrieval_augmented_generation_with_llamaindex/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/run_cluster/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/sagemaker-entrypoint/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/streamlit_openai_chatbot_webserver/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/structured_outputs/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/token_generation_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/utils/",
    "https://docs.vllm.ai/en/latest/examples/others/lmcache/",
    "https://docs.vllm.ai/en/latest/examples/others/logging_configuration/",
    "https://docs.vllm.ai/en/latest/examples/others/tensorize_vllm_model/",
    "https://docs.vllm.ai/en/latest/examples/pooling/classify/",
    "https://docs.vllm.ai/en/latest/examples/pooling/embed/",
    "https://docs.vllm.ai/en/latest/examples/pooling/plugin/",
    "https://docs.vllm.ai/en/latest/examples/pooling/pooling/",
    "https://docs.vllm.ai/en/latest/examples/pooling/score/",
    "https://docs.vllm.ai/en/latest/examples/pooling/token_classify/",
    "https://docs.vllm.ai/en/latest/examples/pooling/token_embed/",
    "https://docs.vllm.ai/en/latest/usage/v1_guide/",
    "https://docs.vllm.ai/en/latest/usage/faq/",
    "https://docs.vllm.ai/en/latest/usage/metrics/",
    "https://docs.vllm.ai/en/latest/usage/reproducibility/",
    "https://docs.vllm.ai/en/latest/usage/security/",
    "https://docs.vllm.ai/en/latest/usage/troubleshooting/",
    "https://docs.vllm.ai/en/latest/usage/usage_stats/",
    "https://docs.vllm.ai/en/latest/serving/offline_inference/",
    "https://docs.vllm.ai/en/latest/serving/openai_compatible_server/",
    "https://docs.vllm.ai/en/latest/serving/context_parallel_deployment/",
    "https://docs.vllm.ai/en/latest/serving/data_parallel_deployment/",
    "https://docs.vllm.ai/en/latest/serving/distributed_troubleshooting/",
    "https://docs.vllm.ai/en/latest/serving/expert_parallel_deployment/",
    "https://docs.vllm.ai/en/latest/serving/parallelism_scaling/",
    "https://docs.vllm.ai/en/latest/serving/integrations/langchain/",
    "https://docs.vllm.ai/en/latest/serving/integrations/llamaindex/",
    "https://docs.vllm.ai/en/latest/deployment/docker/",
    "https://docs.vllm.ai/en/latest/deployment/k8s/",
    "https://docs.vllm.ai/en/latest/deployment/nginx/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/anyscale/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/anything-llm/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/autogen/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/bentoml/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/cerebrium/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/chatbox/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/dify/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/dstack/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/haystack/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/helm/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/hf_inference_endpoints/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/litellm/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/lobe-chat/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/lws/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/modal/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/open-webui/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/retrieval_augmented_generation/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/skypilot/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/streamlit/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/triton/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kaito/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kserve/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kthena/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kubeai/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kuberay/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/llamastack/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/llm-d/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/llmaz/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/production-stack/",
    "https://docs.vllm.ai/en/latest/training/rlhf/",
    "https://docs.vllm.ai/en/latest/training/trl/",
    "https://docs.vllm.ai/en/latest/configuration/",
    "https://docs.vllm.ai/en/latest/configuration/conserving_memory/",
    "https://docs.vllm.ai/en/latest/configuration/engine_args/",
    "https://docs.vllm.ai/en/latest/configuration/env_vars/",
    "https://docs.vllm.ai/en/latest/configuration/model_resolution/",
    "https://docs.vllm.ai/en/latest/configuration/optimization/",
    "https://docs.vllm.ai/en/latest/configuration/serve_args/",
    "https://docs.vllm.ai/en/latest/models/supported_models/",
    "https://docs.vllm.ai/en/latest/models/generative_models/",
    "https://docs.vllm.ai/en/latest/models/pooling_models/",
    "https://docs.vllm.ai/en/latest/models/extensions/fastsafetensor/",
    "https://docs.vllm.ai/en/latest/models/extensions/runai_model_streamer/",
    "https://docs.vllm.ai/en/latest/models/extensions/tensorizer/",
    "https://docs.vllm.ai/en/latest/models/hardware_supported_models/cpu/",
    "https://docs.vllm.ai/en/latest/models/hardware_supported_models/xpu/",
    "https://docs.vllm.ai/en/latest/features/",
    "https://docs.vllm.ai/en/latest/features/automatic_prefix_caching/",
    "https://docs.vllm.ai/en/latest/features/batch_invariance/",
    "https://docs.vllm.ai/en/latest/features/custom_arguments/",
    "https://docs.vllm.ai/en/latest/features/custom_logitsprocs/",
    "https://docs.vllm.ai/en/latest/features/disagg_encoder/",
    "https://docs.vllm.ai/en/latest/features/disagg_prefill/",
    "https://docs.vllm.ai/en/latest/features/interleaved_thinking/",
    "https://docs.vllm.ai/en/latest/features/lora/",
    "https://docs.vllm.ai/en/latest/features/mooncake_connector_usage/",
    "https://docs.vllm.ai/en/latest/features/multimodal_inputs/",
    "https://docs.vllm.ai/en/latest/features/nixl_connector_usage/",
    "https://docs.vllm.ai/en/latest/features/prompt_embeds/",
    "https://docs.vllm.ai/en/latest/features/reasoning_outputs/",
    "https://docs.vllm.ai/en/latest/features/sleep_mode/",
    "https://docs.vllm.ai/en/latest/features/spec_decode/",
    "https://docs.vllm.ai/en/latest/features/structured_outputs/",
    "https://docs.vllm.ai/en/latest/features/tool_calling/",
    "https://docs.vllm.ai/en/latest/features/quantization/",
    "https://docs.vllm.ai/en/latest/features/quantization/auto_awq/",
    "https://docs.vllm.ai/en/latest/features/quantization/auto_round/",
    "https://docs.vllm.ai/en/latest/features/quantization/bitblas/",
    "https://docs.vllm.ai/en/latest/features/quantization/bnb/",
    "https://docs.vllm.ai/en/latest/features/quantization/fp8/",
    "https://docs.vllm.ai/en/latest/features/quantization/gguf/",
    "https://docs.vllm.ai/en/latest/features/quantization/gptqmodel/",
    "https://docs.vllm.ai/en/latest/features/quantization/inc/",
    "https://docs.vllm.ai/en/latest/features/quantization/int4/",
    "https://docs.vllm.ai/en/latest/features/quantization/int8/",
    "https://docs.vllm.ai/en/latest/features/quantization/modelopt/",
    "https://docs.vllm.ai/en/latest/features/quantization/quantized_kvcache/",
    "https://docs.vllm.ai/en/latest/features/quantization/quark/",
    "https://docs.vllm.ai/en/latest/features/quantization/torchao/",
    "https://docs.vllm.ai/en/latest/contributing/deprecation_policy/",
    "https://docs.vllm.ai/en/latest/contributing/dockerfile/dockerfile/",
    "https://docs.vllm.ai/en/latest/contributing/incremental_build/",
    "https://docs.vllm.ai/en/latest/contributing/profiling/",
    "https://docs.vllm.ai/en/latest/contributing/vulnerability_management/",
    "https://docs.vllm.ai/en/latest/contributing/model/",
    "https://docs.vllm.ai/en/latest/contributing/model/basic/",
    "https://docs.vllm.ai/en/latest/contributing/model/registration/",
    "https://docs.vllm.ai/en/latest/contributing/model/tests/",
    "https://docs.vllm.ai/en/latest/contributing/model/multimodal/",
    "https://docs.vllm.ai/en/latest/contributing/model/transcription/",
    "https://docs.vllm.ai/en/latest/contributing/ci/failures/",
    "https://docs.vllm.ai/en/latest/contributing/ci/nightly_builds/",
    "https://docs.vllm.ai/en/latest/contributing/ci/update_pytorch_version/",
    "https://docs.vllm.ai/en/latest/design/io_processor_plugins/",
    "https://docs.vllm.ai/en/latest/design/lora_resolver_plugins/",
    "https://docs.vllm.ai/en/latest/design/plugin_system/",
    "https://docs.vllm.ai/en/latest/design/arch_overview/",
    "https://docs.vllm.ai/en/latest/design/cuda_graphs/",
    "https://docs.vllm.ai/en/latest/design/dbo/",
    "https://docs.vllm.ai/en/latest/design/debug_vllm_compile/",
    "https://docs.vllm.ai/en/latest/design/fused_moe_modular_kernel/",
    "https://docs.vllm.ai/en/latest/design/huggingface_integration/",
    "https://docs.vllm.ai/en/latest/design/hybrid_kv_cache_manager/",
    "https://docs.vllm.ai/en/latest/design/logits_processors/",
    "https://docs.vllm.ai/en/latest/design/metrics/",
    "https://docs.vllm.ai/en/latest/design/mm_processing/",
    "https://docs.vllm.ai/en/latest/design/moe_kernel_features/",
    "https://docs.vllm.ai/en/latest/design/multiprocessing/",
    "https://docs.vllm.ai/en/latest/design/optimization_levels/",
    "https://docs.vllm.ai/en/latest/design/p2p_nccl_connector/",
    "https://docs.vllm.ai/en/latest/design/paged_attention/",
    "https://docs.vllm.ai/en/latest/design/prefix_caching/",
    "https://docs.vllm.ai/en/latest/design/torch_compile/",
    "https://docs.vllm.ai/en/latest/benchmarking/cli/",
    "https://docs.vllm.ai/en/latest/benchmarking/sweeps/",
    "https://docs.vllm.ai/en/latest/benchmarking/dashboard/",
    "https://docs.vllm.ai/en/latest/api/vllm/",
    "https://docs.vllm.ai/en/latest/api/vllm/beam_search/",
    "https://docs.vllm.ai/en/latest/api/vllm/collect_env/",
    "https://docs.vllm.ai/en/latest/api/vllm/connections/",
    "https://docs.vllm.ai/en/latest/api/vllm/env_override/",
    "https://docs.vllm.ai/en/latest/api/vllm/envs/",
    "https://docs.vllm.ai/en/latest/api/vllm/forward_context/",
    "https://docs.vllm.ai/en/latest/api/vllm/logger/",
    "https://docs.vllm.ai/en/latest/api/vllm/logits_process/",
    "https://docs.vllm.ai/en/latest/api/vllm/logprobs/",
    "https://docs.vllm.ai/en/latest/api/vllm/outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/pooling_params/",
    "https://docs.vllm.ai/en/latest/api/vllm/sampling_params/",
    "https://docs.vllm.ai/en/latest/api/vllm/scalar_type/",
    "https://docs.vllm.ai/en/latest/api/vllm/scripts/",
    "https://docs.vllm.ai/en/latest/api/vllm/sequence/",
    "https://docs.vllm.ai/en/latest/api/vllm/tasks/",
    "https://docs.vllm.ai/en/latest/api/vllm/tracing/",
    "https://docs.vllm.ai/en/latest/api/vllm/version/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/image/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/video/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layer/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/selector/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/chunked_local_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/cross_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/encoder_only_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/mm_encoder_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/chunked_prefill_paged_decode/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/common/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/flashmla/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/merge_attn_states/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/paged_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/pallas_kv_cache_update/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/prefix_prefill/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/rocm_aiter_mla_sparse/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_decode_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_merge_attn_states/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_reshape_and_cache_flash/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_unified_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/vit_attn_wrappers/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/fa_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/kv_sharing_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/kv_transfer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/latency/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/startup/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/throughput/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/endpoint_request_func/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/ready_checker/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/cli/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/param_sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/plot/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/plot_pareto/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/serve_sla/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/server/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/sla_sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/activation_quant_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/base_static_graph/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/caching/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/collective_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/compiler_interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/counter/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/cuda_graph/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/decorators/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fix_functionalization/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fusion_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fx_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/inductor_pass/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/matcher_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/monitor/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/noop_elimination/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/partition_rules/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/pass_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/piecewise_backend/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/post_cleanup/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/qk_norm_rope_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/rocm_aiter_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/sequence_parallelism/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/torch25_custom_graph_pass/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/vllm_inductor_pass/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/compilation/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/device/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/ec_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/kv_events/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/kv_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/load/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/lora/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/model/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/multimodal/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/observability/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/parallel/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/pooler/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/profiler/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/scheduler/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/speculative/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/speech_to_text/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/structured_outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/vllm/",
    "https://docs.vllm.ai/en/latest/api/vllm/device_allocator/",
    "https://docs.vllm.ai/en/latest/api/vllm/device_allocator/cumem/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/communication_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_events/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/parallel_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/tpu_distributed_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/all2all/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/all_reduce_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/base_device_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/cpu_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/cuda_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/cuda_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/custom_all_reduce/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/mnnvl_compat/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/pynccl/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/pynccl_allocator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/pynccl_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/quick_all_reduce/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/ray_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/shm_broadcast/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/shm_object_storage/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/symm_mem/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/tpu_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/xpu_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_transfer_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/example_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/factory/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/async_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/eplb_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/rebalance_execute/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/policy/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/policy/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/policy/default/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_transfer_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/factory/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/decode_bench_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/example_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_mp_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/mooncake_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/multi_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/offloading_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/multi_process_adapter/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/vllm_v1_adapter/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/tensor_memory_pool/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/arg_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/async_llm_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/llm_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/api_server/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/chat_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/constants/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/context/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/launcher/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/llm/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/logger/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/renderer/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/responses_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/score_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/ssl/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/tool/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/tool_server/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/anthropic/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/anthropic/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/anthropic/serving_messages/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/collect_env/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/main/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/openai/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/run_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/types/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/latency/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/main/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/startup/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/throughput/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/api_server/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/cli_args/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/orca_metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/run_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_chat/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_chat_stream_harmony/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_completion/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_models/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_responses/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_transcription/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/speech_to_text/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/parser/harmony_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/parser/responses_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/conftest/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/sagemaker/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/sagemaker/routes/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/cache/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/elastic_ep/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/elastic_ep/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/elastic_ep/middleware/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/health/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/server_info/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/lora/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/lora/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/profile/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/profile/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rlhf/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rlhf/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rpc/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rpc/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/sleep/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/sleep/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/tokenize/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/tokenize/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/tokenize/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/data/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/parse/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/preprocess/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/dump_input/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/formatter/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/lazy/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/log_time/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/lora_model/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/lora_weights/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/model_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/peft_helper/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/request/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/resolver/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/worker_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/base_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/column_parallel_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/logits_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/replicated_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/row_parallel_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/vocal_parallel_embedding/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/ipex_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/ipex_ops/lora_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/torch_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/torch_ops/lora_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/fused_moe_lora_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/kernel_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/lora_expand_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/lora_kernel_metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/lora_shrink_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/xla_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/xla_ops/lora_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_gpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_selector/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_xpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/custom_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/parameter/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/activation/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/attention_layer_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/batch_invariant/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/conv/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/kda/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/layernorm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/lightning_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/logits_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/pooler/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/resampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/vocab_parallel_embedding/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk_delta_h/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk_o/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk_scaled_dot_kkt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/cumsum/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/fused_recurrent/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/index_py/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/kda/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/l2norm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/layernorm_guard/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/op/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/solve_tril/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/wy_fast/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/all2all_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/cpu_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/cutlass_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deep_gemm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deep_gemm_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deepep_ll_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_cutedsl_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_cutlass_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_trtllm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_batched_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_marlin_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_moe_method_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_moe_modular_method/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/gpt_oss_triton_kernels_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/layer/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/modular_kernel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_align_block_size/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_pallas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_permute_unpermute/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_torch_iterative/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/pplx_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/routing_simulator/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/shared_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/topk_weight_and_reduce/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/trtllm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/zero_expert_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/linear_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/mamba_mixer/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/mamba_mixer2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/mamba_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/short_conv/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/causal_conv1d/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/layernorm_gated/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/mamba_ssm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_bmm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_chunk_scan/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_chunk_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_combined/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_state_passing/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/auto_round/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/awq/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/awq_marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/awq_triton/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/base_config/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/bitblas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/bitsandbytes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/cpu_wna16/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/deepspeedfp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/experts_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/fbgemm_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/fp_quant/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gguf/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq_bitblas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq_marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq_marlin_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/hqq_marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/inc/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/input_quant_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/ipex_quant/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kv_cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/modelopt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/moe_wna16/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/mxfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/petit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/ptpc_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/qutlass_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/rtn/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/schema/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/torchao/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/tpu_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/triton_scaled_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_scheme/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a4_nvfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a8_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a8_int/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_nvfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a16_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_wNa16/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/module/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/schemes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/schemes/linear_qutlass_nvfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/allspark/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/bitblas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/conch/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/cutlass/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/dynamic_4bit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/exllama/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/MPLinearKernel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/machete/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/xpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/aiter/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/cutlass/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/ScaledMMLinearKernel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/triton/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/xla/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/quark/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/quark_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_ocp_mx/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_scheme/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/allspark_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/bitblas_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/flashinfer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/fp8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/gptq_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/int8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/layer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/machete_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_fp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_test/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_test_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/mxfp4_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/mxfp6_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/mxfp8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/nvfp4_emulation_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/nvfp4_moe_support/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/ocp_mx_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/petit_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/quant_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/w8a8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/common/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/deepseek_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/dual_chunk_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/dynamic_ntk_alpha_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/dynamic_ntk_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/ernie45_vl_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/linear_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/llama3_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/llama4_vision_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/mrope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/ntk_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/phi3_long_rope_scaled_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/xdrope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/yarn_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/base_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/bitsandbytes_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/default_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/dummy_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/gguf_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/online_quantization/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/runai_streamer_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/sharded_state_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/tensorizer/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/tensorizer_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/weight_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/adapters/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/afmoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/aimv2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/apertus/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/arcee/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/arctic/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/aria/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/audioflamingo3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/aya_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bagel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/baichuan/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bailing_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bee/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bert/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bert_with_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/blip/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/blip2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bloom/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/chameleon/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/chatglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/clip/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/cohere2_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/commandr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/dbrx/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepencoder/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_ocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_v2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_vl2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/dots1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/dots_ocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45_vl_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/exaone/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/exaone4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/fairseq2_llama/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/falcon/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/falcon_h1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/flex_olmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/fuyu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3n/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3n_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4_1v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4_moe_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_bigcode/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_j/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_neox/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_oss/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granite/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granite_speech/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granitemoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granitemoehybrid/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granitemoeshared/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gritlm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/grok1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/h2ovl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/hunyuan_v1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/hunyuan_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/hyperclovax_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/idefics2_vision_model/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/idefics3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interfaces/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interfaces_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/intern_vit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/internlm2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/internlm2_ve/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interns1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interns1_vit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/internvl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jais/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jais2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jina_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/keye/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/keye_vl1_5/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/kimi_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/kimi_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/lfm2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/lfm2_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/lightonocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama4_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama_eagle3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava_next/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava_next_video/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava_onevision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/longcat_flash/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/longcat_flash_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mamba2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/medusa/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/midashenglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mimo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mimo_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mimo_v2_flash/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpm3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpm_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpmv/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minimax_m2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minimax_text_01/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minimax_vl_01/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mistral3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mistral_large_3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mistral_large_3_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mixtral/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mllama4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mlp_speculator/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/modernbert/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/module_mapping/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/molmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/moonvit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mpt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nano_nemotron_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron_h/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron_nas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nvlm_d/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/olmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/olmo2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/olmoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/opencua/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/openpangu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/openpangu_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/opt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/orion/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ouro/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ovis/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ovis2_5/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/paddleocr_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/paligemma/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/persimmon/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi3v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi4mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi4mm_audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi4mm_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phimoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/pixtral/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/plamo2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/plamo3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_5_omni_thinker/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_5_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_rm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_next/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_next_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_omni_moe_thinker/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_vl_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/radio/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/roberta/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/rvl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/seed_oss/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/siglip/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/siglip2navit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/skyworkr1v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/smolvlm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/solar/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/stablelm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/starcoder2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/step3_text/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/step3_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/swin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/tarsier/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/telechat2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/teleflm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/terratorch/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ultravox/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/voxtral/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/voxtral_streaming/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/whisper/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/whisper_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/zamba2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/causal/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/legacy/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/multimodal/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/pooling/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/warmup/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/warmup/deep_gemm_warmup/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/warmup/kernel_warmup/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/evs/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/hasher/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/image/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/inputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/parse/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/processing/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/profiling/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/video/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/cuda/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/rocm/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/xpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/io_processors/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/io_processors/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/lora_resolvers/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/lora_resolvers/filesystem_resolver/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/layerwise_profile/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/ray/",
    "https://docs.vllm.ai/en/latest/api/vllm/ray/lazy_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/ray/ray_env/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/abs_reasoning_parsers/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/basic_parsers/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/deepseek_r1_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/deepseek_v3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/ernie45_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/glm4_moe_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/gptoss_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/granite_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/holo2_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/hunyuan_a13b_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/identity_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/minimax_m2_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/mistral_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/olmo3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/qwen3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/seedoss_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/step3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/deepseek_v32/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/deepseek_v32_encoding/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/detokenizer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/hf/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/mistral/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/abstract_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/deepseekv3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/deepseekv31_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/deepseekv32_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/ernie45_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/functiongemma_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/gigachat3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/glm4_moe_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/glm47_moe_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/granite_20b_fc_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/granite_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/hermes_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/hunyuan_a13b_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/internlm2_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/jamba_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/kimi_k2_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/llama4_pythonic_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/llama_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/longcat_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/minimax_m2_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/minimax_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/mistral_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/olmo3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/openai_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/phi4mini_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/pythonic_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/qwen3coder_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/qwen3xml_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/seed_oss_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/step3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/xlam_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/config_parser_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/dynamic_module/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/gguf_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/repo_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/runai_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/s3_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/tokenizer/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/chat_templates/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/chat_templates/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/afmoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/arctic/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/bagel/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/chatglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/deepseek_vl2/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/dotsocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/falcon/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/flex_olmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/hunyuan_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/jais/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/kimi_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/kimi_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/lfm2_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/medusa/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/midashenglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/mistral/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/mlp_speculator/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/moonvit/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/nemotron/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/nemotron_h/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/olmo3/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/ovis/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/qwen3_next/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/radio/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/step3_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/tarsier2/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/ultravox/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/speculators/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/speculators/algos/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/speculators/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/bagel/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/deepseek_ocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/deepseek_vl2/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/hunyuan_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/hunyuan_vl_image/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/ovis/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/ovis2_5/",
    "https://docs.vllm.ai/en/latest/api/vllm/triton_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/triton_utils/importing/",
    "https://docs.vllm.ai/en/latest/api/vllm/usage/",
    "https://docs.vllm.ai/en/latest/api/vllm/usage/usage_lib/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/argparse_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/async_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/collection_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/counter/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/deep_gemm/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/flashinfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/func_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/gc_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/hashing/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/import_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/jsontree/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/math_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/mem_constants/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/mem_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/nccl/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/network_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/nvtx_pytorch_hooks/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/platform_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/profiling/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/serial_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/system_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/tensor_schema/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/torch_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/cudagraph_dispatcher/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_cache_interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/request/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/serial_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/cpu_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/flash_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/flashinfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/flex_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/gdn_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/linear_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mamba1_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mamba2_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mamba_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/pallas/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/rocm_aiter_fa/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/rocm_aiter_unified_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/rocm_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/short_conv_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/tree_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/triton_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/aiter_triton_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/common/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/cutlass_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashattn_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashinfer_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashmla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashmla_sparse/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/indexer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/rocm_aiter_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/rocm_aiter_mla_sparse/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/triton_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/block_pool/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/encoder_cache_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_coordinator/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/single_type_kv_cache_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/async_scheduler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/output/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/request_queue/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/scheduler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/async_llm/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/coordinator/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/core/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/core_client/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/detokenizer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/exceptions/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/input_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/llm_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/logprobs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/output_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/parallel_sampling/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/multiproc_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/ray_distributed_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/ray_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/ray_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/uniproc_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/arc_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/backend/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/factory/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/lru_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/mediums/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/spec/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/backends/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/worker/cpu_gpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/worker/worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/loggers/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/perf/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/prometheus/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/ray_wrappers/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/reader/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/stats/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/pool/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/pool/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/rejection_sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/builtin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/state/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/bad_words/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/logprobs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/penalties/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/topk_topp_sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/tpu/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/tpu/sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/medusa/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/ngram_proposer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/suffix_decoding/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_guidance/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_lm_format_enforcer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_outlines/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_types/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_xgrammar/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/request/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/block_table/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/cp_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/cpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/cpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/dp_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/ec_connector_model_runner_mixin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_input_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_ubatch_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/kv_connector_model_runner_mixin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/lora_model_runner_mixin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/tpu_input_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/tpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/tpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/ubatch_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/ubatching/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/worker_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/workspace/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/xpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/xpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/async_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/attn_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/block_table/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/cudagraph_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/dp_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/input_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/states/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/structured_outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/metrics/logits/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/gumbel/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/logprob/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/min_p/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/output/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/penalties/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/eagle_cudagraph/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/rejection_sample/",
    "https://docs.vllm.ai/en/latest/cli/serve/",
    "https://docs.vllm.ai/en/latest/cli/chat/",
    "https://docs.vllm.ai/en/latest/cli/complete/",
    "https://docs.vllm.ai/en/latest/cli/run-batch/",
    "https://docs.vllm.ai/en/latest/cli/bench/latency/",
    "https://docs.vllm.ai/en/latest/cli/bench/serve/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/plot/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/plot_pareto/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/serve/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/serve_sla/",
    "https://docs.vllm.ai/en/latest/cli/bench/throughput/",
    "https://docs.vllm.ai/en/latest/community/meetups/",
    "https://docs.vllm.ai/en/latest/community/sponsors/",
    "https://docs.vllm.ai/en/latest/governance/collaboration/",
    "https://docs.vllm.ai/en/latest/governance/committers/",
    "https://docs.vllm.ai/en/latest/governance/process/"
  ]
}