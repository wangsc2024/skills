{
  "url": "https://docs.vllm.ai/en/latest/api/vllm/config/parallel/",
  "title": "parallel - vLLM",
  "content": "Configuration for Expert Parallel Load Balancing (EP).\n\nLog the balancedness each step of expert parallelism. This is turned off by default since it will cause communication overhead.\n\nNumber of redundant experts to use for expert parallelism.\n\nThe policy type for expert parallel load balancing (EPLB).\n\nInterval for rearranging experts in expert parallelism.\n\nNote that if this is greater than the EPLB window size, only the metrics of the last lb_window_size steps will be used for rearranging experts.\n\nWhether to use non-blocking EPLB.\n\nWindow size for expert load recording.\n\nConfiguration for the distributed execution.\n\nThe number of API processes initialized.\n\nThis is an internal config that is only valid for and should only be set by API server scale-out.\n\nThe rank of this API process, or -1 for engine core processes under API server scale-out.\n\nThis is an internal config that is only valid for and should only be set by API server scale-out.\n\nList of open port auto-queried for data parallel messaging. Set to be private as it's not intended to be configured by users.\n\nAll2All backend for MoE expert parallel communication. Available options:\n\n\"naive\": Naive all2all implementation using broadcasts\n\n\"allgather_reducescatter\": All2all based on allgather and reducescatter\n\n\"pplx\": Use pplx kernels\n\n\"deepep_high_throughput\": Use deepep high-throughput kernels\n\n\"deepep_low_latency\": Use deepep low-latency kernels\n\n\"flashinfer_all2allv\": Use flashinfer alltoallv kernels for mnnvl\n\nInterleave size of kv_cache storage while using DCP or PCP. For total_cp_rank = pcp_rank * dcp_world_size + dcp_rank, and total_cp_world_size = pcp_world_size * dcp_world_size. store interleave_size tokens on total_cp_rank i, then store next interleave_size tokens on total_cp_rank i+1. Interleave_size=1: token-level alignment, where token i is stored on total_cp_rank i % total_cp_world_size. Interleave_size=block_size: block-level alignment, where tokens are first populated to the preceding ranks. Tokens are then stored in (rank i+1, block j) only after (rank i, block j) is fully occupied. Block_size should be greater than or equal to cp_kv_cache_interleave_size. Block_size should be divisible by cp_kv_cache_interleave_size.\n\nBackend to use for data parallel, either \"mp\" or \"ray\".\n\nWhether to use \"external\" DP LB mode. Applies only to online serving and when data_parallel_size > 0. This is useful for a \"one-pod-per-rank\" wide-EP setup in Kubernetes. Set implicitly when --data-parallel-rank is provided explicitly to vllm serve.\n\nWhether to use \"hybrid\" DP LB mode. Applies only to online serving and when data_parallel_size > 0. Enables running an AsyncLLM and API server on a \"per-node\" basis where vLLM load balances between local data parallel ranks, but an external LB balances between vLLM nodes/replicas. Set explicitly in conjunction with --data-parallel-start-rank.\n\nIP of the data parallel master.\n\nPort of the data parallel master.\n\nRank of the data parallel group.\n\nLocal rank of the data parallel group, set only in SPMD mode.\n\nPort for data parallel messaging.\n\nNumber of data parallel groups. MoE layers will be sharded according to the product of the tensor parallel size and data parallel size.\n\nNumber of local data parallel groups.\n\nThe threshold for dual batch overlap for batches only containing decodes. If the number of tokens in the request is greater than this threshold, microbatching will be used. Otherwise, the request will be processed in a single batch.\n\nThe threshold for dual batch overlap for batches that contain one or more prefills. If the number of tokens in the request is greater than this threshold, microbatching will be used. Otherwise, the request will be processed in a single batch.\n\nInterleave size of kv_cache storage while using DCP. dcp_kv_cache_interleave_size has been replaced by cp_kv_cache_interleave_size, and will be deprecated when PCP is fully supported.\n\nNumber of decode context parallel groups, because the world size does not change by dcp, it simply reuse the GPUs of TP group, and tp_size needs to be divisible by dcp_size.\n\nDisable the custom all-reduce kernel and fall back to NCCL.\n\nForces the dp synchronization logic in vllm/v1/worker/dp_utils.py to use Gloo instead of NCCL for its all reduce\n\nBackend to use for distributed model workers, either \"ray\" or \"mp\" (multiprocessing). If the product of pipeline_parallel_size and tensor_parallel_size is less than or equal to the number of GPUs available, \"mp\" will be used to keep processing on a single host. Otherwise, an error will be raised. To use \"mp\" you must also set nnodes, and to use \"ray\" you must manually set distributed_executor_backend to \"ray\".\n\nNote that tpu only support Ray for distributed inference.\n\nEnable dual batch overlap for the model executor.\n\nEnable expert parallelism load balancing for MoE layers.\n\nUse expert parallelism instead of tensor parallelism for MoE layers.\n\nExpert parallelism configuration.\n\nThe expert placement strategy for MoE layers:\n\n\"linear\": Experts are placed in a contiguous manner. For example, with 4 experts and 2 ranks, rank 0 will have experts [0, 1] and rank 1 will have experts [2, 3].\n\n\"round_robin\": Experts are placed in a round-robin manner. For example, with 4 experts and 2 ranks, rank 0 will have experts [0, 2] and rank 1 will have experts [1, 3]. This strategy can help improve load balancing for grouped expert models with no redundant experts.\n\ndistributed master address for multi-node distributed inference when distributed_executor_backend is mp.\n\ndistributed master port for multi-node distributed inference when distributed_executor_backend is mp.\n\nMaximum number of parallel loading workers when loading model sequentially in multiple batches. To avoid RAM OOM when using tensor parallel and large models.\n\nnum of nodes for multi-node distributed inference when distributed_executor_backend is mp.\n\ndistributed node rank for multi-node distributed inference when distributed_executor_backend is mp.\n\nNumber of pipeline parallel groups.\n\nray distributed model workers placement group.\n\nNumber of prefill context parallel groups.\n\nGlobal rank in distributed setup.\n\nRay runtime environment to pass to distributed workers.\n\nWhether to profile Ray workers with nsight, see https://docs.ray.io/en/latest/ray-observability/user-guides/profiling.html#profiling-nsight-profiler.\n\nThe full name of the worker class to use for speculative decoding. If \"auto\", the worker class will be determined based on the platform.\n\nNumber of tensor parallel groups.\n\nNumber of ubatch size.\n\nThe full name of the worker class to use. If \"auto\", the worker class will be determined based on the platform.\n\nThe full name of the worker extension class to use. The worker extension class is dynamically inherited by the worker class. This is used to inject new attributes and methods to the worker class for use in collective_rpc calls.\n\nworld_size is TPxPP, it affects the number of workers we create.\n\nworld_size_across_dp is TPxPPxDP, it is the size of the world including data parallelism.\n\nProvide a hash that uniquely identifies all the configs that affect the structure of the computation graph from input ids/embeddings to the final hidden states, excluding anything before input ids/embeddings and after the final hidden states.\n\nThis hash is also used for DP worker configuration validation to prevent hangs from mismatched collective communication patterns.\n\nWe might need to initialize process groups in multiple processes that is related to data parallelism, e.g. both in the worker and in the engine, which can live in different processes. To avoid port conflicts, we pop a new port from the prepared port list each time we need to initialize a new process group related to data parallelism.",
  "headings": [
    {
      "level": "h1",
      "text": "vllm.config.parallel ¶",
      "id": "vllm.config.parallel"
    },
    {
      "level": "h2",
      "text": "All2AllBackend module-attribute ¶",
      "id": "vllm.config.parallel.All2AllBackend"
    },
    {
      "level": "h2",
      "text": "DataParallelBackend module-attribute ¶",
      "id": "vllm.config.parallel.DataParallelBackend"
    },
    {
      "level": "h2",
      "text": "DistributedExecutorBackend module-attribute ¶",
      "id": "vllm.config.parallel.DistributedExecutorBackend"
    },
    {
      "level": "h2",
      "text": "EPLBPolicyOption module-attribute ¶",
      "id": "vllm.config.parallel.EPLBPolicyOption"
    },
    {
      "level": "h2",
      "text": "ExpertPlacementStrategy module-attribute ¶",
      "id": "vllm.config.parallel.ExpertPlacementStrategy"
    },
    {
      "level": "h2",
      "text": "logger module-attribute ¶",
      "id": "vllm.config.parallel.logger"
    },
    {
      "level": "h2",
      "text": "EPLBConfig ¶",
      "id": "vllm.config.parallel.EPLBConfig"
    },
    {
      "level": "h3",
      "text": "log_balancedness class-attribute instance-attribute ¶",
      "id": "vllm.config.parallel.EPLBConfig.log_balancedness"
    },
    {
      "level": "h3",
      "text": "num_redundant_experts class-attribute instance-attribute ¶",
      "id": "vllm.config.parallel.EPLBConfig.num_redundant_experts"
    },
    {
      "level": "h3",
      "text": "policy class-attribute instance-attribute ¶",
      "id": "vllm.config.parallel.EPLBConfig.policy"
    },
    {
      "level": "h3",
      "text": "step_interval class-attribute instance-attribute ¶",
      "id": "vllm.config.parallel.EPLBConfig.step_interval"
    },
    {
      "level": "h3",
      "text": "use_async class-attribute instance-attribute ¶",
      "id": "vllm.config.parallel.EPLBConfig.use_async"
    },
    {
      "level": "h3",
      "text": "window_size class-attribute instance-attribute ¶",
      "id": "vllm.config.parallel.EPLBConfig.window_size"
    },
    {
      "level": "h2",
      "text": "ParallelConfig ¶",
      "id": "vllm.config.parallel.ParallelConfig"
    },
    {
      "level": "h3",
      "text": "_api_process_count class-attribute instance-attribute ¶",
      "id": "vllm.config.parallel.ParallelConfig._api_process_count"
    },
    {
      "level": "h3",
      "text": "_api_process_rank class-attribute instance-attribute ¶",
      "id": "vllm.config.parallel.ParallelConfig._api_process_rank"
    },
    {
      "level": "h3",
      "text": "_data_parallel_master_port_list class-attribute instance-attribute ¶",
      "id": "vllm.config.parallel.ParallelConfig._data_parallel_master_port_list"
    },
    {
      "level": "h3",
      "text": "all2all_backend class-attribute instance-attribute ¶",
      "id": "vllm.config.parallel.ParallelConfig.all2all_backend"
    },
    {
      "level": "h3",
      "text": "cp_kv_cache_interleave_size class-attribute instance-attribute ¶",
      "id": "vllm.config.parallel.ParallelConfig.cp_kv_cache_interleave_size"
    },
    {
      "level": "h3",
      "text": "data_parallel_backend class-attribute instance-attribute ¶",
      "id": "vllm.config.parallel.ParallelConfig.data_parallel_backend"
    },
    {
      "level": "h3",
      "text": "data_parallel_external_lb class-attribute instance-attribute ¶",
      "id": "vllm.config.parallel.ParallelConfig.data_parallel_external_lb"
    },
    {
      "level": "h3",
      "text": "data_parallel_hybrid_lb class-attribute instance-attribute ¶",
      "id": "vllm.config.parallel.ParallelConfig.data_parallel_hybrid_lb"
    },
    {
      "level": "h3",
      "text": "data_parallel_master_ip class-attribute instance-attribute ¶",
      "id": "vllm.config.parallel.ParallelConfig.data_parallel_master_ip"
    },
    {
      "level": "h3",
      "text": "data_parallel_master_port class-attribute instance-attribute ¶",
      "id": "vllm.config.parallel.ParallelConfig.data_parallel_master_port"
    },
    {
      "level": "h3",
      "text": "data_parallel_rank class-attribute instance-attribute ¶",
      "id": "vllm.config.parallel.ParallelConfig.data_parallel_rank"
    },
    {
      "level": "h3",
      "text": "data_parallel_rank_local class-attribute instance-attribute ¶",
      "id": "vllm.config.parallel.ParallelConfig.data_parallel_rank_local"
    },
    {
      "level": "h3",
      "text": "data_parallel_rpc_port class-attribute instance-attribute ¶",
      "id": "vllm.config.parallel.ParallelConfig.data_parallel_rpc_port"
    },
    {
      "level": "h3",
      "text": "data_parallel_size class-attribute instance-attribute ¶",
      "id": "vllm.config.parallel.ParallelConfig.data_parallel_size"
    },
    {
      "level": "h3",
      "text": "data_parallel_size_local class-attribute instance-attribute ¶",
      "id": "vllm.config.parallel.ParallelConfig.data_parallel_size_local"
    },
    {
      "level": "h3",
      "text": "dbo_decode_token_threshold class-attribute instance-attribute ¶",
      "id": "vllm.config.parallel.ParallelConfig.dbo_decode_token_threshold"
    },
    {
      "level": "h3",
      "text": "dbo_prefill_token_threshold class-attribute instance-attribute ¶",
      "id": "vllm.config.parallel.ParallelConfig.dbo_prefill_token_threshold"
    },
    {
      "level": "h3",
      "text": "dcp_kv_cache_interleave_size class-attribute instance-attribute ¶",
      "id": "vllm.config.parallel.ParallelConfig.dcp_kv_cache_interleave_size"
    },
    {
      "level": "h3",
      "text": "decode_context_parallel_size class-attribute instance-attribute ¶",
      "id": "vllm.config.parallel.ParallelConfig.decode_context_parallel_size"
    },
    {
      "level": "h3",
      "text": "disable_custom_all_reduce class-attribute instance-attribute ¶",
      "id": "vllm.config.parallel.ParallelConfig.disable_custom_all_reduce"
    },
    {
      "level": "h3",
      "text": "disable_nccl_for_dp_synchronization class-attribute instance-attribute ¶",
      "id": "vllm.config.parallel.ParallelConfig.disable_nccl_for_dp_synchronization"
    },
    {
      "level": "h3",
      "text": "distributed_executor_backend class-attribute instance-attribute ¶",
      "id": "vllm.config.parallel.ParallelConfig.distributed_executor_backend"
    },
    {
      "level": "h3",
      "text": "enable_dbo class-attribute instance-attribute ¶",
      "id": "vllm.config.parallel.ParallelConfig.enable_dbo"
    },
    {
      "level": "h3",
      "text": "enable_eplb class-attribute instance-attribute ¶",
      "id": "vllm.config.parallel.ParallelConfig.enable_eplb"
    },
    {
      "level": "h3",
      "text": "enable_expert_parallel class-attribute instance-attribute ¶",
      "id": "vllm.config.parallel.ParallelConfig.enable_expert_parallel"
    },
    {
      "level": "h3",
      "text": "eplb_config class-attribute instance-attribute ¶",
      "id": "vllm.config.parallel.ParallelConfig.eplb_config"
    },
    {
      "level": "h3",
      "text": "expert_placement_strategy class-attribute instance-attribute ¶",
      "id": "vllm.config.parallel.ParallelConfig.expert_placement_strategy"
    },
    {
      "level": "h3",
      "text": "local_world_size property ¶",
      "id": "vllm.config.parallel.ParallelConfig.local_world_size"
    },
    {
      "level": "h3",
      "text": "master_addr class-attribute instance-attribute ¶",
      "id": "vllm.config.parallel.ParallelConfig.master_addr"
    },
    {
      "level": "h3",
      "text": "master_port class-attribute instance-attribute ¶",
      "id": "vllm.config.parallel.ParallelConfig.master_port"
    },
    {
      "level": "h3",
      "text": "max_parallel_loading_workers class-attribute instance-attribute ¶",
      "id": "vllm.config.parallel.ParallelConfig.max_parallel_loading_workers"
    },
    {
      "level": "h3",
      "text": "nnodes class-attribute instance-attribute ¶",
      "id": "vllm.config.parallel.ParallelConfig.nnodes"
    },
    {
      "level": "h3",
      "text": "nnodes_within_dp property ¶",
      "id": "vllm.config.parallel.ParallelConfig.nnodes_within_dp"
    },
    {
      "level": "h3",
      "text": "node_rank class-attribute instance-attribute ¶",
      "id": "vllm.config.parallel.ParallelConfig.node_rank"
    },
    {
      "level": "h3",
      "text": "node_rank_within_dp property ¶",
      "id": "vllm.config.parallel.ParallelConfig.node_rank_within_dp"
    },
    {
      "level": "h3",
      "text": "num_ubatches property ¶",
      "id": "vllm.config.parallel.ParallelConfig.num_ubatches"
    },
    {
      "level": "h3",
      "text": "pipeline_parallel_size class-attribute instance-attribute ¶",
      "id": "vllm.config.parallel.ParallelConfig.pipeline_parallel_size"
    },
    {
      "level": "h3",
      "text": "placement_group class-attribute instance-attribute ¶",
      "id": "vllm.config.parallel.ParallelConfig.placement_group"
    },
    {
      "level": "h3",
      "text": "prefill_context_parallel_size class-attribute instance-attribute ¶",
      "id": "vllm.config.parallel.ParallelConfig.prefill_context_parallel_size"
    },
    {
      "level": "h3",
      "text": "rank class-attribute instance-attribute ¶",
      "id": "vllm.config.parallel.ParallelConfig.rank"
    },
    {
      "level": "h3",
      "text": "ray_runtime_env class-attribute instance-attribute ¶",
      "id": "vllm.config.parallel.ParallelConfig.ray_runtime_env"
    },
    {
      "level": "h3",
      "text": "ray_workers_use_nsight class-attribute instance-attribute ¶",
      "id": "vllm.config.parallel.ParallelConfig.ray_workers_use_nsight"
    },
    {
      "level": "h3",
      "text": "sd_worker_cls class-attribute instance-attribute ¶",
      "id": "vllm.config.parallel.ParallelConfig.sd_worker_cls"
    },
    {
      "level": "h3",
      "text": "tensor_parallel_size class-attribute instance-attribute ¶",
      "id": "vllm.config.parallel.ParallelConfig.tensor_parallel_size"
    },
    {
      "level": "h3",
      "text": "ubatch_size class-attribute instance-attribute ¶",
      "id": "vllm.config.parallel.ParallelConfig.ubatch_size"
    },
    {
      "level": "h3",
      "text": "use_ray property ¶",
      "id": "vllm.config.parallel.ParallelConfig.use_ray"
    },
    {
      "level": "h3",
      "text": "use_sequence_parallel_moe property ¶",
      "id": "vllm.config.parallel.ParallelConfig.use_sequence_parallel_moe"
    },
    {
      "level": "h3",
      "text": "use_ubatching property ¶",
      "id": "vllm.config.parallel.ParallelConfig.use_ubatching"
    },
    {
      "level": "h3",
      "text": "worker_cls class-attribute instance-attribute ¶",
      "id": "vllm.config.parallel.ParallelConfig.worker_cls"
    },
    {
      "level": "h3",
      "text": "worker_extension_cls class-attribute instance-attribute ¶",
      "id": "vllm.config.parallel.ParallelConfig.worker_extension_cls"
    },
    {
      "level": "h3",
      "text": "world_size class-attribute instance-attribute ¶",
      "id": "vllm.config.parallel.ParallelConfig.world_size"
    },
    {
      "level": "h3",
      "text": "world_size_across_dp property ¶",
      "id": "vllm.config.parallel.ParallelConfig.world_size_across_dp"
    },
    {
      "level": "h3",
      "text": "__post_init__ ¶",
      "id": "vllm.config.parallel.ParallelConfig.__post_init__"
    },
    {
      "level": "h3",
      "text": "_validate_parallel_config ¶",
      "id": "vllm.config.parallel.ParallelConfig._validate_parallel_config"
    },
    {
      "level": "h3",
      "text": "_verify_args ¶",
      "id": "vllm.config.parallel.ParallelConfig._verify_args"
    },
    {
      "level": "h3",
      "text": "compute_hash ¶",
      "id": "vllm.config.parallel.ParallelConfig.compute_hash"
    },
    {
      "level": "h3",
      "text": "get_next_dp_init_port ¶",
      "id": "vllm.config.parallel.ParallelConfig.get_next_dp_init_port"
    },
    {
      "level": "h3",
      "text": "has_unfinished_dp staticmethod ¶",
      "id": "vllm.config.parallel.ParallelConfig.has_unfinished_dp"
    },
    {
      "level": "h3",
      "text": "stateless_init_dp_group ¶",
      "id": "vllm.config.parallel.ParallelConfig.stateless_init_dp_group"
    },
    {
      "level": "h3",
      "text": "sync_kv_cache_memory_size staticmethod ¶",
      "id": "vllm.config.parallel.ParallelConfig.sync_kv_cache_memory_size"
    }
  ],
  "code_samples": [
    {
      "code": "All2AllBackend = Literal[\n    \"naive\",\n    \"pplx\",\n    \"deepep_high_throughput\",\n    \"deepep_low_latency\",\n    \"allgather_reducescatter\",\n    \"flashinfer_all2allv\",\n]",
      "language": "unknown"
    },
    {
      "code": "All2AllBackend = Literal[\n    \"naive\",\n    \"pplx\",\n    \"deepep_high_throughput\",\n    \"deepep_low_latency\",\n    \"allgather_reducescatter\",\n    \"flashinfer_all2allv\",\n]",
      "language": "unknown"
    },
    {
      "code": "DataParallelBackend = Literal['ray', 'mp']",
      "language": "unknown"
    },
    {
      "code": "DataParallelBackend = Literal['ray', 'mp']",
      "language": "unknown"
    },
    {
      "code": "DistributedExecutorBackend = Literal[\n    \"ray\", \"mp\", \"uni\", \"external_launcher\"\n]",
      "language": "unknown"
    },
    {
      "code": "DistributedExecutorBackend = Literal[\n    \"ray\", \"mp\", \"uni\", \"external_launcher\"\n]",
      "language": "unknown"
    },
    {
      "code": "EPLBPolicyOption = Literal['default']",
      "language": "unknown"
    },
    {
      "code": "EPLBPolicyOption = Literal['default']",
      "language": "unknown"
    },
    {
      "code": "ExpertPlacementStrategy = Literal['linear', 'round_robin']",
      "language": "unknown"
    },
    {
      "code": "ExpertPlacementStrategy = Literal['linear', 'round_robin']",
      "language": "unknown"
    },
    {
      "code": "logger = init_logger(__name__)",
      "language": "unknown"
    },
    {
      "code": "logger = init_logger(__name__)",
      "language": "unknown"
    },
    {
      "code": "49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78",
      "language": "unknown"
    },
    {
      "code": "@config\n@dataclass\nclass EPLBConfig:\n    \"\"\"Configuration for Expert Parallel Load Balancing (EP).\"\"\"\n\n    window_size: int = 1000\n    \"\"\"Window size for expert load recording.\"\"\"\n    step_interval: int = 3000\n    \"\"\"\n    Interval for rearranging experts in expert parallelism.\n\n    Note that if this is greater than the EPLB window size, only the metrics\n    of the last `lb_window_size` steps will be used for rearranging experts.\n    \"\"\"\n\n    num_redundant_experts: int = Field(default=0, ge=0)\n    \"\"\"Number of redundant experts to use for expert parallelism.\"\"\"\n\n    log_balancedness: bool = False\n    \"\"\"\n    Log the balancedness each step of expert parallelism.\n    This is turned off by default since it will cause communication overhead.\n    \"\"\"\n    use_async: bool = False\n    \"\"\"\n    Whether to use non-blocking EPLB.\n    \"\"\"\n\n    policy: EPLBPolicyOption = \"default\"\n    \"\"\"The policy type for expert parallel load balancing (EPLB).\"\"\"",
      "language": "python"
    },
    {
      "code": "@config\n@dataclass\nclass EPLBConfig:\n    \"\"\"Configuration for Expert Parallel Load Balancing (EP).\"\"\"\n\n    window_size: int = 1000\n    \"\"\"Window size for expert load recording.\"\"\"\n    step_interval: int = 3000\n    \"\"\"\n    Interval for rearranging experts in expert parallelism.\n\n    Note that if this is greater than the EPLB window size, only the metrics\n    of the last `lb_window_size` steps will be used for rearranging experts.\n    \"\"\"\n\n    num_redundant_experts: int = Field(default=0, ge=0)\n    \"\"\"Number of redundant experts to use for expert parallelism.\"\"\"\n\n    log_balancedness: bool = False\n    \"\"\"\n    Log the balancedness each step of expert parallelism.\n    This is turned off by default since it will cause communication overhead.\n    \"\"\"\n    use_async: bool = False\n    \"\"\"\n    Whether to use non-blocking EPLB.\n    \"\"\"\n\n    policy: EPLBPolicyOption = \"default\"\n    \"\"\"The policy type for expert parallel load balancing (EPLB).\"\"\"",
      "language": "python"
    },
    {
      "code": "log_balancedness: bool = False",
      "language": "typescript"
    },
    {
      "code": "log_balancedness: bool = False",
      "language": "typescript"
    },
    {
      "code": "num_redundant_experts: int = Field(default=0, ge=0)",
      "language": "typescript"
    },
    {
      "code": "num_redundant_experts: int = Field(default=0, ge=0)",
      "language": "typescript"
    },
    {
      "code": "policy: EPLBPolicyOption = 'default'",
      "language": "typescript"
    },
    {
      "code": "policy: EPLBPolicyOption = 'default'",
      "language": "typescript"
    },
    {
      "code": "step_interval: int = 3000",
      "language": "typescript"
    },
    {
      "code": "step_interval: int = 3000",
      "language": "typescript"
    },
    {
      "code": "use_async: bool = False",
      "language": "typescript"
    },
    {
      "code": "use_async: bool = False",
      "language": "typescript"
    },
    {
      "code": "window_size: int = 1000",
      "language": "typescript"
    },
    {
      "code": "window_size: int = 1000",
      "language": "typescript"
    },
    {
      "code": "81\n 82\n 83\n 84\n 85\n 86\n 87\n 88\n 89\n 90\n 91\n 92\n 93\n 94\n 95\n 96\n 97\n 98\n 99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n500\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\n600\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666",
      "language": "unknown"
    },
    {
      "code": "@config\n@dataclass\nclass ParallelConfig:\n    \"\"\"Configuration for the distributed execution.\"\"\"\n\n    pipeline_parallel_size: int = 1\n    \"\"\"Number of pipeline parallel groups.\"\"\"\n    tensor_parallel_size: int = 1\n    \"\"\"Number of tensor parallel groups.\"\"\"\n    prefill_context_parallel_size: int = 1\n    \"\"\"Number of prefill context parallel groups.\"\"\"\n    data_parallel_size: int = 1\n    \"\"\"Number of data parallel groups. MoE layers will be sharded according to\n    the product of the tensor parallel size and data parallel size.\"\"\"\n    data_parallel_size_local: int = 1\n    \"\"\"Number of local data parallel groups.\"\"\"\n    data_parallel_rank: int = 0\n    \"\"\"Rank of the data parallel group.\"\"\"\n    data_parallel_rank_local: int | None = None\n    \"\"\"Local rank of the data parallel group,\n    set only in SPMD mode.\"\"\"\n    data_parallel_master_ip: str = \"127.0.0.1\"\n    \"\"\"IP of the data parallel master.\"\"\"\n    data_parallel_rpc_port: int = 29550\n    \"\"\"Port for data parallel messaging.\"\"\"\n    data_parallel_master_port: int = 29500\n    \"\"\"Port of the data parallel master.\"\"\"\n    data_parallel_backend: DataParallelBackend = \"mp\"\n    \"\"\"Backend to use for data parallel, either \"mp\" or \"ray\".\"\"\"\n    data_parallel_external_lb: bool = False\n    \"\"\"Whether to use \"external\" DP LB mode. Applies only to online serving\n    and when data_parallel_size > 0. This is useful for a \"one-pod-per-rank\"\n    wide-EP setup in Kubernetes. Set implicitly when --data-parallel-rank\n    is provided explicitly to vllm serve.\"\"\"\n    data_parallel_hybrid_lb: bool = False\n    \"\"\"Whether to use \"hybrid\" DP LB mode. Applies only to online serving\n    and when data_parallel_size > 0. Enables running an AsyncLLM\n    and API server on a \"per-node\" basis where vLLM load balances\n    between local data parallel ranks, but an external LB balances\n    between vLLM nodes/replicas. Set explicitly in conjunction with\n    --data-parallel-start-rank.\"\"\"\n    enable_expert_parallel: bool = False\n    \"\"\"Use expert parallelism instead of tensor parallelism for MoE layers.\"\"\"\n    enable_eplb: bool = False\n    \"\"\"Enable expert parallelism load balancing for MoE layers.\"\"\"\n    eplb_config: EPLBConfig = Field(default_factory=EPLBConfig)\n    \"\"\"Expert parallelism configuration.\"\"\"\n    expert_placement_strategy: ExpertPlacementStrategy = \"linear\"\n    \"\"\"The expert placement strategy for MoE layers:\\n\n    - \"linear\": Experts are placed in a contiguous manner. For example, with 4\n      experts and 2 ranks, rank 0 will have experts [0, 1] and rank 1 will have\n      experts [2, 3].\\n\n    - \"round_robin\": Experts are placed in a round-robin manner. For example,\n      with 4 experts and 2 ranks, rank 0 will have experts [0, 2] and rank 1\n      will have experts [1, 3]. This strategy can help improve load balancing\n      for grouped expert models with no redundant experts.\"\"\"\n    all2all_backend: All2AllBackend = \"allgather_reducescatter\"\n    \"\"\"All2All backend for MoE expert parallel communication. Available options:\n\n    - \"naive\": Naive all2all implementation using broadcasts\\n\n    - \"allgather_reducescatter\": All2all based on allgather and reducescatter\\n\n    - \"pplx\": Use pplx kernels\\n\n    - \"deepep_high_throughput\": Use deepep high-throughput kernels\\n\n    - \"deepep_low_latency\": Use deepep low-latency kernels\\n\n    - \"flashinfer_all2allv\": Use flashinfer alltoallv kernels for mnnvl\"\"\"\n\n    max_parallel_loading_workers: int | None = None\n    \"\"\"Maximum number of parallel loading workers when loading model\n    sequentially in multiple batches. To avoid RAM OOM when using tensor\n    parallel and large models.\"\"\"\n\n    disable_custom_all_reduce: bool = False\n    \"\"\"Disable the custom all-reduce kernel and fall back to NCCL.\"\"\"\n\n    enable_dbo: bool = False\n    \"\"\"Enable dual batch overlap for the model executor.\"\"\"\n    ubatch_size: int = 0\n    \"\"\"Number of ubatch size.\"\"\"\n\n    dbo_decode_token_threshold: int = 32\n    \"\"\"The threshold for dual batch overlap for batches only containing decodes.\n    If the number of tokens in the request is greater than this threshold,\n    microbatching will be used. Otherwise, the request will be processed in a\n    single batch.\"\"\"\n    dbo_prefill_token_threshold: int = 512  # TODO(lucas): tune\n    \"\"\"The threshold for dual batch overlap for batches that contain one or more\n    prefills. If the number of tokens in the request is greater than this\n    threshold, microbatching will be used. Otherwise, the request will be\n    processed in a single batch.\"\"\"\n\n    disable_nccl_for_dp_synchronization: bool = False\n    \"\"\"Forces the dp synchronization logic in vllm/v1/worker/dp_utils.py \n    to use Gloo instead of NCCL for its all reduce\"\"\"\n\n    ray_workers_use_nsight: bool = False\n    \"\"\"Whether to profile Ray workers with nsight, see https://docs.ray.io/en/latest/ray-observability/user-guides/profiling.html#profiling-nsight-profiler.\"\"\"\n\n    ray_runtime_env: RuntimeEnv | None = None\n    \"\"\"Ray runtime environment to pass to distributed workers.\"\"\"\n\n    placement_group: PlacementGroup | None = None\n    \"\"\"ray distributed model workers placement group.\"\"\"\n\n    distributed_executor_backend: (\n        str | DistributedExecutorBackend | type[Executor] | None\n    ) = None\n    \"\"\"Backend to use for distributed model workers, either \"ray\" or \"mp\"\n    (multiprocessing). If the product of pipeline_parallel_size and tensor_parallel_size\n    is less than or equal to the number of GPUs available, \"mp\" will be used to\n    keep processing on a single host. Otherwise, an error will be raised. To use \"mp\"\n    you must also set nnodes, and to use \"ray\" you must manually set\n    distributed_executor_backend to \"ray\".\n\n    Note that tpu only support Ray for distributed inference.\"\"\"\n\n    worker_cls: str = \"auto\"\n    \"\"\"The full name of the worker class to use. If \"auto\", the worker class\n    will be determined based on the platform.\"\"\"\n    sd_worker_cls: str = \"auto\"\n    \"\"\"The full name of the worker class to use for speculative decoding.\n    If \"auto\", the worker class will be determined based on the platform.\"\"\"\n    worker_extension_cls: str = \"\"\n    \"\"\"The full name of the worker extension class to use. The worker extension\n    class is dynamically inherited by the worker class. This is used to inject\n    new attributes and methods to the worker class for use in collective_rpc\n    calls.\"\"\"\n    master_addr: str = \"127.0.0.1\"\n    \"\"\"distributed master address for multi-node distributed \n    inference when distributed_executor_backend is mp.\"\"\"\n    master_port: int = 29501\n    \"\"\"distributed master port for multi-node distributed \n    inference when distributed_executor_backend is mp.\"\"\"\n    node_rank: int = 0\n    \"\"\"distributed node rank for multi-node distributed \n    inference when distributed_executor_backend is mp.\"\"\"\n    nnodes: int = 1\n    \"\"\"num of nodes for multi-node distributed \n    inference when distributed_executor_backend is mp.\"\"\"\n\n    world_size: int = Field(init=False)\n    \"\"\"world_size is TPxPP, it affects the number of workers we create.\"\"\"\n\n    rank: int = 0\n    \"\"\"Global rank in distributed setup.\"\"\"\n\n    _data_parallel_master_port_list: list[int] = Field(default_factory=list)\n    \"\"\"List of open port auto-queried for data parallel messaging.\n    Set to be private as it's not intended to be configured by users.\n    \"\"\"\n\n    decode_context_parallel_size: int = 1\n    \"\"\"Number of decode context parallel groups, because the world size does\n    not change by dcp, it simply reuse the GPUs of TP group, and tp_size\n    needs to be divisible by dcp_size.\"\"\"\n\n    dcp_kv_cache_interleave_size: int = 1\n    \"\"\"\n    Interleave size of kv_cache storage while using DCP.\n    dcp_kv_cache_interleave_size has been replaced by cp_kv_cache_interleave_size,\n    and will be deprecated when PCP is fully supported.\n\n    \"\"\"\n    cp_kv_cache_interleave_size: int = 1\n    \"\"\"Interleave size of kv_cache storage while using DCP or PCP.\n    For `total_cp_rank = pcp_rank * dcp_world_size + dcp_rank`,\n        and `total_cp_world_size = pcp_world_size * dcp_world_size`.\n    store interleave_size tokens on total_cp_rank i,\n    then store next interleave_size tokens on total_cp_rank i+1.\n    Interleave_size=1: token-level alignment, where token `i` is stored on\n        total_cp_rank `i % total_cp_world_size`.\n    Interleave_size=block_size: block-level alignment, where tokens are\n        first populated to the preceding ranks. Tokens are then stored\n        in (rank i+1, block j) only after (rank i, block j) is fully occupied.\n    Block_size should be greater than or equal to cp_kv_cache_interleave_size.\n    Block_size should be divisible by cp_kv_cache_interleave_size.\n    \"\"\"\n\n    _api_process_count: int = Field(default=1, gt=0)\n    \"\"\"\n    The number of API processes initialized.\n\n    Note:\n        This is an internal config that is only valid for and\n        should only be set by API server scale-out.\n    \"\"\"\n\n    _api_process_rank: int = Field(default=0, ge=-1)\n    \"\"\"\n    The rank of this API process, or `-1` for engine core processes\n    under API server scale-out.\n\n    Note:\n        This is an internal config that is only valid for and\n        should only be set by API server scale-out.\n    \"\"\"\n\n    @model_validator(mode=\"after\")\n    def _validate_parallel_config(self) -> Self:\n        if self._api_process_rank >= self._api_process_count:\n            raise ValueError(\n                \"Invalid value of `_api_process_rank`. \"\n                f\"Expected to be `-1` or `[0, {self._api_process_count})`, \"\n                f\"but found: {self._api_process_rank}\"\n            )\n\n        if self.data_parallel_size_local > self.data_parallel_size:\n            raise ValueError(\n                f\"data_parallel_size_local ({self.data_parallel_size_local}) \"\n                f\"must be <= data_parallel_size ({self.data_parallel_size})\"\n            )\n\n        if self.data_parallel_size <= 1 and self.data_parallel_external_lb:\n            raise ValueError(\n                \"data_parallel_external_lb can only be set when data_parallel_size > 1\"\n            )\n\n        if self.enable_eplb:\n            if not current_platform.is_cuda_alike():\n                raise ValueError(\n                    \"Expert parallelism load balancing is only supported on \"\n                    \"CUDA devices or ROCm devices now.\"\n                )\n            if not self.enable_expert_parallel:\n                raise ValueError(\"enable_expert_parallel must be True to use EPLB.\")\n            if self.tensor_parallel_size * self.data_parallel_size <= 1:\n                raise ValueError(\n                    \"EPLB requires tensor_parallel_size or data_parallel_size \"\n                    f\"to be greater than 1, but got \"\n                    f\"TP={self.tensor_parallel_size},DP={self.data_parallel_size}.\"\n                )\n        else:\n            if self.eplb_config.num_redundant_experts != 0:\n                raise ValueError(\n                    \"num_redundant_experts is set to \"\n                    f\"{self.eplb_config.num_redundant_experts} but EPLB is not \"\n                    \"enabled. Either enable EPLB or unset \"\n                    \"num_redundant_experts.\"\n                )\n\n        return self\n\n    @property\n    def world_size_across_dp(self) -> int:\n        \"\"\"world_size_across_dp is TPxPPxDP, it is the size of the world\n        including data parallelism.\"\"\"\n        return self.world_size * self.data_parallel_size\n\n    @property\n    def use_ubatching(self) -> bool:\n        return self.enable_dbo or self.ubatch_size > 1\n\n    @property\n    def num_ubatches(self) -> int:\n        return 2 if self.enable_dbo else self.ubatch_size\n\n    def get_next_dp_init_port(self) -> int:\n        \"\"\"\n        We might need to initialize process groups in multiple\n        processes that is related to data parallelism,\n        e.g. both in the worker and in the engine, which\n        can live in different processes. To avoid port conflicts, we\n        pop a new port from the prepared port list each time we need to\n        initialize a new process group related to data parallelism.\n        \"\"\"\n        if self._data_parallel_master_port_list:\n            answer = self._data_parallel_master_port_list.pop()\n        else:\n            answer = self.data_parallel_master_port\n            self.data_parallel_master_port += 1\n\n        return answer\n\n    def stateless_init_dp_group(self) -> ProcessGroup:\n        # NOTE: In high-concurrency scenarios multiple processes\n        # can pick the same (currently free) port through a race\n        # condition when calling `get_open_port()`. When the first\n        # process binds the port the others will subsequently fail\n        # with `torch.distributed.DistNetworkError: EADDRINUSE`.\n        # To make the initialization more robust we retry a few times\n        # with a fresh port whenever this specific error is observed.\n        from torch.distributed import DistNetworkError\n\n        from vllm.distributed.utils import (\n            stateless_init_torch_distributed_process_group,\n        )\n\n        max_retries = 5\n        last_exc: Exception | None = None\n        for _ in range(max_retries):\n            try:\n                # use gloo since the engine process might not have cuda device\n                return stateless_init_torch_distributed_process_group(\n                    self.data_parallel_master_ip,\n                    self.get_next_dp_init_port(),\n                    self.data_parallel_rank,\n                    self.data_parallel_size,\n                    backend=current_platform.dist_backend,\n                )\n            except DistNetworkError as e:\n                # We only want to retry when the root cause is EADDRINUSE.\n                if \"EADDRINUSE\" in str(e):\n                    logger.warning(\"Address already in use. Retrying with a new port.\")\n                    last_exc = e\n                    continue  # try again with a new port\n                raise e\n\n        # If we get here all retries have failed.\n        assert last_exc is not None\n        raise last_exc\n\n    # The all_reduce at the end of attention (during o_proj) means that\n    # inputs are replicated across each rank of the tensor parallel group.\n    # If using expert-parallelism with DeepEP All2All ops, replicated\n    # tokens results in useless duplicate computation and communication.\n    #\n    # In this case, ensure the input to the experts is sequence parallel\n    # to avoid the excess work.\n    #\n    # Not needed for pplx-kernels as it can handle duplicate input tokens.\n    @property\n    def use_sequence_parallel_moe(self) -> bool:\n        return (\n            self.all2all_backend\n            in (\n                \"allgather_reducescatter\",\n                \"naive\",\n                \"deepep_high_throughput\",\n                \"deepep_low_latency\",\n            )\n            and self.enable_expert_parallel\n            and self.tensor_parallel_size > 1\n            and self.data_parallel_size > 1\n        )\n\n    @property\n    def node_rank_within_dp(self) -> int:\n        return self.node_rank % self.nnodes_within_dp\n\n    @property\n    def nnodes_within_dp(self) -> int:\n        if self.nnodes == 1:\n            return 1\n        data_parallel_node_size = (\n            self.data_parallel_size // self.data_parallel_size_local\n        )\n        return self.nnodes // data_parallel_node_size\n\n    @property\n    def local_world_size(self) -> int:\n        return self.world_size // self.nnodes_within_dp\n\n    @staticmethod\n    def has_unfinished_dp(dp_group: ProcessGroup, has_unfinished: bool) -> bool:\n        tensor = torch.tensor([has_unfinished], dtype=torch.int32, device=\"cpu\")\n        # dp rank 0: has_unfinished_seqs=True\n        # dp rank 1: has_unfinished_seqs=False\n        # aggregated: has_unfinished_seqs=True\n        # so this is an OR operation, i.e. MAX in integers\n        torch.distributed.all_reduce(tensor, op=ReduceOp.MAX, group=dp_group)\n        aggregated_has_unfinished = bool(tensor.item())\n        return aggregated_has_unfinished\n\n    @staticmethod\n    def sync_kv_cache_memory_size(dp_group: ProcessGroup, kv_cache_memory: int) -> int:\n        if kv_cache_memory == -1:\n            kv_cache_memory = torch.iinfo(torch.int64).max\n        tensor = torch.tensor([kv_cache_memory], dtype=torch.int64, device=\"cpu\")\n        # we cannot use broadcast for stateless dp group since it depends\n        # on global rank\n        torch.distributed.all_reduce(tensor, op=ReduceOp.MIN, group=dp_group)\n        return tensor.item()\n\n    def compute_hash(self):\n        \"\"\"\n        Provide a hash that uniquely identifies all the configs\n        that affect the structure of the computation\n        graph from input ids/embeddings to the final hidden states,\n        excluding anything before input ids/embeddings and after\n        the final hidden states.\n\n        This hash is also used for DP worker configuration validation\n        to prevent hangs from mismatched collective communication patterns.\n        \"\"\"\n        ignored_factors = {\n            # Derived/runtime topology, networking, or launch details\n            \"data_parallel_rank\",\n            \"data_parallel_rank_local\",\n            \"data_parallel_size_local\",\n            \"data_parallel_backend\",\n            \"data_parallel_external_lb\",\n            \"data_parallel_hybrid_lb\",\n            \"data_parallel_master_ip\",\n            \"data_parallel_master_port\",\n            \"_data_parallel_master_port_list\",\n            \"data_parallel_rpc_port\",\n            \"rank\",\n            \"master_addr\",\n            \"master_port\",\n            \"node_rank\",\n            \"nnodes\",\n            \"max_parallel_loading_workers\",\n            \"disable_custom_all_reduce\",\n            \"ray_workers_use_nsight\",\n            \"ray_runtime_env\",\n            \"placement_group\",\n            \"distributed_executor_backend\",\n            \"worker_cls\",\n            \"sd_worker_cls\",\n            \"worker_extension_cls\",\n            \"_api_process_count\",\n            \"_api_process_rank\",\n        }\n\n        from vllm.config.utils import get_hash_factors, hash_factors\n\n        factors = get_hash_factors(self, ignored_factors)\n        return hash_factors(factors)\n\n    def __post_init__(self) -> None:\n        # Set all2all_backend from env var if not specified, with deprecation warning\n        if envs.is_set(\"VLLM_ALL2ALL_BACKEND\"):\n            logger.warning_once(\n                \"VLLM_ALL2ALL_BACKEND environment variable is deprecated and \"\n                \"will be removed in v0.15.0. Please use the \"\n                \"--all2all-backend command-line argument instead.\"\n            )\n            self.all2all_backend = envs.VLLM_ALL2ALL_BACKEND\n\n        # Continue with the rest of the initialization\n        self.world_size = (\n            self.pipeline_parallel_size\n            * self.tensor_parallel_size\n            * self.prefill_context_parallel_size\n        )\n\n        if self.distributed_executor_backend == \"external_launcher\":\n            logger.info(\"Using external launcher for distributed inference.\")\n            self.world_size *= self.data_parallel_size\n\n        if self.data_parallel_size > 1 or self.data_parallel_size_local == 0:\n            # Data parallel was specified in the engine args.\n            if self.distributed_executor_backend == \"external_launcher\":\n                # For external launcher,\n                # we need to set the data parallel rank automatically\n                self.data_parallel_rank = int(os.environ[\"RANK\"]) // (\n                    self.world_size // self.data_parallel_size\n                )\n                logger.info(\n                    \"Set data_parallel_rank to %d automatically.\",\n                    self.data_parallel_rank,\n                )\n            if not self._data_parallel_master_port_list:\n                self._data_parallel_master_port_list = get_open_ports_list(5)\n            self.data_parallel_master_port = self._data_parallel_master_port_list.pop()\n\n            if not (0 <= self.data_parallel_rank < self.data_parallel_size):\n                raise ValueError(\n                    f\"data_parallel_rank ({self.data_parallel_rank})\"\n                    f\" must be in the range [0, {self.data_parallel_size})\"\n                )\n        else:\n            # Otherwise fall back to env vars (e.g. for offline SPMD case).\n            self.data_parallel_size = envs.VLLM_DP_SIZE\n            self.data_parallel_rank = envs.VLLM_DP_RANK\n            self.data_parallel_rank_local = envs.VLLM_DP_RANK_LOCAL\n            self.data_parallel_master_ip = envs.VLLM_DP_MASTER_IP\n            self.data_parallel_master_port = envs.VLLM_DP_MASTER_PORT\n\n        if self.distributed_executor_backend == \"external_launcher\":\n            os.environ[\"VLLM_ENABLE_V1_MULTIPROCESSING\"] = \"0\"\n            logger.info(\"Disabling V1 multiprocessing for external launcher.\")\n\n        if self.distributed_executor_backend is None and self.world_size > 1:\n            # We use multiprocessing by default if world_size fits on the\n            # current node and we aren't in a ray placement group.\n\n            from vllm.v1.executor import ray_utils\n\n            backend: DistributedExecutorBackend = \"mp\"\n            ray_found = ray_utils.ray_is_available()\n            if current_platform.is_tpu() and envs.VLLM_XLA_USE_SPMD:\n                backend = \"uni\"\n            elif current_platform.is_cuda() and self.nnodes > 1:\n                backend = \"mp\"\n            elif (\n                current_platform.is_cuda()\n                and cuda_device_count_stateless() < self.world_size\n            ):\n                gpu_count = cuda_device_count_stateless()\n                raise ValueError(\n                    f\"World size ({self.world_size}) is larger than the number of \"\n                    f\"available GPUs ({gpu_count}) in this node. If this is \"\n                    \"intentional and you are using:\\n\"\n                    \"- ray, set '--distributed-executor-backend ray'.\\n\"\n                    \"- multiprocessing, set '--nnodes' appropriately.\"\n                )\n            elif self.data_parallel_backend == \"ray\":\n                logger.info(\n                    \"Using ray distributed inference because \"\n                    \"data_parallel_backend is ray\"\n                )\n                backend = \"ray\"\n            elif ray_found:\n                if self.placement_group:\n                    backend = \"ray\"\n                else:\n                    from ray import is_initialized as ray_is_initialized\n\n                    if ray_is_initialized():\n                        from ray.util import get_current_placement_group\n\n                        if get_current_placement_group():\n                            backend = \"ray\"\n            self.distributed_executor_backend = backend\n            logger.debug(\"Defaulting to use %s for distributed inference\", backend)\n\n        if self.distributed_executor_backend is None and self.world_size == 1:\n            self.distributed_executor_backend = \"uni\"\n\n        if self.max_parallel_loading_workers is not None:\n            logger.warning(\n                \"max_parallel_loading_workers is currently \"\n                \"not supported and will be ignored.\"\n            )\n        allowed_backends = (\"mp\", \"uni\", \"external_launcher\")\n        if (\n            self.distributed_executor_backend not in allowed_backends\n            and self.nnodes > 1\n        ):\n            raise ValueError(\n                \"nnodes > 1 can only be set when distributed executor \"\n                \"backend is mp, uni or external_launcher.\"\n            )\n\n    @property\n    def use_ray(self) -> bool:\n        return self.distributed_executor_backend == \"ray\" or (\n            isinstance(self.distributed_executor_backend, type)\n            and getattr(self.distributed_executor_backend, \"uses_ray\", False)\n        )\n\n    @model_validator(mode=\"after\")\n    def _verify_args(self) -> Self:\n        # Lazy import to avoid circular import\n        from vllm.v1.executor import Executor\n\n        # Enable batch invariance settings if requested\n        if vllm_is_batch_invariant():\n            self.disable_custom_all_reduce = True\n\n        if (\n            self.distributed_executor_backend is not None\n            and not isinstance(self.distributed_executor_backend, str)\n            and not (\n                isinstance(self.distributed_executor_backend, type)\n                and issubclass(self.distributed_executor_backend, Executor)\n            )\n        ):\n            raise ValueError(\n                \"Unrecognized distributed executor backend \"\n                f\"{self.distributed_executor_backend}. Supported \"\n                \"values are 'ray', 'mp' 'uni', 'external_launcher', \"\n                \" custom Executor subclass or its import path.\"\n            )\n        if self.use_ray:\n            from vllm.v1.executor import ray_utils\n\n            ray_utils.assert_ray_available()\n\n        if not current_platform.use_custom_allreduce():\n            self.disable_custom_all_reduce = True\n            logger.debug(\n                \"Disabled the custom all-reduce kernel because it is not \"\n                \"supported on current platform.\"\n            )\n        if self.nnodes > 1:\n            self.disable_custom_all_reduce = True\n            logger.debug(\n                \"Disabled the custom all-reduce since we are running on multi-node.\"\n            )\n        if self.ray_workers_use_nsight and not self.use_ray:\n            raise ValueError(\n                \"Unable to use nsight profiling unless workers run with Ray.\"\n            )\n\n        return self",
      "language": "python"
    },
    {
      "code": "@config\n@dataclass\nclass ParallelConfig:\n    \"\"\"Configuration for the distributed execution.\"\"\"\n\n    pipeline_parallel_size: int = 1\n    \"\"\"Number of pipeline parallel groups.\"\"\"\n    tensor_parallel_size: int = 1\n    \"\"\"Number of tensor parallel groups.\"\"\"\n    prefill_context_parallel_size: int = 1\n    \"\"\"Number of prefill context parallel groups.\"\"\"\n    data_parallel_size: int = 1\n    \"\"\"Number of data parallel groups. MoE layers will be sharded according to\n    the product of the tensor parallel size and data parallel size.\"\"\"\n    data_parallel_size_local: int = 1\n    \"\"\"Number of local data parallel groups.\"\"\"\n    data_parallel_rank: int = 0\n    \"\"\"Rank of the data parallel group.\"\"\"\n    data_parallel_rank_local: int | None = None\n    \"\"\"Local rank of the data parallel group,\n    set only in SPMD mode.\"\"\"\n    data_parallel_master_ip: str = \"127.0.0.1\"\n    \"\"\"IP of the data parallel master.\"\"\"\n    data_parallel_rpc_port: int = 29550\n    \"\"\"Port for data parallel messaging.\"\"\"\n    data_parallel_master_port: int = 29500\n    \"\"\"Port of the data parallel master.\"\"\"\n    data_parallel_backend: DataParallelBackend = \"mp\"\n    \"\"\"Backend to use for data parallel, either \"mp\" or \"ray\".\"\"\"\n    data_parallel_external_lb: bool = False\n    \"\"\"Whether to use \"external\" DP LB mode. Applies only to online serving\n    and when data_parallel_size > 0. This is useful for a \"one-pod-per-rank\"\n    wide-EP setup in Kubernetes. Set implicitly when --data-parallel-rank\n    is provided explicitly to vllm serve.\"\"\"\n    data_parallel_hybrid_lb: bool = False\n    \"\"\"Whether to use \"hybrid\" DP LB mode. Applies only to online serving\n    and when data_parallel_size > 0. Enables running an AsyncLLM\n    and API server on a \"per-node\" basis where vLLM load balances\n    between local data parallel ranks, but an external LB balances\n    between vLLM nodes/replicas. Set explicitly in conjunction with\n    --data-parallel-start-rank.\"\"\"\n    enable_expert_parallel: bool = False\n    \"\"\"Use expert parallelism instead of tensor parallelism for MoE layers.\"\"\"\n    enable_eplb: bool = False\n    \"\"\"Enable expert parallelism load balancing for MoE layers.\"\"\"\n    eplb_config: EPLBConfig = Field(default_factory=EPLBConfig)\n    \"\"\"Expert parallelism configuration.\"\"\"\n    expert_placement_strategy: ExpertPlacementStrategy = \"linear\"\n    \"\"\"The expert placement strategy for MoE layers:\\n\n    - \"linear\": Experts are placed in a contiguous manner. For example, with 4\n      experts and 2 ranks, rank 0 will have experts [0, 1] and rank 1 will have\n      experts [2, 3].\\n\n    - \"round_robin\": Experts are placed in a round-robin manner. For example,\n      with 4 experts and 2 ranks, rank 0 will have experts [0, 2] and rank 1\n      will have experts [1, 3]. This strategy can help improve load balancing\n      for grouped expert models with no redundant experts.\"\"\"\n    all2all_backend: All2AllBackend = \"allgather_reducescatter\"\n    \"\"\"All2All backend for MoE expert parallel communication. Available options:\n\n    - \"naive\": Naive all2all implementation using broadcasts\\n\n    - \"allgather_reducescatter\": All2all based on allgather and reducescatter\\n\n    - \"pplx\": Use pplx kernels\\n\n    - \"deepep_high_throughput\": Use deepep high-throughput kernels\\n\n    - \"deepep_low_latency\": Use deepep low-latency kernels\\n\n    - \"flashinfer_all2allv\": Use flashinfer alltoallv kernels for mnnvl\"\"\"\n\n    max_parallel_loading_workers: int | None = None\n    \"\"\"Maximum number of parallel loading workers when loading model\n    sequentially in multiple batches. To avoid RAM OOM when using tensor\n    parallel and large models.\"\"\"\n\n    disable_custom_all_reduce: bool = False\n    \"\"\"Disable the custom all-reduce kernel and fall back to NCCL.\"\"\"\n\n    enable_dbo: bool = False\n    \"\"\"Enable dual batch overlap for the model executor.\"\"\"\n    ubatch_size: int = 0\n    \"\"\"Number of ubatch size.\"\"\"\n\n    dbo_decode_token_threshold: int = 32\n    \"\"\"The threshold for dual batch overlap for batches only containing decodes.\n    If the number of tokens in the request is greater than this threshold,\n    microbatching will be used. Otherwise, the request will be processed in a\n    single batch.\"\"\"\n    dbo_prefill_token_threshold: int = 512  # TODO(lucas): tune\n    \"\"\"The threshold for dual batch overlap for batches that contain one or more\n    prefills. If the number of tokens in the request is greater than this\n    threshold, microbatching will be used. Otherwise, the request will be\n    processed in a single batch.\"\"\"\n\n    disable_nccl_for_dp_synchronization: bool = False\n    \"\"\"Forces the dp synchronization logic in vllm/v1/worker/dp_utils.py \n    to use Gloo instead of NCCL for its all reduce\"\"\"\n\n    ray_workers_use_nsight: bool = False\n    \"\"\"Whether to profile Ray workers with nsight, see https://docs.ray.io/en/latest/ray-observability/user-guides/profiling.html#profiling-nsight-profiler.\"\"\"\n\n    ray_runtime_env: RuntimeEnv | None = None\n    \"\"\"Ray runtime environment to pass to distributed workers.\"\"\"\n\n    placement_group: PlacementGroup | None = None\n    \"\"\"ray distributed model workers placement group.\"\"\"\n\n    distributed_executor_backend: (\n        str | DistributedExecutorBackend | type[Executor] | None\n    ) = None\n    \"\"\"Backend to use for distributed model workers, either \"ray\" or \"mp\"\n    (multiprocessing). If the product of pipeline_parallel_size and tensor_parallel_size\n    is less than or equal to the number of GPUs available, \"mp\" will be used to\n    keep processing on a single host. Otherwise, an error will be raised. To use \"mp\"\n    you must also set nnodes, and to use \"ray\" you must manually set\n    distributed_executor_backend to \"ray\".\n\n    Note that tpu only support Ray for distributed inference.\"\"\"\n\n    worker_cls: str = \"auto\"\n    \"\"\"The full name of the worker class to use. If \"auto\", the worker class\n    will be determined based on the platform.\"\"\"\n    sd_worker_cls: str = \"auto\"\n    \"\"\"The full name of the worker class to use for speculative decoding.\n    If \"auto\", the worker class will be determined based on the platform.\"\"\"\n    worker_extension_cls: str = \"\"\n    \"\"\"The full name of the worker extension class to use. The worker extension\n    class is dynamically inherited by the worker class. This is used to inject\n    new attributes and methods to the worker class for use in collective_rpc\n    calls.\"\"\"\n    master_addr: str = \"127.0.0.1\"\n    \"\"\"distributed master address for multi-node distributed \n    inference when distributed_executor_backend is mp.\"\"\"\n    master_port: int = 29501\n    \"\"\"distributed master port for multi-node distributed \n    inference when distributed_executor_backend is mp.\"\"\"\n    node_rank: int = 0\n    \"\"\"distributed node rank for multi-node distributed \n    inference when distributed_executor_backend is mp.\"\"\"\n    nnodes: int = 1\n    \"\"\"num of nodes for multi-node distributed \n    inference when distributed_executor_backend is mp.\"\"\"\n\n    world_size: int = Field(init=False)\n    \"\"\"world_size is TPxPP, it affects the number of workers we create.\"\"\"\n\n    rank: int = 0\n    \"\"\"Global rank in distributed setup.\"\"\"\n\n    _data_parallel_master_port_list: list[int] = Field(default_factory=list)\n    \"\"\"List of open port auto-queried for data parallel messaging.\n    Set to be private as it's not intended to be configured by users.\n    \"\"\"\n\n    decode_context_parallel_size: int = 1\n    \"\"\"Number of decode context parallel groups, because the world size does\n    not change by dcp, it simply reuse the GPUs of TP group, and tp_size\n    needs to be divisible by dcp_size.\"\"\"\n\n    dcp_kv_cache_interleave_size: int = 1\n    \"\"\"\n    Interleave size of kv_cache storage while using DCP.\n    dcp_kv_cache_interleave_size has been replaced by cp_kv_cache_interleave_size,\n    and will be deprecated when PCP is fully supported.\n\n    \"\"\"\n    cp_kv_cache_interleave_size: int = 1\n    \"\"\"Interleave size of kv_cache storage while using DCP or PCP.\n    For `total_cp_rank = pcp_rank * dcp_world_size + dcp_rank`,\n        and `total_cp_world_size = pcp_world_size * dcp_world_size`.\n    store interleave_size tokens on total_cp_rank i,\n    then store next interleave_size tokens on total_cp_rank i+1.\n    Interleave_size=1: token-level alignment, where token `i` is stored on\n        total_cp_rank `i % total_cp_world_size`.\n    Interleave_size=block_size: block-level alignment, where tokens are\n        first populated to the preceding ranks. Tokens are then stored\n        in (rank i+1, block j) only after (rank i, block j) is fully occupied.\n    Block_size should be greater than or equal to cp_kv_cache_interleave_size.\n    Block_size should be divisible by cp_kv_cache_interleave_size.\n    \"\"\"\n\n    _api_process_count: int = Field(default=1, gt=0)\n    \"\"\"\n    The number of API processes initialized.\n\n    Note:\n        This is an internal config that is only valid for and\n        should only be set by API server scale-out.\n    \"\"\"\n\n    _api_process_rank: int = Field(default=0, ge=-1)\n    \"\"\"\n    The rank of this API process, or `-1` for engine core processes\n    under API server scale-out.\n\n    Note:\n        This is an internal config that is only valid for and\n        should only be set by API server scale-out.\n    \"\"\"\n\n    @model_validator(mode=\"after\")\n    def _validate_parallel_config(self) -> Self:\n        if self._api_process_rank >= self._api_process_count:\n            raise ValueError(\n                \"Invalid value of `_api_process_rank`. \"\n                f\"Expected to be `-1` or `[0, {self._api_process_count})`, \"\n                f\"but found: {self._api_process_rank}\"\n            )\n\n        if self.data_parallel_size_local > self.data_parallel_size:\n            raise ValueError(\n                f\"data_parallel_size_local ({self.data_parallel_size_local}) \"\n                f\"must be <= data_parallel_size ({self.data_parallel_size})\"\n            )\n\n        if self.data_parallel_size <= 1 and self.data_parallel_external_lb:\n            raise ValueError(\n                \"data_parallel_external_lb can only be set when data_parallel_size > 1\"\n            )\n\n        if self.enable_eplb:\n            if not current_platform.is_cuda_alike():\n                raise ValueError(\n                    \"Expert parallelism load balancing is only supported on \"\n                    \"CUDA devices or ROCm devices now.\"\n                )\n            if not self.enable_expert_parallel:\n                raise ValueError(\"enable_expert_parallel must be True to use EPLB.\")\n            if self.tensor_parallel_size * self.data_parallel_size <= 1:\n                raise ValueError(\n                    \"EPLB requires tensor_parallel_size or data_parallel_size \"\n                    f\"to be greater than 1, but got \"\n                    f\"TP={self.tensor_parallel_size},DP={self.data_parallel_size}.\"\n                )\n        else:\n            if self.eplb_config.num_redundant_experts != 0:\n                raise ValueError(\n                    \"num_redundant_experts is set to \"\n                    f\"{self.eplb_config.num_redundant_experts} but EPLB is not \"\n                    \"enabled. Either enable EPLB or unset \"\n                    \"num_redundant_experts.\"\n                )\n\n        return self\n\n    @property\n    def world_size_across_dp(self) -> int:\n        \"\"\"world_size_across_dp is TPxPPxDP, it is the size of the world\n        including data parallelism.\"\"\"\n        return self.world_size * self.data_parallel_size\n\n    @property\n    def use_ubatching(self) -> bool:\n        return self.enable_dbo or self.ubatch_size > 1\n\n    @property\n    def num_ubatches(self) -> int:\n        return 2 if self.enable_dbo else self.ubatch_size\n\n    def get_next_dp_init_port(self) -> int:\n        \"\"\"\n        We might need to initialize process groups in multiple\n        processes that is related to data parallelism,\n        e.g. both in the worker and in the engine, which\n        can live in different processes. To avoid port conflicts, we\n        pop a new port from the prepared port list each time we need to\n        initialize a new process group related to data parallelism.\n        \"\"\"\n        if self._data_parallel_master_port_list:\n            answer = self._data_parallel_master_port_list.pop()\n        else:\n            answer = self.data_parallel_master_port\n            self.data_parallel_master_port += 1\n\n        return answer\n\n    def stateless_init_dp_group(self) -> ProcessGroup:\n        # NOTE: In high-concurrency scenarios multiple processes\n        # can pick the same (currently free) port through a race\n        # condition when calling `get_open_port()`. When the first\n        # process binds the port the others will subsequently fail\n        # with `torch.distributed.DistNetworkError: EADDRINUSE`.\n        # To make the initialization more robust we retry a few times\n        # with a fresh port whenever this specific error is observed.\n        from torch.distributed import DistNetworkError\n\n        from vllm.distributed.utils import (\n            stateless_init_torch_distributed_process_group,\n        )\n\n        max_retries = 5\n        last_exc: Exception | None = None\n        for _ in range(max_retries):\n            try:\n                # use gloo since the engine process might not have cuda device\n                return stateless_init_torch_distributed_process_group(\n                    self.data_parallel_master_ip,\n                    self.get_next_dp_init_port(),\n                    self.data_parallel_rank,\n                    self.data_parallel_size,\n                    backend=current_platform.dist_backend,\n                )\n            except DistNetworkError as e:\n                # We only want to retry when the root cause is EADDRINUSE.\n                if \"EADDRINUSE\" in str(e):\n                    logger.warning(\"Address already in use. Retrying with a new port.\")\n                    last_exc = e\n                    continue  # try again with a new port\n                raise e\n\n        # If we get here all retries have failed.\n        assert last_exc is not None\n        raise last_exc\n\n    # The all_reduce at the end of attention (during o_proj) means that\n    # inputs are replicated across each rank of the tensor parallel group.\n    # If using expert-parallelism with DeepEP All2All ops, replicated\n    # tokens results in useless duplicate computation and communication.\n    #\n    # In this case, ensure the input to the experts is sequence parallel\n    # to avoid the excess work.\n    #\n    # Not needed for pplx-kernels as it can handle duplicate input tokens.\n    @property\n    def use_sequence_parallel_moe(self) -> bool:\n        return (\n            self.all2all_backend\n            in (\n                \"allgather_reducescatter\",\n                \"naive\",\n                \"deepep_high_throughput\",\n                \"deepep_low_latency\",\n            )\n            and self.enable_expert_parallel\n            and self.tensor_parallel_size > 1\n            and self.data_parallel_size > 1\n        )\n\n    @property\n    def node_rank_within_dp(self) -> int:\n        return self.node_rank % self.nnodes_within_dp\n\n    @property\n    def nnodes_within_dp(self) -> int:\n        if self.nnodes == 1:\n            return 1\n        data_parallel_node_size = (\n            self.data_parallel_size // self.data_parallel_size_local\n        )\n        return self.nnodes // data_parallel_node_size\n\n    @property\n    def local_world_size(self) -> int:\n        return self.world_size // self.nnodes_within_dp\n\n    @staticmethod\n    def has_unfinished_dp(dp_group: ProcessGroup, has_unfinished: bool) -> bool:\n        tensor = torch.tensor([has_unfinished], dtype=torch.int32, device=\"cpu\")\n        # dp rank 0: has_unfinished_seqs=True\n        # dp rank 1: has_unfinished_seqs=False\n        # aggregated: has_unfinished_seqs=True\n        # so this is an OR operation, i.e. MAX in integers\n        torch.distributed.all_reduce(tensor, op=ReduceOp.MAX, group=dp_group)\n        aggregated_has_unfinished = bool(tensor.item())\n        return aggregated_has_unfinished\n\n    @staticmethod\n    def sync_kv_cache_memory_size(dp_group: ProcessGroup, kv_cache_memory: int) -> int:\n        if kv_cache_memory == -1:\n            kv_cache_memory = torch.iinfo(torch.int64).max\n        tensor = torch.tensor([kv_cache_memory], dtype=torch.int64, device=\"cpu\")\n        # we cannot use broadcast for stateless dp group since it depends\n        # on global rank\n        torch.distributed.all_reduce(tensor, op=ReduceOp.MIN, group=dp_group)\n        return tensor.item()\n\n    def compute_hash(self):\n        \"\"\"\n        Provide a hash that uniquely identifies all the configs\n        that affect the structure of the computation\n        graph from input ids/embeddings to the final hidden states,\n        excluding anything before input ids/embeddings and after\n        the final hidden states.\n\n        This hash is also used for DP worker configuration validation\n        to prevent hangs from mismatched collective communication patterns.\n        \"\"\"\n        ignored_factors = {\n            # Derived/runtime topology, networking, or launch details\n            \"data_parallel_rank\",\n            \"data_parallel_rank_local\",\n            \"data_parallel_size_local\",\n            \"data_parallel_backend\",\n            \"data_parallel_external_lb\",\n            \"data_parallel_hybrid_lb\",\n            \"data_parallel_master_ip\",\n            \"data_parallel_master_port\",\n            \"_data_parallel_master_port_list\",\n            \"data_parallel_rpc_port\",\n            \"rank\",\n            \"master_addr\",\n            \"master_port\",\n            \"node_rank\",\n            \"nnodes\",\n            \"max_parallel_loading_workers\",\n            \"disable_custom_all_reduce\",\n            \"ray_workers_use_nsight\",\n            \"ray_runtime_env\",\n            \"placement_group\",\n            \"distributed_executor_backend\",\n            \"worker_cls\",\n            \"sd_worker_cls\",\n            \"worker_extension_cls\",\n            \"_api_process_count\",\n            \"_api_process_rank\",\n        }\n\n        from vllm.config.utils import get_hash_factors, hash_factors\n\n        factors = get_hash_factors(self, ignored_factors)\n        return hash_factors(factors)\n\n    def __post_init__(self) -> None:\n        # Set all2all_backend from env var if not specified, with deprecation warning\n        if envs.is_set(\"VLLM_ALL2ALL_BACKEND\"):\n            logger.warning_once(\n                \"VLLM_ALL2ALL_BACKEND environment variable is deprecated and \"\n                \"will be removed in v0.15.0. Please use the \"\n                \"--all2all-backend command-line argument instead.\"\n            )\n            self.all2all_backend = envs.VLLM_ALL2ALL_BACKEND\n\n        # Continue with the rest of the initialization\n        self.world_size = (\n            self.pipeline_parallel_size\n            * self.tensor_parallel_size\n            * self.prefill_context_parallel_size\n        )\n\n        if self.distributed_executor_backend == \"external_launcher\":\n            logger.info(\"Using external launcher for distributed inference.\")\n            self.world_size *= self.data_parallel_size\n\n        if self.data_parallel_size > 1 or self.data_parallel_size_local == 0:\n            # Data parallel was specified in the engine args.\n            if self.distributed_executor_backend == \"external_launcher\":\n                # For external launcher,\n                # we need to set the data parallel rank automatically\n                self.data_parallel_rank = int(os.environ[\"RANK\"]) // (\n                    self.world_size // self.data_parallel_size\n                )\n                logger.info(\n                    \"Set data_parallel_rank to %d automatically.\",\n                    self.data_parallel_rank,\n                )\n            if not self._data_parallel_master_port_list:\n                self._data_parallel_master_port_list = get_open_ports_list(5)\n            self.data_parallel_master_port = self._data_parallel_master_port_list.pop()\n\n            if not (0 <= self.data_parallel_rank < self.data_parallel_size):\n                raise ValueError(\n                    f\"data_parallel_rank ({self.data_parallel_rank})\"\n                    f\" must be in the range [0, {self.data_parallel_size})\"\n                )\n        else:\n            # Otherwise fall back to env vars (e.g. for offline SPMD case).\n            self.data_parallel_size = envs.VLLM_DP_SIZE\n            self.data_parallel_rank = envs.VLLM_DP_RANK\n            self.data_parallel_rank_local = envs.VLLM_DP_RANK_LOCAL\n            self.data_parallel_master_ip = envs.VLLM_DP_MASTER_IP\n            self.data_parallel_master_port = envs.VLLM_DP_MASTER_PORT\n\n        if self.distributed_executor_backend == \"external_launcher\":\n            os.environ[\"VLLM_ENABLE_V1_MULTIPROCESSING\"] = \"0\"\n            logger.info(\"Disabling V1 multiprocessing for external launcher.\")\n\n        if self.distributed_executor_backend is None and self.world_size > 1:\n            # We use multiprocessing by default if world_size fits on the\n            # current node and we aren't in a ray placement group.\n\n            from vllm.v1.executor import ray_utils\n\n            backend: DistributedExecutorBackend = \"mp\"\n            ray_found = ray_utils.ray_is_available()\n            if current_platform.is_tpu() and envs.VLLM_XLA_USE_SPMD:\n                backend = \"uni\"\n            elif current_platform.is_cuda() and self.nnodes > 1:\n                backend = \"mp\"\n            elif (\n                current_platform.is_cuda()\n                and cuda_device_count_stateless() < self.world_size\n            ):\n                gpu_count = cuda_device_count_stateless()\n                raise ValueError(\n                    f\"World size ({self.world_size}) is larger than the number of \"\n                    f\"available GPUs ({gpu_count}) in this node. If this is \"\n                    \"intentional and you are using:\\n\"\n                    \"- ray, set '--distributed-executor-backend ray'.\\n\"\n                    \"- multiprocessing, set '--nnodes' appropriately.\"\n                )\n            elif self.data_parallel_backend == \"ray\":\n                logger.info(\n                    \"Using ray distributed inference because \"\n                    \"data_parallel_backend is ray\"\n                )\n                backend = \"ray\"\n            elif ray_found:\n                if self.placement_group:\n                    backend = \"ray\"\n                else:\n                    from ray import is_initialized as ray_is_initialized\n\n                    if ray_is_initialized():\n                        from ray.util import get_current_placement_group\n\n                        if get_current_placement_group():\n                            backend = \"ray\"\n            self.distributed_executor_backend = backend\n            logger.debug(\"Defaulting to use %s for distributed inference\", backend)\n\n        if self.distributed_executor_backend is None and self.world_size == 1:\n            self.distributed_executor_backend = \"uni\"\n\n        if self.max_parallel_loading_workers is not None:\n            logger.warning(\n                \"max_parallel_loading_workers is currently \"\n                \"not supported and will be ignored.\"\n            )\n        allowed_backends = (\"mp\", \"uni\", \"external_launcher\")\n        if (\n            self.distributed_executor_backend not in allowed_backends\n            and self.nnodes > 1\n        ):\n            raise ValueError(\n                \"nnodes > 1 can only be set when distributed executor \"\n                \"backend is mp, uni or external_launcher.\"\n            )\n\n    @property\n    def use_ray(self) -> bool:\n        return self.distributed_executor_backend == \"ray\" or (\n            isinstance(self.distributed_executor_backend, type)\n            and getattr(self.distributed_executor_backend, \"uses_ray\", False)\n        )\n\n    @model_validator(mode=\"after\")\n    def _verify_args(self) -> Self:\n        # Lazy import to avoid circular import\n        from vllm.v1.executor import Executor\n\n        # Enable batch invariance settings if requested\n        if vllm_is_batch_invariant():\n            self.disable_custom_all_reduce = True\n\n        if (\n            self.distributed_executor_backend is not None\n            and not isinstance(self.distributed_executor_backend, str)\n            and not (\n                isinstance(self.distributed_executor_backend, type)\n                and issubclass(self.distributed_executor_backend, Executor)\n            )\n        ):\n            raise ValueError(\n                \"Unrecognized distributed executor backend \"\n                f\"{self.distributed_executor_backend}. Supported \"\n                \"values are 'ray', 'mp' 'uni', 'external_launcher', \"\n                \" custom Executor subclass or its import path.\"\n            )\n        if self.use_ray:\n            from vllm.v1.executor import ray_utils\n\n            ray_utils.assert_ray_available()\n\n        if not current_platform.use_custom_allreduce():\n            self.disable_custom_all_reduce = True\n            logger.debug(\n                \"Disabled the custom all-reduce kernel because it is not \"\n                \"supported on current platform.\"\n            )\n        if self.nnodes > 1:\n            self.disable_custom_all_reduce = True\n            logger.debug(\n                \"Disabled the custom all-reduce since we are running on multi-node.\"\n            )\n        if self.ray_workers_use_nsight and not self.use_ray:\n            raise ValueError(\n                \"Unable to use nsight profiling unless workers run with Ray.\"\n            )\n\n        return self",
      "language": "python"
    },
    {
      "code": "_api_process_count: int = Field(default=1, gt=0)",
      "language": "typescript"
    },
    {
      "code": "_api_process_count: int = Field(default=1, gt=0)",
      "language": "typescript"
    },
    {
      "code": "_api_process_rank: int = Field(default=0, ge=-1)",
      "language": "typescript"
    },
    {
      "code": "_api_process_rank: int = Field(default=0, ge=-1)",
      "language": "typescript"
    },
    {
      "code": "_data_parallel_master_port_list: list[int] = Field(\n    default_factory=list\n)",
      "language": "yaml"
    },
    {
      "code": "_data_parallel_master_port_list: list[int] = Field(\n    default_factory=list\n)",
      "language": "yaml"
    },
    {
      "code": "all2all_backend: All2AllBackend = 'allgather_reducescatter'",
      "language": "typescript"
    },
    {
      "code": "all2all_backend: All2AllBackend = 'allgather_reducescatter'",
      "language": "typescript"
    },
    {
      "code": "cp_kv_cache_interleave_size: int = 1",
      "language": "typescript"
    },
    {
      "code": "cp_kv_cache_interleave_size: int = 1",
      "language": "typescript"
    },
    {
      "code": "data_parallel_backend: DataParallelBackend = 'mp'",
      "language": "typescript"
    },
    {
      "code": "data_parallel_backend: DataParallelBackend = 'mp'",
      "language": "typescript"
    },
    {
      "code": "data_parallel_external_lb: bool = False",
      "language": "typescript"
    },
    {
      "code": "data_parallel_external_lb: bool = False",
      "language": "typescript"
    },
    {
      "code": "data_parallel_hybrid_lb: bool = False",
      "language": "typescript"
    },
    {
      "code": "data_parallel_hybrid_lb: bool = False",
      "language": "typescript"
    },
    {
      "code": "data_parallel_master_ip: str = '127.0.0.1'",
      "language": "typescript"
    },
    {
      "code": "data_parallel_master_ip: str = '127.0.0.1'",
      "language": "typescript"
    },
    {
      "code": "data_parallel_master_port: int = 29500",
      "language": "typescript"
    },
    {
      "code": "data_parallel_master_port: int = 29500",
      "language": "typescript"
    },
    {
      "code": "data_parallel_rank: int = 0",
      "language": "typescript"
    },
    {
      "code": "data_parallel_rank: int = 0",
      "language": "typescript"
    },
    {
      "code": "data_parallel_rank_local: int | None = None",
      "language": "yaml"
    },
    {
      "code": "data_parallel_rank_local: int | None = None",
      "language": "yaml"
    },
    {
      "code": "data_parallel_rpc_port: int = 29550",
      "language": "typescript"
    },
    {
      "code": "data_parallel_rpc_port: int = 29550",
      "language": "typescript"
    },
    {
      "code": "data_parallel_size: int = 1",
      "language": "typescript"
    },
    {
      "code": "data_parallel_size: int = 1",
      "language": "typescript"
    },
    {
      "code": "data_parallel_size_local: int = 1",
      "language": "typescript"
    },
    {
      "code": "data_parallel_size_local: int = 1",
      "language": "typescript"
    },
    {
      "code": "dbo_decode_token_threshold: int = 32",
      "language": "typescript"
    },
    {
      "code": "dbo_decode_token_threshold: int = 32",
      "language": "typescript"
    },
    {
      "code": "dbo_prefill_token_threshold: int = 512",
      "language": "typescript"
    },
    {
      "code": "dbo_prefill_token_threshold: int = 512",
      "language": "typescript"
    },
    {
      "code": "dcp_kv_cache_interleave_size: int = 1",
      "language": "typescript"
    },
    {
      "code": "dcp_kv_cache_interleave_size: int = 1",
      "language": "typescript"
    },
    {
      "code": "decode_context_parallel_size: int = 1",
      "language": "typescript"
    },
    {
      "code": "decode_context_parallel_size: int = 1",
      "language": "typescript"
    },
    {
      "code": "disable_custom_all_reduce: bool = False",
      "language": "typescript"
    },
    {
      "code": "disable_custom_all_reduce: bool = False",
      "language": "typescript"
    },
    {
      "code": "disable_nccl_for_dp_synchronization: bool = False",
      "language": "typescript"
    },
    {
      "code": "disable_nccl_for_dp_synchronization: bool = False",
      "language": "typescript"
    },
    {
      "code": "distributed_executor_backend: (\n    str | DistributedExecutorBackend | type[Executor] | None\n) = None",
      "language": "yaml"
    },
    {
      "code": "distributed_executor_backend: (\n    str | DistributedExecutorBackend | type[Executor] | None\n) = None",
      "language": "yaml"
    },
    {
      "code": "enable_dbo: bool = False",
      "language": "typescript"
    },
    {
      "code": "enable_dbo: bool = False",
      "language": "typescript"
    },
    {
      "code": "enable_eplb: bool = False",
      "language": "typescript"
    },
    {
      "code": "enable_eplb: bool = False",
      "language": "typescript"
    },
    {
      "code": "enable_expert_parallel: bool = False",
      "language": "typescript"
    },
    {
      "code": "enable_expert_parallel: bool = False",
      "language": "typescript"
    },
    {
      "code": "eplb_config: EPLBConfig = Field(default_factory=EPLBConfig)",
      "language": "typescript"
    },
    {
      "code": "eplb_config: EPLBConfig = Field(default_factory=EPLBConfig)",
      "language": "typescript"
    },
    {
      "code": "expert_placement_strategy: ExpertPlacementStrategy = (\n    \"linear\"\n)",
      "language": "typescript"
    },
    {
      "code": "expert_placement_strategy: ExpertPlacementStrategy = (\n    \"linear\"\n)",
      "language": "typescript"
    },
    {
      "code": "local_world_size: int",
      "language": "yaml"
    },
    {
      "code": "local_world_size: int",
      "language": "yaml"
    },
    {
      "code": "master_addr: str = '127.0.0.1'",
      "language": "typescript"
    },
    {
      "code": "master_addr: str = '127.0.0.1'",
      "language": "typescript"
    },
    {
      "code": "master_port: int = 29501",
      "language": "typescript"
    },
    {
      "code": "master_port: int = 29501",
      "language": "typescript"
    },
    {
      "code": "max_parallel_loading_workers: int | None = None",
      "language": "yaml"
    },
    {
      "code": "max_parallel_loading_workers: int | None = None",
      "language": "yaml"
    },
    {
      "code": "nnodes: int = 1",
      "language": "typescript"
    },
    {
      "code": "nnodes: int = 1",
      "language": "typescript"
    },
    {
      "code": "nnodes_within_dp: int",
      "language": "yaml"
    },
    {
      "code": "nnodes_within_dp: int",
      "language": "yaml"
    },
    {
      "code": "node_rank: int = 0",
      "language": "typescript"
    },
    {
      "code": "node_rank: int = 0",
      "language": "typescript"
    },
    {
      "code": "node_rank_within_dp: int",
      "language": "yaml"
    },
    {
      "code": "node_rank_within_dp: int",
      "language": "yaml"
    },
    {
      "code": "num_ubatches: int",
      "language": "yaml"
    },
    {
      "code": "num_ubatches: int",
      "language": "yaml"
    },
    {
      "code": "pipeline_parallel_size: int = 1",
      "language": "typescript"
    },
    {
      "code": "pipeline_parallel_size: int = 1",
      "language": "typescript"
    },
    {
      "code": "placement_group: PlacementGroup | None = None",
      "language": "yaml"
    },
    {
      "code": "placement_group: PlacementGroup | None = None",
      "language": "yaml"
    },
    {
      "code": "prefill_context_parallel_size: int = 1",
      "language": "typescript"
    },
    {
      "code": "prefill_context_parallel_size: int = 1",
      "language": "typescript"
    },
    {
      "code": "rank: int = 0",
      "language": "typescript"
    },
    {
      "code": "rank: int = 0",
      "language": "typescript"
    },
    {
      "code": "ray_runtime_env: RuntimeEnv | None = None",
      "language": "yaml"
    },
    {
      "code": "ray_runtime_env: RuntimeEnv | None = None",
      "language": "yaml"
    },
    {
      "code": "ray_workers_use_nsight: bool = False",
      "language": "typescript"
    },
    {
      "code": "ray_workers_use_nsight: bool = False",
      "language": "typescript"
    },
    {
      "code": "sd_worker_cls: str = 'auto'",
      "language": "typescript"
    },
    {
      "code": "sd_worker_cls: str = 'auto'",
      "language": "typescript"
    },
    {
      "code": "tensor_parallel_size: int = 1",
      "language": "typescript"
    },
    {
      "code": "tensor_parallel_size: int = 1",
      "language": "typescript"
    },
    {
      "code": "ubatch_size: int = 0",
      "language": "typescript"
    },
    {
      "code": "ubatch_size: int = 0",
      "language": "typescript"
    },
    {
      "code": "use_ray: bool",
      "language": "yaml"
    },
    {
      "code": "use_ray: bool",
      "language": "yaml"
    },
    {
      "code": "use_sequence_parallel_moe: bool",
      "language": "yaml"
    },
    {
      "code": "use_sequence_parallel_moe: bool",
      "language": "yaml"
    },
    {
      "code": "use_ubatching: bool",
      "language": "yaml"
    },
    {
      "code": "use_ubatching: bool",
      "language": "yaml"
    },
    {
      "code": "worker_cls: str = 'auto'",
      "language": "typescript"
    },
    {
      "code": "worker_cls: str = 'auto'",
      "language": "typescript"
    },
    {
      "code": "worker_extension_cls: str = ''",
      "language": "typescript"
    },
    {
      "code": "worker_extension_cls: str = ''",
      "language": "typescript"
    },
    {
      "code": "world_size: int = Field(init=False)",
      "language": "typescript"
    },
    {
      "code": "world_size: int = Field(init=False)",
      "language": "typescript"
    },
    {
      "code": "world_size_across_dp: int",
      "language": "yaml"
    },
    {
      "code": "world_size_across_dp: int",
      "language": "yaml"
    },
    {
      "code": "__post_init__() -> None",
      "language": "rust"
    },
    {
      "code": "__post_init__() -> None",
      "language": "rust"
    },
    {
      "code": "499\n500\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\n600\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613",
      "language": "unknown"
    },
    {
      "code": "def __post_init__(self) -> None:\n    # Set all2all_backend from env var if not specified, with deprecation warning\n    if envs.is_set(\"VLLM_ALL2ALL_BACKEND\"):\n        logger.warning_once(\n            \"VLLM_ALL2ALL_BACKEND environment variable is deprecated and \"\n            \"will be removed in v0.15.0. Please use the \"\n            \"--all2all-backend command-line argument instead.\"\n        )\n        self.all2all_backend = envs.VLLM_ALL2ALL_BACKEND\n\n    # Continue with the rest of the initialization\n    self.world_size = (\n        self.pipeline_parallel_size\n        * self.tensor_parallel_size\n        * self.prefill_context_parallel_size\n    )\n\n    if self.distributed_executor_backend == \"external_launcher\":\n        logger.info(\"Using external launcher for distributed inference.\")\n        self.world_size *= self.data_parallel_size\n\n    if self.data_parallel_size > 1 or self.data_parallel_size_local == 0:\n        # Data parallel was specified in the engine args.\n        if self.distributed_executor_backend == \"external_launcher\":\n            # For external launcher,\n            # we need to set the data parallel rank automatically\n            self.data_parallel_rank = int(os.environ[\"RANK\"]) // (\n                self.world_size // self.data_parallel_size\n            )\n            logger.info(\n                \"Set data_parallel_rank to %d automatically.\",\n                self.data_parallel_rank,\n            )\n        if not self._data_parallel_master_port_list:\n            self._data_parallel_master_port_list = get_open_ports_list(5)\n        self.data_parallel_master_port = self._data_parallel_master_port_list.pop()\n\n        if not (0 <= self.data_parallel_rank < self.data_parallel_size):\n            raise ValueError(\n                f\"data_parallel_rank ({self.data_parallel_rank})\"\n                f\" must be in the range [0, {self.data_parallel_size})\"\n            )\n    else:\n        # Otherwise fall back to env vars (e.g. for offline SPMD case).\n        self.data_parallel_size = envs.VLLM_DP_SIZE\n        self.data_parallel_rank = envs.VLLM_DP_RANK\n        self.data_parallel_rank_local = envs.VLLM_DP_RANK_LOCAL\n        self.data_parallel_master_ip = envs.VLLM_DP_MASTER_IP\n        self.data_parallel_master_port = envs.VLLM_DP_MASTER_PORT\n\n    if self.distributed_executor_backend == \"external_launcher\":\n        os.environ[\"VLLM_ENABLE_V1_MULTIPROCESSING\"] = \"0\"\n        logger.info(\"Disabling V1 multiprocessing for external launcher.\")\n\n    if self.distributed_executor_backend is None and self.world_size > 1:\n        # We use multiprocessing by default if world_size fits on the\n        # current node and we aren't in a ray placement group.\n\n        from vllm.v1.executor import ray_utils\n\n        backend: DistributedExecutorBackend = \"mp\"\n        ray_found = ray_utils.ray_is_available()\n        if current_platform.is_tpu() and envs.VLLM_XLA_USE_SPMD:\n            backend = \"uni\"\n        elif current_platform.is_cuda() and self.nnodes > 1:\n            backend = \"mp\"\n        elif (\n            current_platform.is_cuda()\n            and cuda_device_count_stateless() < self.world_size\n        ):\n            gpu_count = cuda_device_count_stateless()\n            raise ValueError(\n                f\"World size ({self.world_size}) is larger than the number of \"\n                f\"available GPUs ({gpu_count}) in this node. If this is \"\n                \"intentional and you are using:\\n\"\n                \"- ray, set '--distributed-executor-backend ray'.\\n\"\n                \"- multiprocessing, set '--nnodes' appropriately.\"\n            )\n        elif self.data_parallel_backend == \"ray\":\n            logger.info(\n                \"Using ray distributed inference because \"\n                \"data_parallel_backend is ray\"\n            )\n            backend = \"ray\"\n        elif ray_found:\n            if self.placement_group:\n                backend = \"ray\"\n            else:\n                from ray import is_initialized as ray_is_initialized\n\n                if ray_is_initialized():\n                    from ray.util import get_current_placement_group\n\n                    if get_current_placement_group():\n                        backend = \"ray\"\n        self.distributed_executor_backend = backend\n        logger.debug(\"Defaulting to use %s for distributed inference\", backend)\n\n    if self.distributed_executor_backend is None and self.world_size == 1:\n        self.distributed_executor_backend = \"uni\"\n\n    if self.max_parallel_loading_workers is not None:\n        logger.warning(\n            \"max_parallel_loading_workers is currently \"\n            \"not supported and will be ignored.\"\n        )\n    allowed_backends = (\"mp\", \"uni\", \"external_launcher\")\n    if (\n        self.distributed_executor_backend not in allowed_backends\n        and self.nnodes > 1\n    ):\n        raise ValueError(\n            \"nnodes > 1 can only be set when distributed executor \"\n            \"backend is mp, uni or external_launcher.\"\n        )",
      "language": "python"
    },
    {
      "code": "def __post_init__(self) -> None:\n    # Set all2all_backend from env var if not specified, with deprecation warning\n    if envs.is_set(\"VLLM_ALL2ALL_BACKEND\"):\n        logger.warning_once(\n            \"VLLM_ALL2ALL_BACKEND environment variable is deprecated and \"\n            \"will be removed in v0.15.0. Please use the \"\n            \"--all2all-backend command-line argument instead.\"\n        )\n        self.all2all_backend = envs.VLLM_ALL2ALL_BACKEND\n\n    # Continue with the rest of the initialization\n    self.world_size = (\n        self.pipeline_parallel_size\n        * self.tensor_parallel_size\n        * self.prefill_context_parallel_size\n    )\n\n    if self.distributed_executor_backend == \"external_launcher\":\n        logger.info(\"Using external launcher for distributed inference.\")\n        self.world_size *= self.data_parallel_size\n\n    if self.data_parallel_size > 1 or self.data_parallel_size_local == 0:\n        # Data parallel was specified in the engine args.\n        if self.distributed_executor_backend == \"external_launcher\":\n            # For external launcher,\n            # we need to set the data parallel rank automatically\n            self.data_parallel_rank = int(os.environ[\"RANK\"]) // (\n                self.world_size // self.data_parallel_size\n            )\n            logger.info(\n                \"Set data_parallel_rank to %d automatically.\",\n                self.data_parallel_rank,\n            )\n        if not self._data_parallel_master_port_list:\n            self._data_parallel_master_port_list = get_open_ports_list(5)\n        self.data_parallel_master_port = self._data_parallel_master_port_list.pop()\n\n        if not (0 <= self.data_parallel_rank < self.data_parallel_size):\n            raise ValueError(\n                f\"data_parallel_rank ({self.data_parallel_rank})\"\n                f\" must be in the range [0, {self.data_parallel_size})\"\n            )\n    else:\n        # Otherwise fall back to env vars (e.g. for offline SPMD case).\n        self.data_parallel_size = envs.VLLM_DP_SIZE\n        self.data_parallel_rank = envs.VLLM_DP_RANK\n        self.data_parallel_rank_local = envs.VLLM_DP_RANK_LOCAL\n        self.data_parallel_master_ip = envs.VLLM_DP_MASTER_IP\n        self.data_parallel_master_port = envs.VLLM_DP_MASTER_PORT\n\n    if self.distributed_executor_backend == \"external_launcher\":\n        os.environ[\"VLLM_ENABLE_V1_MULTIPROCESSING\"] = \"0\"\n        logger.info(\"Disabling V1 multiprocessing for external launcher.\")\n\n    if self.distributed_executor_backend is None and self.world_size > 1:\n        # We use multiprocessing by default if world_size fits on the\n        # current node and we aren't in a ray placement group.\n\n        from vllm.v1.executor import ray_utils\n\n        backend: DistributedExecutorBackend = \"mp\"\n        ray_found = ray_utils.ray_is_available()\n        if current_platform.is_tpu() and envs.VLLM_XLA_USE_SPMD:\n            backend = \"uni\"\n        elif current_platform.is_cuda() and self.nnodes > 1:\n            backend = \"mp\"\n        elif (\n            current_platform.is_cuda()\n            and cuda_device_count_stateless() < self.world_size\n        ):\n            gpu_count = cuda_device_count_stateless()\n            raise ValueError(\n                f\"World size ({self.world_size}) is larger than the number of \"\n                f\"available GPUs ({gpu_count}) in this node. If this is \"\n                \"intentional and you are using:\\n\"\n                \"- ray, set '--distributed-executor-backend ray'.\\n\"\n                \"- multiprocessing, set '--nnodes' appropriately.\"\n            )\n        elif self.data_parallel_backend == \"ray\":\n            logger.info(\n                \"Using ray distributed inference because \"\n                \"data_parallel_backend is ray\"\n            )\n            backend = \"ray\"\n        elif ray_found:\n            if self.placement_group:\n                backend = \"ray\"\n            else:\n                from ray import is_initialized as ray_is_initialized\n\n                if ray_is_initialized():\n                    from ray.util import get_current_placement_group\n\n                    if get_current_placement_group():\n                        backend = \"ray\"\n        self.distributed_executor_backend = backend\n        logger.debug(\"Defaulting to use %s for distributed inference\", backend)\n\n    if self.distributed_executor_backend is None and self.world_size == 1:\n        self.distributed_executor_backend = \"uni\"\n\n    if self.max_parallel_loading_workers is not None:\n        logger.warning(\n            \"max_parallel_loading_workers is currently \"\n            \"not supported and will be ignored.\"\n        )\n    allowed_backends = (\"mp\", \"uni\", \"external_launcher\")\n    if (\n        self.distributed_executor_backend not in allowed_backends\n        and self.nnodes > 1\n    ):\n        raise ValueError(\n            \"nnodes > 1 can only be set when distributed executor \"\n            \"backend is mp, uni or external_launcher.\"\n        )",
      "language": "python"
    },
    {
      "code": "_validate_parallel_config() -> Self",
      "language": "php"
    },
    {
      "code": "_validate_parallel_config() -> Self",
      "language": "php"
    },
    {
      "code": "277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320",
      "language": "unknown"
    },
    {
      "code": "@model_validator(mode=\"after\")\ndef _validate_parallel_config(self) -> Self:\n    if self._api_process_rank >= self._api_process_count:\n        raise ValueError(\n            \"Invalid value of `_api_process_rank`. \"\n            f\"Expected to be `-1` or `[0, {self._api_process_count})`, \"\n            f\"but found: {self._api_process_rank}\"\n        )\n\n    if self.data_parallel_size_local > self.data_parallel_size:\n        raise ValueError(\n            f\"data_parallel_size_local ({self.data_parallel_size_local}) \"\n            f\"must be <= data_parallel_size ({self.data_parallel_size})\"\n        )\n\n    if self.data_parallel_size <= 1 and self.data_parallel_external_lb:\n        raise ValueError(\n            \"data_parallel_external_lb can only be set when data_parallel_size > 1\"\n        )\n\n    if self.enable_eplb:\n        if not current_platform.is_cuda_alike():\n            raise ValueError(\n                \"Expert parallelism load balancing is only supported on \"\n                \"CUDA devices or ROCm devices now.\"\n            )\n        if not self.enable_expert_parallel:\n            raise ValueError(\"enable_expert_parallel must be True to use EPLB.\")\n        if self.tensor_parallel_size * self.data_parallel_size <= 1:\n            raise ValueError(\n                \"EPLB requires tensor_parallel_size or data_parallel_size \"\n                f\"to be greater than 1, but got \"\n                f\"TP={self.tensor_parallel_size},DP={self.data_parallel_size}.\"\n            )\n    else:\n        if self.eplb_config.num_redundant_experts != 0:\n            raise ValueError(\n                \"num_redundant_experts is set to \"\n                f\"{self.eplb_config.num_redundant_experts} but EPLB is not \"\n                \"enabled. Either enable EPLB or unset \"\n                \"num_redundant_experts.\"\n            )\n\n    return self",
      "language": "python"
    },
    {
      "code": "@model_validator(mode=\"after\")\ndef _validate_parallel_config(self) -> Self:\n    if self._api_process_rank >= self._api_process_count:\n        raise ValueError(\n            \"Invalid value of `_api_process_rank`. \"\n            f\"Expected to be `-1` or `[0, {self._api_process_count})`, \"\n            f\"but found: {self._api_process_rank}\"\n        )\n\n    if self.data_parallel_size_local > self.data_parallel_size:\n        raise ValueError(\n            f\"data_parallel_size_local ({self.data_parallel_size_local}) \"\n            f\"must be <= data_parallel_size ({self.data_parallel_size})\"\n        )\n\n    if self.data_parallel_size <= 1 and self.data_parallel_external_lb:\n        raise ValueError(\n            \"data_parallel_external_lb can only be set when data_parallel_size > 1\"\n        )\n\n    if self.enable_eplb:\n        if not current_platform.is_cuda_alike():\n            raise ValueError(\n                \"Expert parallelism load balancing is only supported on \"\n                \"CUDA devices or ROCm devices now.\"\n            )\n        if not self.enable_expert_parallel:\n            raise ValueError(\"enable_expert_parallel must be True to use EPLB.\")\n        if self.tensor_parallel_size * self.data_parallel_size <= 1:\n            raise ValueError(\n                \"EPLB requires tensor_parallel_size or data_parallel_size \"\n                f\"to be greater than 1, but got \"\n                f\"TP={self.tensor_parallel_size},DP={self.data_parallel_size}.\"\n            )\n    else:\n        if self.eplb_config.num_redundant_experts != 0:\n            raise ValueError(\n                \"num_redundant_experts is set to \"\n                f\"{self.eplb_config.num_redundant_experts} but EPLB is not \"\n                \"enabled. Either enable EPLB or unset \"\n                \"num_redundant_experts.\"\n            )\n\n    return self",
      "language": "python"
    },
    {
      "code": "_verify_args() -> Self",
      "language": "php"
    },
    {
      "code": "_verify_args() -> Self",
      "language": "php"
    },
    {
      "code": "622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666",
      "language": "unknown"
    },
    {
      "code": "@model_validator(mode=\"after\")\ndef _verify_args(self) -> Self:\n    # Lazy import to avoid circular import\n    from vllm.v1.executor import Executor\n\n    # Enable batch invariance settings if requested\n    if vllm_is_batch_invariant():\n        self.disable_custom_all_reduce = True\n\n    if (\n        self.distributed_executor_backend is not None\n        and not isinstance(self.distributed_executor_backend, str)\n        and not (\n            isinstance(self.distributed_executor_backend, type)\n            and issubclass(self.distributed_executor_backend, Executor)\n        )\n    ):\n        raise ValueError(\n            \"Unrecognized distributed executor backend \"\n            f\"{self.distributed_executor_backend}. Supported \"\n            \"values are 'ray', 'mp' 'uni', 'external_launcher', \"\n            \" custom Executor subclass or its import path.\"\n        )\n    if self.use_ray:\n        from vllm.v1.executor import ray_utils\n\n        ray_utils.assert_ray_available()\n\n    if not current_platform.use_custom_allreduce():\n        self.disable_custom_all_reduce = True\n        logger.debug(\n            \"Disabled the custom all-reduce kernel because it is not \"\n            \"supported on current platform.\"\n        )\n    if self.nnodes > 1:\n        self.disable_custom_all_reduce = True\n        logger.debug(\n            \"Disabled the custom all-reduce since we are running on multi-node.\"\n        )\n    if self.ray_workers_use_nsight and not self.use_ray:\n        raise ValueError(\n            \"Unable to use nsight profiling unless workers run with Ray.\"\n        )\n\n    return self",
      "language": "python"
    },
    {
      "code": "@model_validator(mode=\"after\")\ndef _verify_args(self) -> Self:\n    # Lazy import to avoid circular import\n    from vllm.v1.executor import Executor\n\n    # Enable batch invariance settings if requested\n    if vllm_is_batch_invariant():\n        self.disable_custom_all_reduce = True\n\n    if (\n        self.distributed_executor_backend is not None\n        and not isinstance(self.distributed_executor_backend, str)\n        and not (\n            isinstance(self.distributed_executor_backend, type)\n            and issubclass(self.distributed_executor_backend, Executor)\n        )\n    ):\n        raise ValueError(\n            \"Unrecognized distributed executor backend \"\n            f\"{self.distributed_executor_backend}. Supported \"\n            \"values are 'ray', 'mp' 'uni', 'external_launcher', \"\n            \" custom Executor subclass or its import path.\"\n        )\n    if self.use_ray:\n        from vllm.v1.executor import ray_utils\n\n        ray_utils.assert_ray_available()\n\n    if not current_platform.use_custom_allreduce():\n        self.disable_custom_all_reduce = True\n        logger.debug(\n            \"Disabled the custom all-reduce kernel because it is not \"\n            \"supported on current platform.\"\n        )\n    if self.nnodes > 1:\n        self.disable_custom_all_reduce = True\n        logger.debug(\n            \"Disabled the custom all-reduce since we are running on multi-node.\"\n        )\n    if self.ray_workers_use_nsight and not self.use_ray:\n        raise ValueError(\n            \"Unable to use nsight profiling unless workers run with Ray.\"\n        )\n\n    return self",
      "language": "python"
    },
    {
      "code": "compute_hash()",
      "language": "unknown"
    },
    {
      "code": "compute_hash()",
      "language": "unknown"
    },
    {
      "code": "453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497",
      "language": "unknown"
    },
    {
      "code": "def compute_hash(self):\n    \"\"\"\n    Provide a hash that uniquely identifies all the configs\n    that affect the structure of the computation\n    graph from input ids/embeddings to the final hidden states,\n    excluding anything before input ids/embeddings and after\n    the final hidden states.\n\n    This hash is also used for DP worker configuration validation\n    to prevent hangs from mismatched collective communication patterns.\n    \"\"\"\n    ignored_factors = {\n        # Derived/runtime topology, networking, or launch details\n        \"data_parallel_rank\",\n        \"data_parallel_rank_local\",\n        \"data_parallel_size_local\",\n        \"data_parallel_backend\",\n        \"data_parallel_external_lb\",\n        \"data_parallel_hybrid_lb\",\n        \"data_parallel_master_ip\",\n        \"data_parallel_master_port\",\n        \"_data_parallel_master_port_list\",\n        \"data_parallel_rpc_port\",\n        \"rank\",\n        \"master_addr\",\n        \"master_port\",\n        \"node_rank\",\n        \"nnodes\",\n        \"max_parallel_loading_workers\",\n        \"disable_custom_all_reduce\",\n        \"ray_workers_use_nsight\",\n        \"ray_runtime_env\",\n        \"placement_group\",\n        \"distributed_executor_backend\",\n        \"worker_cls\",\n        \"sd_worker_cls\",\n        \"worker_extension_cls\",\n        \"_api_process_count\",\n        \"_api_process_rank\",\n    }\n\n    from vllm.config.utils import get_hash_factors, hash_factors\n\n    factors = get_hash_factors(self, ignored_factors)\n    return hash_factors(factors)",
      "language": "python"
    },
    {
      "code": "def compute_hash(self):\n    \"\"\"\n    Provide a hash that uniquely identifies all the configs\n    that affect the structure of the computation\n    graph from input ids/embeddings to the final hidden states,\n    excluding anything before input ids/embeddings and after\n    the final hidden states.\n\n    This hash is also used for DP worker configuration validation\n    to prevent hangs from mismatched collective communication patterns.\n    \"\"\"\n    ignored_factors = {\n        # Derived/runtime topology, networking, or launch details\n        \"data_parallel_rank\",\n        \"data_parallel_rank_local\",\n        \"data_parallel_size_local\",\n        \"data_parallel_backend\",\n        \"data_parallel_external_lb\",\n        \"data_parallel_hybrid_lb\",\n        \"data_parallel_master_ip\",\n        \"data_parallel_master_port\",\n        \"_data_parallel_master_port_list\",\n        \"data_parallel_rpc_port\",\n        \"rank\",\n        \"master_addr\",\n        \"master_port\",\n        \"node_rank\",\n        \"nnodes\",\n        \"max_parallel_loading_workers\",\n        \"disable_custom_all_reduce\",\n        \"ray_workers_use_nsight\",\n        \"ray_runtime_env\",\n        \"placement_group\",\n        \"distributed_executor_backend\",\n        \"worker_cls\",\n        \"sd_worker_cls\",\n        \"worker_extension_cls\",\n        \"_api_process_count\",\n        \"_api_process_rank\",\n    }\n\n    from vllm.config.utils import get_hash_factors, hash_factors\n\n    factors = get_hash_factors(self, ignored_factors)\n    return hash_factors(factors)",
      "language": "python"
    },
    {
      "code": "get_next_dp_init_port() -> int",
      "language": "php"
    },
    {
      "code": "get_next_dp_init_port() -> int",
      "language": "php"
    },
    {
      "code": "336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351",
      "language": "unknown"
    },
    {
      "code": "def get_next_dp_init_port(self) -> int:\n    \"\"\"\n    We might need to initialize process groups in multiple\n    processes that is related to data parallelism,\n    e.g. both in the worker and in the engine, which\n    can live in different processes. To avoid port conflicts, we\n    pop a new port from the prepared port list each time we need to\n    initialize a new process group related to data parallelism.\n    \"\"\"\n    if self._data_parallel_master_port_list:\n        answer = self._data_parallel_master_port_list.pop()\n    else:\n        answer = self.data_parallel_master_port\n        self.data_parallel_master_port += 1\n\n    return answer",
      "language": "python"
    },
    {
      "code": "def get_next_dp_init_port(self) -> int:\n    \"\"\"\n    We might need to initialize process groups in multiple\n    processes that is related to data parallelism,\n    e.g. both in the worker and in the engine, which\n    can live in different processes. To avoid port conflicts, we\n    pop a new port from the prepared port list each time we need to\n    initialize a new process group related to data parallelism.\n    \"\"\"\n    if self._data_parallel_master_port_list:\n        answer = self._data_parallel_master_port_list.pop()\n    else:\n        answer = self.data_parallel_master_port\n        self.data_parallel_master_port += 1\n\n    return answer",
      "language": "python"
    },
    {
      "code": "has_unfinished_dp(\n    dp_group: ProcessGroup, has_unfinished: bool\n) -> bool",
      "language": "php"
    },
    {
      "code": "has_unfinished_dp(\n    dp_group: ProcessGroup, has_unfinished: bool\n) -> bool",
      "language": "php"
    },
    {
      "code": "432\n433\n434\n435\n436\n437\n438\n439\n440\n441",
      "language": "unknown"
    },
    {
      "code": "@staticmethod\ndef has_unfinished_dp(dp_group: ProcessGroup, has_unfinished: bool) -> bool:\n    tensor = torch.tensor([has_unfinished], dtype=torch.int32, device=\"cpu\")\n    # dp rank 0: has_unfinished_seqs=True\n    # dp rank 1: has_unfinished_seqs=False\n    # aggregated: has_unfinished_seqs=True\n    # so this is an OR operation, i.e. MAX in integers\n    torch.distributed.all_reduce(tensor, op=ReduceOp.MAX, group=dp_group)\n    aggregated_has_unfinished = bool(tensor.item())\n    return aggregated_has_unfinished",
      "language": "python"
    },
    {
      "code": "@staticmethod\ndef has_unfinished_dp(dp_group: ProcessGroup, has_unfinished: bool) -> bool:\n    tensor = torch.tensor([has_unfinished], dtype=torch.int32, device=\"cpu\")\n    # dp rank 0: has_unfinished_seqs=True\n    # dp rank 1: has_unfinished_seqs=False\n    # aggregated: has_unfinished_seqs=True\n    # so this is an OR operation, i.e. MAX in integers\n    torch.distributed.all_reduce(tensor, op=ReduceOp.MAX, group=dp_group)\n    aggregated_has_unfinished = bool(tensor.item())\n    return aggregated_has_unfinished",
      "language": "python"
    },
    {
      "code": "stateless_init_dp_group() -> ProcessGroup",
      "language": "php"
    },
    {
      "code": "stateless_init_dp_group() -> ProcessGroup",
      "language": "php"
    },
    {
      "code": "353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389",
      "language": "unknown"
    },
    {
      "code": "def stateless_init_dp_group(self) -> ProcessGroup:\n    # NOTE: In high-concurrency scenarios multiple processes\n    # can pick the same (currently free) port through a race\n    # condition when calling `get_open_port()`. When the first\n    # process binds the port the others will subsequently fail\n    # with `torch.distributed.DistNetworkError: EADDRINUSE`.\n    # To make the initialization more robust we retry a few times\n    # with a fresh port whenever this specific error is observed.\n    from torch.distributed import DistNetworkError\n\n    from vllm.distributed.utils import (\n        stateless_init_torch_distributed_process_group,\n    )\n\n    max_retries = 5\n    last_exc: Exception | None = None\n    for _ in range(max_retries):\n        try:\n            # use gloo since the engine process might not have cuda device\n            return stateless_init_torch_distributed_process_group(\n                self.data_parallel_master_ip,\n                self.get_next_dp_init_port(),\n                self.data_parallel_rank,\n                self.data_parallel_size,\n                backend=current_platform.dist_backend,\n            )\n        except DistNetworkError as e:\n            # We only want to retry when the root cause is EADDRINUSE.\n            if \"EADDRINUSE\" in str(e):\n                logger.warning(\"Address already in use. Retrying with a new port.\")\n                last_exc = e\n                continue  # try again with a new port\n            raise e\n\n    # If we get here all retries have failed.\n    assert last_exc is not None\n    raise last_exc",
      "language": "python"
    },
    {
      "code": "def stateless_init_dp_group(self) -> ProcessGroup:\n    # NOTE: In high-concurrency scenarios multiple processes\n    # can pick the same (currently free) port through a race\n    # condition when calling `get_open_port()`. When the first\n    # process binds the port the others will subsequently fail\n    # with `torch.distributed.DistNetworkError: EADDRINUSE`.\n    # To make the initialization more robust we retry a few times\n    # with a fresh port whenever this specific error is observed.\n    from torch.distributed import DistNetworkError\n\n    from vllm.distributed.utils import (\n        stateless_init_torch_distributed_process_group,\n    )\n\n    max_retries = 5\n    last_exc: Exception | None = None\n    for _ in range(max_retries):\n        try:\n            # use gloo since the engine process might not have cuda device\n            return stateless_init_torch_distributed_process_group(\n                self.data_parallel_master_ip,\n                self.get_next_dp_init_port(),\n                self.data_parallel_rank,\n                self.data_parallel_size,\n                backend=current_platform.dist_backend,\n            )\n        except DistNetworkError as e:\n            # We only want to retry when the root cause is EADDRINUSE.\n            if \"EADDRINUSE\" in str(e):\n                logger.warning(\"Address already in use. Retrying with a new port.\")\n                last_exc = e\n                continue  # try again with a new port\n            raise e\n\n    # If we get here all retries have failed.\n    assert last_exc is not None\n    raise last_exc",
      "language": "python"
    },
    {
      "code": "sync_kv_cache_memory_size(\n    dp_group: ProcessGroup, kv_cache_memory: int\n) -> int",
      "language": "php"
    },
    {
      "code": "sync_kv_cache_memory_size(\n    dp_group: ProcessGroup, kv_cache_memory: int\n) -> int",
      "language": "php"
    },
    {
      "code": "443\n444\n445\n446\n447\n448\n449\n450\n451",
      "language": "unknown"
    },
    {
      "code": "@staticmethod\ndef sync_kv_cache_memory_size(dp_group: ProcessGroup, kv_cache_memory: int) -> int:\n    if kv_cache_memory == -1:\n        kv_cache_memory = torch.iinfo(torch.int64).max\n    tensor = torch.tensor([kv_cache_memory], dtype=torch.int64, device=\"cpu\")\n    # we cannot use broadcast for stateless dp group since it depends\n    # on global rank\n    torch.distributed.all_reduce(tensor, op=ReduceOp.MIN, group=dp_group)\n    return tensor.item()",
      "language": "python"
    },
    {
      "code": "@staticmethod\ndef sync_kv_cache_memory_size(dp_group: ProcessGroup, kv_cache_memory: int) -> int:\n    if kv_cache_memory == -1:\n        kv_cache_memory = torch.iinfo(torch.int64).max\n    tensor = torch.tensor([kv_cache_memory], dtype=torch.int64, device=\"cpu\")\n    # we cannot use broadcast for stateless dp group since it depends\n    # on global rank\n    torch.distributed.all_reduce(tensor, op=ReduceOp.MIN, group=dp_group)\n    return tensor.item()",
      "language": "python"
    }
  ],
  "patterns": [],
  "links": [
    "https://docs.vllm.ai/en/latest/api/vllm/config/parallel/",
    "https://docs.vllm.ai/en/latest/",
    "https://docs.vllm.ai/en/latest/usage/",
    "https://docs.vllm.ai/en/latest/contributing/",
    "https://docs.vllm.ai/en/latest/benchmarking/",
    "https://docs.vllm.ai/en/latest/api/",
    "https://docs.vllm.ai/en/latest/cli/",
    "https://docs.vllm.ai/en/latest/community/contact_us/",
    "https://docs.vllm.ai/en/latest/getting_started/quickstart/",
    "https://docs.vllm.ai/en/latest/getting_started/installation/",
    "https://docs.vllm.ai/en/latest/getting_started/installation/gpu/",
    "https://docs.vllm.ai/en/latest/getting_started/installation/cpu/",
    "https://docs.vllm.ai/en/latest/examples/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/async_llm_streaming/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/audio_language/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/automatic_prefix_caching/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/basic/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/batch_llm_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/chat_with_tools/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/context_extension/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/data_parallel/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/disaggregated-prefill-v1/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/disaggregated_prefill/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/encoder_decoder_multimodal/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/kv_load_failure_recovery/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/llm_engine_example/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/llm_engine_reset_kv/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/load_sharded_state/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/logits_processor/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/lora_with_quantization_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/metrics/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/mistral-small/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/mlpspeculator/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/multilora_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/openai_batch/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/prefix_caching/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/prompt_embed_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/qwen2_5_omni/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/qwen3_omni/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/qwen_1m/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/reproducibility/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf_colocate/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf_online_quant/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf_utils/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/save_sharded_state/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/simple_profiling/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/skip_loading_weights_in_engine_init/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/spec_decode/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/structured_outputs/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/torchrun_dp_example/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/torchrun_example/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/vision_language/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/vision_language_multi_image/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/api_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/chart-helm/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/dashboards/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_encoder/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_prefill/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_serving/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/elastic_ep/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/gradio_openai_chatbot_webserver/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/gradio_webserver/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/kv_events_subscriber/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/multi-node-serving/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/multi_instance_data_parallel/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_for_multimodal/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools_required/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools_xlam/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools_xlam_streaming/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_tool_calls_with_reasoning/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_with_reasoning/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_with_reasoning_streaming/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_completion_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_responses_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_responses_client_with_mcp_tools/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_responses_client_with_tools/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_transcription_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_translation_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/opentelemetry/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/prometheus_grafana/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/prompt_embed_inference_with_openai_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/ray_serve_deepseek/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/retrieval_augmented_generation_with_langchain/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/retrieval_augmented_generation_with_llamaindex/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/run_cluster/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/sagemaker-entrypoint/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/streamlit_openai_chatbot_webserver/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/structured_outputs/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/token_generation_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/utils/",
    "https://docs.vllm.ai/en/latest/examples/others/lmcache/",
    "https://docs.vllm.ai/en/latest/examples/others/logging_configuration/",
    "https://docs.vllm.ai/en/latest/examples/others/tensorize_vllm_model/",
    "https://docs.vllm.ai/en/latest/examples/pooling/classify/",
    "https://docs.vllm.ai/en/latest/examples/pooling/embed/",
    "https://docs.vllm.ai/en/latest/examples/pooling/plugin/",
    "https://docs.vllm.ai/en/latest/examples/pooling/pooling/",
    "https://docs.vllm.ai/en/latest/examples/pooling/score/",
    "https://docs.vllm.ai/en/latest/examples/pooling/token_classify/",
    "https://docs.vllm.ai/en/latest/examples/pooling/token_embed/",
    "https://docs.vllm.ai/en/latest/usage/v1_guide/",
    "https://docs.vllm.ai/en/latest/usage/faq/",
    "https://docs.vllm.ai/en/latest/usage/metrics/",
    "https://docs.vllm.ai/en/latest/usage/reproducibility/",
    "https://docs.vllm.ai/en/latest/usage/security/",
    "https://docs.vllm.ai/en/latest/usage/troubleshooting/",
    "https://docs.vllm.ai/en/latest/usage/usage_stats/",
    "https://docs.vllm.ai/en/latest/serving/offline_inference/",
    "https://docs.vllm.ai/en/latest/serving/openai_compatible_server/",
    "https://docs.vllm.ai/en/latest/serving/context_parallel_deployment/",
    "https://docs.vllm.ai/en/latest/serving/data_parallel_deployment/",
    "https://docs.vllm.ai/en/latest/serving/distributed_troubleshooting/",
    "https://docs.vllm.ai/en/latest/serving/expert_parallel_deployment/",
    "https://docs.vllm.ai/en/latest/serving/parallelism_scaling/",
    "https://docs.vllm.ai/en/latest/serving/integrations/langchain/",
    "https://docs.vllm.ai/en/latest/serving/integrations/llamaindex/",
    "https://docs.vllm.ai/en/latest/deployment/docker/",
    "https://docs.vllm.ai/en/latest/deployment/k8s/",
    "https://docs.vllm.ai/en/latest/deployment/nginx/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/anyscale/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/anything-llm/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/autogen/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/bentoml/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/cerebrium/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/chatbox/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/dify/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/dstack/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/haystack/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/helm/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/hf_inference_endpoints/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/litellm/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/lobe-chat/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/lws/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/modal/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/open-webui/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/retrieval_augmented_generation/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/skypilot/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/streamlit/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/triton/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kaito/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kserve/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kthena/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kubeai/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kuberay/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/llamastack/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/llm-d/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/llmaz/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/production-stack/",
    "https://docs.vllm.ai/en/latest/training/rlhf/",
    "https://docs.vllm.ai/en/latest/training/trl/",
    "https://docs.vllm.ai/en/latest/configuration/",
    "https://docs.vllm.ai/en/latest/configuration/conserving_memory/",
    "https://docs.vllm.ai/en/latest/configuration/engine_args/",
    "https://docs.vllm.ai/en/latest/configuration/env_vars/",
    "https://docs.vllm.ai/en/latest/configuration/model_resolution/",
    "https://docs.vllm.ai/en/latest/configuration/optimization/",
    "https://docs.vllm.ai/en/latest/configuration/serve_args/",
    "https://docs.vllm.ai/en/latest/models/supported_models/",
    "https://docs.vllm.ai/en/latest/models/generative_models/",
    "https://docs.vllm.ai/en/latest/models/pooling_models/",
    "https://docs.vllm.ai/en/latest/models/extensions/fastsafetensor/",
    "https://docs.vllm.ai/en/latest/models/extensions/runai_model_streamer/",
    "https://docs.vllm.ai/en/latest/models/extensions/tensorizer/",
    "https://docs.vllm.ai/en/latest/models/hardware_supported_models/cpu/",
    "https://docs.vllm.ai/en/latest/models/hardware_supported_models/xpu/",
    "https://docs.vllm.ai/en/latest/features/",
    "https://docs.vllm.ai/en/latest/features/automatic_prefix_caching/",
    "https://docs.vllm.ai/en/latest/features/batch_invariance/",
    "https://docs.vllm.ai/en/latest/features/custom_arguments/",
    "https://docs.vllm.ai/en/latest/features/custom_logitsprocs/",
    "https://docs.vllm.ai/en/latest/features/disagg_encoder/",
    "https://docs.vllm.ai/en/latest/features/disagg_prefill/",
    "https://docs.vllm.ai/en/latest/features/interleaved_thinking/",
    "https://docs.vllm.ai/en/latest/features/lora/",
    "https://docs.vllm.ai/en/latest/features/mooncake_connector_usage/",
    "https://docs.vllm.ai/en/latest/features/multimodal_inputs/",
    "https://docs.vllm.ai/en/latest/features/nixl_connector_usage/",
    "https://docs.vllm.ai/en/latest/features/prompt_embeds/",
    "https://docs.vllm.ai/en/latest/features/reasoning_outputs/",
    "https://docs.vllm.ai/en/latest/features/sleep_mode/",
    "https://docs.vllm.ai/en/latest/features/spec_decode/",
    "https://docs.vllm.ai/en/latest/features/structured_outputs/",
    "https://docs.vllm.ai/en/latest/features/tool_calling/",
    "https://docs.vllm.ai/en/latest/features/quantization/",
    "https://docs.vllm.ai/en/latest/features/quantization/auto_awq/",
    "https://docs.vllm.ai/en/latest/features/quantization/auto_round/",
    "https://docs.vllm.ai/en/latest/features/quantization/bitblas/",
    "https://docs.vllm.ai/en/latest/features/quantization/bnb/",
    "https://docs.vllm.ai/en/latest/features/quantization/fp8/",
    "https://docs.vllm.ai/en/latest/features/quantization/gguf/",
    "https://docs.vllm.ai/en/latest/features/quantization/gptqmodel/",
    "https://docs.vllm.ai/en/latest/features/quantization/inc/",
    "https://docs.vllm.ai/en/latest/features/quantization/int4/",
    "https://docs.vllm.ai/en/latest/features/quantization/int8/",
    "https://docs.vllm.ai/en/latest/features/quantization/modelopt/",
    "https://docs.vllm.ai/en/latest/features/quantization/quantized_kvcache/",
    "https://docs.vllm.ai/en/latest/features/quantization/quark/",
    "https://docs.vllm.ai/en/latest/features/quantization/torchao/",
    "https://docs.vllm.ai/en/latest/contributing/deprecation_policy/",
    "https://docs.vllm.ai/en/latest/contributing/dockerfile/dockerfile/",
    "https://docs.vllm.ai/en/latest/contributing/incremental_build/",
    "https://docs.vllm.ai/en/latest/contributing/profiling/",
    "https://docs.vllm.ai/en/latest/contributing/vulnerability_management/",
    "https://docs.vllm.ai/en/latest/contributing/model/",
    "https://docs.vllm.ai/en/latest/contributing/model/basic/",
    "https://docs.vllm.ai/en/latest/contributing/model/registration/",
    "https://docs.vllm.ai/en/latest/contributing/model/tests/",
    "https://docs.vllm.ai/en/latest/contributing/model/multimodal/",
    "https://docs.vllm.ai/en/latest/contributing/model/transcription/",
    "https://docs.vllm.ai/en/latest/contributing/ci/failures/",
    "https://docs.vllm.ai/en/latest/contributing/ci/nightly_builds/",
    "https://docs.vllm.ai/en/latest/contributing/ci/update_pytorch_version/",
    "https://docs.vllm.ai/en/latest/design/io_processor_plugins/",
    "https://docs.vllm.ai/en/latest/design/lora_resolver_plugins/",
    "https://docs.vllm.ai/en/latest/design/plugin_system/",
    "https://docs.vllm.ai/en/latest/design/arch_overview/",
    "https://docs.vllm.ai/en/latest/design/cuda_graphs/",
    "https://docs.vllm.ai/en/latest/design/dbo/",
    "https://docs.vllm.ai/en/latest/design/debug_vllm_compile/",
    "https://docs.vllm.ai/en/latest/design/fused_moe_modular_kernel/",
    "https://docs.vllm.ai/en/latest/design/huggingface_integration/",
    "https://docs.vllm.ai/en/latest/design/hybrid_kv_cache_manager/",
    "https://docs.vllm.ai/en/latest/design/logits_processors/",
    "https://docs.vllm.ai/en/latest/design/metrics/",
    "https://docs.vllm.ai/en/latest/design/mm_processing/",
    "https://docs.vllm.ai/en/latest/design/moe_kernel_features/",
    "https://docs.vllm.ai/en/latest/design/multiprocessing/",
    "https://docs.vllm.ai/en/latest/design/optimization_levels/",
    "https://docs.vllm.ai/en/latest/design/p2p_nccl_connector/",
    "https://docs.vllm.ai/en/latest/design/paged_attention/",
    "https://docs.vllm.ai/en/latest/design/prefix_caching/",
    "https://docs.vllm.ai/en/latest/design/torch_compile/",
    "https://docs.vllm.ai/en/latest/benchmarking/cli/",
    "https://docs.vllm.ai/en/latest/benchmarking/sweeps/",
    "https://docs.vllm.ai/en/latest/benchmarking/dashboard/",
    "https://docs.vllm.ai/en/latest/api/vllm/",
    "https://docs.vllm.ai/en/latest/api/vllm/beam_search/",
    "https://docs.vllm.ai/en/latest/api/vllm/collect_env/",
    "https://docs.vllm.ai/en/latest/api/vllm/connections/",
    "https://docs.vllm.ai/en/latest/api/vllm/env_override/",
    "https://docs.vllm.ai/en/latest/api/vllm/envs/",
    "https://docs.vllm.ai/en/latest/api/vllm/forward_context/",
    "https://docs.vllm.ai/en/latest/api/vllm/logger/",
    "https://docs.vllm.ai/en/latest/api/vllm/logits_process/",
    "https://docs.vllm.ai/en/latest/api/vllm/logprobs/",
    "https://docs.vllm.ai/en/latest/api/vllm/outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/pooling_params/",
    "https://docs.vllm.ai/en/latest/api/vllm/sampling_params/",
    "https://docs.vllm.ai/en/latest/api/vllm/scalar_type/",
    "https://docs.vllm.ai/en/latest/api/vllm/scripts/",
    "https://docs.vllm.ai/en/latest/api/vllm/sequence/",
    "https://docs.vllm.ai/en/latest/api/vllm/tasks/",
    "https://docs.vllm.ai/en/latest/api/vllm/tracing/",
    "https://docs.vllm.ai/en/latest/api/vllm/version/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/image/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/video/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layer/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/selector/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/chunked_local_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/cross_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/encoder_only_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/mm_encoder_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/chunked_prefill_paged_decode/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/common/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/flashmla/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/merge_attn_states/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/paged_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/pallas_kv_cache_update/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/prefix_prefill/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/rocm_aiter_mla_sparse/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_decode_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_merge_attn_states/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_reshape_and_cache_flash/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_unified_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/vit_attn_wrappers/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/fa_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/kv_sharing_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/kv_transfer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/datasets/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/latency/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/startup/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/throughput/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/endpoint_request_func/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/ready_checker/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/cli/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/param_sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/plot/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/plot_pareto/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/serve_sla/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/server/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/sla_sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/activation_quant_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/base_static_graph/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/caching/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/collective_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/compiler_interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/counter/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/cuda_graph/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/decorators/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fix_functionalization/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fusion_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fx_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/inductor_pass/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/matcher_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/monitor/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/noop_elimination/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/partition_rules/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/pass_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/piecewise_backend/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/post_cleanup/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/qk_norm_rope_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/rocm_aiter_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/sequence_parallelism/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/torch25_custom_graph_pass/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/vllm_inductor_pass/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/compilation/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/device/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/ec_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/kv_events/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/kv_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/load/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/lora/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/model/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/multimodal/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/observability/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/pooler/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/profiler/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/scheduler/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/speculative/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/speech_to_text/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/structured_outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/vllm/",
    "https://docs.vllm.ai/en/latest/api/vllm/device_allocator/",
    "https://docs.vllm.ai/en/latest/api/vllm/device_allocator/cumem/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/communication_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_events/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/parallel_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/tpu_distributed_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/all2all/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/all_reduce_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/base_device_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/cpu_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/cuda_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/cuda_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/custom_all_reduce/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/mnnvl_compat/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/pynccl/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/pynccl_allocator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/pynccl_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/quick_all_reduce/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/ray_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/shm_broadcast/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/shm_object_storage/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/symm_mem/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/tpu_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/xpu_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_transfer_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/example_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/factory/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/async_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/eplb_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/rebalance_execute/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/policy/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/policy/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/policy/default/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_transfer_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/factory/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/decode_bench_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/example_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_mp_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/mooncake_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/multi_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/offloading_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/multi_process_adapter/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/vllm_v1_adapter/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/tensor_memory_pool/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/arg_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/async_llm_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/llm_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/api_server/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/chat_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/constants/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/context/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/launcher/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/llm/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/logger/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/renderer/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/responses_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/score_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/ssl/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/tool/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/tool_server/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/anthropic/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/anthropic/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/anthropic/serving_messages/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/collect_env/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/main/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/openai/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/run_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/types/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/latency/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/main/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/startup/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/throughput/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/api_server/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/cli_args/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/orca_metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/run_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_chat/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_chat_stream_harmony/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_completion/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_models/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_responses/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_transcription/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/speech_to_text/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/parser/harmony_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/parser/responses_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/conftest/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/sagemaker/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/sagemaker/routes/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/cache/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/elastic_ep/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/elastic_ep/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/elastic_ep/middleware/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/health/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/server_info/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/lora/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/lora/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/profile/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/profile/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rlhf/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rlhf/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rpc/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rpc/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/sleep/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/sleep/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/tokenize/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/tokenize/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/tokenize/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/data/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/parse/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/preprocess/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/dump_input/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/formatter/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/lazy/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/log_time/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/lora_model/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/lora_weights/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/model_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/peft_helper/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/request/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/resolver/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/worker_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/base_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/column_parallel_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/logits_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/replicated_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/row_parallel_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/vocal_parallel_embedding/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/ipex_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/ipex_ops/lora_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/torch_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/torch_ops/lora_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/fused_moe_lora_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/kernel_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/lora_expand_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/lora_kernel_metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/lora_shrink_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/xla_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/xla_ops/lora_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_gpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_selector/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_xpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/custom_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/parameter/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/activation/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/attention_layer_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/batch_invariant/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/conv/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/kda/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/layernorm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/lightning_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/logits_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/pooler/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/resampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/vocab_parallel_embedding/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk_delta_h/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk_o/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk_scaled_dot_kkt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/cumsum/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/fused_recurrent/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/index_py/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/kda/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/l2norm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/layernorm_guard/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/op/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/solve_tril/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/wy_fast/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/all2all_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/cpu_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/cutlass_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deep_gemm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deep_gemm_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deepep_ll_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_cutedsl_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_cutlass_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_trtllm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_batched_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_marlin_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_moe_method_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_moe_modular_method/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/gpt_oss_triton_kernels_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/layer/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/modular_kernel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_align_block_size/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_pallas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_permute_unpermute/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_torch_iterative/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/pplx_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/routing_simulator/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/shared_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/topk_weight_and_reduce/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/trtllm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/zero_expert_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/linear_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/mamba_mixer/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/mamba_mixer2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/mamba_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/short_conv/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/causal_conv1d/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/layernorm_gated/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/mamba_ssm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_bmm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_chunk_scan/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_chunk_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_combined/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_state_passing/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/auto_round/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/awq/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/awq_marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/awq_triton/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/base_config/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/bitblas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/bitsandbytes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/cpu_wna16/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/deepspeedfp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/experts_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/fbgemm_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/fp_quant/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gguf/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq_bitblas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq_marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq_marlin_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/hqq_marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/inc/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/input_quant_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/ipex_quant/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kv_cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/modelopt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/moe_wna16/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/mxfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/petit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/ptpc_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/qutlass_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/rtn/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/schema/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/torchao/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/tpu_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/triton_scaled_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_scheme/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a4_nvfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a8_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a8_int/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_nvfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a16_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_wNa16/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/module/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/schemes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/schemes/linear_qutlass_nvfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/allspark/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/bitblas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/conch/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/cutlass/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/dynamic_4bit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/exllama/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/MPLinearKernel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/machete/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/xpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/aiter/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/cutlass/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/ScaledMMLinearKernel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/triton/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/xla/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/quark/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/quark_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_ocp_mx/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_scheme/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/allspark_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/bitblas_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/flashinfer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/fp8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/gptq_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/int8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/layer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/machete_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_fp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_test/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_test_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/mxfp4_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/mxfp6_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/mxfp8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/nvfp4_emulation_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/nvfp4_moe_support/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/ocp_mx_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/petit_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/quant_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/w8a8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/common/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/deepseek_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/dual_chunk_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/dynamic_ntk_alpha_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/dynamic_ntk_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/ernie45_vl_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/linear_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/llama3_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/llama4_vision_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/mrope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/ntk_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/phi3_long_rope_scaled_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/xdrope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/yarn_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/base_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/bitsandbytes_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/default_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/dummy_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/gguf_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/online_quantization/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/runai_streamer_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/sharded_state_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/tensorizer/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/tensorizer_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/weight_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/adapters/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/afmoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/aimv2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/apertus/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/arcee/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/arctic/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/aria/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/audioflamingo3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/aya_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bagel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/baichuan/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bailing_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bee/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bert/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bert_with_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/blip/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/blip2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bloom/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/chameleon/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/chatglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/clip/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/cohere2_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/commandr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/dbrx/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepencoder/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_ocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_v2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_vl2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/dots1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/dots_ocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45_vl_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/exaone/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/exaone4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/fairseq2_llama/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/falcon/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/falcon_h1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/flex_olmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/fuyu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3n/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3n_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4_1v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4_moe_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_bigcode/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_j/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_neox/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_oss/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granite/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granite_speech/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granitemoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granitemoehybrid/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granitemoeshared/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gritlm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/grok1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/h2ovl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/hunyuan_v1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/hunyuan_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/hyperclovax_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/idefics2_vision_model/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/idefics3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interfaces/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interfaces_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/intern_vit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/internlm2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/internlm2_ve/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interns1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interns1_vit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/internvl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jais/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jais2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jina_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/keye/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/keye_vl1_5/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/kimi_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/kimi_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/lfm2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/lfm2_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/lightonocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama4_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama_eagle3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava_next/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava_next_video/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava_onevision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/longcat_flash/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/longcat_flash_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mamba2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/medusa/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/midashenglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mimo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mimo_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mimo_v2_flash/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpm3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpm_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpmv/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minimax_m2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minimax_text_01/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minimax_vl_01/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mistral3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mistral_large_3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mistral_large_3_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mixtral/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mllama4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mlp_speculator/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/modernbert/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/module_mapping/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/molmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/moonvit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mpt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nano_nemotron_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron_h/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron_nas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nvlm_d/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/olmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/olmo2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/olmoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/opencua/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/openpangu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/openpangu_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/opt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/orion/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ouro/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ovis/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ovis2_5/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/paddleocr_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/paligemma/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/persimmon/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi3v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi4mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi4mm_audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi4mm_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phimoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/pixtral/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/plamo2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/plamo3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_5_omni_thinker/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_5_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_rm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_next/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_next_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_omni_moe_thinker/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_vl_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/radio/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/roberta/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/rvl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/seed_oss/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/siglip/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/siglip2navit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/skyworkr1v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/smolvlm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/solar/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/stablelm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/starcoder2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/step3_text/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/step3_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/swin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/tarsier/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/telechat2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/teleflm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/terratorch/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ultravox/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/voxtral/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/voxtral_streaming/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/whisper/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/whisper_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/zamba2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/causal/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/legacy/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/multimodal/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/pooling/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/warmup/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/warmup/deep_gemm_warmup/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/warmup/kernel_warmup/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/evs/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/hasher/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/image/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/inputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/parse/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/processing/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/profiling/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/video/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/cuda/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/rocm/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/xpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/io_processors/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/io_processors/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/lora_resolvers/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/lora_resolvers/filesystem_resolver/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/layerwise_profile/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/ray/",
    "https://docs.vllm.ai/en/latest/api/vllm/ray/lazy_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/ray/ray_env/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/abs_reasoning_parsers/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/basic_parsers/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/deepseek_r1_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/deepseek_v3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/ernie45_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/glm4_moe_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/gptoss_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/granite_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/holo2_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/hunyuan_a13b_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/identity_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/minimax_m2_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/mistral_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/olmo3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/qwen3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/seedoss_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/step3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/deepseek_v32/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/deepseek_v32_encoding/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/detokenizer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/hf/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/mistral/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/abstract_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/deepseekv3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/deepseekv31_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/deepseekv32_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/ernie45_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/functiongemma_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/gigachat3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/glm4_moe_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/glm47_moe_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/granite_20b_fc_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/granite_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/hermes_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/hunyuan_a13b_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/internlm2_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/jamba_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/kimi_k2_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/llama4_pythonic_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/llama_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/longcat_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/minimax_m2_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/minimax_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/mistral_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/olmo3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/openai_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/phi4mini_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/pythonic_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/qwen3coder_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/qwen3xml_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/seed_oss_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/step3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/xlam_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/config_parser_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/dynamic_module/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/gguf_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/repo_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/runai_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/s3_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/tokenizer/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/chat_templates/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/chat_templates/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/afmoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/arctic/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/bagel/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/chatglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/deepseek_vl2/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/dotsocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/falcon/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/flex_olmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/hunyuan_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/jais/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/kimi_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/kimi_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/lfm2_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/medusa/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/midashenglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/mistral/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/mlp_speculator/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/moonvit/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/nemotron/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/nemotron_h/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/olmo3/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/ovis/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/qwen3_next/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/radio/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/step3_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/tarsier2/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/ultravox/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/speculators/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/speculators/algos/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/speculators/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/bagel/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/deepseek_ocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/deepseek_vl2/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/hunyuan_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/hunyuan_vl_image/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/ovis/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/ovis2_5/",
    "https://docs.vllm.ai/en/latest/api/vllm/triton_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/triton_utils/importing/",
    "https://docs.vllm.ai/en/latest/api/vllm/usage/",
    "https://docs.vllm.ai/en/latest/api/vllm/usage/usage_lib/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/argparse_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/async_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/collection_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/counter/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/deep_gemm/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/flashinfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/func_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/gc_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/hashing/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/import_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/jsontree/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/math_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/mem_constants/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/mem_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/nccl/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/network_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/nvtx_pytorch_hooks/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/platform_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/profiling/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/serial_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/system_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/tensor_schema/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/torch_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/cudagraph_dispatcher/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_cache_interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/request/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/serial_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/cpu_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/flash_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/flashinfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/flex_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/gdn_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/linear_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mamba1_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mamba2_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mamba_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/pallas/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/rocm_aiter_fa/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/rocm_aiter_unified_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/rocm_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/short_conv_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/tree_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/triton_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/aiter_triton_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/common/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/cutlass_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashattn_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashinfer_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashmla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashmla_sparse/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/indexer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/rocm_aiter_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/rocm_aiter_mla_sparse/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/triton_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/block_pool/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/encoder_cache_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_coordinator/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/single_type_kv_cache_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/async_scheduler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/output/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/request_queue/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/scheduler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/async_llm/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/coordinator/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/core/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/core_client/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/detokenizer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/exceptions/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/input_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/llm_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/logprobs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/output_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/parallel_sampling/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/multiproc_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/ray_distributed_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/ray_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/ray_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/uniproc_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/arc_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/backend/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/factory/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/lru_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/mediums/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/spec/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/backends/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/worker/cpu_gpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/worker/worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/loggers/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/perf/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/prometheus/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/ray_wrappers/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/reader/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/stats/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/pool/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/pool/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/rejection_sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/builtin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/state/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/bad_words/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/logprobs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/penalties/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/topk_topp_sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/tpu/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/tpu/sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/medusa/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/ngram_proposer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/suffix_decoding/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_guidance/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_lm_format_enforcer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_outlines/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_types/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_xgrammar/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/request/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/block_table/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/cp_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/cpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/cpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/dp_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/ec_connector_model_runner_mixin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_input_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_ubatch_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/kv_connector_model_runner_mixin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/lora_model_runner_mixin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/tpu_input_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/tpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/tpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/ubatch_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/ubatching/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/worker_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/workspace/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/xpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/xpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/async_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/attn_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/block_table/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/cudagraph_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/dp_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/input_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/states/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/structured_outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/metrics/logits/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/gumbel/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/logprob/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/min_p/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/output/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/penalties/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/eagle_cudagraph/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/rejection_sample/",
    "https://docs.vllm.ai/en/latest/cli/serve/",
    "https://docs.vllm.ai/en/latest/cli/chat/",
    "https://docs.vllm.ai/en/latest/cli/complete/",
    "https://docs.vllm.ai/en/latest/cli/run-batch/",
    "https://docs.vllm.ai/en/latest/cli/bench/latency/",
    "https://docs.vllm.ai/en/latest/cli/bench/serve/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/plot/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/plot_pareto/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/serve/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/serve_sla/",
    "https://docs.vllm.ai/en/latest/cli/bench/throughput/",
    "https://docs.vllm.ai/en/latest/community/meetups/",
    "https://docs.vllm.ai/en/latest/community/sponsors/",
    "https://docs.vllm.ai/en/latest/governance/collaboration/",
    "https://docs.vllm.ai/en/latest/governance/committers/",
    "https://docs.vllm.ai/en/latest/governance/process/"
  ]
}