{
  "url": "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/mooncake_connector/",
  "title": "mooncake_connector - vLLM",
  "content": "Bases: KVConnectorBase_V1\n\nGet the finished recving and sending requests.\n\nMooncakeConnector does not save explicitly.\n\nMooncakeConnector does not do layerwise saving.\n\nBases: KVConnectorMetadata\n\nImplementation of Scheduler side methods\n\nFor remote prefill, pull all prompt blocks from remote asynchronously relative to engine execution.\n\nthe number of locally computed tokens for this request\n\nReturns: * the number of tokens that can be loaded from the external KV cache beyond what is already computed. * true if the external KV cache tokens will be loaded asynchronously (between scheduler steps).\n\nOnce a request is finished, determine whether request blocks should be freed now or will be sent asynchronously and freed later.\n\nImplementation of Worker side methods\n\nBackground thread that listens for Mooncake requests, dispatches them to a thread pool, and sends acknowledgments upon completion.\n\nGet requests that are done sending or recving on this specific worker. The scheduler process (via the MultiprocExecutor) will use this output to track which workers are done.\n\nRegister the KV Cache data in mooncake.\n\nCleanup background threads on destruction.\n\nVectorised NumPy implementation.",
  "headings": [
    {
      "level": "h1",
      "text": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector"
    },
    {
      "level": "h2",
      "text": "EngineId module-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.EngineId"
    },
    {
      "level": "h2",
      "text": "ReqId module-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.ReqId"
    },
    {
      "level": "h2",
      "text": "TRANS_DONE module-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.TRANS_DONE"
    },
    {
      "level": "h2",
      "text": "TRANS_ERROR module-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.TRANS_ERROR"
    },
    {
      "level": "h2",
      "text": "logger module-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.logger"
    },
    {
      "level": "h2",
      "text": "FinishedReceiveReqSet dataclass ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.FinishedReceiveReqSet"
    },
    {
      "level": "h3",
      "text": "lock instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.FinishedReceiveReqSet.lock"
    },
    {
      "level": "h3",
      "text": "set instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.FinishedReceiveReqSet.set"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.FinishedReceiveReqSet.__init__"
    },
    {
      "level": "h2",
      "text": "FinishedSendReqSet dataclass ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.FinishedSendReqSet"
    },
    {
      "level": "h3",
      "text": "lock instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.FinishedSendReqSet.lock"
    },
    {
      "level": "h3",
      "text": "set instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.FinishedSendReqSet.set"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.FinishedSendReqSet.__init__"
    },
    {
      "level": "h2",
      "text": "MooncakeAgentMetadata ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeAgentMetadata"
    },
    {
      "level": "h3",
      "text": "block_ids instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeAgentMetadata.block_ids"
    },
    {
      "level": "h3",
      "text": "kv_caches_base_addr instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeAgentMetadata.kv_caches_base_addr"
    },
    {
      "level": "h3",
      "text": "remote_hostname instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeAgentMetadata.remote_hostname"
    },
    {
      "level": "h3",
      "text": "remote_port instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeAgentMetadata.remote_port"
    },
    {
      "level": "h3",
      "text": "request_ids instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeAgentMetadata.request_ids"
    },
    {
      "level": "h2",
      "text": "MooncakeConnector ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnector"
    },
    {
      "level": "h3",
      "text": "connector_scheduler instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnector.connector_scheduler"
    },
    {
      "level": "h3",
      "text": "connector_worker instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnector.connector_worker"
    },
    {
      "level": "h3",
      "text": "engine_id instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnector.engine_id"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnector.__init__"
    },
    {
      "level": "h3",
      "text": "build_connector_meta ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnector.build_connector_meta"
    },
    {
      "level": "h3",
      "text": "get_finished ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnector.get_finished"
    },
    {
      "level": "h3",
      "text": "get_num_new_matched_tokens ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnector.get_num_new_matched_tokens"
    },
    {
      "level": "h3",
      "text": "register_kv_caches ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnector.register_kv_caches"
    },
    {
      "level": "h3",
      "text": "request_finished ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnector.request_finished"
    },
    {
      "level": "h3",
      "text": "save_kv_layer ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnector.save_kv_layer"
    },
    {
      "level": "h3",
      "text": "start_load_kv ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnector.start_load_kv"
    },
    {
      "level": "h3",
      "text": "update_state_after_alloc ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnector.update_state_after_alloc"
    },
    {
      "level": "h3",
      "text": "wait_for_layer_load ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnector.wait_for_layer_load"
    },
    {
      "level": "h3",
      "text": "wait_for_save ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnector.wait_for_save"
    },
    {
      "level": "h2",
      "text": "MooncakeConnectorMetadata ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorMetadata"
    },
    {
      "level": "h3",
      "text": "reqs_to_recv instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorMetadata.reqs_to_recv"
    },
    {
      "level": "h3",
      "text": "reqs_to_send instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorMetadata.reqs_to_send"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorMetadata.__init__"
    },
    {
      "level": "h3",
      "text": "add_new_req ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorMetadata.add_new_req"
    },
    {
      "level": "h2",
      "text": "MooncakeConnectorScheduler ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorScheduler"
    },
    {
      "level": "h3",
      "text": "_reqs_need_recv instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorScheduler._reqs_need_recv"
    },
    {
      "level": "h3",
      "text": "_reqs_need_send instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorScheduler._reqs_need_send"
    },
    {
      "level": "h3",
      "text": "engine_id instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorScheduler.engine_id"
    },
    {
      "level": "h3",
      "text": "kv_role instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorScheduler.kv_role"
    },
    {
      "level": "h3",
      "text": "side_channel_host instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorScheduler.side_channel_host"
    },
    {
      "level": "h3",
      "text": "side_channel_port instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorScheduler.side_channel_port"
    },
    {
      "level": "h3",
      "text": "vllm_config instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorScheduler.vllm_config"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorScheduler.__init__"
    },
    {
      "level": "h3",
      "text": "build_connector_meta ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorScheduler.build_connector_meta"
    },
    {
      "level": "h3",
      "text": "get_num_new_matched_tokens ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorScheduler.get_num_new_matched_tokens"
    },
    {
      "level": "h3",
      "text": "request_finished ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorScheduler.request_finished"
    },
    {
      "level": "h3",
      "text": "update_state_after_alloc ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorScheduler.update_state_after_alloc"
    },
    {
      "level": "h2",
      "text": "MooncakeConnectorWorker ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorWorker"
    },
    {
      "level": "h3",
      "text": "_block_size instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorWorker._block_size"
    },
    {
      "level": "h3",
      "text": "_decoder instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorWorker._decoder"
    },
    {
      "level": "h3",
      "text": "_encoder instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorWorker._encoder"
    },
    {
      "level": "h3",
      "text": "_mooncake_receiver_t instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorWorker._mooncake_receiver_t"
    },
    {
      "level": "h3",
      "text": "_mooncake_sender_t instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorWorker._mooncake_sender_t"
    },
    {
      "level": "h3",
      "text": "_sender_executor instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorWorker._sender_executor"
    },
    {
      "level": "h3",
      "text": "_tp_size instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorWorker._tp_size"
    },
    {
      "level": "h3",
      "text": "_use_pallas instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorWorker._use_pallas"
    },
    {
      "level": "h3",
      "text": "async_zmq_ctx instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorWorker.async_zmq_ctx"
    },
    {
      "level": "h3",
      "text": "backend_name instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorWorker.backend_name"
    },
    {
      "level": "h3",
      "text": "block_size instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorWorker.block_size"
    },
    {
      "level": "h3",
      "text": "cache_config instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorWorker.cache_config"
    },
    {
      "level": "h3",
      "text": "device_kv_caches instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorWorker.device_kv_caches"
    },
    {
      "level": "h3",
      "text": "engine instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorWorker.engine"
    },
    {
      "level": "h3",
      "text": "engine_id instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorWorker.engine_id"
    },
    {
      "level": "h3",
      "text": "finished_recving_reqs instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorWorker.finished_recving_reqs"
    },
    {
      "level": "h3",
      "text": "finished_sending_reqs instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorWorker.finished_sending_reqs"
    },
    {
      "level": "h3",
      "text": "hostname instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorWorker.hostname"
    },
    {
      "level": "h3",
      "text": "kv_cache_layout instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorWorker.kv_cache_layout"
    },
    {
      "level": "h3",
      "text": "kv_caches_base_addr instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorWorker.kv_caches_base_addr"
    },
    {
      "level": "h3",
      "text": "kv_role instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorWorker.kv_role"
    },
    {
      "level": "h3",
      "text": "kv_topo instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorWorker.kv_topo"
    },
    {
      "level": "h3",
      "text": "model_config instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorWorker.model_config"
    },
    {
      "level": "h3",
      "text": "num_blocks instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorWorker.num_blocks"
    },
    {
      "level": "h3",
      "text": "num_workers instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorWorker.num_workers"
    },
    {
      "level": "h3",
      "text": "receiver_loop instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorWorker.receiver_loop"
    },
    {
      "level": "h3",
      "text": "reqs_need_send instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorWorker.reqs_need_send"
    },
    {
      "level": "h3",
      "text": "rpc_port instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorWorker.rpc_port"
    },
    {
      "level": "h3",
      "text": "side_channel_port instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorWorker.side_channel_port"
    },
    {
      "level": "h3",
      "text": "tp_group instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorWorker.tp_group"
    },
    {
      "level": "h3",
      "text": "tp_rank instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorWorker.tp_rank"
    },
    {
      "level": "h3",
      "text": "use_mla instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorWorker.use_mla"
    },
    {
      "level": "h3",
      "text": "vllm_config instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorWorker.vllm_config"
    },
    {
      "level": "h3",
      "text": "world_size instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorWorker.world_size"
    },
    {
      "level": "h3",
      "text": "zmq_ctx instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorWorker.zmq_ctx"
    },
    {
      "level": "h3",
      "text": "__del__ ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorWorker.__del__"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorWorker.__init__"
    },
    {
      "level": "h3",
      "text": "_mooncake_sender ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorWorker._mooncake_sender"
    },
    {
      "level": "h3",
      "text": "_receiver_loop ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorWorker._receiver_loop"
    },
    {
      "level": "h3",
      "text": "_send_blocks ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorWorker._send_blocks"
    },
    {
      "level": "h3",
      "text": "_sender_worker ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorWorker._sender_worker"
    },
    {
      "level": "h3",
      "text": "fetch_finished_recving_reqs async ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorWorker.fetch_finished_recving_reqs"
    },
    {
      "level": "h3",
      "text": "get_finished ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorWorker.get_finished"
    },
    {
      "level": "h3",
      "text": "group_kv_pull ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorWorker.group_kv_pull"
    },
    {
      "level": "h3",
      "text": "receive_kv async ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorWorker.receive_kv"
    },
    {
      "level": "h3",
      "text": "register_kv_caches ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorWorker.register_kv_caches"
    },
    {
      "level": "h3",
      "text": "send_kv_to_decode ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorWorker.send_kv_to_decode"
    },
    {
      "level": "h3",
      "text": "shutdown ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorWorker.shutdown"
    },
    {
      "level": "h3",
      "text": "start_load_kv ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.MooncakeConnectorWorker.start_load_kv"
    },
    {
      "level": "h2",
      "text": "RecvReqMeta dataclass ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.RecvReqMeta"
    },
    {
      "level": "h3",
      "text": "local_block_ids instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.RecvReqMeta.local_block_ids"
    },
    {
      "level": "h3",
      "text": "remote_host instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.RecvReqMeta.remote_host"
    },
    {
      "level": "h3",
      "text": "remote_port instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.RecvReqMeta.remote_port"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.RecvReqMeta.__init__"
    },
    {
      "level": "h2",
      "text": "SendBlockMeta dataclass ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.SendBlockMeta"
    },
    {
      "level": "h3",
      "text": "expire_time class-attribute instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.SendBlockMeta.expire_time"
    },
    {
      "level": "h3",
      "text": "local_block_ids instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.SendBlockMeta.local_block_ids"
    },
    {
      "level": "h3",
      "text": "ready instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.SendBlockMeta.ready"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.SendBlockMeta.__init__"
    },
    {
      "level": "h2",
      "text": "SendReqMeta dataclass ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.SendReqMeta"
    },
    {
      "level": "h3",
      "text": "lock instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.SendReqMeta.lock"
    },
    {
      "level": "h3",
      "text": "reqs instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.SendReqMeta.reqs"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.SendReqMeta.__init__"
    },
    {
      "level": "h2",
      "text": "get_mooncake_side_channel_port ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.get_mooncake_side_channel_port"
    },
    {
      "level": "h2",
      "text": "group_concurrent_contiguous ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.mooncake_connector.group_concurrent_contiguous"
    }
  ],
  "code_samples": [
    {
      "code": "EngineId = str",
      "language": "unknown"
    },
    {
      "code": "EngineId = str",
      "language": "unknown"
    },
    {
      "code": "ReqId = str",
      "language": "unknown"
    },
    {
      "code": "ReqId = str",
      "language": "unknown"
    },
    {
      "code": "TRANS_DONE = b'trans_done'",
      "language": "unknown"
    },
    {
      "code": "TRANS_DONE = b'trans_done'",
      "language": "unknown"
    },
    {
      "code": "TRANS_ERROR = b'trans_error'",
      "language": "unknown"
    },
    {
      "code": "TRANS_ERROR = b'trans_error'",
      "language": "unknown"
    },
    {
      "code": "logger = init_logger(__name__)",
      "language": "unknown"
    },
    {
      "code": "logger = init_logger(__name__)",
      "language": "unknown"
    },
    {
      "code": "102\n103\n104\n105",
      "language": "unknown"
    },
    {
      "code": "@dataclass\nclass FinishedReceiveReqSet:\n    set: set[ReqId]\n    lock: asyncio.Lock",
      "language": "python"
    },
    {
      "code": "@dataclass\nclass FinishedReceiveReqSet:\n    set: set[ReqId]\n    lock: asyncio.Lock",
      "language": "python"
    },
    {
      "code": "set: set[ReqId]",
      "language": "yaml"
    },
    {
      "code": "set: set[ReqId]",
      "language": "yaml"
    },
    {
      "code": "__init__(set: set[ReqId], lock: Lock) -> None",
      "language": "python"
    },
    {
      "code": "__init__(set: set[ReqId], lock: Lock) -> None",
      "language": "python"
    },
    {
      "code": "96\n97\n98\n99",
      "language": "unknown"
    },
    {
      "code": "@dataclass\nclass FinishedSendReqSet:\n    set: set[ReqId]\n    lock: threading.Lock",
      "language": "python"
    },
    {
      "code": "@dataclass\nclass FinishedSendReqSet:\n    set: set[ReqId]\n    lock: threading.Lock",
      "language": "python"
    },
    {
      "code": "set: set[ReqId]",
      "language": "yaml"
    },
    {
      "code": "set: set[ReqId]",
      "language": "yaml"
    },
    {
      "code": "__init__(set: set[ReqId], lock: Lock) -> None",
      "language": "python"
    },
    {
      "code": "__init__(set: set[ReqId], lock: Lock) -> None",
      "language": "python"
    },
    {
      "code": "63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73",
      "language": "unknown"
    },
    {
      "code": "class MooncakeAgentMetadata(\n    msgspec.Struct,\n    omit_defaults=True,  # type: ignore[call-arg]\n    # required for @cached_property.\n    dict=True,\n):\n    remote_hostname: str\n    remote_port: int\n    request_ids: list[ReqId]\n    kv_caches_base_addr: list[int]\n    block_ids: list[list[int]]",
      "language": "php"
    },
    {
      "code": "class MooncakeAgentMetadata(\n    msgspec.Struct,\n    omit_defaults=True,  # type: ignore[call-arg]\n    # required for @cached_property.\n    dict=True,\n):\n    remote_hostname: str\n    remote_port: int\n    request_ids: list[ReqId]\n    kv_caches_base_addr: list[int]\n    block_ids: list[list[int]]",
      "language": "php"
    },
    {
      "code": "block_ids: list[list[int]]",
      "language": "yaml"
    },
    {
      "code": "block_ids: list[list[int]]",
      "language": "yaml"
    },
    {
      "code": "kv_caches_base_addr: list[int]",
      "language": "yaml"
    },
    {
      "code": "kv_caches_base_addr: list[int]",
      "language": "yaml"
    },
    {
      "code": "remote_hostname: str",
      "language": "yaml"
    },
    {
      "code": "remote_hostname: str",
      "language": "yaml"
    },
    {
      "code": "remote_port: int",
      "language": "yaml"
    },
    {
      "code": "remote_port: int",
      "language": "yaml"
    },
    {
      "code": "request_ids: list[ReqId]",
      "language": "yaml"
    },
    {
      "code": "request_ids: list[ReqId]",
      "language": "yaml"
    },
    {
      "code": "130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221",
      "language": "unknown"
    },
    {
      "code": "class MooncakeConnector(KVConnectorBase_V1):\n    def __init__(\n        self,\n        vllm_config: VllmConfig,\n        role: KVConnectorRole,\n        kv_cache_config: Optional[\"KVCacheConfig\"] = None,\n    ):\n        super().__init__(vllm_config, role, kv_cache_config)\n\n        assert vllm_config.kv_transfer_config is not None\n        assert vllm_config.kv_transfer_config.engine_id is not None\n        self.engine_id: EngineId = vllm_config.kv_transfer_config.engine_id\n\n        if role == KVConnectorRole.SCHEDULER:\n            self.connector_scheduler: MooncakeConnectorScheduler | None = (\n                MooncakeConnectorScheduler(vllm_config, self.engine_id)\n            )\n            self.connector_worker: MooncakeConnectorWorker | None = None\n        elif role == KVConnectorRole.WORKER:\n            self.connector_scheduler = None\n            self.connector_worker = MooncakeConnectorWorker(vllm_config, self.engine_id)\n\n    ############################################################\n    # Scheduler Side Methods\n    ############################################################\n\n    def get_num_new_matched_tokens(\n        self, request: \"Request\", num_computed_tokens: int\n    ) -> tuple[int, bool]:\n        assert self.connector_scheduler is not None\n        return self.connector_scheduler.get_num_new_matched_tokens(\n            request, num_computed_tokens\n        )\n\n    def update_state_after_alloc(\n        self, request: \"Request\", blocks: \"KVCacheBlocks\", num_external_tokens: int\n    ):\n        assert self.connector_scheduler is not None\n        return self.connector_scheduler.update_state_after_alloc(\n            request, blocks, num_external_tokens\n        )\n\n    def build_connector_meta(\n        self,\n        scheduler_output: SchedulerOutput,\n    ) -> KVConnectorMetadata:\n        assert self.connector_scheduler is not None\n        return self.connector_scheduler.build_connector_meta(scheduler_output)\n\n    def request_finished(\n        self,\n        request: \"Request\",\n        block_ids: list[int],\n    ) -> tuple[bool, dict[str, Any] | None]:\n        assert self.connector_scheduler is not None\n        return self.connector_scheduler.request_finished(request, block_ids)\n\n    ############################################################\n    # Worker Side Methods\n    ############################################################\n    def register_kv_caches(self, kv_caches: dict[str, torch.Tensor]):\n        assert self.connector_worker is not None\n        self.connector_worker.register_kv_caches(kv_caches)\n\n    def get_finished(\n        self, finished_req_ids: set[str]\n    ) -> tuple[set[str] | None, set[str] | None]:\n        \"\"\"Get the finished recving and sending requests.\"\"\"\n        assert self.connector_worker is not None\n        return self.connector_worker.get_finished()\n\n    def start_load_kv(self, forward_context: \"ForwardContext\", **kwargs) -> None:\n        assert self.connector_worker is not None\n        assert isinstance(self._connector_metadata, MooncakeConnectorMetadata)\n        self.connector_worker.start_load_kv(self._connector_metadata)\n\n    def wait_for_layer_load(self, layer_name: str) -> None:\n        \"\"\"MooncakeConnector does not do layerwise saving.\"\"\"\n        pass\n\n    def save_kv_layer(\n        self,\n        layer_name: str,\n        kv_layer: torch.Tensor,\n        attn_metadata: AttentionMetadata,\n        **kwargs,\n    ) -> None:\n        \"\"\"MooncakeConnector does not save explicitly.\"\"\"\n        pass\n\n    def wait_for_save(self):\n        pass",
      "language": "python"
    },
    {
      "code": "class MooncakeConnector(KVConnectorBase_V1):\n    def __init__(\n        self,\n        vllm_config: VllmConfig,\n        role: KVConnectorRole,\n        kv_cache_config: Optional[\"KVCacheConfig\"] = None,\n    ):\n        super().__init__(vllm_config, role, kv_cache_config)\n\n        assert vllm_config.kv_transfer_config is not None\n        assert vllm_config.kv_transfer_config.engine_id is not None\n        self.engine_id: EngineId = vllm_config.kv_transfer_config.engine_id\n\n        if role == KVConnectorRole.SCHEDULER:\n            self.connector_scheduler: MooncakeConnectorScheduler | None = (\n                MooncakeConnectorScheduler(vllm_config, self.engine_id)\n            )\n            self.connector_worker: MooncakeConnectorWorker | None = None\n        elif role == KVConnectorRole.WORKER:\n            self.connector_scheduler = None\n            self.connector_worker = MooncakeConnectorWorker(vllm_config, self.engine_id)\n\n    ############################################################\n    # Scheduler Side Methods\n    ############################################################\n\n    def get_num_new_matched_tokens(\n        self, request: \"Request\", num_computed_tokens: int\n    ) -> tuple[int, bool]:\n        assert self.connector_scheduler is not None\n        return self.connector_scheduler.get_num_new_matched_tokens(\n            request, num_computed_tokens\n        )\n\n    def update_state_after_alloc(\n        self, request: \"Request\", blocks: \"KVCacheBlocks\", num_external_tokens: int\n    ):\n        assert self.connector_scheduler is not None\n        return self.connector_scheduler.update_state_after_alloc(\n            request, blocks, num_external_tokens\n        )\n\n    def build_connector_meta(\n        self,\n        scheduler_output: SchedulerOutput,\n    ) -> KVConnectorMetadata:\n        assert self.connector_scheduler is not None\n        return self.connector_scheduler.build_connector_meta(scheduler_output)\n\n    def request_finished(\n        self,\n        request: \"Request\",\n        block_ids: list[int],\n    ) -> tuple[bool, dict[str, Any] | None]:\n        assert self.connector_scheduler is not None\n        return self.connector_scheduler.request_finished(request, block_ids)\n\n    ############################################################\n    # Worker Side Methods\n    ############################################################\n    def register_kv_caches(self, kv_caches: dict[str, torch.Tensor]):\n        assert self.connector_worker is not None\n        self.connector_worker.register_kv_caches(kv_caches)\n\n    def get_finished(\n        self, finished_req_ids: set[str]\n    ) -> tuple[set[str] | None, set[str] | None]:\n        \"\"\"Get the finished recving and sending requests.\"\"\"\n        assert self.connector_worker is not None\n        return self.connector_worker.get_finished()\n\n    def start_load_kv(self, forward_context: \"ForwardContext\", **kwargs) -> None:\n        assert self.connector_worker is not None\n        assert isinstance(self._connector_metadata, MooncakeConnectorMetadata)\n        self.connector_worker.start_load_kv(self._connector_metadata)\n\n    def wait_for_layer_load(self, layer_name: str) -> None:\n        \"\"\"MooncakeConnector does not do layerwise saving.\"\"\"\n        pass\n\n    def save_kv_layer(\n        self,\n        layer_name: str,\n        kv_layer: torch.Tensor,\n        attn_metadata: AttentionMetadata,\n        **kwargs,\n    ) -> None:\n        \"\"\"MooncakeConnector does not save explicitly.\"\"\"\n        pass\n\n    def wait_for_save(self):\n        pass",
      "language": "python"
    },
    {
      "code": "connector_scheduler: MooncakeConnectorScheduler | None = (\n    MooncakeConnectorScheduler(vllm_config, engine_id)\n)",
      "language": "yaml"
    },
    {
      "code": "connector_scheduler: MooncakeConnectorScheduler | None = (\n    MooncakeConnectorScheduler(vllm_config, engine_id)\n)",
      "language": "yaml"
    },
    {
      "code": "connector_worker: MooncakeConnectorWorker | None = None",
      "language": "yaml"
    },
    {
      "code": "connector_worker: MooncakeConnectorWorker | None = None",
      "language": "yaml"
    },
    {
      "code": "engine_id: EngineId = engine_id",
      "language": "typescript"
    },
    {
      "code": "engine_id: EngineId = engine_id",
      "language": "typescript"
    },
    {
      "code": "__init__(\n    vllm_config: VllmConfig,\n    role: KVConnectorRole,\n    kv_cache_config: Optional[KVCacheConfig] = None,\n)",
      "language": "python"
    },
    {
      "code": "__init__(\n    vllm_config: VllmConfig,\n    role: KVConnectorRole,\n    kv_cache_config: Optional[KVCacheConfig] = None,\n)",
      "language": "python"
    },
    {
      "code": "131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150",
      "language": "unknown"
    },
    {
      "code": "def __init__(\n    self,\n    vllm_config: VllmConfig,\n    role: KVConnectorRole,\n    kv_cache_config: Optional[\"KVCacheConfig\"] = None,\n):\n    super().__init__(vllm_config, role, kv_cache_config)\n\n    assert vllm_config.kv_transfer_config is not None\n    assert vllm_config.kv_transfer_config.engine_id is not None\n    self.engine_id: EngineId = vllm_config.kv_transfer_config.engine_id\n\n    if role == KVConnectorRole.SCHEDULER:\n        self.connector_scheduler: MooncakeConnectorScheduler | None = (\n            MooncakeConnectorScheduler(vllm_config, self.engine_id)\n        )\n        self.connector_worker: MooncakeConnectorWorker | None = None\n    elif role == KVConnectorRole.WORKER:\n        self.connector_scheduler = None\n        self.connector_worker = MooncakeConnectorWorker(vllm_config, self.engine_id)",
      "language": "python"
    },
    {
      "code": "def __init__(\n    self,\n    vllm_config: VllmConfig,\n    role: KVConnectorRole,\n    kv_cache_config: Optional[\"KVCacheConfig\"] = None,\n):\n    super().__init__(vllm_config, role, kv_cache_config)\n\n    assert vllm_config.kv_transfer_config is not None\n    assert vllm_config.kv_transfer_config.engine_id is not None\n    self.engine_id: EngineId = vllm_config.kv_transfer_config.engine_id\n\n    if role == KVConnectorRole.SCHEDULER:\n        self.connector_scheduler: MooncakeConnectorScheduler | None = (\n            MooncakeConnectorScheduler(vllm_config, self.engine_id)\n        )\n        self.connector_worker: MooncakeConnectorWorker | None = None\n    elif role == KVConnectorRole.WORKER:\n        self.connector_scheduler = None\n        self.connector_worker = MooncakeConnectorWorker(vllm_config, self.engine_id)",
      "language": "python"
    },
    {
      "code": "build_connector_meta(\n    scheduler_output: SchedulerOutput,\n) -> KVConnectorMetadata",
      "language": "php"
    },
    {
      "code": "build_connector_meta(\n    scheduler_output: SchedulerOutput,\n) -> KVConnectorMetadata",
      "language": "php"
    },
    {
      "code": "172\n173\n174\n175\n176\n177",
      "language": "unknown"
    },
    {
      "code": "def build_connector_meta(\n    self,\n    scheduler_output: SchedulerOutput,\n) -> KVConnectorMetadata:\n    assert self.connector_scheduler is not None\n    return self.connector_scheduler.build_connector_meta(scheduler_output)",
      "language": "python"
    },
    {
      "code": "def build_connector_meta(\n    self,\n    scheduler_output: SchedulerOutput,\n) -> KVConnectorMetadata:\n    assert self.connector_scheduler is not None\n    return self.connector_scheduler.build_connector_meta(scheduler_output)",
      "language": "python"
    },
    {
      "code": "get_finished(\n    finished_req_ids: set[str],\n) -> tuple[set[str] | None, set[str] | None]",
      "language": "rust"
    },
    {
      "code": "get_finished(\n    finished_req_ids: set[str],\n) -> tuple[set[str] | None, set[str] | None]",
      "language": "rust"
    },
    {
      "code": "194\n195\n196\n197\n198\n199",
      "language": "unknown"
    },
    {
      "code": "def get_finished(\n    self, finished_req_ids: set[str]\n) -> tuple[set[str] | None, set[str] | None]:\n    \"\"\"Get the finished recving and sending requests.\"\"\"\n    assert self.connector_worker is not None\n    return self.connector_worker.get_finished()",
      "language": "python"
    },
    {
      "code": "def get_finished(\n    self, finished_req_ids: set[str]\n) -> tuple[set[str] | None, set[str] | None]:\n    \"\"\"Get the finished recving and sending requests.\"\"\"\n    assert self.connector_worker is not None\n    return self.connector_worker.get_finished()",
      "language": "python"
    },
    {
      "code": "get_num_new_matched_tokens(\n    request: Request, num_computed_tokens: int\n) -> tuple[int, bool]",
      "language": "php"
    },
    {
      "code": "get_num_new_matched_tokens(\n    request: Request, num_computed_tokens: int\n) -> tuple[int, bool]",
      "language": "php"
    },
    {
      "code": "156\n157\n158\n159\n160\n161\n162",
      "language": "unknown"
    },
    {
      "code": "def get_num_new_matched_tokens(\n    self, request: \"Request\", num_computed_tokens: int\n) -> tuple[int, bool]:\n    assert self.connector_scheduler is not None\n    return self.connector_scheduler.get_num_new_matched_tokens(\n        request, num_computed_tokens\n    )",
      "language": "python"
    },
    {
      "code": "def get_num_new_matched_tokens(\n    self, request: \"Request\", num_computed_tokens: int\n) -> tuple[int, bool]:\n    assert self.connector_scheduler is not None\n    return self.connector_scheduler.get_num_new_matched_tokens(\n        request, num_computed_tokens\n    )",
      "language": "python"
    },
    {
      "code": "register_kv_caches(kv_caches: dict[str, Tensor])",
      "language": "unknown"
    },
    {
      "code": "register_kv_caches(kv_caches: dict[str, Tensor])",
      "language": "unknown"
    },
    {
      "code": "190\n191\n192",
      "language": "unknown"
    },
    {
      "code": "def register_kv_caches(self, kv_caches: dict[str, torch.Tensor]):\n    assert self.connector_worker is not None\n    self.connector_worker.register_kv_caches(kv_caches)",
      "language": "python"
    },
    {
      "code": "def register_kv_caches(self, kv_caches: dict[str, torch.Tensor]):\n    assert self.connector_worker is not None\n    self.connector_worker.register_kv_caches(kv_caches)",
      "language": "python"
    },
    {
      "code": "request_finished(\n    request: Request, block_ids: list[int]\n) -> tuple[bool, dict[str, Any] | None]",
      "language": "rust"
    },
    {
      "code": "request_finished(\n    request: Request, block_ids: list[int]\n) -> tuple[bool, dict[str, Any] | None]",
      "language": "rust"
    },
    {
      "code": "179\n180\n181\n182\n183\n184\n185",
      "language": "unknown"
    },
    {
      "code": "def request_finished(\n    self,\n    request: \"Request\",\n    block_ids: list[int],\n) -> tuple[bool, dict[str, Any] | None]:\n    assert self.connector_scheduler is not None\n    return self.connector_scheduler.request_finished(request, block_ids)",
      "language": "python"
    },
    {
      "code": "def request_finished(\n    self,\n    request: \"Request\",\n    block_ids: list[int],\n) -> tuple[bool, dict[str, Any] | None]:\n    assert self.connector_scheduler is not None\n    return self.connector_scheduler.request_finished(request, block_ids)",
      "language": "python"
    },
    {
      "code": "save_kv_layer(\n    layer_name: str,\n    kv_layer: Tensor,\n    attn_metadata: AttentionMetadata,\n    **kwargs,\n) -> None",
      "language": "rust"
    },
    {
      "code": "save_kv_layer(\n    layer_name: str,\n    kv_layer: Tensor,\n    attn_metadata: AttentionMetadata,\n    **kwargs,\n) -> None",
      "language": "rust"
    },
    {
      "code": "210\n211\n212\n213\n214\n215\n216\n217\n218",
      "language": "unknown"
    },
    {
      "code": "def save_kv_layer(\n    self,\n    layer_name: str,\n    kv_layer: torch.Tensor,\n    attn_metadata: AttentionMetadata,\n    **kwargs,\n) -> None:\n    \"\"\"MooncakeConnector does not save explicitly.\"\"\"\n    pass",
      "language": "python"
    },
    {
      "code": "def save_kv_layer(\n    self,\n    layer_name: str,\n    kv_layer: torch.Tensor,\n    attn_metadata: AttentionMetadata,\n    **kwargs,\n) -> None:\n    \"\"\"MooncakeConnector does not save explicitly.\"\"\"\n    pass",
      "language": "python"
    },
    {
      "code": "start_load_kv(\n    forward_context: ForwardContext, **kwargs\n) -> None",
      "language": "rust"
    },
    {
      "code": "start_load_kv(\n    forward_context: ForwardContext, **kwargs\n) -> None",
      "language": "rust"
    },
    {
      "code": "201\n202\n203\n204",
      "language": "unknown"
    },
    {
      "code": "def start_load_kv(self, forward_context: \"ForwardContext\", **kwargs) -> None:\n    assert self.connector_worker is not None\n    assert isinstance(self._connector_metadata, MooncakeConnectorMetadata)\n    self.connector_worker.start_load_kv(self._connector_metadata)",
      "language": "python"
    },
    {
      "code": "def start_load_kv(self, forward_context: \"ForwardContext\", **kwargs) -> None:\n    assert self.connector_worker is not None\n    assert isinstance(self._connector_metadata, MooncakeConnectorMetadata)\n    self.connector_worker.start_load_kv(self._connector_metadata)",
      "language": "python"
    },
    {
      "code": "update_state_after_alloc(\n    request: Request,\n    blocks: KVCacheBlocks,\n    num_external_tokens: int,\n)",
      "language": "yaml"
    },
    {
      "code": "update_state_after_alloc(\n    request: Request,\n    blocks: KVCacheBlocks,\n    num_external_tokens: int,\n)",
      "language": "yaml"
    },
    {
      "code": "164\n165\n166\n167\n168\n169\n170",
      "language": "unknown"
    },
    {
      "code": "def update_state_after_alloc(\n    self, request: \"Request\", blocks: \"KVCacheBlocks\", num_external_tokens: int\n):\n    assert self.connector_scheduler is not None\n    return self.connector_scheduler.update_state_after_alloc(\n        request, blocks, num_external_tokens\n    )",
      "language": "python"
    },
    {
      "code": "def update_state_after_alloc(\n    self, request: \"Request\", blocks: \"KVCacheBlocks\", num_external_tokens: int\n):\n    assert self.connector_scheduler is not None\n    return self.connector_scheduler.update_state_after_alloc(\n        request, blocks, num_external_tokens\n    )",
      "language": "python"
    },
    {
      "code": "wait_for_layer_load(layer_name: str) -> None",
      "language": "rust"
    },
    {
      "code": "wait_for_layer_load(layer_name: str) -> None",
      "language": "rust"
    },
    {
      "code": "206\n207\n208",
      "language": "unknown"
    },
    {
      "code": "def wait_for_layer_load(self, layer_name: str) -> None:\n    \"\"\"MooncakeConnector does not do layerwise saving.\"\"\"\n    pass",
      "language": "python"
    },
    {
      "code": "def wait_for_layer_load(self, layer_name: str) -> None:\n    \"\"\"MooncakeConnector does not do layerwise saving.\"\"\"\n    pass",
      "language": "python"
    },
    {
      "code": "wait_for_save()",
      "language": "unknown"
    },
    {
      "code": "wait_for_save()",
      "language": "unknown"
    },
    {
      "code": "def wait_for_save(self):\n    pass",
      "language": "python"
    },
    {
      "code": "def wait_for_save(self):\n    pass",
      "language": "python"
    },
    {
      "code": "108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127",
      "language": "unknown"
    },
    {
      "code": "class MooncakeConnectorMetadata(KVConnectorMetadata):\n    def __init__(self):\n        self.reqs_to_recv: dict[ReqId, RecvReqMeta] = {}\n        self.reqs_to_send: dict[ReqId, list[int]] = {}\n\n    def add_new_req(\n        self,\n        request_id: ReqId,\n        local_block_ids: list[int],\n        kv_transfer_params: dict[str, Any],\n        load_remote_cache: bool = True,\n    ):\n        if load_remote_cache:\n            self.reqs_to_recv[request_id] = RecvReqMeta(\n                local_block_ids=local_block_ids,\n                remote_host=kv_transfer_params[\"remote_host\"],\n                remote_port=kv_transfer_params[\"remote_port\"],\n            )\n        else:\n            self.reqs_to_send[request_id] = local_block_ids",
      "language": "python"
    },
    {
      "code": "class MooncakeConnectorMetadata(KVConnectorMetadata):\n    def __init__(self):\n        self.reqs_to_recv: dict[ReqId, RecvReqMeta] = {}\n        self.reqs_to_send: dict[ReqId, list[int]] = {}\n\n    def add_new_req(\n        self,\n        request_id: ReqId,\n        local_block_ids: list[int],\n        kv_transfer_params: dict[str, Any],\n        load_remote_cache: bool = True,\n    ):\n        if load_remote_cache:\n            self.reqs_to_recv[request_id] = RecvReqMeta(\n                local_block_ids=local_block_ids,\n                remote_host=kv_transfer_params[\"remote_host\"],\n                remote_port=kv_transfer_params[\"remote_port\"],\n            )\n        else:\n            self.reqs_to_send[request_id] = local_block_ids",
      "language": "python"
    },
    {
      "code": "reqs_to_recv: dict[ReqId, RecvReqMeta] = {}",
      "language": "yaml"
    },
    {
      "code": "reqs_to_recv: dict[ReqId, RecvReqMeta] = {}",
      "language": "yaml"
    },
    {
      "code": "reqs_to_send: dict[ReqId, list[int]] = {}",
      "language": "yaml"
    },
    {
      "code": "reqs_to_send: dict[ReqId, list[int]] = {}",
      "language": "yaml"
    },
    {
      "code": "109\n110\n111",
      "language": "unknown"
    },
    {
      "code": "def __init__(self):\n    self.reqs_to_recv: dict[ReqId, RecvReqMeta] = {}\n    self.reqs_to_send: dict[ReqId, list[int]] = {}",
      "language": "python"
    },
    {
      "code": "def __init__(self):\n    self.reqs_to_recv: dict[ReqId, RecvReqMeta] = {}\n    self.reqs_to_send: dict[ReqId, list[int]] = {}",
      "language": "python"
    },
    {
      "code": "add_new_req(\n    request_id: ReqId,\n    local_block_ids: list[int],\n    kv_transfer_params: dict[str, Any],\n    load_remote_cache: bool = True,\n)",
      "language": "typescript"
    },
    {
      "code": "add_new_req(\n    request_id: ReqId,\n    local_block_ids: list[int],\n    kv_transfer_params: dict[str, Any],\n    load_remote_cache: bool = True,\n)",
      "language": "typescript"
    },
    {
      "code": "113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127",
      "language": "unknown"
    },
    {
      "code": "def add_new_req(\n    self,\n    request_id: ReqId,\n    local_block_ids: list[int],\n    kv_transfer_params: dict[str, Any],\n    load_remote_cache: bool = True,\n):\n    if load_remote_cache:\n        self.reqs_to_recv[request_id] = RecvReqMeta(\n            local_block_ids=local_block_ids,\n            remote_host=kv_transfer_params[\"remote_host\"],\n            remote_port=kv_transfer_params[\"remote_port\"],\n        )\n    else:\n        self.reqs_to_send[request_id] = local_block_ids",
      "language": "python"
    },
    {
      "code": "def add_new_req(\n    self,\n    request_id: ReqId,\n    local_block_ids: list[int],\n    kv_transfer_params: dict[str, Any],\n    load_remote_cache: bool = True,\n):\n    if load_remote_cache:\n        self.reqs_to_recv[request_id] = RecvReqMeta(\n            local_block_ids=local_block_ids,\n            remote_host=kv_transfer_params[\"remote_host\"],\n            remote_port=kv_transfer_params[\"remote_port\"],\n        )\n    else:\n        self.reqs_to_send[request_id] = local_block_ids",
      "language": "python"
    },
    {
      "code": "224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398",
      "language": "unknown"
    },
    {
      "code": "class MooncakeConnectorScheduler:\n    \"\"\"Implementation of Scheduler side methods\"\"\"\n\n    def __init__(self, vllm_config: VllmConfig, engine_id: str):\n        self.vllm_config = vllm_config\n        self.engine_id: EngineId = engine_id\n        self.side_channel_host = get_ip()\n        self.side_channel_port = get_mooncake_side_channel_port(vllm_config)\n\n        assert vllm_config.kv_transfer_config\n        self.kv_role = vllm_config.kv_transfer_config.kv_role\n        logger.info(\"Initializing Mooncake Transfer Engine Scheduler %s\", engine_id)\n\n        # Requests that need to start recv/send.\n        # New requests are added by update_state_after_alloc in\n        # the scheduler. Used to make metadata passed to Worker.\n        self._reqs_need_recv: dict[ReqId, tuple[Request, list[int]]] = {}\n        self._reqs_need_send: dict[ReqId, list[int]] = {}\n\n    def get_num_new_matched_tokens(\n        self, request: \"Request\", num_computed_tokens: int\n    ) -> tuple[int, bool]:\n        \"\"\"\n        For remote prefill, pull all prompt blocks from remote\n        asynchronously relative to engine execution.\n\n        Args:\n            request (Request): the request object.\n            num_computed_tokens (int): the number of locally\n                computed tokens for this request\n        Returns:\n            * the number of tokens that can be loaded from the\n              external KV cache beyond what is already computed.\n            * true if the external KV cache tokens will be loaded\n              asynchronously (between scheduler steps).\n        \"\"\"\n\n        params = request.kv_transfer_params\n        logger.debug(\n            \"MooncakeConnector get_num_new_matched_tokens: \"\n            \"num_computed_tokens=%s, kv_transfer_params=%s\",\n            num_computed_tokens,\n            params,\n        )\n\n        if params is not None and params.get(\"do_remote_prefill\"):\n            # Remote prefill: get all prompt blocks from remote.\n            token_ids = request.prompt_token_ids or []\n            count = len(token_ids) - num_computed_tokens\n            if count > 0:\n                return count, True\n\n        # No remote prefill for this request.\n        return 0, False\n\n    def update_state_after_alloc(\n        self, request: \"Request\", blocks: \"KVCacheBlocks\", num_external_tokens: int\n    ):\n        params = request.kv_transfer_params\n        logger.debug(\n            \"MooncakeConnector update_state_after_alloc: \"\n            \"num_external_tokens=%s, kv_transfer_params=%s\",\n            num_external_tokens,\n            params,\n        )\n\n        if not params:\n            return\n\n        if params.get(\"do_remote_prefill\"):\n            assert self.kv_role != \"kv_producer\"\n            if all(p in params for p in (\"remote_host\", \"remote_port\")):\n                # If remote_blocks and num_external_tokens = 0, we have\n                # a full prefix cache hit on the D worker. We need to call\n                # send_notif in _read_blocks to free the memory on the P.\n                local_block_ids = (\n                    blocks.get_unhashed_block_ids() if num_external_tokens > 0 else []\n                )\n                # Get unhashed blocks to pull from remote.\n                self._reqs_need_recv[request.request_id] = (request, local_block_ids)\n            else:\n                logger.warning(\n                    \"Got invalid KVTransferParams: %s. This \"\n                    \"request will not utilize KVTransfer\",\n                    params,\n                )\n            # Only trigger 1 KV transfer per request.\n            params[\"do_remote_prefill\"] = False\n\n        elif params.get(\"do_remote_decode\"):\n            # Add an empty list to worker to create event.\n            self._reqs_need_send[request.request_id] = []\n\n    def build_connector_meta(\n        self,\n        scheduler_output: SchedulerOutput,\n    ) -> KVConnectorMetadata:\n        meta = MooncakeConnectorMetadata()\n\n        # Loop through scheduled reqs and convert to RecvReqMeta.\n        if self.kv_role != \"kv_producer\":\n            for req_id, (req, block_ids) in self._reqs_need_recv.items():\n                assert req.kv_transfer_params is not None\n                meta.add_new_req(\n                    request_id=req_id,\n                    local_block_ids=block_ids,\n                    kv_transfer_params=req.kv_transfer_params,\n                )\n            self._reqs_need_recv.clear()\n\n        if self.kv_role != \"kv_consumer\":\n            for req_id, block_ids in self._reqs_need_send.items():\n                meta.add_new_req(\n                    request_id=req_id,\n                    local_block_ids=block_ids,\n                    kv_transfer_params={},\n                    load_remote_cache=False,\n                )\n            self._reqs_need_send.clear()\n\n        return meta\n\n    def request_finished(\n        self,\n        request: \"Request\",\n        block_ids: list[int],\n    ) -> tuple[bool, dict[str, Any] | None]:\n        \"\"\"\n        Once a request is finished, determine whether request blocks\n        should be freed now or will be sent asynchronously and freed later.\n        \"\"\"\n\n        params = request.kv_transfer_params\n        logger.debug(\n            \"MooncakeConnector request_finished, request_status=%s, \"\n            \"kv_transfer_params=%s\",\n            request.status,\n            params,\n        )\n        if not params:\n            return False, None\n\n        if params.get(\"do_remote_prefill\"):\n            # If do_remote_prefill is still True when the request is finished,\n            # update_state_after_alloc must not have been called (the request\n            # must have been aborted before it was scheduled).\n            # To avoid stranding the prefill blocks in the prefill instance,\n            # we must add empty block_ids to _reqs_need_recv so that our\n            # worker side will notify and free blocks in the prefill instance.\n            assert self.kv_role != \"kv_producer\"\n            self._reqs_need_recv[request.request_id] = (request, [])\n            params[\"do_remote_prefill\"] = False\n            return False, None\n\n        if (\n            not params.get(\"do_remote_decode\")\n            or request.status != RequestStatus.FINISHED_LENGTH_CAPPED\n        ):\n            return False, None\n\n        assert self.kv_role != \"kv_consumer\"\n\n        # TODO: check whether block_ids actually ever be 0. If not we could\n        # remove the conditional below\n        delay_free_blocks = len(block_ids) > 0\n\n        if delay_free_blocks:\n            self._reqs_need_send[request.request_id] = block_ids\n\n        return delay_free_blocks, dict(\n            do_remote_prefill=True,\n            do_remote_decode=False,\n            remote_host=self.side_channel_host,\n            remote_port=self.side_channel_port,\n        )",
      "language": "python"
    },
    {
      "code": "class MooncakeConnectorScheduler:\n    \"\"\"Implementation of Scheduler side methods\"\"\"\n\n    def __init__(self, vllm_config: VllmConfig, engine_id: str):\n        self.vllm_config = vllm_config\n        self.engine_id: EngineId = engine_id\n        self.side_channel_host = get_ip()\n        self.side_channel_port = get_mooncake_side_channel_port(vllm_config)\n\n        assert vllm_config.kv_transfer_config\n        self.kv_role = vllm_config.kv_transfer_config.kv_role\n        logger.info(\"Initializing Mooncake Transfer Engine Scheduler %s\", engine_id)\n\n        # Requests that need to start recv/send.\n        # New requests are added by update_state_after_alloc in\n        # the scheduler. Used to make metadata passed to Worker.\n        self._reqs_need_recv: dict[ReqId, tuple[Request, list[int]]] = {}\n        self._reqs_need_send: dict[ReqId, list[int]] = {}\n\n    def get_num_new_matched_tokens(\n        self, request: \"Request\", num_computed_tokens: int\n    ) -> tuple[int, bool]:\n        \"\"\"\n        For remote prefill, pull all prompt blocks from remote\n        asynchronously relative to engine execution.\n\n        Args:\n            request (Request): the request object.\n            num_computed_tokens (int): the number of locally\n                computed tokens for this request\n        Returns:\n            * the number of tokens that can be loaded from the\n              external KV cache beyond what is already computed.\n            * true if the external KV cache tokens will be loaded\n              asynchronously (between scheduler steps).\n        \"\"\"\n\n        params = request.kv_transfer_params\n        logger.debug(\n            \"MooncakeConnector get_num_new_matched_tokens: \"\n            \"num_computed_tokens=%s, kv_transfer_params=%s\",\n            num_computed_tokens,\n            params,\n        )\n\n        if params is not None and params.get(\"do_remote_prefill\"):\n            # Remote prefill: get all prompt blocks from remote.\n            token_ids = request.prompt_token_ids or []\n            count = len(token_ids) - num_computed_tokens\n            if count > 0:\n                return count, True\n\n        # No remote prefill for this request.\n        return 0, False\n\n    def update_state_after_alloc(\n        self, request: \"Request\", blocks: \"KVCacheBlocks\", num_external_tokens: int\n    ):\n        params = request.kv_transfer_params\n        logger.debug(\n            \"MooncakeConnector update_state_after_alloc: \"\n            \"num_external_tokens=%s, kv_transfer_params=%s\",\n            num_external_tokens,\n            params,\n        )\n\n        if not params:\n            return\n\n        if params.get(\"do_remote_prefill\"):\n            assert self.kv_role != \"kv_producer\"\n            if all(p in params for p in (\"remote_host\", \"remote_port\")):\n                # If remote_blocks and num_external_tokens = 0, we have\n                # a full prefix cache hit on the D worker. We need to call\n                # send_notif in _read_blocks to free the memory on the P.\n                local_block_ids = (\n                    blocks.get_unhashed_block_ids() if num_external_tokens > 0 else []\n                )\n                # Get unhashed blocks to pull from remote.\n                self._reqs_need_recv[request.request_id] = (request, local_block_ids)\n            else:\n                logger.warning(\n                    \"Got invalid KVTransferParams: %s. This \"\n                    \"request will not utilize KVTransfer\",\n                    params,\n                )\n            # Only trigger 1 KV transfer per request.\n            params[\"do_remote_prefill\"] = False\n\n        elif params.get(\"do_remote_decode\"):\n            # Add an empty list to worker to create event.\n            self._reqs_need_send[request.request_id] = []\n\n    def build_connector_meta(\n        self,\n        scheduler_output: SchedulerOutput,\n    ) -> KVConnectorMetadata:\n        meta = MooncakeConnectorMetadata()\n\n        # Loop through scheduled reqs and convert to RecvReqMeta.\n        if self.kv_role != \"kv_producer\":\n            for req_id, (req, block_ids) in self._reqs_need_recv.items():\n                assert req.kv_transfer_params is not None\n                meta.add_new_req(\n                    request_id=req_id,\n                    local_block_ids=block_ids,\n                    kv_transfer_params=req.kv_transfer_params,\n                )\n            self._reqs_need_recv.clear()\n\n        if self.kv_role != \"kv_consumer\":\n            for req_id, block_ids in self._reqs_need_send.items():\n                meta.add_new_req(\n                    request_id=req_id,\n                    local_block_ids=block_ids,\n                    kv_transfer_params={},\n                    load_remote_cache=False,\n                )\n            self._reqs_need_send.clear()\n\n        return meta\n\n    def request_finished(\n        self,\n        request: \"Request\",\n        block_ids: list[int],\n    ) -> tuple[bool, dict[str, Any] | None]:\n        \"\"\"\n        Once a request is finished, determine whether request blocks\n        should be freed now or will be sent asynchronously and freed later.\n        \"\"\"\n\n        params = request.kv_transfer_params\n        logger.debug(\n            \"MooncakeConnector request_finished, request_status=%s, \"\n            \"kv_transfer_params=%s\",\n            request.status,\n            params,\n        )\n        if not params:\n            return False, None\n\n        if params.get(\"do_remote_prefill\"):\n            # If do_remote_prefill is still True when the request is finished,\n            # update_state_after_alloc must not have been called (the request\n            # must have been aborted before it was scheduled).\n            # To avoid stranding the prefill blocks in the prefill instance,\n            # we must add empty block_ids to _reqs_need_recv so that our\n            # worker side will notify and free blocks in the prefill instance.\n            assert self.kv_role != \"kv_producer\"\n            self._reqs_need_recv[request.request_id] = (request, [])\n            params[\"do_remote_prefill\"] = False\n            return False, None\n\n        if (\n            not params.get(\"do_remote_decode\")\n            or request.status != RequestStatus.FINISHED_LENGTH_CAPPED\n        ):\n            return False, None\n\n        assert self.kv_role != \"kv_consumer\"\n\n        # TODO: check whether block_ids actually ever be 0. If not we could\n        # remove the conditional below\n        delay_free_blocks = len(block_ids) > 0\n\n        if delay_free_blocks:\n            self._reqs_need_send[request.request_id] = block_ids\n\n        return delay_free_blocks, dict(\n            do_remote_prefill=True,\n            do_remote_decode=False,\n            remote_host=self.side_channel_host,\n            remote_port=self.side_channel_port,\n        )",
      "language": "python"
    },
    {
      "code": "_reqs_need_recv: dict[ReqId, tuple[Request, list[int]]] = {}",
      "language": "yaml"
    },
    {
      "code": "_reqs_need_recv: dict[ReqId, tuple[Request, list[int]]] = {}",
      "language": "yaml"
    },
    {
      "code": "_reqs_need_send: dict[ReqId, list[int]] = {}",
      "language": "yaml"
    },
    {
      "code": "_reqs_need_send: dict[ReqId, list[int]] = {}",
      "language": "yaml"
    },
    {
      "code": "engine_id: EngineId = engine_id",
      "language": "typescript"
    },
    {
      "code": "engine_id: EngineId = engine_id",
      "language": "typescript"
    },
    {
      "code": "kv_role = kv_role",
      "language": "unknown"
    },
    {
      "code": "kv_role = kv_role",
      "language": "unknown"
    },
    {
      "code": "side_channel_host = get_ip()",
      "language": "unknown"
    },
    {
      "code": "side_channel_host = get_ip()",
      "language": "unknown"
    },
    {
      "code": "side_channel_port = get_mooncake_side_channel_port(\n    vllm_config\n)",
      "language": "unknown"
    },
    {
      "code": "side_channel_port = get_mooncake_side_channel_port(\n    vllm_config\n)",
      "language": "unknown"
    },
    {
      "code": "vllm_config = vllm_config",
      "language": "unknown"
    },
    {
      "code": "vllm_config = vllm_config",
      "language": "unknown"
    },
    {
      "code": "__init__(vllm_config: VllmConfig, engine_id: str)",
      "language": "python"
    },
    {
      "code": "__init__(vllm_config: VllmConfig, engine_id: str)",
      "language": "python"
    },
    {
      "code": "227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241",
      "language": "unknown"
    },
    {
      "code": "def __init__(self, vllm_config: VllmConfig, engine_id: str):\n    self.vllm_config = vllm_config\n    self.engine_id: EngineId = engine_id\n    self.side_channel_host = get_ip()\n    self.side_channel_port = get_mooncake_side_channel_port(vllm_config)\n\n    assert vllm_config.kv_transfer_config\n    self.kv_role = vllm_config.kv_transfer_config.kv_role\n    logger.info(\"Initializing Mooncake Transfer Engine Scheduler %s\", engine_id)\n\n    # Requests that need to start recv/send.\n    # New requests are added by update_state_after_alloc in\n    # the scheduler. Used to make metadata passed to Worker.\n    self._reqs_need_recv: dict[ReqId, tuple[Request, list[int]]] = {}\n    self._reqs_need_send: dict[ReqId, list[int]] = {}",
      "language": "python"
    },
    {
      "code": "def __init__(self, vllm_config: VllmConfig, engine_id: str):\n    self.vllm_config = vllm_config\n    self.engine_id: EngineId = engine_id\n    self.side_channel_host = get_ip()\n    self.side_channel_port = get_mooncake_side_channel_port(vllm_config)\n\n    assert vllm_config.kv_transfer_config\n    self.kv_role = vllm_config.kv_transfer_config.kv_role\n    logger.info(\"Initializing Mooncake Transfer Engine Scheduler %s\", engine_id)\n\n    # Requests that need to start recv/send.\n    # New requests are added by update_state_after_alloc in\n    # the scheduler. Used to make metadata passed to Worker.\n    self._reqs_need_recv: dict[ReqId, tuple[Request, list[int]]] = {}\n    self._reqs_need_send: dict[ReqId, list[int]] = {}",
      "language": "python"
    },
    {
      "code": "build_connector_meta(\n    scheduler_output: SchedulerOutput,\n) -> KVConnectorMetadata",
      "language": "php"
    },
    {
      "code": "build_connector_meta(\n    scheduler_output: SchedulerOutput,\n) -> KVConnectorMetadata",
      "language": "php"
    },
    {
      "code": "317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344",
      "language": "unknown"
    },
    {
      "code": "def build_connector_meta(\n    self,\n    scheduler_output: SchedulerOutput,\n) -> KVConnectorMetadata:\n    meta = MooncakeConnectorMetadata()\n\n    # Loop through scheduled reqs and convert to RecvReqMeta.\n    if self.kv_role != \"kv_producer\":\n        for req_id, (req, block_ids) in self._reqs_need_recv.items():\n            assert req.kv_transfer_params is not None\n            meta.add_new_req(\n                request_id=req_id,\n                local_block_ids=block_ids,\n                kv_transfer_params=req.kv_transfer_params,\n            )\n        self._reqs_need_recv.clear()\n\n    if self.kv_role != \"kv_consumer\":\n        for req_id, block_ids in self._reqs_need_send.items():\n            meta.add_new_req(\n                request_id=req_id,\n                local_block_ids=block_ids,\n                kv_transfer_params={},\n                load_remote_cache=False,\n            )\n        self._reqs_need_send.clear()\n\n    return meta",
      "language": "python"
    },
    {
      "code": "def build_connector_meta(\n    self,\n    scheduler_output: SchedulerOutput,\n) -> KVConnectorMetadata:\n    meta = MooncakeConnectorMetadata()\n\n    # Loop through scheduled reqs and convert to RecvReqMeta.\n    if self.kv_role != \"kv_producer\":\n        for req_id, (req, block_ids) in self._reqs_need_recv.items():\n            assert req.kv_transfer_params is not None\n            meta.add_new_req(\n                request_id=req_id,\n                local_block_ids=block_ids,\n                kv_transfer_params=req.kv_transfer_params,\n            )\n        self._reqs_need_recv.clear()\n\n    if self.kv_role != \"kv_consumer\":\n        for req_id, block_ids in self._reqs_need_send.items():\n            meta.add_new_req(\n                request_id=req_id,\n                local_block_ids=block_ids,\n                kv_transfer_params={},\n                load_remote_cache=False,\n            )\n        self._reqs_need_send.clear()\n\n    return meta",
      "language": "python"
    },
    {
      "code": "get_num_new_matched_tokens(\n    request: Request, num_computed_tokens: int\n) -> tuple[int, bool]",
      "language": "php"
    },
    {
      "code": "get_num_new_matched_tokens(\n    request: Request, num_computed_tokens: int\n) -> tuple[int, bool]",
      "language": "php"
    },
    {
      "code": "243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277",
      "language": "unknown"
    },
    {
      "code": "def get_num_new_matched_tokens(\n    self, request: \"Request\", num_computed_tokens: int\n) -> tuple[int, bool]:\n    \"\"\"\n    For remote prefill, pull all prompt blocks from remote\n    asynchronously relative to engine execution.\n\n    Args:\n        request (Request): the request object.\n        num_computed_tokens (int): the number of locally\n            computed tokens for this request\n    Returns:\n        * the number of tokens that can be loaded from the\n          external KV cache beyond what is already computed.\n        * true if the external KV cache tokens will be loaded\n          asynchronously (between scheduler steps).\n    \"\"\"\n\n    params = request.kv_transfer_params\n    logger.debug(\n        \"MooncakeConnector get_num_new_matched_tokens: \"\n        \"num_computed_tokens=%s, kv_transfer_params=%s\",\n        num_computed_tokens,\n        params,\n    )\n\n    if params is not None and params.get(\"do_remote_prefill\"):\n        # Remote prefill: get all prompt blocks from remote.\n        token_ids = request.prompt_token_ids or []\n        count = len(token_ids) - num_computed_tokens\n        if count > 0:\n            return count, True\n\n    # No remote prefill for this request.\n    return 0, False",
      "language": "python"
    },
    {
      "code": "def get_num_new_matched_tokens(\n    self, request: \"Request\", num_computed_tokens: int\n) -> tuple[int, bool]:\n    \"\"\"\n    For remote prefill, pull all prompt blocks from remote\n    asynchronously relative to engine execution.\n\n    Args:\n        request (Request): the request object.\n        num_computed_tokens (int): the number of locally\n            computed tokens for this request\n    Returns:\n        * the number of tokens that can be loaded from the\n          external KV cache beyond what is already computed.\n        * true if the external KV cache tokens will be loaded\n          asynchronously (between scheduler steps).\n    \"\"\"\n\n    params = request.kv_transfer_params\n    logger.debug(\n        \"MooncakeConnector get_num_new_matched_tokens: \"\n        \"num_computed_tokens=%s, kv_transfer_params=%s\",\n        num_computed_tokens,\n        params,\n    )\n\n    if params is not None and params.get(\"do_remote_prefill\"):\n        # Remote prefill: get all prompt blocks from remote.\n        token_ids = request.prompt_token_ids or []\n        count = len(token_ids) - num_computed_tokens\n        if count > 0:\n            return count, True\n\n    # No remote prefill for this request.\n    return 0, False",
      "language": "python"
    },
    {
      "code": "request_finished(\n    request: Request, block_ids: list[int]\n) -> tuple[bool, dict[str, Any] | None]",
      "language": "rust"
    },
    {
      "code": "request_finished(\n    request: Request, block_ids: list[int]\n) -> tuple[bool, dict[str, Any] | None]",
      "language": "rust"
    },
    {
      "code": "346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398",
      "language": "unknown"
    },
    {
      "code": "def request_finished(\n    self,\n    request: \"Request\",\n    block_ids: list[int],\n) -> tuple[bool, dict[str, Any] | None]:\n    \"\"\"\n    Once a request is finished, determine whether request blocks\n    should be freed now or will be sent asynchronously and freed later.\n    \"\"\"\n\n    params = request.kv_transfer_params\n    logger.debug(\n        \"MooncakeConnector request_finished, request_status=%s, \"\n        \"kv_transfer_params=%s\",\n        request.status,\n        params,\n    )\n    if not params:\n        return False, None\n\n    if params.get(\"do_remote_prefill\"):\n        # If do_remote_prefill is still True when the request is finished,\n        # update_state_after_alloc must not have been called (the request\n        # must have been aborted before it was scheduled).\n        # To avoid stranding the prefill blocks in the prefill instance,\n        # we must add empty block_ids to _reqs_need_recv so that our\n        # worker side will notify and free blocks in the prefill instance.\n        assert self.kv_role != \"kv_producer\"\n        self._reqs_need_recv[request.request_id] = (request, [])\n        params[\"do_remote_prefill\"] = False\n        return False, None\n\n    if (\n        not params.get(\"do_remote_decode\")\n        or request.status != RequestStatus.FINISHED_LENGTH_CAPPED\n    ):\n        return False, None\n\n    assert self.kv_role != \"kv_consumer\"\n\n    # TODO: check whether block_ids actually ever be 0. If not we could\n    # remove the conditional below\n    delay_free_blocks = len(block_ids) > 0\n\n    if delay_free_blocks:\n        self._reqs_need_send[request.request_id] = block_ids\n\n    return delay_free_blocks, dict(\n        do_remote_prefill=True,\n        do_remote_decode=False,\n        remote_host=self.side_channel_host,\n        remote_port=self.side_channel_port,\n    )",
      "language": "python"
    },
    {
      "code": "def request_finished(\n    self,\n    request: \"Request\",\n    block_ids: list[int],\n) -> tuple[bool, dict[str, Any] | None]:\n    \"\"\"\n    Once a request is finished, determine whether request blocks\n    should be freed now or will be sent asynchronously and freed later.\n    \"\"\"\n\n    params = request.kv_transfer_params\n    logger.debug(\n        \"MooncakeConnector request_finished, request_status=%s, \"\n        \"kv_transfer_params=%s\",\n        request.status,\n        params,\n    )\n    if not params:\n        return False, None\n\n    if params.get(\"do_remote_prefill\"):\n        # If do_remote_prefill is still True when the request is finished,\n        # update_state_after_alloc must not have been called (the request\n        # must have been aborted before it was scheduled).\n        # To avoid stranding the prefill blocks in the prefill instance,\n        # we must add empty block_ids to _reqs_need_recv so that our\n        # worker side will notify and free blocks in the prefill instance.\n        assert self.kv_role != \"kv_producer\"\n        self._reqs_need_recv[request.request_id] = (request, [])\n        params[\"do_remote_prefill\"] = False\n        return False, None\n\n    if (\n        not params.get(\"do_remote_decode\")\n        or request.status != RequestStatus.FINISHED_LENGTH_CAPPED\n    ):\n        return False, None\n\n    assert self.kv_role != \"kv_consumer\"\n\n    # TODO: check whether block_ids actually ever be 0. If not we could\n    # remove the conditional below\n    delay_free_blocks = len(block_ids) > 0\n\n    if delay_free_blocks:\n        self._reqs_need_send[request.request_id] = block_ids\n\n    return delay_free_blocks, dict(\n        do_remote_prefill=True,\n        do_remote_decode=False,\n        remote_host=self.side_channel_host,\n        remote_port=self.side_channel_port,\n    )",
      "language": "python"
    },
    {
      "code": "update_state_after_alloc(\n    request: Request,\n    blocks: KVCacheBlocks,\n    num_external_tokens: int,\n)",
      "language": "yaml"
    },
    {
      "code": "update_state_after_alloc(\n    request: Request,\n    blocks: KVCacheBlocks,\n    num_external_tokens: int,\n)",
      "language": "yaml"
    },
    {
      "code": "279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315",
      "language": "unknown"
    },
    {
      "code": "def update_state_after_alloc(\n    self, request: \"Request\", blocks: \"KVCacheBlocks\", num_external_tokens: int\n):\n    params = request.kv_transfer_params\n    logger.debug(\n        \"MooncakeConnector update_state_after_alloc: \"\n        \"num_external_tokens=%s, kv_transfer_params=%s\",\n        num_external_tokens,\n        params,\n    )\n\n    if not params:\n        return\n\n    if params.get(\"do_remote_prefill\"):\n        assert self.kv_role != \"kv_producer\"\n        if all(p in params for p in (\"remote_host\", \"remote_port\")):\n            # If remote_blocks and num_external_tokens = 0, we have\n            # a full prefix cache hit on the D worker. We need to call\n            # send_notif in _read_blocks to free the memory on the P.\n            local_block_ids = (\n                blocks.get_unhashed_block_ids() if num_external_tokens > 0 else []\n            )\n            # Get unhashed blocks to pull from remote.\n            self._reqs_need_recv[request.request_id] = (request, local_block_ids)\n        else:\n            logger.warning(\n                \"Got invalid KVTransferParams: %s. This \"\n                \"request will not utilize KVTransfer\",\n                params,\n            )\n        # Only trigger 1 KV transfer per request.\n        params[\"do_remote_prefill\"] = False\n\n    elif params.get(\"do_remote_decode\"):\n        # Add an empty list to worker to create event.\n        self._reqs_need_send[request.request_id] = []",
      "language": "python"
    },
    {
      "code": "def update_state_after_alloc(\n    self, request: \"Request\", blocks: \"KVCacheBlocks\", num_external_tokens: int\n):\n    params = request.kv_transfer_params\n    logger.debug(\n        \"MooncakeConnector update_state_after_alloc: \"\n        \"num_external_tokens=%s, kv_transfer_params=%s\",\n        num_external_tokens,\n        params,\n    )\n\n    if not params:\n        return\n\n    if params.get(\"do_remote_prefill\"):\n        assert self.kv_role != \"kv_producer\"\n        if all(p in params for p in (\"remote_host\", \"remote_port\")):\n            # If remote_blocks and num_external_tokens = 0, we have\n            # a full prefix cache hit on the D worker. We need to call\n            # send_notif in _read_blocks to free the memory on the P.\n            local_block_ids = (\n                blocks.get_unhashed_block_ids() if num_external_tokens > 0 else []\n            )\n            # Get unhashed blocks to pull from remote.\n            self._reqs_need_recv[request.request_id] = (request, local_block_ids)\n        else:\n            logger.warning(\n                \"Got invalid KVTransferParams: %s. This \"\n                \"request will not utilize KVTransfer\",\n                params,\n            )\n        # Only trigger 1 KV transfer per request.\n        params[\"do_remote_prefill\"] = False\n\n    elif params.get(\"do_remote_decode\"):\n        # Add an empty list to worker to create event.\n        self._reqs_need_send[request.request_id] = []",
      "language": "python"
    },
    {
      "code": "401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n500\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\n600\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\n700\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\n800\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894",
      "language": "unknown"
    },
    {
      "code": "class MooncakeConnectorWorker:\n    \"\"\"Implementation of Worker side methods\"\"\"\n\n    def __init__(self, vllm_config: VllmConfig, engine_id: str):\n        logger.info(\"Initializing Mooncake Transfer Engine worker %s\", engine_id)\n\n        self.vllm_config = vllm_config\n\n        self.engine = TransferEngine()\n        self.hostname = get_ip()\n        protocol = self.vllm_config.kv_transfer_config.kv_connector_extra_config.get(  # type: ignore[union-attr]\n            \"mooncake_protocol\", \"rdma\"\n        )\n        logger.info(\n            \"The Mooncake Transfer Engine is using %s as its protocol.\", protocol\n        )\n        ret_value = self.engine.initialize(self.hostname, \"P2PHANDSHAKE\", protocol, \"\")\n        if ret_value != 0:\n            raise RuntimeError(\"Mooncake Transfer Engine initialization failed.\")\n\n        self.rpc_port = self.engine.get_rpc_port()\n\n        logger.debug(\n            \"Mooncake Transfer Engine initialized at %s:%d\",\n            self.hostname,\n            self.rpc_port,\n        )\n\n        # Mooncake handshake port.\n        self.side_channel_port: int = get_mooncake_side_channel_port(vllm_config)\n\n        self.engine_id: EngineId = engine_id\n        self.tp_rank = get_tensor_model_parallel_rank()\n        self.world_size = get_tensor_model_parallel_world_size()\n        self.tp_group = get_tp_group()\n        self.num_blocks = 0\n\n        assert vllm_config.kv_transfer_config\n        self.kv_role = vllm_config.kv_transfer_config.kv_role\n        self.num_workers = vllm_config.kv_transfer_config.kv_connector_extra_config.get(\n            \"num_workers\", 10\n        )\n\n        self.kv_caches_base_addr: list[int] = []\n        self.device_kv_caches: dict[str, torch.Tensor] = {}\n        self.reqs_need_send: SendReqMeta = SendReqMeta(reqs={}, lock=threading.Lock())\n\n        # For kv_both, we will act both prefiller and decoder.\n        if self.kv_role != \"kv_consumer\":\n            # Background thread for sending kvcaches to D.\n            self._mooncake_sender_t: threading.Thread | None = None\n            # Background thread for processing new sending requests.\n            self._sender_executor = ThreadPoolExecutor(\n                max_workers=self.num_workers, thread_name_prefix=\"vllm-mooncake-sender\"\n            )\n            logger.debug(\n                \"Mooncake Prefiller: use %d workers to send kvcaches\", self.num_workers\n            )\n        if self.kv_role != \"kv_producer\":\n            self.receiver_loop = asyncio.new_event_loop()\n            self._mooncake_receiver_t = threading.Thread(\n                target=self._receiver_loop, args=(self.receiver_loop,), daemon=True\n            )\n            self._mooncake_receiver_t.start()\n            logger.debug(\"Mooncake Decoder: start receiver thread\")\n\n        self.finished_sending_reqs: FinishedSendReqSet = FinishedSendReqSet(\n            set(), threading.Lock()\n        )\n        self.finished_recving_reqs: FinishedReceiveReqSet = FinishedReceiveReqSet(\n            set(), asyncio.Lock()\n        )\n\n        self.block_size = vllm_config.cache_config.block_size\n        self.model_config = vllm_config.model_config\n        self.cache_config = vllm_config.cache_config\n        self.use_mla = self.model_config.use_mla\n\n        backend = get_attn_backend(\n            self.model_config.get_head_size(),\n            self.model_config.dtype,\n            self.cache_config.cache_dtype,\n            self.block_size,\n            use_mla=self.use_mla,\n        )\n        self.backend_name = backend.get_name()\n        self.kv_cache_layout = get_kv_cache_layout()\n        logger.debug(\"Detected attention backend %s\", self.backend_name)\n        logger.debug(\"Detected kv cache layout %s\", self.kv_cache_layout)\n\n        self._tp_size: dict[EngineId, int] = {self.engine_id: self.world_size}\n        self._block_size: dict[EngineId, int] = {self.engine_id: self.block_size}\n        self.kv_topo = TpKVTopology(\n            tp_rank=self.tp_rank,\n            engine_id=self.engine_id,\n            remote_tp_size=self._tp_size,  # shared state\n            remote_block_size=self._block_size,  # shared state\n            is_mla=self.use_mla,\n            total_num_kv_heads=self.model_config.get_total_num_kv_heads(),\n            attn_backend=backend,\n        )\n        self._use_pallas = self.kv_topo._use_pallas\n\n        self.zmq_ctx = zmq.Context()\n        self.async_zmq_ctx = zmq.asyncio.Context()\n        self._encoder = msgspec.msgpack.Encoder()\n        self._decoder = msgspec.msgpack.Decoder(MooncakeAgentMetadata)\n\n    def __del__(self):\n        self.shutdown()\n\n    def shutdown(self):\n        \"\"\"Cleanup background threads on destruction.\"\"\"\n        self.zmq_ctx.term()\n        self.async_zmq_ctx.term()\n        if self.kv_role != \"kv_consumer\":\n            self._sender_executor.shutdown(wait=False)\n            if self._mooncake_sender_t:\n                self._mooncake_sender_t.join()\n        if self.kv_role != \"kv_producer\" and self.receiver_loop.is_running():\n            self.receiver_loop.call_soon_threadsafe(self.receiver_loop.stop)\n            self._mooncake_receiver_t.join()\n\n    def _receiver_loop(self, loop: asyncio.AbstractEventLoop):\n        asyncio.set_event_loop(loop)\n        loop.run_forever()\n\n    def _mooncake_sender(\n        self, ready_event: threading.Event, base_port: int, tp_rank: int\n    ):\n        \"\"\"\n        Background thread that listens for Mooncake requests, dispatches them\n        to a thread pool, and sends acknowledgments upon completion.\n        \"\"\"\n\n        frontend_path = make_zmq_path(\"tcp\", self.hostname, base_port + tp_rank)\n        frontend = make_zmq_socket(self.zmq_ctx, frontend_path, zmq.ROUTER)\n        logger.debug(\"Mooncake sender starting listening on path: %s\", frontend_path)\n\n        backend_path = make_zmq_path(\"inproc\", str(uuid.uuid4()))\n        backend = make_zmq_socket(self.zmq_ctx, backend_path, zmq.PULL)\n\n        poller = zmq.Poller()\n        poller.register(frontend, zmq.POLLIN)\n        poller.register(backend, zmq.POLLIN)\n\n        ready_event.set()\n\n        try:\n            while True:\n                sockets = dict(poller.poll())\n\n                if frontend in sockets:\n                    identity, _, metadata_bytes = frontend.recv_multipart()\n                    self._sender_executor.submit(\n                        self._sender_worker,\n                        identity,\n                        metadata_bytes,\n                        backend_path,\n                    )\n\n                if backend in sockets:\n                    identity, status = backend.recv_multipart()\n                    frontend.send_multipart((identity, b\"\", status))\n\n        except zmq.ContextTerminated:\n            logger.debug(\"ZMQ context terminated, exiting Mooncake sender thread.\")\n        except Exception as e:\n            logger.error(\"Error in Mooncake sender thread: %s. Exiting thread.\", str(e))\n        finally:\n            frontend.close()\n            backend.close()\n\n    def _sender_worker(\n        self, identity: bytes, metadata_bytes: bytes, worker_channel_path: str\n    ):\n        status = TRANS_ERROR\n\n        try:\n            metadata = self._decoder.decode(metadata_bytes)\n            self.send_kv_to_decode(metadata)\n            status = TRANS_DONE\n        except Exception as e:\n            logger.error(\"Error processing Mooncake handshake: %s\", e)\n        finally:\n            pusher = make_zmq_socket(self.zmq_ctx, worker_channel_path, zmq.PUSH)\n            try:\n                pusher.send_multipart((identity, status))\n            except zmq.ZMQError as e:\n                logger.warning(\n                    \"Internal error, maybe the server is shutting down. Error: %s\",\n                    e,\n                )\n            finally:\n                pusher.close()\n\n    def send_kv_to_decode(self, meta: MooncakeAgentMetadata):\n        send_reqs: list[tuple[ReqId, SendBlockMeta]] = []\n        with self.reqs_need_send.lock:\n            for req_id in meta.request_ids:\n                send_meta = self.reqs_need_send.reqs.get(req_id)\n                if send_meta is None:\n                    logger.warning(\"Request %s not found in reqs_need_send\", req_id)\n                    return\n                # Mark it as not expired. We will send it now.\n                send_meta.expire_time = float(\"inf\")\n                send_reqs.append((req_id, send_meta))\n\n        self._send_blocks(send_reqs, meta)\n\n        with self.reqs_need_send.lock:\n            for req_id in meta.request_ids:\n                del self.reqs_need_send.reqs[req_id]\n\n        with self.finished_sending_reqs.lock:\n            self.finished_sending_reqs.set.update(meta.request_ids)\n\n    def _send_blocks(\n        self,\n        send_reqs: list[tuple[ReqId, SendBlockMeta]],\n        agent_meta: MooncakeAgentMetadata,\n    ):\n        src_ptrs = []\n        dst_ptrs = []\n        lengths = []\n        local_base_addr = self.kv_caches_base_addr\n        remote_base_addr = agent_meta.kv_caches_base_addr\n        block_len = self.block_len\n        remote_session = f\"{agent_meta.remote_hostname}:{agent_meta.remote_port}\"\n\n        assert len(send_reqs) == len(agent_meta.block_ids)\n        for (req_id, send_meta), remote_block_ids in zip(\n            send_reqs, agent_meta.block_ids\n        ):\n            send_meta.ready.wait()\n\n            num_remote_blocks = len(remote_block_ids)\n            if num_remote_blocks == 0:\n                continue\n\n            local_block_ids = send_meta.local_block_ids\n            # Partial prefix cache hit: just read uncomputed blocks.\n            num_local_blocks = len(local_block_ids)\n            assert num_local_blocks >= num_remote_blocks\n            if num_local_blocks > num_remote_blocks:\n                local_block_ids = local_block_ids[-num_remote_blocks:]\n\n            # Group by indices\n            group_local_block_ids, group_remote_block_ids = group_concurrent_contiguous(\n                local_block_ids, remote_block_ids\n            )\n\n            for local_layer_addr, remote_layer_addr in zip(\n                local_base_addr, remote_base_addr\n            ):\n                for group_local_block_id, group_remote_block_id in zip(\n                    group_local_block_ids, group_remote_block_ids\n                ):\n                    src_ptrs.append(\n                        local_layer_addr + group_local_block_id[0] * block_len\n                    )\n                    dst_ptrs.append(\n                        remote_layer_addr + group_remote_block_id[0] * block_len\n                    )\n                    lengths.append(block_len * len(group_local_block_id))\n\n            logger.debug(\n                \"Sending kv_caches for request %s (%d blocks) to %s\",\n                req_id,\n                num_remote_blocks,\n                remote_session,\n            )\n\n        start_time = time.perf_counter()\n        ret_value = self.engine.batch_transfer_sync_write(\n            remote_session, src_ptrs, dst_ptrs, lengths\n        )\n        if ret_value != 0:\n            raise RuntimeError(f\"Error in batch_transfer_sync_write: {ret_value}\")\n\n        logger.debug(\n            \"Sending to %s done, took %s\",\n            remote_session,\n            time.perf_counter() - start_time,\n        )\n\n    def register_kv_caches(self, kv_caches: dict[str, torch.Tensor]):\n        \"\"\"Register the KV Cache data in mooncake.\"\"\"\n\n        logger.info(\"Registering KV_Caches. use_mla: %s\", self.use_mla)\n\n        kv_data_ptrs = []\n        kv_data_lens = []\n        seen_base_addresses = []\n\n        split_k_and_v = self.kv_topo.split_k_and_v\n        tensor_size_bytes = None\n        for layer_name, cache_or_caches in kv_caches.items():\n            logger.debug(\n                \"registering layer %s with shape %s\", layer_name, cache_or_caches.shape\n            )\n            cache_list = cache_or_caches if split_k_and_v else [cache_or_caches]\n\n            for cache in cache_list:\n                base_addr = cache.data_ptr()\n                if base_addr in seen_base_addresses:\n                    continue\n\n                seen_base_addresses.append(base_addr)\n                curr_tensor_size_bytes = cache.nbytes\n\n                if tensor_size_bytes is None:\n                    tensor_size_bytes = curr_tensor_size_bytes\n                    self.num_blocks = cache.shape[0]\n\n                assert tensor_size_bytes == curr_tensor_size_bytes, (\n                    \"All kv cache tensors must have the same size\"\n                )\n                kernel_block_size = cache.shape[-2 if self.use_mla else -3]\n                assert self.block_size == kernel_block_size\n                kv_data_ptrs.append(base_addr)\n                kv_data_lens.append(tensor_size_bytes)\n\n        self.kv_caches_base_addr = seen_base_addresses\n\n        ret_value = self.engine.batch_register_memory(kv_data_ptrs, kv_data_lens)\n        if ret_value != 0:\n            raise RuntimeError(\"Mooncake batch memory registration failed.\")\n\n        assert tensor_size_bytes is not None\n        assert self.num_blocks != 0\n        assert tensor_size_bytes % self.num_blocks == 0\n        self.block_len = tensor_size_bytes // self.num_blocks\n        self.device_kv_caches = kv_caches\n        logger.debug(\n            \"registered num_blocks=%d block_len=%d\", self.num_blocks, self.block_len\n        )\n\n        # No need to launch server for D node.\n        if self.kv_role == \"kv_consumer\":\n            return\n\n        ready_event = threading.Event()\n        self._mooncake_sender_t = threading.Thread(\n            target=self._mooncake_sender,\n            args=(ready_event, self.side_channel_port, self.tp_rank),\n            daemon=True,\n            name=\"mooncake_sender\",\n        )\n        self._mooncake_sender_t.start()\n        ready_event.wait()  # Wait for listener ZMQ socket to be ready.\n\n    async def fetch_finished_recving_reqs(self) -> set[ReqId]:\n        async with self.finished_recving_reqs.lock:\n            finished_recving_reqs = self.finished_recving_reqs.set\n            self.finished_recving_reqs.set = set()\n        return finished_recving_reqs\n\n    def get_finished(self) -> tuple[set[str] | None, set[str] | None]:\n        \"\"\"\n        Get requests that are done sending or recving on this specific worker.\n        The scheduler process (via the MultiprocExecutor) will use this output\n        to track which workers are done.\n        \"\"\"\n        fut = None\n        if self.kv_role != \"kv_producer\":\n            fut = asyncio.run_coroutine_threadsafe(\n                self.fetch_finished_recving_reqs(), self.receiver_loop\n            )\n\n        if self.kv_role != \"kv_consumer\":\n            with self.finished_sending_reqs.lock:\n                finished_sending_reqs = self.finished_sending_reqs.set\n                self.finished_sending_reqs.set = set()\n        else:\n            finished_sending_reqs = set()\n\n        finished_recving_reqs = fut.result() if fut else set()\n\n        if finished_sending_reqs or finished_recving_reqs:\n            logger.debug(\n                \"Rank %s, get_finished: %s requests done sending \"\n                \"and %s requests done recving\",\n                self.tp_rank,\n                len(finished_sending_reqs),\n                len(finished_recving_reqs),\n            )\n\n        # Handle timeout to avoid stranding blocks on remote.\n        now = time.perf_counter()\n        with self.reqs_need_send.lock:\n            expired_reqs = [\n                req_id\n                for req_id, send_meta in self.reqs_need_send.reqs.items()\n                if send_meta.expire_time < now\n            ]\n            for req_id in expired_reqs:\n                logger.warning(\n                    \"Request %s timed out after %d seconds without \"\n                    \"being sent. Freeing its blocks on the producer side.\",\n                    req_id,\n                    envs.VLLM_MOONCAKE_ABORT_REQUEST_TIMEOUT,\n                )\n                del self.reqs_need_send.reqs[req_id]\n            if expired_reqs:\n                finished_sending_reqs.update(expired_reqs)\n\n        return finished_sending_reqs or None, finished_recving_reqs or None\n\n    async def receive_kv(self, path: str, req_blocks: list[tuple[str, list[int]]]):\n        req_ids, block_ids = map(list, zip(*req_blocks))\n        metadata = MooncakeAgentMetadata(\n            remote_hostname=self.hostname,\n            remote_port=self.rpc_port,\n            request_ids=req_ids,\n            kv_caches_base_addr=self.kv_caches_base_addr,\n            block_ids=block_ids,\n        )\n\n        encoded_data = self._encoder.encode(metadata)\n        logger.debug(\n            \"Size of encoded MooncakeAgentMetadata: %d bytes\", len(encoded_data)\n        )\n        logger.debug(\"Sending kv transfer request for %s on path: %s\", req_ids, path)\n\n        # Send query for the request.\n        sock: zmq.asyncio.Socket = make_zmq_socket(\n            self.async_zmq_ctx, path, zmq.REQ, bind=False, linger=0\n        )\n        sock.setsockopt(zmq.RCVTIMEO, 60000)\n        try:\n            await sock.send(encoded_data)\n            ret_msg = await sock.recv()\n            if ret_msg != TRANS_DONE:\n                logger.error(\n                    \"Error happens during tranfering kvcache for %s, see logs in prefiller.\",  # noqa: E501\n                    req_ids,\n                )\n                return\n        except zmq.ContextTerminated:\n            logger.debug(\"ZMQ context terminated, exiting Mooncake receiver thread.\")\n        except Exception as e:\n            logger.error(\"MooncakeAgentMetadata transfer failed for %s: %s\", req_ids, e)\n            return\n        finally:\n            sock.close()\n\n        async with self.finished_recving_reqs.lock:\n            self.finished_recving_reqs.set.update(req_ids)\n\n        logger.debug(\"pulling kv_caches for %s finished\", req_ids)\n\n    def group_kv_pull(self, metadata: MooncakeConnectorMetadata):\n        kv_pulls = defaultdict(list)\n        for req_id, meta in metadata.reqs_to_recv.items():\n            logger.debug(\n                \"start_load_kv for request %s from remote engine. \"\n                \"Num local_block_ids: %s.\",\n                req_id,\n                len(meta.local_block_ids),\n            )\n            path = make_zmq_path(\n                \"tcp\", meta.remote_host, meta.remote_port + self.tp_rank\n            )\n            kv_pulls[path].append((req_id, meta.local_block_ids))\n\n        return kv_pulls\n\n    def start_load_kv(self, metadata: MooncakeConnectorMetadata):\n        if self.kv_role != \"kv_producer\":\n            kv_pulls = self.group_kv_pull(metadata)\n            for path, req_blocks in kv_pulls.items():\n                asyncio.run_coroutine_threadsafe(\n                    self.receive_kv(path, req_blocks), self.receiver_loop\n                )\n\n        if self.kv_role != \"kv_consumer\":\n            with self.reqs_need_send.lock:\n                for req_id, block_ids in metadata.reqs_to_send.items():\n                    if block_ids:\n                        # Already gone through request_finished()\n                        send_meta = self.reqs_need_send.reqs[req_id]\n                        send_meta.local_block_ids = block_ids\n                        send_meta.ready.set()\n                        send_meta.expire_time = (\n                            time.perf_counter()\n                            + envs.VLLM_MOONCAKE_ABORT_REQUEST_TIMEOUT\n                        )\n                    else:\n                        # From update_state_after_alloc(),\n                        # but not reach request_finished() yet\n                        self.reqs_need_send.reqs[req_id] = SendBlockMeta(\n                            local_block_ids=[], ready=threading.Event()\n                        )",
      "language": "python"
    },
    {
      "code": "class MooncakeConnectorWorker:\n    \"\"\"Implementation of Worker side methods\"\"\"\n\n    def __init__(self, vllm_config: VllmConfig, engine_id: str):\n        logger.info(\"Initializing Mooncake Transfer Engine worker %s\", engine_id)\n\n        self.vllm_config = vllm_config\n\n        self.engine = TransferEngine()\n        self.hostname = get_ip()\n        protocol = self.vllm_config.kv_transfer_config.kv_connector_extra_config.get(  # type: ignore[union-attr]\n            \"mooncake_protocol\", \"rdma\"\n        )\n        logger.info(\n            \"The Mooncake Transfer Engine is using %s as its protocol.\", protocol\n        )\n        ret_value = self.engine.initialize(self.hostname, \"P2PHANDSHAKE\", protocol, \"\")\n        if ret_value != 0:\n            raise RuntimeError(\"Mooncake Transfer Engine initialization failed.\")\n\n        self.rpc_port = self.engine.get_rpc_port()\n\n        logger.debug(\n            \"Mooncake Transfer Engine initialized at %s:%d\",\n            self.hostname,\n            self.rpc_port,\n        )\n\n        # Mooncake handshake port.\n        self.side_channel_port: int = get_mooncake_side_channel_port(vllm_config)\n\n        self.engine_id: EngineId = engine_id\n        self.tp_rank = get_tensor_model_parallel_rank()\n        self.world_size = get_tensor_model_parallel_world_size()\n        self.tp_group = get_tp_group()\n        self.num_blocks = 0\n\n        assert vllm_config.kv_transfer_config\n        self.kv_role = vllm_config.kv_transfer_config.kv_role\n        self.num_workers = vllm_config.kv_transfer_config.kv_connector_extra_config.get(\n            \"num_workers\", 10\n        )\n\n        self.kv_caches_base_addr: list[int] = []\n        self.device_kv_caches: dict[str, torch.Tensor] = {}\n        self.reqs_need_send: SendReqMeta = SendReqMeta(reqs={}, lock=threading.Lock())\n\n        # For kv_both, we will act both prefiller and decoder.\n        if self.kv_role != \"kv_consumer\":\n            # Background thread for sending kvcaches to D.\n            self._mooncake_sender_t: threading.Thread | None = None\n            # Background thread for processing new sending requests.\n            self._sender_executor = ThreadPoolExecutor(\n                max_workers=self.num_workers, thread_name_prefix=\"vllm-mooncake-sender\"\n            )\n            logger.debug(\n                \"Mooncake Prefiller: use %d workers to send kvcaches\", self.num_workers\n            )\n        if self.kv_role != \"kv_producer\":\n            self.receiver_loop = asyncio.new_event_loop()\n            self._mooncake_receiver_t = threading.Thread(\n                target=self._receiver_loop, args=(self.receiver_loop,), daemon=True\n            )\n            self._mooncake_receiver_t.start()\n            logger.debug(\"Mooncake Decoder: start receiver thread\")\n\n        self.finished_sending_reqs: FinishedSendReqSet = FinishedSendReqSet(\n            set(), threading.Lock()\n        )\n        self.finished_recving_reqs: FinishedReceiveReqSet = FinishedReceiveReqSet(\n            set(), asyncio.Lock()\n        )\n\n        self.block_size = vllm_config.cache_config.block_size\n        self.model_config = vllm_config.model_config\n        self.cache_config = vllm_config.cache_config\n        self.use_mla = self.model_config.use_mla\n\n        backend = get_attn_backend(\n            self.model_config.get_head_size(),\n            self.model_config.dtype,\n            self.cache_config.cache_dtype,\n            self.block_size,\n            use_mla=self.use_mla,\n        )\n        self.backend_name = backend.get_name()\n        self.kv_cache_layout = get_kv_cache_layout()\n        logger.debug(\"Detected attention backend %s\", self.backend_name)\n        logger.debug(\"Detected kv cache layout %s\", self.kv_cache_layout)\n\n        self._tp_size: dict[EngineId, int] = {self.engine_id: self.world_size}\n        self._block_size: dict[EngineId, int] = {self.engine_id: self.block_size}\n        self.kv_topo = TpKVTopology(\n            tp_rank=self.tp_rank,\n            engine_id=self.engine_id,\n            remote_tp_size=self._tp_size,  # shared state\n            remote_block_size=self._block_size,  # shared state\n            is_mla=self.use_mla,\n            total_num_kv_heads=self.model_config.get_total_num_kv_heads(),\n            attn_backend=backend,\n        )\n        self._use_pallas = self.kv_topo._use_pallas\n\n        self.zmq_ctx = zmq.Context()\n        self.async_zmq_ctx = zmq.asyncio.Context()\n        self._encoder = msgspec.msgpack.Encoder()\n        self._decoder = msgspec.msgpack.Decoder(MooncakeAgentMetadata)\n\n    def __del__(self):\n        self.shutdown()\n\n    def shutdown(self):\n        \"\"\"Cleanup background threads on destruction.\"\"\"\n        self.zmq_ctx.term()\n        self.async_zmq_ctx.term()\n        if self.kv_role != \"kv_consumer\":\n            self._sender_executor.shutdown(wait=False)\n            if self._mooncake_sender_t:\n                self._mooncake_sender_t.join()\n        if self.kv_role != \"kv_producer\" and self.receiver_loop.is_running():\n            self.receiver_loop.call_soon_threadsafe(self.receiver_loop.stop)\n            self._mooncake_receiver_t.join()\n\n    def _receiver_loop(self, loop: asyncio.AbstractEventLoop):\n        asyncio.set_event_loop(loop)\n        loop.run_forever()\n\n    def _mooncake_sender(\n        self, ready_event: threading.Event, base_port: int, tp_rank: int\n    ):\n        \"\"\"\n        Background thread that listens for Mooncake requests, dispatches them\n        to a thread pool, and sends acknowledgments upon completion.\n        \"\"\"\n\n        frontend_path = make_zmq_path(\"tcp\", self.hostname, base_port + tp_rank)\n        frontend = make_zmq_socket(self.zmq_ctx, frontend_path, zmq.ROUTER)\n        logger.debug(\"Mooncake sender starting listening on path: %s\", frontend_path)\n\n        backend_path = make_zmq_path(\"inproc\", str(uuid.uuid4()))\n        backend = make_zmq_socket(self.zmq_ctx, backend_path, zmq.PULL)\n\n        poller = zmq.Poller()\n        poller.register(frontend, zmq.POLLIN)\n        poller.register(backend, zmq.POLLIN)\n\n        ready_event.set()\n\n        try:\n            while True:\n                sockets = dict(poller.poll())\n\n                if frontend in sockets:\n                    identity, _, metadata_bytes = frontend.recv_multipart()\n                    self._sender_executor.submit(\n                        self._sender_worker,\n                        identity,\n                        metadata_bytes,\n                        backend_path,\n                    )\n\n                if backend in sockets:\n                    identity, status = backend.recv_multipart()\n                    frontend.send_multipart((identity, b\"\", status))\n\n        except zmq.ContextTerminated:\n            logger.debug(\"ZMQ context terminated, exiting Mooncake sender thread.\")\n        except Exception as e:\n            logger.error(\"Error in Mooncake sender thread: %s. Exiting thread.\", str(e))\n        finally:\n            frontend.close()\n            backend.close()\n\n    def _sender_worker(\n        self, identity: bytes, metadata_bytes: bytes, worker_channel_path: str\n    ):\n        status = TRANS_ERROR\n\n        try:\n            metadata = self._decoder.decode(metadata_bytes)\n            self.send_kv_to_decode(metadata)\n            status = TRANS_DONE\n        except Exception as e:\n            logger.error(\"Error processing Mooncake handshake: %s\", e)\n        finally:\n            pusher = make_zmq_socket(self.zmq_ctx, worker_channel_path, zmq.PUSH)\n            try:\n                pusher.send_multipart((identity, status))\n            except zmq.ZMQError as e:\n                logger.warning(\n                    \"Internal error, maybe the server is shutting down. Error: %s\",\n                    e,\n                )\n            finally:\n                pusher.close()\n\n    def send_kv_to_decode(self, meta: MooncakeAgentMetadata):\n        send_reqs: list[tuple[ReqId, SendBlockMeta]] = []\n        with self.reqs_need_send.lock:\n            for req_id in meta.request_ids:\n                send_meta = self.reqs_need_send.reqs.get(req_id)\n                if send_meta is None:\n                    logger.warning(\"Request %s not found in reqs_need_send\", req_id)\n                    return\n                # Mark it as not expired. We will send it now.\n                send_meta.expire_time = float(\"inf\")\n                send_reqs.append((req_id, send_meta))\n\n        self._send_blocks(send_reqs, meta)\n\n        with self.reqs_need_send.lock:\n            for req_id in meta.request_ids:\n                del self.reqs_need_send.reqs[req_id]\n\n        with self.finished_sending_reqs.lock:\n            self.finished_sending_reqs.set.update(meta.request_ids)\n\n    def _send_blocks(\n        self,\n        send_reqs: list[tuple[ReqId, SendBlockMeta]],\n        agent_meta: MooncakeAgentMetadata,\n    ):\n        src_ptrs = []\n        dst_ptrs = []\n        lengths = []\n        local_base_addr = self.kv_caches_base_addr\n        remote_base_addr = agent_meta.kv_caches_base_addr\n        block_len = self.block_len\n        remote_session = f\"{agent_meta.remote_hostname}:{agent_meta.remote_port}\"\n\n        assert len(send_reqs) == len(agent_meta.block_ids)\n        for (req_id, send_meta), remote_block_ids in zip(\n            send_reqs, agent_meta.block_ids\n        ):\n            send_meta.ready.wait()\n\n            num_remote_blocks = len(remote_block_ids)\n            if num_remote_blocks == 0:\n                continue\n\n            local_block_ids = send_meta.local_block_ids\n            # Partial prefix cache hit: just read uncomputed blocks.\n            num_local_blocks = len(local_block_ids)\n            assert num_local_blocks >= num_remote_blocks\n            if num_local_blocks > num_remote_blocks:\n                local_block_ids = local_block_ids[-num_remote_blocks:]\n\n            # Group by indices\n            group_local_block_ids, group_remote_block_ids = group_concurrent_contiguous(\n                local_block_ids, remote_block_ids\n            )\n\n            for local_layer_addr, remote_layer_addr in zip(\n                local_base_addr, remote_base_addr\n            ):\n                for group_local_block_id, group_remote_block_id in zip(\n                    group_local_block_ids, group_remote_block_ids\n                ):\n                    src_ptrs.append(\n                        local_layer_addr + group_local_block_id[0] * block_len\n                    )\n                    dst_ptrs.append(\n                        remote_layer_addr + group_remote_block_id[0] * block_len\n                    )\n                    lengths.append(block_len * len(group_local_block_id))\n\n            logger.debug(\n                \"Sending kv_caches for request %s (%d blocks) to %s\",\n                req_id,\n                num_remote_blocks,\n                remote_session,\n            )\n\n        start_time = time.perf_counter()\n        ret_value = self.engine.batch_transfer_sync_write(\n            remote_session, src_ptrs, dst_ptrs, lengths\n        )\n        if ret_value != 0:\n            raise RuntimeError(f\"Error in batch_transfer_sync_write: {ret_value}\")\n\n        logger.debug(\n            \"Sending to %s done, took %s\",\n            remote_session,\n            time.perf_counter() - start_time,\n        )\n\n    def register_kv_caches(self, kv_caches: dict[str, torch.Tensor]):\n        \"\"\"Register the KV Cache data in mooncake.\"\"\"\n\n        logger.info(\"Registering KV_Caches. use_mla: %s\", self.use_mla)\n\n        kv_data_ptrs = []\n        kv_data_lens = []\n        seen_base_addresses = []\n\n        split_k_and_v = self.kv_topo.split_k_and_v\n        tensor_size_bytes = None\n        for layer_name, cache_or_caches in kv_caches.items():\n            logger.debug(\n                \"registering layer %s with shape %s\", layer_name, cache_or_caches.shape\n            )\n            cache_list = cache_or_caches if split_k_and_v else [cache_or_caches]\n\n            for cache in cache_list:\n                base_addr = cache.data_ptr()\n                if base_addr in seen_base_addresses:\n                    continue\n\n                seen_base_addresses.append(base_addr)\n                curr_tensor_size_bytes = cache.nbytes\n\n                if tensor_size_bytes is None:\n                    tensor_size_bytes = curr_tensor_size_bytes\n                    self.num_blocks = cache.shape[0]\n\n                assert tensor_size_bytes == curr_tensor_size_bytes, (\n                    \"All kv cache tensors must have the same size\"\n                )\n                kernel_block_size = cache.shape[-2 if self.use_mla else -3]\n                assert self.block_size == kernel_block_size\n                kv_data_ptrs.append(base_addr)\n                kv_data_lens.append(tensor_size_bytes)\n\n        self.kv_caches_base_addr = seen_base_addresses\n\n        ret_value = self.engine.batch_register_memory(kv_data_ptrs, kv_data_lens)\n        if ret_value != 0:\n            raise RuntimeError(\"Mooncake batch memory registration failed.\")\n\n        assert tensor_size_bytes is not None\n        assert self.num_blocks != 0\n        assert tensor_size_bytes % self.num_blocks == 0\n        self.block_len = tensor_size_bytes // self.num_blocks\n        self.device_kv_caches = kv_caches\n        logger.debug(\n            \"registered num_blocks=%d block_len=%d\", self.num_blocks, self.block_len\n        )\n\n        # No need to launch server for D node.\n        if self.kv_role == \"kv_consumer\":\n            return\n\n        ready_event = threading.Event()\n        self._mooncake_sender_t = threading.Thread(\n            target=self._mooncake_sender,\n            args=(ready_event, self.side_channel_port, self.tp_rank),\n            daemon=True,\n            name=\"mooncake_sender\",\n        )\n        self._mooncake_sender_t.start()\n        ready_event.wait()  # Wait for listener ZMQ socket to be ready.\n\n    async def fetch_finished_recving_reqs(self) -> set[ReqId]:\n        async with self.finished_recving_reqs.lock:\n            finished_recving_reqs = self.finished_recving_reqs.set\n            self.finished_recving_reqs.set = set()\n        return finished_recving_reqs\n\n    def get_finished(self) -> tuple[set[str] | None, set[str] | None]:\n        \"\"\"\n        Get requests that are done sending or recving on this specific worker.\n        The scheduler process (via the MultiprocExecutor) will use this output\n        to track which workers are done.\n        \"\"\"\n        fut = None\n        if self.kv_role != \"kv_producer\":\n            fut = asyncio.run_coroutine_threadsafe(\n                self.fetch_finished_recving_reqs(), self.receiver_loop\n            )\n\n        if self.kv_role != \"kv_consumer\":\n            with self.finished_sending_reqs.lock:\n                finished_sending_reqs = self.finished_sending_reqs.set\n                self.finished_sending_reqs.set = set()\n        else:\n            finished_sending_reqs = set()\n\n        finished_recving_reqs = fut.result() if fut else set()\n\n        if finished_sending_reqs or finished_recving_reqs:\n            logger.debug(\n                \"Rank %s, get_finished: %s requests done sending \"\n                \"and %s requests done recving\",\n                self.tp_rank,\n                len(finished_sending_reqs),\n                len(finished_recving_reqs),\n            )\n\n        # Handle timeout to avoid stranding blocks on remote.\n        now = time.perf_counter()\n        with self.reqs_need_send.lock:\n            expired_reqs = [\n                req_id\n                for req_id, send_meta in self.reqs_need_send.reqs.items()\n                if send_meta.expire_time < now\n            ]\n            for req_id in expired_reqs:\n                logger.warning(\n                    \"Request %s timed out after %d seconds without \"\n                    \"being sent. Freeing its blocks on the producer side.\",\n                    req_id,\n                    envs.VLLM_MOONCAKE_ABORT_REQUEST_TIMEOUT,\n                )\n                del self.reqs_need_send.reqs[req_id]\n            if expired_reqs:\n                finished_sending_reqs.update(expired_reqs)\n\n        return finished_sending_reqs or None, finished_recving_reqs or None\n\n    async def receive_kv(self, path: str, req_blocks: list[tuple[str, list[int]]]):\n        req_ids, block_ids = map(list, zip(*req_blocks))\n        metadata = MooncakeAgentMetadata(\n            remote_hostname=self.hostname,\n            remote_port=self.rpc_port,\n            request_ids=req_ids,\n            kv_caches_base_addr=self.kv_caches_base_addr,\n            block_ids=block_ids,\n        )\n\n        encoded_data = self._encoder.encode(metadata)\n        logger.debug(\n            \"Size of encoded MooncakeAgentMetadata: %d bytes\", len(encoded_data)\n        )\n        logger.debug(\"Sending kv transfer request for %s on path: %s\", req_ids, path)\n\n        # Send query for the request.\n        sock: zmq.asyncio.Socket = make_zmq_socket(\n            self.async_zmq_ctx, path, zmq.REQ, bind=False, linger=0\n        )\n        sock.setsockopt(zmq.RCVTIMEO, 60000)\n        try:\n            await sock.send(encoded_data)\n            ret_msg = await sock.recv()\n            if ret_msg != TRANS_DONE:\n                logger.error(\n                    \"Error happens during tranfering kvcache for %s, see logs in prefiller.\",  # noqa: E501\n                    req_ids,\n                )\n                return\n        except zmq.ContextTerminated:\n            logger.debug(\"ZMQ context terminated, exiting Mooncake receiver thread.\")\n        except Exception as e:\n            logger.error(\"MooncakeAgentMetadata transfer failed for %s: %s\", req_ids, e)\n            return\n        finally:\n            sock.close()\n\n        async with self.finished_recving_reqs.lock:\n            self.finished_recving_reqs.set.update(req_ids)\n\n        logger.debug(\"pulling kv_caches for %s finished\", req_ids)\n\n    def group_kv_pull(self, metadata: MooncakeConnectorMetadata):\n        kv_pulls = defaultdict(list)\n        for req_id, meta in metadata.reqs_to_recv.items():\n            logger.debug(\n                \"start_load_kv for request %s from remote engine. \"\n                \"Num local_block_ids: %s.\",\n                req_id,\n                len(meta.local_block_ids),\n            )\n            path = make_zmq_path(\n                \"tcp\", meta.remote_host, meta.remote_port + self.tp_rank\n            )\n            kv_pulls[path].append((req_id, meta.local_block_ids))\n\n        return kv_pulls\n\n    def start_load_kv(self, metadata: MooncakeConnectorMetadata):\n        if self.kv_role != \"kv_producer\":\n            kv_pulls = self.group_kv_pull(metadata)\n            for path, req_blocks in kv_pulls.items():\n                asyncio.run_coroutine_threadsafe(\n                    self.receive_kv(path, req_blocks), self.receiver_loop\n                )\n\n        if self.kv_role != \"kv_consumer\":\n            with self.reqs_need_send.lock:\n                for req_id, block_ids in metadata.reqs_to_send.items():\n                    if block_ids:\n                        # Already gone through request_finished()\n                        send_meta = self.reqs_need_send.reqs[req_id]\n                        send_meta.local_block_ids = block_ids\n                        send_meta.ready.set()\n                        send_meta.expire_time = (\n                            time.perf_counter()\n                            + envs.VLLM_MOONCAKE_ABORT_REQUEST_TIMEOUT\n                        )\n                    else:\n                        # From update_state_after_alloc(),\n                        # but not reach request_finished() yet\n                        self.reqs_need_send.reqs[req_id] = SendBlockMeta(\n                            local_block_ids=[], ready=threading.Event()\n                        )",
      "language": "python"
    },
    {
      "code": "_block_size: dict[EngineId, int] = {engine_id: block_size}",
      "language": "css"
    },
    {
      "code": "_block_size: dict[EngineId, int] = {engine_id: block_size}",
      "language": "css"
    },
    {
      "code": "_decoder = Decoder(MooncakeAgentMetadata)",
      "language": "unknown"
    },
    {
      "code": "_decoder = Decoder(MooncakeAgentMetadata)",
      "language": "unknown"
    },
    {
      "code": "_encoder = Encoder()",
      "language": "unknown"
    },
    {
      "code": "_encoder = Encoder()",
      "language": "unknown"
    },
    {
      "code": "_mooncake_receiver_t = Thread(\n    target=_receiver_loop,\n    args=(receiver_loop,),\n    daemon=True,\n)",
      "language": "unknown"
    },
    {
      "code": "_mooncake_receiver_t = Thread(\n    target=_receiver_loop,\n    args=(receiver_loop,),\n    daemon=True,\n)",
      "language": "unknown"
    },
    {
      "code": "_mooncake_sender_t: Thread | None = None",
      "language": "yaml"
    },
    {
      "code": "_mooncake_sender_t: Thread | None = None",
      "language": "yaml"
    },
    {
      "code": "_sender_executor = ThreadPoolExecutor(\n    max_workers=num_workers,\n    thread_name_prefix=\"vllm-mooncake-sender\",\n)",
      "language": "unknown"
    },
    {
      "code": "_sender_executor = ThreadPoolExecutor(\n    max_workers=num_workers,\n    thread_name_prefix=\"vllm-mooncake-sender\",\n)",
      "language": "unknown"
    },
    {
      "code": "_tp_size: dict[EngineId, int] = {engine_id: world_size}",
      "language": "css"
    },
    {
      "code": "_tp_size: dict[EngineId, int] = {engine_id: world_size}",
      "language": "css"
    },
    {
      "code": "_use_pallas = _use_pallas",
      "language": "unknown"
    },
    {
      "code": "_use_pallas = _use_pallas",
      "language": "unknown"
    },
    {
      "code": "async_zmq_ctx = Context()",
      "language": "unknown"
    },
    {
      "code": "async_zmq_ctx = Context()",
      "language": "unknown"
    },
    {
      "code": "backend_name = get_name()",
      "language": "unknown"
    },
    {
      "code": "backend_name = get_name()",
      "language": "unknown"
    },
    {
      "code": "block_size = block_size",
      "language": "unknown"
    },
    {
      "code": "block_size = block_size",
      "language": "unknown"
    },
    {
      "code": "cache_config = cache_config",
      "language": "unknown"
    },
    {
      "code": "cache_config = cache_config",
      "language": "unknown"
    },
    {
      "code": "device_kv_caches: dict[str, Tensor] = {}",
      "language": "yaml"
    },
    {
      "code": "device_kv_caches: dict[str, Tensor] = {}",
      "language": "yaml"
    },
    {
      "code": "engine = TransferEngine()",
      "language": "unknown"
    },
    {
      "code": "engine = TransferEngine()",
      "language": "unknown"
    },
    {
      "code": "engine_id: EngineId = engine_id",
      "language": "typescript"
    },
    {
      "code": "engine_id: EngineId = engine_id",
      "language": "typescript"
    },
    {
      "code": "finished_recving_reqs: FinishedReceiveReqSet = (\n    FinishedReceiveReqSet(set(), Lock())\n)",
      "language": "typescript"
    },
    {
      "code": "finished_recving_reqs: FinishedReceiveReqSet = (\n    FinishedReceiveReqSet(set(), Lock())\n)",
      "language": "typescript"
    },
    {
      "code": "finished_sending_reqs: FinishedSendReqSet = (\n    FinishedSendReqSet(set(), Lock())\n)",
      "language": "typescript"
    },
    {
      "code": "finished_sending_reqs: FinishedSendReqSet = (\n    FinishedSendReqSet(set(), Lock())\n)",
      "language": "typescript"
    },
    {
      "code": "hostname = get_ip()",
      "language": "unknown"
    },
    {
      "code": "hostname = get_ip()",
      "language": "unknown"
    },
    {
      "code": "kv_cache_layout = get_kv_cache_layout()",
      "language": "unknown"
    },
    {
      "code": "kv_cache_layout = get_kv_cache_layout()",
      "language": "unknown"
    },
    {
      "code": "kv_caches_base_addr: list[int] = []",
      "language": "yaml"
    },
    {
      "code": "kv_caches_base_addr: list[int] = []",
      "language": "yaml"
    },
    {
      "code": "kv_role = kv_role",
      "language": "unknown"
    },
    {
      "code": "kv_role = kv_role",
      "language": "unknown"
    },
    {
      "code": "kv_topo = TpKVTopology(\n    tp_rank=tp_rank,\n    engine_id=engine_id,\n    remote_tp_size=_tp_size,\n    remote_block_size=_block_size,\n    is_mla=use_mla,\n    total_num_kv_heads=get_total_num_kv_heads(),\n    attn_backend=backend,\n)",
      "language": "unknown"
    },
    {
      "code": "kv_topo = TpKVTopology(\n    tp_rank=tp_rank,\n    engine_id=engine_id,\n    remote_tp_size=_tp_size,\n    remote_block_size=_block_size,\n    is_mla=use_mla,\n    total_num_kv_heads=get_total_num_kv_heads(),\n    attn_backend=backend,\n)",
      "language": "unknown"
    },
    {
      "code": "model_config = model_config",
      "language": "unknown"
    },
    {
      "code": "model_config = model_config",
      "language": "unknown"
    },
    {
      "code": "num_blocks = 0",
      "language": "unknown"
    },
    {
      "code": "num_blocks = 0",
      "language": "unknown"
    },
    {
      "code": "num_workers = get('num_workers', 10)",
      "language": "unknown"
    },
    {
      "code": "num_workers = get('num_workers', 10)",
      "language": "unknown"
    },
    {
      "code": "receiver_loop = new_event_loop()",
      "language": "unknown"
    },
    {
      "code": "receiver_loop = new_event_loop()",
      "language": "unknown"
    },
    {
      "code": "reqs_need_send: SendReqMeta = SendReqMeta(\n    reqs={}, lock=Lock()\n)",
      "language": "typescript"
    },
    {
      "code": "reqs_need_send: SendReqMeta = SendReqMeta(\n    reqs={}, lock=Lock()\n)",
      "language": "typescript"
    },
    {
      "code": "rpc_port = get_rpc_port()",
      "language": "unknown"
    },
    {
      "code": "rpc_port = get_rpc_port()",
      "language": "unknown"
    },
    {
      "code": "side_channel_port: int = get_mooncake_side_channel_port(\n    vllm_config\n)",
      "language": "typescript"
    },
    {
      "code": "side_channel_port: int = get_mooncake_side_channel_port(\n    vllm_config\n)",
      "language": "typescript"
    },
    {
      "code": "tp_group = get_tp_group()",
      "language": "unknown"
    },
    {
      "code": "tp_group = get_tp_group()",
      "language": "unknown"
    },
    {
      "code": "tp_rank = get_tensor_model_parallel_rank()",
      "language": "unknown"
    },
    {
      "code": "tp_rank = get_tensor_model_parallel_rank()",
      "language": "unknown"
    },
    {
      "code": "use_mla = use_mla",
      "language": "unknown"
    },
    {
      "code": "use_mla = use_mla",
      "language": "unknown"
    },
    {
      "code": "vllm_config = vllm_config",
      "language": "unknown"
    },
    {
      "code": "vllm_config = vllm_config",
      "language": "unknown"
    },
    {
      "code": "world_size = get_tensor_model_parallel_world_size()",
      "language": "unknown"
    },
    {
      "code": "world_size = get_tensor_model_parallel_world_size()",
      "language": "unknown"
    },
    {
      "code": "zmq_ctx = Context()",
      "language": "unknown"
    },
    {
      "code": "zmq_ctx = Context()",
      "language": "unknown"
    },
    {
      "code": "def __del__(self):\n    self.shutdown()",
      "language": "python"
    },
    {
      "code": "def __del__(self):\n    self.shutdown()",
      "language": "python"
    },
    {
      "code": "__init__(vllm_config: VllmConfig, engine_id: str)",
      "language": "python"
    },
    {
      "code": "__init__(vllm_config: VllmConfig, engine_id: str)",
      "language": "python"
    },
    {
      "code": "404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n500\n501\n502\n503\n504\n505\n506\n507",
      "language": "unknown"
    },
    {
      "code": "def __init__(self, vllm_config: VllmConfig, engine_id: str):\n    logger.info(\"Initializing Mooncake Transfer Engine worker %s\", engine_id)\n\n    self.vllm_config = vllm_config\n\n    self.engine = TransferEngine()\n    self.hostname = get_ip()\n    protocol = self.vllm_config.kv_transfer_config.kv_connector_extra_config.get(  # type: ignore[union-attr]\n        \"mooncake_protocol\", \"rdma\"\n    )\n    logger.info(\n        \"The Mooncake Transfer Engine is using %s as its protocol.\", protocol\n    )\n    ret_value = self.engine.initialize(self.hostname, \"P2PHANDSHAKE\", protocol, \"\")\n    if ret_value != 0:\n        raise RuntimeError(\"Mooncake Transfer Engine initialization failed.\")\n\n    self.rpc_port = self.engine.get_rpc_port()\n\n    logger.debug(\n        \"Mooncake Transfer Engine initialized at %s:%d\",\n        self.hostname,\n        self.rpc_port,\n    )\n\n    # Mooncake handshake port.\n    self.side_channel_port: int = get_mooncake_side_channel_port(vllm_config)\n\n    self.engine_id: EngineId = engine_id\n    self.tp_rank = get_tensor_model_parallel_rank()\n    self.world_size = get_tensor_model_parallel_world_size()\n    self.tp_group = get_tp_group()\n    self.num_blocks = 0\n\n    assert vllm_config.kv_transfer_config\n    self.kv_role = vllm_config.kv_transfer_config.kv_role\n    self.num_workers = vllm_config.kv_transfer_config.kv_connector_extra_config.get(\n        \"num_workers\", 10\n    )\n\n    self.kv_caches_base_addr: list[int] = []\n    self.device_kv_caches: dict[str, torch.Tensor] = {}\n    self.reqs_need_send: SendReqMeta = SendReqMeta(reqs={}, lock=threading.Lock())\n\n    # For kv_both, we will act both prefiller and decoder.\n    if self.kv_role != \"kv_consumer\":\n        # Background thread for sending kvcaches to D.\n        self._mooncake_sender_t: threading.Thread | None = None\n        # Background thread for processing new sending requests.\n        self._sender_executor = ThreadPoolExecutor(\n            max_workers=self.num_workers, thread_name_prefix=\"vllm-mooncake-sender\"\n        )\n        logger.debug(\n            \"Mooncake Prefiller: use %d workers to send kvcaches\", self.num_workers\n        )\n    if self.kv_role != \"kv_producer\":\n        self.receiver_loop = asyncio.new_event_loop()\n        self._mooncake_receiver_t = threading.Thread(\n            target=self._receiver_loop, args=(self.receiver_loop,), daemon=True\n        )\n        self._mooncake_receiver_t.start()\n        logger.debug(\"Mooncake Decoder: start receiver thread\")\n\n    self.finished_sending_reqs: FinishedSendReqSet = FinishedSendReqSet(\n        set(), threading.Lock()\n    )\n    self.finished_recving_reqs: FinishedReceiveReqSet = FinishedReceiveReqSet(\n        set(), asyncio.Lock()\n    )\n\n    self.block_size = vllm_config.cache_config.block_size\n    self.model_config = vllm_config.model_config\n    self.cache_config = vllm_config.cache_config\n    self.use_mla = self.model_config.use_mla\n\n    backend = get_attn_backend(\n        self.model_config.get_head_size(),\n        self.model_config.dtype,\n        self.cache_config.cache_dtype,\n        self.block_size,\n        use_mla=self.use_mla,\n    )\n    self.backend_name = backend.get_name()\n    self.kv_cache_layout = get_kv_cache_layout()\n    logger.debug(\"Detected attention backend %s\", self.backend_name)\n    logger.debug(\"Detected kv cache layout %s\", self.kv_cache_layout)\n\n    self._tp_size: dict[EngineId, int] = {self.engine_id: self.world_size}\n    self._block_size: dict[EngineId, int] = {self.engine_id: self.block_size}\n    self.kv_topo = TpKVTopology(\n        tp_rank=self.tp_rank,\n        engine_id=self.engine_id,\n        remote_tp_size=self._tp_size,  # shared state\n        remote_block_size=self._block_size,  # shared state\n        is_mla=self.use_mla,\n        total_num_kv_heads=self.model_config.get_total_num_kv_heads(),\n        attn_backend=backend,\n    )\n    self._use_pallas = self.kv_topo._use_pallas\n\n    self.zmq_ctx = zmq.Context()\n    self.async_zmq_ctx = zmq.asyncio.Context()\n    self._encoder = msgspec.msgpack.Encoder()\n    self._decoder = msgspec.msgpack.Decoder(MooncakeAgentMetadata)",
      "language": "python"
    },
    {
      "code": "def __init__(self, vllm_config: VllmConfig, engine_id: str):\n    logger.info(\"Initializing Mooncake Transfer Engine worker %s\", engine_id)\n\n    self.vllm_config = vllm_config\n\n    self.engine = TransferEngine()\n    self.hostname = get_ip()\n    protocol = self.vllm_config.kv_transfer_config.kv_connector_extra_config.get(  # type: ignore[union-attr]\n        \"mooncake_protocol\", \"rdma\"\n    )\n    logger.info(\n        \"The Mooncake Transfer Engine is using %s as its protocol.\", protocol\n    )\n    ret_value = self.engine.initialize(self.hostname, \"P2PHANDSHAKE\", protocol, \"\")\n    if ret_value != 0:\n        raise RuntimeError(\"Mooncake Transfer Engine initialization failed.\")\n\n    self.rpc_port = self.engine.get_rpc_port()\n\n    logger.debug(\n        \"Mooncake Transfer Engine initialized at %s:%d\",\n        self.hostname,\n        self.rpc_port,\n    )\n\n    # Mooncake handshake port.\n    self.side_channel_port: int = get_mooncake_side_channel_port(vllm_config)\n\n    self.engine_id: EngineId = engine_id\n    self.tp_rank = get_tensor_model_parallel_rank()\n    self.world_size = get_tensor_model_parallel_world_size()\n    self.tp_group = get_tp_group()\n    self.num_blocks = 0\n\n    assert vllm_config.kv_transfer_config\n    self.kv_role = vllm_config.kv_transfer_config.kv_role\n    self.num_workers = vllm_config.kv_transfer_config.kv_connector_extra_config.get(\n        \"num_workers\", 10\n    )\n\n    self.kv_caches_base_addr: list[int] = []\n    self.device_kv_caches: dict[str, torch.Tensor] = {}\n    self.reqs_need_send: SendReqMeta = SendReqMeta(reqs={}, lock=threading.Lock())\n\n    # For kv_both, we will act both prefiller and decoder.\n    if self.kv_role != \"kv_consumer\":\n        # Background thread for sending kvcaches to D.\n        self._mooncake_sender_t: threading.Thread | None = None\n        # Background thread for processing new sending requests.\n        self._sender_executor = ThreadPoolExecutor(\n            max_workers=self.num_workers, thread_name_prefix=\"vllm-mooncake-sender\"\n        )\n        logger.debug(\n            \"Mooncake Prefiller: use %d workers to send kvcaches\", self.num_workers\n        )\n    if self.kv_role != \"kv_producer\":\n        self.receiver_loop = asyncio.new_event_loop()\n        self._mooncake_receiver_t = threading.Thread(\n            target=self._receiver_loop, args=(self.receiver_loop,), daemon=True\n        )\n        self._mooncake_receiver_t.start()\n        logger.debug(\"Mooncake Decoder: start receiver thread\")\n\n    self.finished_sending_reqs: FinishedSendReqSet = FinishedSendReqSet(\n        set(), threading.Lock()\n    )\n    self.finished_recving_reqs: FinishedReceiveReqSet = FinishedReceiveReqSet(\n        set(), asyncio.Lock()\n    )\n\n    self.block_size = vllm_config.cache_config.block_size\n    self.model_config = vllm_config.model_config\n    self.cache_config = vllm_config.cache_config\n    self.use_mla = self.model_config.use_mla\n\n    backend = get_attn_backend(\n        self.model_config.get_head_size(),\n        self.model_config.dtype,\n        self.cache_config.cache_dtype,\n        self.block_size,\n        use_mla=self.use_mla,\n    )\n    self.backend_name = backend.get_name()\n    self.kv_cache_layout = get_kv_cache_layout()\n    logger.debug(\"Detected attention backend %s\", self.backend_name)\n    logger.debug(\"Detected kv cache layout %s\", self.kv_cache_layout)\n\n    self._tp_size: dict[EngineId, int] = {self.engine_id: self.world_size}\n    self._block_size: dict[EngineId, int] = {self.engine_id: self.block_size}\n    self.kv_topo = TpKVTopology(\n        tp_rank=self.tp_rank,\n        engine_id=self.engine_id,\n        remote_tp_size=self._tp_size,  # shared state\n        remote_block_size=self._block_size,  # shared state\n        is_mla=self.use_mla,\n        total_num_kv_heads=self.model_config.get_total_num_kv_heads(),\n        attn_backend=backend,\n    )\n    self._use_pallas = self.kv_topo._use_pallas\n\n    self.zmq_ctx = zmq.Context()\n    self.async_zmq_ctx = zmq.asyncio.Context()\n    self._encoder = msgspec.msgpack.Encoder()\n    self._decoder = msgspec.msgpack.Decoder(MooncakeAgentMetadata)",
      "language": "python"
    },
    {
      "code": "_mooncake_sender(\n    ready_event: Event, base_port: int, tp_rank: int\n)",
      "language": "yaml"
    },
    {
      "code": "_mooncake_sender(\n    ready_event: Event, base_port: int, tp_rank: int\n)",
      "language": "yaml"
    },
    {
      "code": "528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572",
      "language": "unknown"
    },
    {
      "code": "def _mooncake_sender(\n    self, ready_event: threading.Event, base_port: int, tp_rank: int\n):\n    \"\"\"\n    Background thread that listens for Mooncake requests, dispatches them\n    to a thread pool, and sends acknowledgments upon completion.\n    \"\"\"\n\n    frontend_path = make_zmq_path(\"tcp\", self.hostname, base_port + tp_rank)\n    frontend = make_zmq_socket(self.zmq_ctx, frontend_path, zmq.ROUTER)\n    logger.debug(\"Mooncake sender starting listening on path: %s\", frontend_path)\n\n    backend_path = make_zmq_path(\"inproc\", str(uuid.uuid4()))\n    backend = make_zmq_socket(self.zmq_ctx, backend_path, zmq.PULL)\n\n    poller = zmq.Poller()\n    poller.register(frontend, zmq.POLLIN)\n    poller.register(backend, zmq.POLLIN)\n\n    ready_event.set()\n\n    try:\n        while True:\n            sockets = dict(poller.poll())\n\n            if frontend in sockets:\n                identity, _, metadata_bytes = frontend.recv_multipart()\n                self._sender_executor.submit(\n                    self._sender_worker,\n                    identity,\n                    metadata_bytes,\n                    backend_path,\n                )\n\n            if backend in sockets:\n                identity, status = backend.recv_multipart()\n                frontend.send_multipart((identity, b\"\", status))\n\n    except zmq.ContextTerminated:\n        logger.debug(\"ZMQ context terminated, exiting Mooncake sender thread.\")\n    except Exception as e:\n        logger.error(\"Error in Mooncake sender thread: %s. Exiting thread.\", str(e))\n    finally:\n        frontend.close()\n        backend.close()",
      "language": "python"
    },
    {
      "code": "def _mooncake_sender(\n    self, ready_event: threading.Event, base_port: int, tp_rank: int\n):\n    \"\"\"\n    Background thread that listens for Mooncake requests, dispatches them\n    to a thread pool, and sends acknowledgments upon completion.\n    \"\"\"\n\n    frontend_path = make_zmq_path(\"tcp\", self.hostname, base_port + tp_rank)\n    frontend = make_zmq_socket(self.zmq_ctx, frontend_path, zmq.ROUTER)\n    logger.debug(\"Mooncake sender starting listening on path: %s\", frontend_path)\n\n    backend_path = make_zmq_path(\"inproc\", str(uuid.uuid4()))\n    backend = make_zmq_socket(self.zmq_ctx, backend_path, zmq.PULL)\n\n    poller = zmq.Poller()\n    poller.register(frontend, zmq.POLLIN)\n    poller.register(backend, zmq.POLLIN)\n\n    ready_event.set()\n\n    try:\n        while True:\n            sockets = dict(poller.poll())\n\n            if frontend in sockets:\n                identity, _, metadata_bytes = frontend.recv_multipart()\n                self._sender_executor.submit(\n                    self._sender_worker,\n                    identity,\n                    metadata_bytes,\n                    backend_path,\n                )\n\n            if backend in sockets:\n                identity, status = backend.recv_multipart()\n                frontend.send_multipart((identity, b\"\", status))\n\n    except zmq.ContextTerminated:\n        logger.debug(\"ZMQ context terminated, exiting Mooncake sender thread.\")\n    except Exception as e:\n        logger.error(\"Error in Mooncake sender thread: %s. Exiting thread.\", str(e))\n    finally:\n        frontend.close()\n        backend.close()",
      "language": "python"
    },
    {
      "code": "_receiver_loop(loop: AbstractEventLoop)",
      "language": "unknown"
    },
    {
      "code": "_receiver_loop(loop: AbstractEventLoop)",
      "language": "unknown"
    },
    {
      "code": "524\n525\n526",
      "language": "unknown"
    },
    {
      "code": "def _receiver_loop(self, loop: asyncio.AbstractEventLoop):\n    asyncio.set_event_loop(loop)\n    loop.run_forever()",
      "language": "python"
    },
    {
      "code": "def _receiver_loop(self, loop: asyncio.AbstractEventLoop):\n    asyncio.set_event_loop(loop)\n    loop.run_forever()",
      "language": "python"
    },
    {
      "code": "_send_blocks(\n    send_reqs: list[tuple[ReqId, SendBlockMeta]],\n    agent_meta: MooncakeAgentMetadata,\n)",
      "language": "yaml"
    },
    {
      "code": "_send_blocks(\n    send_reqs: list[tuple[ReqId, SendBlockMeta]],\n    agent_meta: MooncakeAgentMetadata,\n)",
      "language": "yaml"
    },
    {
      "code": "618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685",
      "language": "unknown"
    },
    {
      "code": "def _send_blocks(\n    self,\n    send_reqs: list[tuple[ReqId, SendBlockMeta]],\n    agent_meta: MooncakeAgentMetadata,\n):\n    src_ptrs = []\n    dst_ptrs = []\n    lengths = []\n    local_base_addr = self.kv_caches_base_addr\n    remote_base_addr = agent_meta.kv_caches_base_addr\n    block_len = self.block_len\n    remote_session = f\"{agent_meta.remote_hostname}:{agent_meta.remote_port}\"\n\n    assert len(send_reqs) == len(agent_meta.block_ids)\n    for (req_id, send_meta), remote_block_ids in zip(\n        send_reqs, agent_meta.block_ids\n    ):\n        send_meta.ready.wait()\n\n        num_remote_blocks = len(remote_block_ids)\n        if num_remote_blocks == 0:\n            continue\n\n        local_block_ids = send_meta.local_block_ids\n        # Partial prefix cache hit: just read uncomputed blocks.\n        num_local_blocks = len(local_block_ids)\n        assert num_local_blocks >= num_remote_blocks\n        if num_local_blocks > num_remote_blocks:\n            local_block_ids = local_block_ids[-num_remote_blocks:]\n\n        # Group by indices\n        group_local_block_ids, group_remote_block_ids = group_concurrent_contiguous(\n            local_block_ids, remote_block_ids\n        )\n\n        for local_layer_addr, remote_layer_addr in zip(\n            local_base_addr, remote_base_addr\n        ):\n            for group_local_block_id, group_remote_block_id in zip(\n                group_local_block_ids, group_remote_block_ids\n            ):\n                src_ptrs.append(\n                    local_layer_addr + group_local_block_id[0] * block_len\n                )\n                dst_ptrs.append(\n                    remote_layer_addr + group_remote_block_id[0] * block_len\n                )\n                lengths.append(block_len * len(group_local_block_id))\n\n        logger.debug(\n            \"Sending kv_caches for request %s (%d blocks) to %s\",\n            req_id,\n            num_remote_blocks,\n            remote_session,\n        )\n\n    start_time = time.perf_counter()\n    ret_value = self.engine.batch_transfer_sync_write(\n        remote_session, src_ptrs, dst_ptrs, lengths\n    )\n    if ret_value != 0:\n        raise RuntimeError(f\"Error in batch_transfer_sync_write: {ret_value}\")\n\n    logger.debug(\n        \"Sending to %s done, took %s\",\n        remote_session,\n        time.perf_counter() - start_time,\n    )",
      "language": "python"
    },
    {
      "code": "def _send_blocks(\n    self,\n    send_reqs: list[tuple[ReqId, SendBlockMeta]],\n    agent_meta: MooncakeAgentMetadata,\n):\n    src_ptrs = []\n    dst_ptrs = []\n    lengths = []\n    local_base_addr = self.kv_caches_base_addr\n    remote_base_addr = agent_meta.kv_caches_base_addr\n    block_len = self.block_len\n    remote_session = f\"{agent_meta.remote_hostname}:{agent_meta.remote_port}\"\n\n    assert len(send_reqs) == len(agent_meta.block_ids)\n    for (req_id, send_meta), remote_block_ids in zip(\n        send_reqs, agent_meta.block_ids\n    ):\n        send_meta.ready.wait()\n\n        num_remote_blocks = len(remote_block_ids)\n        if num_remote_blocks == 0:\n            continue\n\n        local_block_ids = send_meta.local_block_ids\n        # Partial prefix cache hit: just read uncomputed blocks.\n        num_local_blocks = len(local_block_ids)\n        assert num_local_blocks >= num_remote_blocks\n        if num_local_blocks > num_remote_blocks:\n            local_block_ids = local_block_ids[-num_remote_blocks:]\n\n        # Group by indices\n        group_local_block_ids, group_remote_block_ids = group_concurrent_contiguous(\n            local_block_ids, remote_block_ids\n        )\n\n        for local_layer_addr, remote_layer_addr in zip(\n            local_base_addr, remote_base_addr\n        ):\n            for group_local_block_id, group_remote_block_id in zip(\n                group_local_block_ids, group_remote_block_ids\n            ):\n                src_ptrs.append(\n                    local_layer_addr + group_local_block_id[0] * block_len\n                )\n                dst_ptrs.append(\n                    remote_layer_addr + group_remote_block_id[0] * block_len\n                )\n                lengths.append(block_len * len(group_local_block_id))\n\n        logger.debug(\n            \"Sending kv_caches for request %s (%d blocks) to %s\",\n            req_id,\n            num_remote_blocks,\n            remote_session,\n        )\n\n    start_time = time.perf_counter()\n    ret_value = self.engine.batch_transfer_sync_write(\n        remote_session, src_ptrs, dst_ptrs, lengths\n    )\n    if ret_value != 0:\n        raise RuntimeError(f\"Error in batch_transfer_sync_write: {ret_value}\")\n\n    logger.debug(\n        \"Sending to %s done, took %s\",\n        remote_session,\n        time.perf_counter() - start_time,\n    )",
      "language": "python"
    },
    {
      "code": "_sender_worker(\n    identity: bytes,\n    metadata_bytes: bytes,\n    worker_channel_path: str,\n)",
      "language": "yaml"
    },
    {
      "code": "_sender_worker(\n    identity: bytes,\n    metadata_bytes: bytes,\n    worker_channel_path: str,\n)",
      "language": "yaml"
    },
    {
      "code": "574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595",
      "language": "unknown"
    },
    {
      "code": "def _sender_worker(\n    self, identity: bytes, metadata_bytes: bytes, worker_channel_path: str\n):\n    status = TRANS_ERROR\n\n    try:\n        metadata = self._decoder.decode(metadata_bytes)\n        self.send_kv_to_decode(metadata)\n        status = TRANS_DONE\n    except Exception as e:\n        logger.error(\"Error processing Mooncake handshake: %s\", e)\n    finally:\n        pusher = make_zmq_socket(self.zmq_ctx, worker_channel_path, zmq.PUSH)\n        try:\n            pusher.send_multipart((identity, status))\n        except zmq.ZMQError as e:\n            logger.warning(\n                \"Internal error, maybe the server is shutting down. Error: %s\",\n                e,\n            )\n        finally:\n            pusher.close()",
      "language": "python"
    },
    {
      "code": "def _sender_worker(\n    self, identity: bytes, metadata_bytes: bytes, worker_channel_path: str\n):\n    status = TRANS_ERROR\n\n    try:\n        metadata = self._decoder.decode(metadata_bytes)\n        self.send_kv_to_decode(metadata)\n        status = TRANS_DONE\n    except Exception as e:\n        logger.error(\"Error processing Mooncake handshake: %s\", e)\n    finally:\n        pusher = make_zmq_socket(self.zmq_ctx, worker_channel_path, zmq.PUSH)\n        try:\n            pusher.send_multipart((identity, status))\n        except zmq.ZMQError as e:\n            logger.warning(\n                \"Internal error, maybe the server is shutting down. Error: %s\",\n                e,\n            )\n        finally:\n            pusher.close()",
      "language": "python"
    },
    {
      "code": "fetch_finished_recving_reqs() -> set[ReqId]",
      "language": "php"
    },
    {
      "code": "fetch_finished_recving_reqs() -> set[ReqId]",
      "language": "php"
    },
    {
      "code": "753\n754\n755\n756\n757",
      "language": "unknown"
    },
    {
      "code": "async def fetch_finished_recving_reqs(self) -> set[ReqId]:\n    async with self.finished_recving_reqs.lock:\n        finished_recving_reqs = self.finished_recving_reqs.set\n        self.finished_recving_reqs.set = set()\n    return finished_recving_reqs",
      "language": "python"
    },
    {
      "code": "async def fetch_finished_recving_reqs(self) -> set[ReqId]:\n    async with self.finished_recving_reqs.lock:\n        finished_recving_reqs = self.finished_recving_reqs.set\n        self.finished_recving_reqs.set = set()\n    return finished_recving_reqs",
      "language": "python"
    },
    {
      "code": "get_finished() -> tuple[set[str] | None, set[str] | None]",
      "language": "rust"
    },
    {
      "code": "get_finished() -> tuple[set[str] | None, set[str] | None]",
      "language": "rust"
    },
    {
      "code": "759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\n800\n801\n802\n803\n804\n805\n806\n807\n808",
      "language": "unknown"
    },
    {
      "code": "def get_finished(self) -> tuple[set[str] | None, set[str] | None]:\n    \"\"\"\n    Get requests that are done sending or recving on this specific worker.\n    The scheduler process (via the MultiprocExecutor) will use this output\n    to track which workers are done.\n    \"\"\"\n    fut = None\n    if self.kv_role != \"kv_producer\":\n        fut = asyncio.run_coroutine_threadsafe(\n            self.fetch_finished_recving_reqs(), self.receiver_loop\n        )\n\n    if self.kv_role != \"kv_consumer\":\n        with self.finished_sending_reqs.lock:\n            finished_sending_reqs = self.finished_sending_reqs.set\n            self.finished_sending_reqs.set = set()\n    else:\n        finished_sending_reqs = set()\n\n    finished_recving_reqs = fut.result() if fut else set()\n\n    if finished_sending_reqs or finished_recving_reqs:\n        logger.debug(\n            \"Rank %s, get_finished: %s requests done sending \"\n            \"and %s requests done recving\",\n            self.tp_rank,\n            len(finished_sending_reqs),\n            len(finished_recving_reqs),\n        )\n\n    # Handle timeout to avoid stranding blocks on remote.\n    now = time.perf_counter()\n    with self.reqs_need_send.lock:\n        expired_reqs = [\n            req_id\n            for req_id, send_meta in self.reqs_need_send.reqs.items()\n            if send_meta.expire_time < now\n        ]\n        for req_id in expired_reqs:\n            logger.warning(\n                \"Request %s timed out after %d seconds without \"\n                \"being sent. Freeing its blocks on the producer side.\",\n                req_id,\n                envs.VLLM_MOONCAKE_ABORT_REQUEST_TIMEOUT,\n            )\n            del self.reqs_need_send.reqs[req_id]\n        if expired_reqs:\n            finished_sending_reqs.update(expired_reqs)\n\n    return finished_sending_reqs or None, finished_recving_reqs or None",
      "language": "python"
    },
    {
      "code": "def get_finished(self) -> tuple[set[str] | None, set[str] | None]:\n    \"\"\"\n    Get requests that are done sending or recving on this specific worker.\n    The scheduler process (via the MultiprocExecutor) will use this output\n    to track which workers are done.\n    \"\"\"\n    fut = None\n    if self.kv_role != \"kv_producer\":\n        fut = asyncio.run_coroutine_threadsafe(\n            self.fetch_finished_recving_reqs(), self.receiver_loop\n        )\n\n    if self.kv_role != \"kv_consumer\":\n        with self.finished_sending_reqs.lock:\n            finished_sending_reqs = self.finished_sending_reqs.set\n            self.finished_sending_reqs.set = set()\n    else:\n        finished_sending_reqs = set()\n\n    finished_recving_reqs = fut.result() if fut else set()\n\n    if finished_sending_reqs or finished_recving_reqs:\n        logger.debug(\n            \"Rank %s, get_finished: %s requests done sending \"\n            \"and %s requests done recving\",\n            self.tp_rank,\n            len(finished_sending_reqs),\n            len(finished_recving_reqs),\n        )\n\n    # Handle timeout to avoid stranding blocks on remote.\n    now = time.perf_counter()\n    with self.reqs_need_send.lock:\n        expired_reqs = [\n            req_id\n            for req_id, send_meta in self.reqs_need_send.reqs.items()\n            if send_meta.expire_time < now\n        ]\n        for req_id in expired_reqs:\n            logger.warning(\n                \"Request %s timed out after %d seconds without \"\n                \"being sent. Freeing its blocks on the producer side.\",\n                req_id,\n                envs.VLLM_MOONCAKE_ABORT_REQUEST_TIMEOUT,\n            )\n            del self.reqs_need_send.reqs[req_id]\n        if expired_reqs:\n            finished_sending_reqs.update(expired_reqs)\n\n    return finished_sending_reqs or None, finished_recving_reqs or None",
      "language": "python"
    },
    {
      "code": "group_kv_pull(metadata: MooncakeConnectorMetadata)",
      "language": "unknown"
    },
    {
      "code": "group_kv_pull(metadata: MooncakeConnectorMetadata)",
      "language": "unknown"
    },
    {
      "code": "853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867",
      "language": "unknown"
    },
    {
      "code": "def group_kv_pull(self, metadata: MooncakeConnectorMetadata):\n    kv_pulls = defaultdict(list)\n    for req_id, meta in metadata.reqs_to_recv.items():\n        logger.debug(\n            \"start_load_kv for request %s from remote engine. \"\n            \"Num local_block_ids: %s.\",\n            req_id,\n            len(meta.local_block_ids),\n        )\n        path = make_zmq_path(\n            \"tcp\", meta.remote_host, meta.remote_port + self.tp_rank\n        )\n        kv_pulls[path].append((req_id, meta.local_block_ids))\n\n    return kv_pulls",
      "language": "python"
    },
    {
      "code": "def group_kv_pull(self, metadata: MooncakeConnectorMetadata):\n    kv_pulls = defaultdict(list)\n    for req_id, meta in metadata.reqs_to_recv.items():\n        logger.debug(\n            \"start_load_kv for request %s from remote engine. \"\n            \"Num local_block_ids: %s.\",\n            req_id,\n            len(meta.local_block_ids),\n        )\n        path = make_zmq_path(\n            \"tcp\", meta.remote_host, meta.remote_port + self.tp_rank\n        )\n        kv_pulls[path].append((req_id, meta.local_block_ids))\n\n    return kv_pulls",
      "language": "python"
    },
    {
      "code": "receive_kv(\n    path: str, req_blocks: list[tuple[str, list[int]]]\n)",
      "language": "yaml"
    },
    {
      "code": "receive_kv(\n    path: str, req_blocks: list[tuple[str, list[int]]]\n)",
      "language": "yaml"
    },
    {
      "code": "810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851",
      "language": "unknown"
    },
    {
      "code": "async def receive_kv(self, path: str, req_blocks: list[tuple[str, list[int]]]):\n    req_ids, block_ids = map(list, zip(*req_blocks))\n    metadata = MooncakeAgentMetadata(\n        remote_hostname=self.hostname,\n        remote_port=self.rpc_port,\n        request_ids=req_ids,\n        kv_caches_base_addr=self.kv_caches_base_addr,\n        block_ids=block_ids,\n    )\n\n    encoded_data = self._encoder.encode(metadata)\n    logger.debug(\n        \"Size of encoded MooncakeAgentMetadata: %d bytes\", len(encoded_data)\n    )\n    logger.debug(\"Sending kv transfer request for %s on path: %s\", req_ids, path)\n\n    # Send query for the request.\n    sock: zmq.asyncio.Socket = make_zmq_socket(\n        self.async_zmq_ctx, path, zmq.REQ, bind=False, linger=0\n    )\n    sock.setsockopt(zmq.RCVTIMEO, 60000)\n    try:\n        await sock.send(encoded_data)\n        ret_msg = await sock.recv()\n        if ret_msg != TRANS_DONE:\n            logger.error(\n                \"Error happens during tranfering kvcache for %s, see logs in prefiller.\",  # noqa: E501\n                req_ids,\n            )\n            return\n    except zmq.ContextTerminated:\n        logger.debug(\"ZMQ context terminated, exiting Mooncake receiver thread.\")\n    except Exception as e:\n        logger.error(\"MooncakeAgentMetadata transfer failed for %s: %s\", req_ids, e)\n        return\n    finally:\n        sock.close()\n\n    async with self.finished_recving_reqs.lock:\n        self.finished_recving_reqs.set.update(req_ids)\n\n    logger.debug(\"pulling kv_caches for %s finished\", req_ids)",
      "language": "python"
    },
    {
      "code": "async def receive_kv(self, path: str, req_blocks: list[tuple[str, list[int]]]):\n    req_ids, block_ids = map(list, zip(*req_blocks))\n    metadata = MooncakeAgentMetadata(\n        remote_hostname=self.hostname,\n        remote_port=self.rpc_port,\n        request_ids=req_ids,\n        kv_caches_base_addr=self.kv_caches_base_addr,\n        block_ids=block_ids,\n    )\n\n    encoded_data = self._encoder.encode(metadata)\n    logger.debug(\n        \"Size of encoded MooncakeAgentMetadata: %d bytes\", len(encoded_data)\n    )\n    logger.debug(\"Sending kv transfer request for %s on path: %s\", req_ids, path)\n\n    # Send query for the request.\n    sock: zmq.asyncio.Socket = make_zmq_socket(\n        self.async_zmq_ctx, path, zmq.REQ, bind=False, linger=0\n    )\n    sock.setsockopt(zmq.RCVTIMEO, 60000)\n    try:\n        await sock.send(encoded_data)\n        ret_msg = await sock.recv()\n        if ret_msg != TRANS_DONE:\n            logger.error(\n                \"Error happens during tranfering kvcache for %s, see logs in prefiller.\",  # noqa: E501\n                req_ids,\n            )\n            return\n    except zmq.ContextTerminated:\n        logger.debug(\"ZMQ context terminated, exiting Mooncake receiver thread.\")\n    except Exception as e:\n        logger.error(\"MooncakeAgentMetadata transfer failed for %s: %s\", req_ids, e)\n        return\n    finally:\n        sock.close()\n\n    async with self.finished_recving_reqs.lock:\n        self.finished_recving_reqs.set.update(req_ids)\n\n    logger.debug(\"pulling kv_caches for %s finished\", req_ids)",
      "language": "python"
    },
    {
      "code": "register_kv_caches(kv_caches: dict[str, Tensor])",
      "language": "unknown"
    },
    {
      "code": "register_kv_caches(kv_caches: dict[str, Tensor])",
      "language": "unknown"
    },
    {
      "code": "687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\n700\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751",
      "language": "unknown"
    },
    {
      "code": "def register_kv_caches(self, kv_caches: dict[str, torch.Tensor]):\n    \"\"\"Register the KV Cache data in mooncake.\"\"\"\n\n    logger.info(\"Registering KV_Caches. use_mla: %s\", self.use_mla)\n\n    kv_data_ptrs = []\n    kv_data_lens = []\n    seen_base_addresses = []\n\n    split_k_and_v = self.kv_topo.split_k_and_v\n    tensor_size_bytes = None\n    for layer_name, cache_or_caches in kv_caches.items():\n        logger.debug(\n            \"registering layer %s with shape %s\", layer_name, cache_or_caches.shape\n        )\n        cache_list = cache_or_caches if split_k_and_v else [cache_or_caches]\n\n        for cache in cache_list:\n            base_addr = cache.data_ptr()\n            if base_addr in seen_base_addresses:\n                continue\n\n            seen_base_addresses.append(base_addr)\n            curr_tensor_size_bytes = cache.nbytes\n\n            if tensor_size_bytes is None:\n                tensor_size_bytes = curr_tensor_size_bytes\n                self.num_blocks = cache.shape[0]\n\n            assert tensor_size_bytes == curr_tensor_size_bytes, (\n                \"All kv cache tensors must have the same size\"\n            )\n            kernel_block_size = cache.shape[-2 if self.use_mla else -3]\n            assert self.block_size == kernel_block_size\n            kv_data_ptrs.append(base_addr)\n            kv_data_lens.append(tensor_size_bytes)\n\n    self.kv_caches_base_addr = seen_base_addresses\n\n    ret_value = self.engine.batch_register_memory(kv_data_ptrs, kv_data_lens)\n    if ret_value != 0:\n        raise RuntimeError(\"Mooncake batch memory registration failed.\")\n\n    assert tensor_size_bytes is not None\n    assert self.num_blocks != 0\n    assert tensor_size_bytes % self.num_blocks == 0\n    self.block_len = tensor_size_bytes // self.num_blocks\n    self.device_kv_caches = kv_caches\n    logger.debug(\n        \"registered num_blocks=%d block_len=%d\", self.num_blocks, self.block_len\n    )\n\n    # No need to launch server for D node.\n    if self.kv_role == \"kv_consumer\":\n        return\n\n    ready_event = threading.Event()\n    self._mooncake_sender_t = threading.Thread(\n        target=self._mooncake_sender,\n        args=(ready_event, self.side_channel_port, self.tp_rank),\n        daemon=True,\n        name=\"mooncake_sender\",\n    )\n    self._mooncake_sender_t.start()\n    ready_event.wait()  # Wait for listener ZMQ socket to be ready.",
      "language": "python"
    },
    {
      "code": "def register_kv_caches(self, kv_caches: dict[str, torch.Tensor]):\n    \"\"\"Register the KV Cache data in mooncake.\"\"\"\n\n    logger.info(\"Registering KV_Caches. use_mla: %s\", self.use_mla)\n\n    kv_data_ptrs = []\n    kv_data_lens = []\n    seen_base_addresses = []\n\n    split_k_and_v = self.kv_topo.split_k_and_v\n    tensor_size_bytes = None\n    for layer_name, cache_or_caches in kv_caches.items():\n        logger.debug(\n            \"registering layer %s with shape %s\", layer_name, cache_or_caches.shape\n        )\n        cache_list = cache_or_caches if split_k_and_v else [cache_or_caches]\n\n        for cache in cache_list:\n            base_addr = cache.data_ptr()\n            if base_addr in seen_base_addresses:\n                continue\n\n            seen_base_addresses.append(base_addr)\n            curr_tensor_size_bytes = cache.nbytes\n\n            if tensor_size_bytes is None:\n                tensor_size_bytes = curr_tensor_size_bytes\n                self.num_blocks = cache.shape[0]\n\n            assert tensor_size_bytes == curr_tensor_size_bytes, (\n                \"All kv cache tensors must have the same size\"\n            )\n            kernel_block_size = cache.shape[-2 if self.use_mla else -3]\n            assert self.block_size == kernel_block_size\n            kv_data_ptrs.append(base_addr)\n            kv_data_lens.append(tensor_size_bytes)\n\n    self.kv_caches_base_addr = seen_base_addresses\n\n    ret_value = self.engine.batch_register_memory(kv_data_ptrs, kv_data_lens)\n    if ret_value != 0:\n        raise RuntimeError(\"Mooncake batch memory registration failed.\")\n\n    assert tensor_size_bytes is not None\n    assert self.num_blocks != 0\n    assert tensor_size_bytes % self.num_blocks == 0\n    self.block_len = tensor_size_bytes // self.num_blocks\n    self.device_kv_caches = kv_caches\n    logger.debug(\n        \"registered num_blocks=%d block_len=%d\", self.num_blocks, self.block_len\n    )\n\n    # No need to launch server for D node.\n    if self.kv_role == \"kv_consumer\":\n        return\n\n    ready_event = threading.Event()\n    self._mooncake_sender_t = threading.Thread(\n        target=self._mooncake_sender,\n        args=(ready_event, self.side_channel_port, self.tp_rank),\n        daemon=True,\n        name=\"mooncake_sender\",\n    )\n    self._mooncake_sender_t.start()\n    ready_event.wait()  # Wait for listener ZMQ socket to be ready.",
      "language": "python"
    },
    {
      "code": "send_kv_to_decode(meta: MooncakeAgentMetadata)",
      "language": "unknown"
    },
    {
      "code": "send_kv_to_decode(meta: MooncakeAgentMetadata)",
      "language": "unknown"
    },
    {
      "code": "597\n598\n599\n600\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616",
      "language": "unknown"
    },
    {
      "code": "def send_kv_to_decode(self, meta: MooncakeAgentMetadata):\n    send_reqs: list[tuple[ReqId, SendBlockMeta]] = []\n    with self.reqs_need_send.lock:\n        for req_id in meta.request_ids:\n            send_meta = self.reqs_need_send.reqs.get(req_id)\n            if send_meta is None:\n                logger.warning(\"Request %s not found in reqs_need_send\", req_id)\n                return\n            # Mark it as not expired. We will send it now.\n            send_meta.expire_time = float(\"inf\")\n            send_reqs.append((req_id, send_meta))\n\n    self._send_blocks(send_reqs, meta)\n\n    with self.reqs_need_send.lock:\n        for req_id in meta.request_ids:\n            del self.reqs_need_send.reqs[req_id]\n\n    with self.finished_sending_reqs.lock:\n        self.finished_sending_reqs.set.update(meta.request_ids)",
      "language": "python"
    },
    {
      "code": "def send_kv_to_decode(self, meta: MooncakeAgentMetadata):\n    send_reqs: list[tuple[ReqId, SendBlockMeta]] = []\n    with self.reqs_need_send.lock:\n        for req_id in meta.request_ids:\n            send_meta = self.reqs_need_send.reqs.get(req_id)\n            if send_meta is None:\n                logger.warning(\"Request %s not found in reqs_need_send\", req_id)\n                return\n            # Mark it as not expired. We will send it now.\n            send_meta.expire_time = float(\"inf\")\n            send_reqs.append((req_id, send_meta))\n\n    self._send_blocks(send_reqs, meta)\n\n    with self.reqs_need_send.lock:\n        for req_id in meta.request_ids:\n            del self.reqs_need_send.reqs[req_id]\n\n    with self.finished_sending_reqs.lock:\n        self.finished_sending_reqs.set.update(meta.request_ids)",
      "language": "python"
    },
    {
      "code": "512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522",
      "language": "unknown"
    },
    {
      "code": "def shutdown(self):\n    \"\"\"Cleanup background threads on destruction.\"\"\"\n    self.zmq_ctx.term()\n    self.async_zmq_ctx.term()\n    if self.kv_role != \"kv_consumer\":\n        self._sender_executor.shutdown(wait=False)\n        if self._mooncake_sender_t:\n            self._mooncake_sender_t.join()\n    if self.kv_role != \"kv_producer\" and self.receiver_loop.is_running():\n        self.receiver_loop.call_soon_threadsafe(self.receiver_loop.stop)\n        self._mooncake_receiver_t.join()",
      "language": "python"
    },
    {
      "code": "def shutdown(self):\n    \"\"\"Cleanup background threads on destruction.\"\"\"\n    self.zmq_ctx.term()\n    self.async_zmq_ctx.term()\n    if self.kv_role != \"kv_consumer\":\n        self._sender_executor.shutdown(wait=False)\n        if self._mooncake_sender_t:\n            self._mooncake_sender_t.join()\n    if self.kv_role != \"kv_producer\" and self.receiver_loop.is_running():\n        self.receiver_loop.call_soon_threadsafe(self.receiver_loop.stop)\n        self._mooncake_receiver_t.join()",
      "language": "python"
    },
    {
      "code": "start_load_kv(metadata: MooncakeConnectorMetadata)",
      "language": "unknown"
    },
    {
      "code": "start_load_kv(metadata: MooncakeConnectorMetadata)",
      "language": "unknown"
    },
    {
      "code": "869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894",
      "language": "unknown"
    },
    {
      "code": "def start_load_kv(self, metadata: MooncakeConnectorMetadata):\n    if self.kv_role != \"kv_producer\":\n        kv_pulls = self.group_kv_pull(metadata)\n        for path, req_blocks in kv_pulls.items():\n            asyncio.run_coroutine_threadsafe(\n                self.receive_kv(path, req_blocks), self.receiver_loop\n            )\n\n    if self.kv_role != \"kv_consumer\":\n        with self.reqs_need_send.lock:\n            for req_id, block_ids in metadata.reqs_to_send.items():\n                if block_ids:\n                    # Already gone through request_finished()\n                    send_meta = self.reqs_need_send.reqs[req_id]\n                    send_meta.local_block_ids = block_ids\n                    send_meta.ready.set()\n                    send_meta.expire_time = (\n                        time.perf_counter()\n                        + envs.VLLM_MOONCAKE_ABORT_REQUEST_TIMEOUT\n                    )\n                else:\n                    # From update_state_after_alloc(),\n                    # but not reach request_finished() yet\n                    self.reqs_need_send.reqs[req_id] = SendBlockMeta(\n                        local_block_ids=[], ready=threading.Event()\n                    )",
      "language": "python"
    },
    {
      "code": "def start_load_kv(self, metadata: MooncakeConnectorMetadata):\n    if self.kv_role != \"kv_producer\":\n        kv_pulls = self.group_kv_pull(metadata)\n        for path, req_blocks in kv_pulls.items():\n            asyncio.run_coroutine_threadsafe(\n                self.receive_kv(path, req_blocks), self.receiver_loop\n            )\n\n    if self.kv_role != \"kv_consumer\":\n        with self.reqs_need_send.lock:\n            for req_id, block_ids in metadata.reqs_to_send.items():\n                if block_ids:\n                    # Already gone through request_finished()\n                    send_meta = self.reqs_need_send.reqs[req_id]\n                    send_meta.local_block_ids = block_ids\n                    send_meta.ready.set()\n                    send_meta.expire_time = (\n                        time.perf_counter()\n                        + envs.VLLM_MOONCAKE_ABORT_REQUEST_TIMEOUT\n                    )\n                else:\n                    # From update_state_after_alloc(),\n                    # but not reach request_finished() yet\n                    self.reqs_need_send.reqs[req_id] = SendBlockMeta(\n                        local_block_ids=[], ready=threading.Event()\n                    )",
      "language": "python"
    },
    {
      "code": "76\n77\n78\n79\n80",
      "language": "unknown"
    },
    {
      "code": "@dataclass\nclass RecvReqMeta:\n    local_block_ids: list[int]\n    remote_host: str\n    remote_port: int",
      "language": "python"
    },
    {
      "code": "@dataclass\nclass RecvReqMeta:\n    local_block_ids: list[int]\n    remote_host: str\n    remote_port: int",
      "language": "python"
    },
    {
      "code": "local_block_ids: list[int]",
      "language": "yaml"
    },
    {
      "code": "local_block_ids: list[int]",
      "language": "yaml"
    },
    {
      "code": "remote_host: str",
      "language": "yaml"
    },
    {
      "code": "remote_host: str",
      "language": "yaml"
    },
    {
      "code": "remote_port: int",
      "language": "yaml"
    },
    {
      "code": "remote_port: int",
      "language": "yaml"
    },
    {
      "code": "__init__(\n    local_block_ids: list[int],\n    remote_host: str,\n    remote_port: int,\n) -> None",
      "language": "python"
    },
    {
      "code": "__init__(\n    local_block_ids: list[int],\n    remote_host: str,\n    remote_port: int,\n) -> None",
      "language": "python"
    },
    {
      "code": "83\n84\n85\n86\n87",
      "language": "unknown"
    },
    {
      "code": "@dataclass\nclass SendBlockMeta:\n    local_block_ids: list[int]\n    ready: threading.Event\n    expire_time: float = float(\"inf\")",
      "language": "python"
    },
    {
      "code": "@dataclass\nclass SendBlockMeta:\n    local_block_ids: list[int]\n    ready: threading.Event\n    expire_time: float = float(\"inf\")",
      "language": "python"
    },
    {
      "code": "expire_time: float = float('inf')",
      "language": "typescript"
    },
    {
      "code": "expire_time: float = float('inf')",
      "language": "typescript"
    },
    {
      "code": "local_block_ids: list[int]",
      "language": "yaml"
    },
    {
      "code": "local_block_ids: list[int]",
      "language": "yaml"
    },
    {
      "code": "ready: Event",
      "language": "yaml"
    },
    {
      "code": "ready: Event",
      "language": "yaml"
    },
    {
      "code": "__init__(\n    local_block_ids: list[int],\n    ready: Event,\n    expire_time: float = float(\"inf\"),\n) -> None",
      "language": "typescript"
    },
    {
      "code": "__init__(\n    local_block_ids: list[int],\n    ready: Event,\n    expire_time: float = float(\"inf\"),\n) -> None",
      "language": "typescript"
    },
    {
      "code": "90\n91\n92\n93",
      "language": "unknown"
    },
    {
      "code": "@dataclass\nclass SendReqMeta:\n    reqs: dict[ReqId, SendBlockMeta]\n    lock: threading.Lock",
      "language": "python"
    },
    {
      "code": "@dataclass\nclass SendReqMeta:\n    reqs: dict[ReqId, SendBlockMeta]\n    lock: threading.Lock",
      "language": "python"
    },
    {
      "code": "reqs: dict[ReqId, SendBlockMeta]",
      "language": "yaml"
    },
    {
      "code": "reqs: dict[ReqId, SendBlockMeta]",
      "language": "yaml"
    },
    {
      "code": "__init__(\n    reqs: dict[ReqId, SendBlockMeta], lock: Lock\n) -> None",
      "language": "python"
    },
    {
      "code": "__init__(\n    reqs: dict[ReqId, SendBlockMeta], lock: Lock\n) -> None",
      "language": "python"
    },
    {
      "code": "get_mooncake_side_channel_port(\n    vllm_config: VllmConfig,\n) -> int",
      "language": "php"
    },
    {
      "code": "get_mooncake_side_channel_port(\n    vllm_config: VllmConfig,\n) -> int",
      "language": "php"
    },
    {
      "code": "914\n915\n916\n917\n918\n919\n920",
      "language": "unknown"
    },
    {
      "code": "def get_mooncake_side_channel_port(vllm_config: VllmConfig) -> int:\n    # This logic is now centralized\n    return (\n        envs.VLLM_MOONCAKE_BOOTSTRAP_PORT\n        + vllm_config.parallel_config.data_parallel_rank\n        * vllm_config.parallel_config.tensor_parallel_size\n    )",
      "language": "python"
    },
    {
      "code": "def get_mooncake_side_channel_port(vllm_config: VllmConfig) -> int:\n    # This logic is now centralized\n    return (\n        envs.VLLM_MOONCAKE_BOOTSTRAP_PORT\n        + vllm_config.parallel_config.data_parallel_rank\n        * vllm_config.parallel_config.tensor_parallel_size\n    )",
      "language": "python"
    },
    {
      "code": "group_concurrent_contiguous(\n    src_indices: list[int], dst_indices: list[int]\n) -> tuple[list[list[int]], list[list[int]]]",
      "language": "php"
    },
    {
      "code": "group_concurrent_contiguous(\n    src_indices: list[int], dst_indices: list[int]\n) -> tuple[list[list[int]], list[list[int]]]",
      "language": "php"
    },
    {
      "code": "897\n898\n899\n900\n901\n902\n903\n904\n905\n906\n907\n908\n909\n910\n911",
      "language": "unknown"
    },
    {
      "code": "def group_concurrent_contiguous(\n    src_indices: list[int], dst_indices: list[int]\n) -> tuple[list[list[int]], list[list[int]]]:\n    \"\"\"Vectorised NumPy implementation.\"\"\"\n    if len(src_indices) == 0:\n        return [], []\n\n    brk = np.where((np.diff(src_indices) != 1) | (np.diff(dst_indices) != 1))[0] + 1\n    src_groups = np.split(src_indices, brk)\n    dst_groups = np.split(dst_indices, brk)\n\n    src_groups = [g.tolist() for g in src_groups]\n    dst_groups = [g.tolist() for g in dst_groups]\n\n    return src_groups, dst_groups",
      "language": "python"
    },
    {
      "code": "def group_concurrent_contiguous(\n    src_indices: list[int], dst_indices: list[int]\n) -> tuple[list[list[int]], list[list[int]]]:\n    \"\"\"Vectorised NumPy implementation.\"\"\"\n    if len(src_indices) == 0:\n        return [], []\n\n    brk = np.where((np.diff(src_indices) != 1) | (np.diff(dst_indices) != 1))[0] + 1\n    src_groups = np.split(src_indices, brk)\n    dst_groups = np.split(dst_indices, brk)\n\n    src_groups = [g.tolist() for g in src_groups]\n    dst_groups = [g.tolist() for g in dst_groups]\n\n    return src_groups, dst_groups",
      "language": "python"
    }
  ],
  "patterns": [],
  "links": [
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/mooncake_connector/",
    "https://docs.vllm.ai/en/latest/",
    "https://docs.vllm.ai/en/latest/usage/",
    "https://docs.vllm.ai/en/latest/contributing/",
    "https://docs.vllm.ai/en/latest/benchmarking/",
    "https://docs.vllm.ai/en/latest/api/",
    "https://docs.vllm.ai/en/latest/cli/",
    "https://docs.vllm.ai/en/latest/community/contact_us/",
    "https://docs.vllm.ai/en/latest/getting_started/quickstart/",
    "https://docs.vllm.ai/en/latest/getting_started/installation/",
    "https://docs.vllm.ai/en/latest/getting_started/installation/gpu/",
    "https://docs.vllm.ai/en/latest/getting_started/installation/cpu/",
    "https://docs.vllm.ai/en/latest/examples/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/async_llm_streaming/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/audio_language/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/automatic_prefix_caching/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/basic/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/batch_llm_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/chat_with_tools/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/context_extension/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/data_parallel/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/disaggregated-prefill-v1/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/disaggregated_prefill/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/encoder_decoder_multimodal/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/kv_load_failure_recovery/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/llm_engine_example/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/llm_engine_reset_kv/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/load_sharded_state/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/logits_processor/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/lora_with_quantization_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/metrics/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/mistral-small/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/mlpspeculator/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/multilora_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/openai_batch/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/prefix_caching/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/prompt_embed_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/qwen2_5_omni/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/qwen3_omni/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/qwen_1m/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/reproducibility/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf_colocate/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf_online_quant/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf_utils/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/save_sharded_state/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/simple_profiling/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/skip_loading_weights_in_engine_init/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/spec_decode/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/structured_outputs/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/torchrun_dp_example/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/torchrun_example/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/vision_language/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/vision_language_multi_image/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/api_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/chart-helm/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/dashboards/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_encoder/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_prefill/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_serving/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/elastic_ep/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/gradio_openai_chatbot_webserver/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/gradio_webserver/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/kv_events_subscriber/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/multi-node-serving/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/multi_instance_data_parallel/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_for_multimodal/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools_required/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools_xlam/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools_xlam_streaming/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_tool_calls_with_reasoning/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_with_reasoning/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_with_reasoning_streaming/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_completion_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_responses_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_responses_client_with_mcp_tools/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_responses_client_with_tools/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_transcription_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_translation_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/opentelemetry/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/prometheus_grafana/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/prompt_embed_inference_with_openai_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/ray_serve_deepseek/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/retrieval_augmented_generation_with_langchain/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/retrieval_augmented_generation_with_llamaindex/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/run_cluster/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/sagemaker-entrypoint/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/streamlit_openai_chatbot_webserver/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/structured_outputs/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/token_generation_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/utils/",
    "https://docs.vllm.ai/en/latest/examples/others/lmcache/",
    "https://docs.vllm.ai/en/latest/examples/others/logging_configuration/",
    "https://docs.vllm.ai/en/latest/examples/others/tensorize_vllm_model/",
    "https://docs.vllm.ai/en/latest/examples/pooling/classify/",
    "https://docs.vllm.ai/en/latest/examples/pooling/embed/",
    "https://docs.vllm.ai/en/latest/examples/pooling/plugin/",
    "https://docs.vllm.ai/en/latest/examples/pooling/pooling/",
    "https://docs.vllm.ai/en/latest/examples/pooling/score/",
    "https://docs.vllm.ai/en/latest/examples/pooling/token_classify/",
    "https://docs.vllm.ai/en/latest/examples/pooling/token_embed/",
    "https://docs.vllm.ai/en/latest/usage/v1_guide/",
    "https://docs.vllm.ai/en/latest/usage/faq/",
    "https://docs.vllm.ai/en/latest/usage/metrics/",
    "https://docs.vllm.ai/en/latest/usage/reproducibility/",
    "https://docs.vllm.ai/en/latest/usage/security/",
    "https://docs.vllm.ai/en/latest/usage/troubleshooting/",
    "https://docs.vllm.ai/en/latest/usage/usage_stats/",
    "https://docs.vllm.ai/en/latest/serving/offline_inference/",
    "https://docs.vllm.ai/en/latest/serving/openai_compatible_server/",
    "https://docs.vllm.ai/en/latest/serving/context_parallel_deployment/",
    "https://docs.vllm.ai/en/latest/serving/data_parallel_deployment/",
    "https://docs.vllm.ai/en/latest/serving/distributed_troubleshooting/",
    "https://docs.vllm.ai/en/latest/serving/expert_parallel_deployment/",
    "https://docs.vllm.ai/en/latest/serving/parallelism_scaling/",
    "https://docs.vllm.ai/en/latest/serving/integrations/langchain/",
    "https://docs.vllm.ai/en/latest/serving/integrations/llamaindex/",
    "https://docs.vllm.ai/en/latest/deployment/docker/",
    "https://docs.vllm.ai/en/latest/deployment/k8s/",
    "https://docs.vllm.ai/en/latest/deployment/nginx/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/anyscale/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/anything-llm/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/autogen/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/bentoml/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/cerebrium/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/chatbox/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/dify/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/dstack/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/haystack/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/helm/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/hf_inference_endpoints/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/litellm/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/lobe-chat/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/lws/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/modal/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/open-webui/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/retrieval_augmented_generation/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/skypilot/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/streamlit/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/triton/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kaito/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kserve/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kthena/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kubeai/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kuberay/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/llamastack/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/llm-d/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/llmaz/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/production-stack/",
    "https://docs.vllm.ai/en/latest/training/rlhf/",
    "https://docs.vllm.ai/en/latest/training/trl/",
    "https://docs.vllm.ai/en/latest/configuration/",
    "https://docs.vllm.ai/en/latest/configuration/conserving_memory/",
    "https://docs.vllm.ai/en/latest/configuration/engine_args/",
    "https://docs.vllm.ai/en/latest/configuration/env_vars/",
    "https://docs.vllm.ai/en/latest/configuration/model_resolution/",
    "https://docs.vllm.ai/en/latest/configuration/optimization/",
    "https://docs.vllm.ai/en/latest/configuration/serve_args/",
    "https://docs.vllm.ai/en/latest/models/supported_models/",
    "https://docs.vllm.ai/en/latest/models/generative_models/",
    "https://docs.vllm.ai/en/latest/models/pooling_models/",
    "https://docs.vllm.ai/en/latest/models/extensions/fastsafetensor/",
    "https://docs.vllm.ai/en/latest/models/extensions/runai_model_streamer/",
    "https://docs.vllm.ai/en/latest/models/extensions/tensorizer/",
    "https://docs.vllm.ai/en/latest/models/hardware_supported_models/cpu/",
    "https://docs.vllm.ai/en/latest/models/hardware_supported_models/xpu/",
    "https://docs.vllm.ai/en/latest/features/",
    "https://docs.vllm.ai/en/latest/features/automatic_prefix_caching/",
    "https://docs.vllm.ai/en/latest/features/batch_invariance/",
    "https://docs.vllm.ai/en/latest/features/custom_arguments/",
    "https://docs.vllm.ai/en/latest/features/custom_logitsprocs/",
    "https://docs.vllm.ai/en/latest/features/disagg_encoder/",
    "https://docs.vllm.ai/en/latest/features/disagg_prefill/",
    "https://docs.vllm.ai/en/latest/features/interleaved_thinking/",
    "https://docs.vllm.ai/en/latest/features/lora/",
    "https://docs.vllm.ai/en/latest/features/mooncake_connector_usage/",
    "https://docs.vllm.ai/en/latest/features/multimodal_inputs/",
    "https://docs.vllm.ai/en/latest/features/nixl_connector_usage/",
    "https://docs.vllm.ai/en/latest/features/prompt_embeds/",
    "https://docs.vllm.ai/en/latest/features/reasoning_outputs/",
    "https://docs.vllm.ai/en/latest/features/sleep_mode/",
    "https://docs.vllm.ai/en/latest/features/spec_decode/",
    "https://docs.vllm.ai/en/latest/features/structured_outputs/",
    "https://docs.vllm.ai/en/latest/features/tool_calling/",
    "https://docs.vllm.ai/en/latest/features/quantization/",
    "https://docs.vllm.ai/en/latest/features/quantization/auto_awq/",
    "https://docs.vllm.ai/en/latest/features/quantization/auto_round/",
    "https://docs.vllm.ai/en/latest/features/quantization/bitblas/",
    "https://docs.vllm.ai/en/latest/features/quantization/bnb/",
    "https://docs.vllm.ai/en/latest/features/quantization/fp8/",
    "https://docs.vllm.ai/en/latest/features/quantization/gguf/",
    "https://docs.vllm.ai/en/latest/features/quantization/gptqmodel/",
    "https://docs.vllm.ai/en/latest/features/quantization/inc/",
    "https://docs.vllm.ai/en/latest/features/quantization/int4/",
    "https://docs.vllm.ai/en/latest/features/quantization/int8/",
    "https://docs.vllm.ai/en/latest/features/quantization/modelopt/",
    "https://docs.vllm.ai/en/latest/features/quantization/quantized_kvcache/",
    "https://docs.vllm.ai/en/latest/features/quantization/quark/",
    "https://docs.vllm.ai/en/latest/features/quantization/torchao/",
    "https://docs.vllm.ai/en/latest/contributing/deprecation_policy/",
    "https://docs.vllm.ai/en/latest/contributing/dockerfile/dockerfile/",
    "https://docs.vllm.ai/en/latest/contributing/incremental_build/",
    "https://docs.vllm.ai/en/latest/contributing/profiling/",
    "https://docs.vllm.ai/en/latest/contributing/vulnerability_management/",
    "https://docs.vllm.ai/en/latest/contributing/model/",
    "https://docs.vllm.ai/en/latest/contributing/model/basic/",
    "https://docs.vllm.ai/en/latest/contributing/model/registration/",
    "https://docs.vllm.ai/en/latest/contributing/model/tests/",
    "https://docs.vllm.ai/en/latest/contributing/model/multimodal/",
    "https://docs.vllm.ai/en/latest/contributing/model/transcription/",
    "https://docs.vllm.ai/en/latest/contributing/ci/failures/",
    "https://docs.vllm.ai/en/latest/contributing/ci/nightly_builds/",
    "https://docs.vllm.ai/en/latest/contributing/ci/update_pytorch_version/",
    "https://docs.vllm.ai/en/latest/design/io_processor_plugins/",
    "https://docs.vllm.ai/en/latest/design/lora_resolver_plugins/",
    "https://docs.vllm.ai/en/latest/design/plugin_system/",
    "https://docs.vllm.ai/en/latest/design/arch_overview/",
    "https://docs.vllm.ai/en/latest/design/cuda_graphs/",
    "https://docs.vllm.ai/en/latest/design/dbo/",
    "https://docs.vllm.ai/en/latest/design/debug_vllm_compile/",
    "https://docs.vllm.ai/en/latest/design/fused_moe_modular_kernel/",
    "https://docs.vllm.ai/en/latest/design/huggingface_integration/",
    "https://docs.vllm.ai/en/latest/design/hybrid_kv_cache_manager/",
    "https://docs.vllm.ai/en/latest/design/logits_processors/",
    "https://docs.vllm.ai/en/latest/design/metrics/",
    "https://docs.vllm.ai/en/latest/design/mm_processing/",
    "https://docs.vllm.ai/en/latest/design/moe_kernel_features/",
    "https://docs.vllm.ai/en/latest/design/multiprocessing/",
    "https://docs.vllm.ai/en/latest/design/optimization_levels/",
    "https://docs.vllm.ai/en/latest/design/p2p_nccl_connector/",
    "https://docs.vllm.ai/en/latest/design/paged_attention/",
    "https://docs.vllm.ai/en/latest/design/prefix_caching/",
    "https://docs.vllm.ai/en/latest/design/torch_compile/",
    "https://docs.vllm.ai/en/latest/benchmarking/cli/",
    "https://docs.vllm.ai/en/latest/benchmarking/sweeps/",
    "https://docs.vllm.ai/en/latest/benchmarking/dashboard/",
    "https://docs.vllm.ai/en/latest/api/vllm/",
    "https://docs.vllm.ai/en/latest/api/vllm/beam_search/",
    "https://docs.vllm.ai/en/latest/api/vllm/collect_env/",
    "https://docs.vllm.ai/en/latest/api/vllm/connections/",
    "https://docs.vllm.ai/en/latest/api/vllm/env_override/",
    "https://docs.vllm.ai/en/latest/api/vllm/envs/",
    "https://docs.vllm.ai/en/latest/api/vllm/forward_context/",
    "https://docs.vllm.ai/en/latest/api/vllm/logger/",
    "https://docs.vllm.ai/en/latest/api/vllm/logits_process/",
    "https://docs.vllm.ai/en/latest/api/vllm/logprobs/",
    "https://docs.vllm.ai/en/latest/api/vllm/outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/pooling_params/",
    "https://docs.vllm.ai/en/latest/api/vllm/sampling_params/",
    "https://docs.vllm.ai/en/latest/api/vllm/scalar_type/",
    "https://docs.vllm.ai/en/latest/api/vllm/scripts/",
    "https://docs.vllm.ai/en/latest/api/vllm/sequence/",
    "https://docs.vllm.ai/en/latest/api/vllm/tasks/",
    "https://docs.vllm.ai/en/latest/api/vllm/tracing/",
    "https://docs.vllm.ai/en/latest/api/vllm/version/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/image/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/video/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layer/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/selector/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/chunked_local_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/cross_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/encoder_only_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/mm_encoder_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/chunked_prefill_paged_decode/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/common/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/flashmla/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/merge_attn_states/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/paged_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/pallas_kv_cache_update/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/prefix_prefill/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/rocm_aiter_mla_sparse/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_decode_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_merge_attn_states/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_reshape_and_cache_flash/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_unified_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/vit_attn_wrappers/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/fa_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/kv_sharing_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/kv_transfer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/datasets/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/latency/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/startup/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/throughput/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/endpoint_request_func/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/ready_checker/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/cli/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/param_sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/plot/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/plot_pareto/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/serve_sla/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/server/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/sla_sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/activation_quant_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/base_static_graph/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/caching/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/collective_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/compiler_interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/counter/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/cuda_graph/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/decorators/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fix_functionalization/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fusion_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fx_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/inductor_pass/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/matcher_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/monitor/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/noop_elimination/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/partition_rules/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/pass_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/piecewise_backend/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/post_cleanup/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/qk_norm_rope_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/rocm_aiter_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/sequence_parallelism/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/torch25_custom_graph_pass/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/vllm_inductor_pass/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/compilation/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/device/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/ec_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/kv_events/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/kv_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/load/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/lora/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/model/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/multimodal/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/observability/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/parallel/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/pooler/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/profiler/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/scheduler/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/speculative/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/speech_to_text/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/structured_outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/vllm/",
    "https://docs.vllm.ai/en/latest/api/vllm/device_allocator/",
    "https://docs.vllm.ai/en/latest/api/vllm/device_allocator/cumem/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/communication_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_events/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/parallel_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/tpu_distributed_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/all2all/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/all_reduce_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/base_device_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/cpu_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/cuda_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/cuda_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/custom_all_reduce/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/mnnvl_compat/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/pynccl/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/pynccl_allocator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/pynccl_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/quick_all_reduce/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/ray_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/shm_broadcast/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/shm_object_storage/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/symm_mem/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/tpu_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/xpu_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_transfer_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/example_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/factory/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/async_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/eplb_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/rebalance_execute/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/policy/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/policy/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/policy/default/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_transfer_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/factory/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/decode_bench_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/example_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_mp_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/multi_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/offloading_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/multi_process_adapter/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/vllm_v1_adapter/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/tensor_memory_pool/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/arg_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/async_llm_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/llm_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/api_server/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/chat_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/constants/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/context/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/launcher/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/llm/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/logger/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/renderer/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/responses_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/score_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/ssl/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/tool/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/tool_server/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/anthropic/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/anthropic/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/anthropic/serving_messages/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/collect_env/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/main/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/openai/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/run_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/types/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/latency/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/main/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/startup/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/throughput/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/api_server/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/cli_args/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/orca_metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/run_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_chat/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_chat_stream_harmony/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_completion/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_models/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_responses/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_transcription/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/speech_to_text/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/parser/harmony_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/parser/responses_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/conftest/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/sagemaker/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/sagemaker/routes/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/cache/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/elastic_ep/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/elastic_ep/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/elastic_ep/middleware/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/health/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/server_info/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/lora/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/lora/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/profile/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/profile/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rlhf/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rlhf/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rpc/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rpc/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/sleep/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/sleep/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/tokenize/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/tokenize/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/tokenize/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/data/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/parse/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/preprocess/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/dump_input/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/formatter/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/lazy/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/log_time/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/lora_model/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/lora_weights/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/model_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/peft_helper/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/request/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/resolver/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/worker_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/base_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/column_parallel_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/logits_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/replicated_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/row_parallel_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/vocal_parallel_embedding/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/ipex_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/ipex_ops/lora_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/torch_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/torch_ops/lora_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/fused_moe_lora_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/kernel_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/lora_expand_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/lora_kernel_metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/lora_shrink_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/xla_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/xla_ops/lora_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_gpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_selector/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_xpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/custom_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/parameter/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/activation/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/attention_layer_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/batch_invariant/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/conv/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/kda/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/layernorm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/lightning_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/logits_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/pooler/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/resampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/vocab_parallel_embedding/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk_delta_h/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk_o/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk_scaled_dot_kkt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/cumsum/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/fused_recurrent/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/index_py/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/kda/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/l2norm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/layernorm_guard/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/op/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/solve_tril/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/wy_fast/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/all2all_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/cpu_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/cutlass_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deep_gemm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deep_gemm_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deepep_ll_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_cutedsl_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_cutlass_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_trtllm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_batched_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_marlin_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_moe_method_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_moe_modular_method/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/gpt_oss_triton_kernels_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/layer/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/modular_kernel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_align_block_size/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_pallas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_permute_unpermute/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_torch_iterative/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/pplx_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/routing_simulator/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/shared_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/topk_weight_and_reduce/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/trtllm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/zero_expert_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/linear_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/mamba_mixer/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/mamba_mixer2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/mamba_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/short_conv/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/causal_conv1d/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/layernorm_gated/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/mamba_ssm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_bmm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_chunk_scan/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_chunk_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_combined/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_state_passing/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/auto_round/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/awq/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/awq_marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/awq_triton/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/base_config/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/bitblas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/bitsandbytes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/cpu_wna16/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/deepspeedfp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/experts_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/fbgemm_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/fp_quant/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gguf/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq_bitblas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq_marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq_marlin_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/hqq_marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/inc/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/input_quant_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/ipex_quant/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kv_cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/modelopt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/moe_wna16/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/mxfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/petit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/ptpc_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/qutlass_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/rtn/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/schema/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/torchao/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/tpu_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/triton_scaled_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_scheme/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a4_nvfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a8_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a8_int/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_nvfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a16_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_wNa16/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/module/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/schemes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/schemes/linear_qutlass_nvfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/allspark/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/bitblas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/conch/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/cutlass/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/dynamic_4bit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/exllama/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/MPLinearKernel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/machete/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/xpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/aiter/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/cutlass/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/ScaledMMLinearKernel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/triton/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/xla/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/quark/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/quark_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_ocp_mx/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_scheme/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/allspark_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/bitblas_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/flashinfer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/fp8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/gptq_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/int8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/layer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/machete_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_fp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_test/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_test_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/mxfp4_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/mxfp6_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/mxfp8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/nvfp4_emulation_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/nvfp4_moe_support/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/ocp_mx_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/petit_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/quant_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/w8a8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/common/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/deepseek_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/dual_chunk_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/dynamic_ntk_alpha_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/dynamic_ntk_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/ernie45_vl_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/linear_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/llama3_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/llama4_vision_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/mrope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/ntk_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/phi3_long_rope_scaled_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/xdrope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/yarn_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/base_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/bitsandbytes_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/default_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/dummy_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/gguf_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/online_quantization/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/runai_streamer_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/sharded_state_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/tensorizer/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/tensorizer_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/weight_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/adapters/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/afmoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/aimv2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/apertus/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/arcee/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/arctic/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/aria/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/audioflamingo3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/aya_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bagel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/baichuan/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bailing_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bee/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bert/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bert_with_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/blip/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/blip2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bloom/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/chameleon/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/chatglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/clip/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/cohere2_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/commandr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/dbrx/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepencoder/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_ocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_v2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_vl2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/dots1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/dots_ocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45_vl_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/exaone/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/exaone4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/fairseq2_llama/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/falcon/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/falcon_h1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/flex_olmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/fuyu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3n/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3n_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4_1v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4_moe_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_bigcode/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_j/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_neox/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_oss/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granite/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granite_speech/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granitemoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granitemoehybrid/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granitemoeshared/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gritlm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/grok1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/h2ovl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/hunyuan_v1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/hunyuan_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/hyperclovax_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/idefics2_vision_model/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/idefics3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interfaces/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interfaces_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/intern_vit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/internlm2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/internlm2_ve/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interns1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interns1_vit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/internvl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jais/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jais2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jina_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/keye/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/keye_vl1_5/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/kimi_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/kimi_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/lfm2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/lfm2_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/lightonocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama4_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama_eagle3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava_next/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava_next_video/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava_onevision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/longcat_flash/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/longcat_flash_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mamba2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/medusa/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/midashenglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mimo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mimo_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mimo_v2_flash/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpm3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpm_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpmv/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minimax_m2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minimax_text_01/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minimax_vl_01/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mistral3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mistral_large_3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mistral_large_3_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mixtral/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mllama4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mlp_speculator/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/modernbert/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/module_mapping/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/molmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/moonvit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mpt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nano_nemotron_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron_h/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron_nas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nvlm_d/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/olmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/olmo2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/olmoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/opencua/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/openpangu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/openpangu_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/opt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/orion/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ouro/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ovis/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ovis2_5/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/paddleocr_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/paligemma/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/persimmon/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi3v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi4mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi4mm_audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi4mm_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phimoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/pixtral/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/plamo2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/plamo3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_5_omni_thinker/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_5_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_rm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_next/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_next_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_omni_moe_thinker/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_vl_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/radio/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/roberta/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/rvl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/seed_oss/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/siglip/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/siglip2navit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/skyworkr1v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/smolvlm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/solar/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/stablelm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/starcoder2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/step3_text/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/step3_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/swin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/tarsier/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/telechat2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/teleflm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/terratorch/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ultravox/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/voxtral/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/voxtral_streaming/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/whisper/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/whisper_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/zamba2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/causal/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/legacy/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/multimodal/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/pooling/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/warmup/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/warmup/deep_gemm_warmup/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/warmup/kernel_warmup/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/evs/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/hasher/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/image/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/inputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/parse/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/processing/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/profiling/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/video/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/cuda/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/rocm/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/xpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/io_processors/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/io_processors/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/lora_resolvers/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/lora_resolvers/filesystem_resolver/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/layerwise_profile/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/ray/",
    "https://docs.vllm.ai/en/latest/api/vllm/ray/lazy_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/ray/ray_env/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/abs_reasoning_parsers/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/basic_parsers/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/deepseek_r1_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/deepseek_v3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/ernie45_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/glm4_moe_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/gptoss_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/granite_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/holo2_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/hunyuan_a13b_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/identity_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/minimax_m2_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/mistral_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/olmo3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/qwen3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/seedoss_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/step3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/deepseek_v32/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/deepseek_v32_encoding/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/detokenizer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/hf/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/mistral/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/abstract_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/deepseekv3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/deepseekv31_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/deepseekv32_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/ernie45_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/functiongemma_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/gigachat3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/glm4_moe_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/glm47_moe_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/granite_20b_fc_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/granite_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/hermes_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/hunyuan_a13b_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/internlm2_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/jamba_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/kimi_k2_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/llama4_pythonic_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/llama_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/longcat_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/minimax_m2_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/minimax_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/mistral_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/olmo3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/openai_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/phi4mini_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/pythonic_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/qwen3coder_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/qwen3xml_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/seed_oss_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/step3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/xlam_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/config_parser_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/dynamic_module/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/gguf_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/repo_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/runai_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/s3_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/tokenizer/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/chat_templates/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/chat_templates/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/afmoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/arctic/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/bagel/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/chatglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/deepseek_vl2/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/dotsocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/falcon/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/flex_olmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/hunyuan_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/jais/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/kimi_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/kimi_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/lfm2_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/medusa/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/midashenglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/mistral/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/mlp_speculator/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/moonvit/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/nemotron/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/nemotron_h/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/olmo3/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/ovis/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/qwen3_next/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/radio/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/step3_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/tarsier2/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/ultravox/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/speculators/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/speculators/algos/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/speculators/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/bagel/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/deepseek_ocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/deepseek_vl2/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/hunyuan_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/hunyuan_vl_image/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/ovis/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/ovis2_5/",
    "https://docs.vllm.ai/en/latest/api/vllm/triton_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/triton_utils/importing/",
    "https://docs.vllm.ai/en/latest/api/vllm/usage/",
    "https://docs.vllm.ai/en/latest/api/vllm/usage/usage_lib/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/argparse_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/async_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/collection_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/counter/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/deep_gemm/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/flashinfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/func_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/gc_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/hashing/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/import_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/jsontree/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/math_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/mem_constants/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/mem_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/nccl/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/network_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/nvtx_pytorch_hooks/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/platform_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/profiling/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/serial_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/system_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/tensor_schema/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/torch_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/cudagraph_dispatcher/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_cache_interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/request/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/serial_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/cpu_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/flash_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/flashinfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/flex_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/gdn_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/linear_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mamba1_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mamba2_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mamba_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/pallas/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/rocm_aiter_fa/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/rocm_aiter_unified_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/rocm_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/short_conv_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/tree_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/triton_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/aiter_triton_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/common/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/cutlass_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashattn_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashinfer_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashmla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashmla_sparse/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/indexer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/rocm_aiter_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/rocm_aiter_mla_sparse/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/triton_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/block_pool/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/encoder_cache_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_coordinator/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/single_type_kv_cache_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/async_scheduler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/output/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/request_queue/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/scheduler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/async_llm/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/coordinator/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/core/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/core_client/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/detokenizer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/exceptions/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/input_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/llm_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/logprobs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/output_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/parallel_sampling/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/multiproc_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/ray_distributed_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/ray_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/ray_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/uniproc_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/arc_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/backend/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/factory/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/lru_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/mediums/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/spec/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/backends/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/worker/cpu_gpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/worker/worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/loggers/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/perf/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/prometheus/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/ray_wrappers/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/reader/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/stats/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/pool/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/pool/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/rejection_sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/builtin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/state/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/bad_words/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/logprobs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/penalties/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/topk_topp_sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/tpu/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/tpu/sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/medusa/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/ngram_proposer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/suffix_decoding/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_guidance/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_lm_format_enforcer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_outlines/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_types/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_xgrammar/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/request/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/block_table/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/cp_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/cpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/cpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/dp_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/ec_connector_model_runner_mixin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_input_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_ubatch_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/kv_connector_model_runner_mixin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/lora_model_runner_mixin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/tpu_input_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/tpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/tpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/ubatch_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/ubatching/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/worker_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/workspace/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/xpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/xpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/async_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/attn_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/block_table/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/cudagraph_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/dp_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/input_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/states/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/structured_outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/metrics/logits/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/gumbel/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/logprob/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/min_p/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/output/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/penalties/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/eagle_cudagraph/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/rejection_sample/",
    "https://docs.vllm.ai/en/latest/cli/serve/",
    "https://docs.vllm.ai/en/latest/cli/chat/",
    "https://docs.vllm.ai/en/latest/cli/complete/",
    "https://docs.vllm.ai/en/latest/cli/run-batch/",
    "https://docs.vllm.ai/en/latest/cli/bench/latency/",
    "https://docs.vllm.ai/en/latest/cli/bench/serve/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/plot/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/plot_pareto/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/serve/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/serve_sla/",
    "https://docs.vllm.ai/en/latest/cli/bench/throughput/",
    "https://docs.vllm.ai/en/latest/community/meetups/",
    "https://docs.vllm.ai/en/latest/community/sponsors/",
    "https://docs.vllm.ai/en/latest/governance/collaboration/",
    "https://docs.vllm.ai/en/latest/governance/committers/",
    "https://docs.vllm.ai/en/latest/governance/process/"
  ]
}