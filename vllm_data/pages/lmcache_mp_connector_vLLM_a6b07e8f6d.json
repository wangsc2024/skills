{
  "url": "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_mp_connector/",
  "title": "lmcache_mp_connector - vLLM",
  "content": "Bases: KVConnectorBase_V1\n\nThe connector for LMCache multi-process mode.\n\nExtra configs (kv_transfer_config.extra_config): - lmcache.mp.host: the host of the LMCache server. - lmcache.mp.port: the port of the LMCache server.\n\nClean up request tracker and associated lookup future for a request. This should be called when a request is finished to prevent memory leak.\n\nGet the connector metadata.\n\nThis function should only be called inside the connector.\n\nthe connector metadata.\n\nBuild the connector metadata for this step.\n\nThis function should NOT modify fields in the scheduler_output. Also, calling this function will reset the state of the connector.\n\nthe scheduler output object.\n\nKVConnectorStats resolution method. This method allows dynamically registered connectors to return their own KVConnectorStats object, which can implement custom aggregation logic on the data dict.\n\nCreate a KVConnectorPromMetrics subclass which should register per-connector Prometheus metrics and implement observe() to expose connector transfer stats via Prometheus.\n\nGet the set of block IDs that failed to load.\n\nSet of block IDs that encountered load errors.\n\nEmpty set if no load errors occurred.\n\nNotifies worker-side connector ids of requests that have finished generating tokens on the worker. The scheduler process (via the Executors) will use this output to track which workers are done.\n\nids of requests that have finished asynchronous transfer\n\n(requests that previously returned True from request_finished()),\n\ntuple of (sending/saving ids, recving/loading ids).\n\nThe finished saves/sends req ids must belong to a set provided in a\n\ncall to this method (this call or a prior one).\n\nGet the count of requests expected to complete send/receive operations via this connector. This method is used to initialize the KVOutputAggregator, overwriting the default world_size.\n\nexpected sending or receiving completion count.\n\nGet the KV connector stats collected during the last interval.\n\nGet number of new tokens that can be loaded from the external KV cache beyond the num_computed_tokens.\n\nthe number of locally computed tokens for this request\n\nA tuple with the following elements: - An optional number of tokens that can be loaded from the external KV cache beyond what is already computed. If None, it means that the connector needs more time to determine the number of matched tokens, and the scheduler should query for this request again later. - True if external KV cache tokens will be loaded asynchronously (between scheduler steps). Must be 'False' if the first element is 0.\n\nThe connector should only consider the largest prefix of prompt- tokens for which KV cache is actually available at the time of the call. If the cache cannot be loaded for some tokens (e.g., due to connectivity issues or eviction), those tokens must not be taken into account.\n\nGet the required KV cache layout for this connector. Args: vllm_config (VllmConfig): the vllm config.\n\nthe required KV cache layout. e.g. HND, or NHD.\n\nNone if the connector does not require a specific layout.\n\nInitialize with the KV caches. Useful for pre-registering the KV Caches in the KVConnector (e.g. for NIXL).\n\ndictionary of layer names, kv cache\n\nCalled exactly once when a request has finished, before its blocks are freed.\n\nThe connector may assumes responsibility for freeing the blocks asynchronously by returning True.\n\nTrue if the request is being saved/sent asynchronously and blocks\n\nshould not be freed until the request_id is returned from\n\nOptional KVTransferParams to be included in the request outputs\n\nreturned by the engine.\n\nStart saving a layer of KV cache from vLLM's paged buffer to the connector. This is called from within attention layer to enable async copying during execution.\n\nthe name of the layer.\n\nthe paged KV buffer of the current layer in vLLM.\n\nthe attention metadata.\n\nadditional arguments for the save operation.\n\nShutdown the connector. This is called when the worker process is shutting down to ensure that all the async operations are completed and the connector is cleaned up properly.\n\nStart loading the KV cache from the connector to vLLM's paged KV buffer. This is called from the forward context before the forward pass to enable async loading during model execution.\n\nadditional arguments for the load operation\n\nThe number of elements in kv_caches and layer_names should be the same.\n\nTake the KV cache events from the connector.\n\nNew KV cache events since the last call.\n\nUpdate KVConnector state from worker-side connectors output.\n\nthe worker-side connectors output.\n\nUpdate KVConnector state after block allocation.\n\nIf get_num_new_matched_tokens previously returned True for a request, this function may be called twice for that same request - first when blocks are allocated for the connector tokens to be asynchronously loaded into, and second when any additional blocks are allocated, after the load/transfer is complete.\n\nthe blocks allocated for the request.\n\nthe number of tokens that will be loaded from the external KV cache.\n\nBlock until the KV for a specific layer is loaded into vLLM's paged buffer. This is called from within attention layer to ensure async copying from start_load_kv is complete.\n\nThis interface will be useful for layer-by-layer pipelining.\n\nthe name of that layer\n\nBlock until all the save operations is done. This is called as the forward context exits to ensure that the async saving from save_kv_layer is complete before finishing the forward.\n\nThis prevents overwrites of paged KV buffer before saving done.\n\nBases: KVConnectorMetadata\n\nGenerate the retrieve metadata for the current request tracker.\n\nThe request tracker to generate the metadata from.\n\nthe number of blocks in a LMCache data chunk\n\nGenerate the store metadata for the current request tracker.\n\nThe request tracker to generate the metadata from.\n\nthe number of blocks in a LMCache data chunk\n\nState machine: PREFETCHING -- update_state_after_alloc --> WAITING_FOR_LOAD WAITING_FOR_LOAD -- process_loading_requests --> READY\n\nUpdate the block ids for the current request This function will be called when processing the cached requests.\n\nIncrease the number of stored blocks for the current request This function will be called when processing the cached requests.\n\nCheck whether the current request is ready for retrieving, will be used in process_loading_requests\n\nCheck whether the current request needs retrieve, will be used update_stage_after_alloc\n\nConvert the rank for the MLA.",
  "headings": [
    {
      "level": "h1",
      "text": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector"
    },
    {
      "level": "h2",
      "text": "logger module-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.logger"
    },
    {
      "level": "h2",
      "text": "LMCacheMPConnector ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPConnector"
    },
    {
      "level": "h3",
      "text": "request_trackers instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPConnector.request_trackers"
    },
    {
      "level": "h3",
      "text": "role property ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPConnector.role"
    },
    {
      "level": "h3",
      "text": "scheduler_adapter instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPConnector.scheduler_adapter"
    },
    {
      "level": "h3",
      "text": "vllm_block_size instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPConnector.vllm_block_size"
    },
    {
      "level": "h3",
      "text": "worker_adapter instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPConnector.worker_adapter"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPConnector.__init__"
    },
    {
      "level": "h3",
      "text": "_cleanup_request_tracker ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPConnector._cleanup_request_tracker"
    },
    {
      "level": "h3",
      "text": "_get_connector_metadata ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPConnector._get_connector_metadata"
    },
    {
      "level": "h3",
      "text": "_get_or_create_request_tracker ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPConnector._get_or_create_request_tracker"
    },
    {
      "level": "h3",
      "text": "_get_request_tracker ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPConnector._get_request_tracker"
    },
    {
      "level": "h3",
      "text": "_process_cached_requests ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPConnector._process_cached_requests"
    },
    {
      "level": "h3",
      "text": "_process_new_requests ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPConnector._process_new_requests"
    },
    {
      "level": "h3",
      "text": "_process_retrieve_requests ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPConnector._process_retrieve_requests"
    },
    {
      "level": "h3",
      "text": "build_connector_meta ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPConnector.build_connector_meta"
    },
    {
      "level": "h3",
      "text": "build_kv_connector_stats classmethod ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPConnector.build_kv_connector_stats"
    },
    {
      "level": "h3",
      "text": "build_prom_metrics classmethod ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPConnector.build_prom_metrics"
    },
    {
      "level": "h3",
      "text": "get_block_ids_with_load_errors ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPConnector.get_block_ids_with_load_errors"
    },
    {
      "level": "h3",
      "text": "get_finished ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPConnector.get_finished"
    },
    {
      "level": "h3",
      "text": "get_finished_count ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPConnector.get_finished_count"
    },
    {
      "level": "h3",
      "text": "get_kv_connector_stats ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPConnector.get_kv_connector_stats"
    },
    {
      "level": "h3",
      "text": "get_num_new_matched_tokens ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPConnector.get_num_new_matched_tokens"
    },
    {
      "level": "h3",
      "text": "get_required_kvcache_layout classmethod ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPConnector.get_required_kvcache_layout"
    },
    {
      "level": "h3",
      "text": "register_kv_caches ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPConnector.register_kv_caches"
    },
    {
      "level": "h3",
      "text": "request_finished ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPConnector.request_finished"
    },
    {
      "level": "h3",
      "text": "save_kv_layer ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPConnector.save_kv_layer"
    },
    {
      "level": "h3",
      "text": "shutdown ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPConnector.shutdown"
    },
    {
      "level": "h3",
      "text": "start_load_kv ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPConnector.start_load_kv"
    },
    {
      "level": "h3",
      "text": "take_events ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPConnector.take_events"
    },
    {
      "level": "h3",
      "text": "update_connector_output ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPConnector.update_connector_output"
    },
    {
      "level": "h3",
      "text": "update_state_after_alloc ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPConnector.update_state_after_alloc"
    },
    {
      "level": "h3",
      "text": "wait_for_layer_load ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPConnector.wait_for_layer_load"
    },
    {
      "level": "h3",
      "text": "wait_for_save ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPConnector.wait_for_save"
    },
    {
      "level": "h2",
      "text": "LMCacheMPConnectorMetadata ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPConnectorMetadata"
    },
    {
      "level": "h3",
      "text": "requests instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPConnectorMetadata.requests"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPConnectorMetadata.__init__"
    },
    {
      "level": "h3",
      "text": "__len__ ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPConnectorMetadata.__len__"
    },
    {
      "level": "h3",
      "text": "__repr__ ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPConnectorMetadata.__repr__"
    },
    {
      "level": "h3",
      "text": "__str__ ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPConnectorMetadata.__str__"
    },
    {
      "level": "h3",
      "text": "add_request_metadata ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPConnectorMetadata.add_request_metadata"
    },
    {
      "level": "h2",
      "text": "LMCacheMPRequestMetadata dataclass ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPRequestMetadata"
    },
    {
      "level": "h3",
      "text": "direction instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPRequestMetadata.direction"
    },
    {
      "level": "h3",
      "text": "op instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPRequestMetadata.op"
    },
    {
      "level": "h3",
      "text": "request_id instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPRequestMetadata.request_id"
    },
    {
      "level": "h3",
      "text": "GetRetrieveMetadata staticmethod ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPRequestMetadata.GetRetrieveMetadata"
    },
    {
      "level": "h3",
      "text": "GetStoreMetadata staticmethod ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPRequestMetadata.GetStoreMetadata"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPRequestMetadata.__init__"
    },
    {
      "level": "h2",
      "text": "LMCacheMPRequestState ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPRequestState"
    },
    {
      "level": "h3",
      "text": "PREFETCHING class-attribute instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPRequestState.PREFETCHING"
    },
    {
      "level": "h3",
      "text": "READY class-attribute instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPRequestState.READY"
    },
    {
      "level": "h3",
      "text": "WAITING_FOR_LOAD class-attribute instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPRequestState.WAITING_FOR_LOAD"
    },
    {
      "level": "h2",
      "text": "LMCacheMPRequestTracker dataclass ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPRequestTracker"
    },
    {
      "level": "h3",
      "text": "all_token_ids instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPRequestTracker.all_token_ids"
    },
    {
      "level": "h3",
      "text": "allocated_block_ids class-attribute instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPRequestTracker.allocated_block_ids"
    },
    {
      "level": "h3",
      "text": "block_hashes instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPRequestTracker.block_hashes"
    },
    {
      "level": "h3",
      "text": "num_lmcache_hit_blocks class-attribute instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPRequestTracker.num_lmcache_hit_blocks"
    },
    {
      "level": "h3",
      "text": "num_scheduled_tokens class-attribute instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPRequestTracker.num_scheduled_tokens"
    },
    {
      "level": "h3",
      "text": "num_stored_blocks class-attribute instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPRequestTracker.num_stored_blocks"
    },
    {
      "level": "h3",
      "text": "num_vllm_hit_blocks class-attribute instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPRequestTracker.num_vllm_hit_blocks"
    },
    {
      "level": "h3",
      "text": "request_id instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPRequestTracker.request_id"
    },
    {
      "level": "h3",
      "text": "state class-attribute instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPRequestTracker.state"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPRequestTracker.__init__"
    },
    {
      "level": "h3",
      "text": "__repr__ ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPRequestTracker.__repr__"
    },
    {
      "level": "h3",
      "text": "__str__ ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPRequestTracker.__str__"
    },
    {
      "level": "h3",
      "text": "append_block_ids ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPRequestTracker.append_block_ids"
    },
    {
      "level": "h3",
      "text": "increase_num_scheduled_tokens ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPRequestTracker.increase_num_scheduled_tokens"
    },
    {
      "level": "h3",
      "text": "increase_num_stored_blocks ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPRequestTracker.increase_num_stored_blocks"
    },
    {
      "level": "h3",
      "text": "is_ready_for_retrieving ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPRequestTracker.is_ready_for_retrieving"
    },
    {
      "level": "h3",
      "text": "needs_retrieve ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.LMCacheMPRequestTracker.needs_retrieve"
    },
    {
      "level": "h2",
      "text": "convert_block_hashes_to_bytes ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.convert_block_hashes_to_bytes"
    },
    {
      "level": "h2",
      "text": "create_scheduler_adapter ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.create_scheduler_adapter"
    },
    {
      "level": "h2",
      "text": "create_worker_adapter ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.create_worker_adapter"
    },
    {
      "level": "h2",
      "text": "extract_world_size_and_kv_rank ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.extract_world_size_and_kv_rank"
    },
    {
      "level": "h2",
      "text": "reformat_block_ids ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_mp_connector.reformat_block_ids"
    }
  ],
  "code_samples": [
    {
      "code": "logger = init_logger(__name__)",
      "language": "unknown"
    },
    {
      "code": "logger = init_logger(__name__)",
      "language": "unknown"
    },
    {
      "code": "367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n500\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\n600\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\n700\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\n800\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899\n900\n901\n902\n903\n904\n905\n906\n907\n908\n909\n910\n911\n912\n913\n914\n915\n916\n917\n918\n919\n920\n921\n922\n923\n924\n925\n926\n927\n928\n929\n930\n931\n932\n933",
      "language": "unknown"
    },
    {
      "code": "class LMCacheMPConnector(KVConnectorBase_V1):\n    \"\"\"\n    The connector for LMCache multi-process mode.\n\n    Extra configs (kv_transfer_config.extra_config):\n    - lmcache.mp.host: the host of the LMCache server.\n    - lmcache.mp.port: the port of the LMCache server.\n    \"\"\"\n\n    def __init__(\n        self,\n        vllm_config: \"VllmConfig\",\n        role: KVConnectorRole,\n        kv_cache_config: Optional[\"KVCacheConfig\"] = None,\n    ):\n        super().__init__(vllm_config, role, kv_cache_config)\n\n        assert vllm_config.kv_transfer_config is not None\n        server_host = vllm_config.kv_transfer_config.get_from_extra_config(\n            \"lmcache.mp.host\", \"tcp://localhost\"\n        )\n        server_port = vllm_config.kv_transfer_config.get_from_extra_config(\n            \"lmcache.mp.port\", 5555\n        )\n\n        server_url = f\"{server_host}:{server_port}\"\n        zmq_context = zmq.Context.instance()\n        if self.role == KVConnectorRole.SCHEDULER:\n            self.scheduler_adapter = create_scheduler_adapter(\n                server_url, zmq_context, vllm_config\n            )\n            self.request_trackers: dict[str, LMCacheMPRequestTracker] = {}\n        elif self.role == KVConnectorRole.WORKER:\n            self.worker_adapter = create_worker_adapter(\n                server_url, zmq_context, vllm_config\n            )\n        else:\n            raise ValueError(f\"Unknown KVConnectorRole: {self.role}\")\n\n        self.vllm_block_size = vllm_config.cache_config.block_size\n\n    @property\n    def role(self) -> KVConnectorRole:\n        return self._role\n\n    # ==============================\n    # Worker-side methods\n    # ==============================\n\n    def _get_connector_metadata(self) -> KVConnectorMetadata:\n        \"\"\"Get the connector metadata.\n\n        This function should only be called inside the connector.\n\n        Returns:\n            ConnectorMetadata: the connector metadata.\n        \"\"\"\n\n        # Should only be called while set to valid metadata.\n        assert self._connector_metadata is not None\n        return self._connector_metadata\n\n    def register_kv_caches(self, kv_caches: dict[str, torch.Tensor]):\n        \"\"\"\n        Initialize with the KV caches. Useful for pre-registering the\n        KV Caches in the KVConnector (e.g. for NIXL).\n\n        Args:\n            kv_caches: dictionary of layer names, kv cache\n        \"\"\"\n        logger.info(\"Registering kv caches!\")\n        self.worker_adapter.register_kv_caches(kv_caches)\n        return\n\n    def start_load_kv(self, forward_context: \"ForwardContext\", **kwargs: Any) -> None:\n        \"\"\"\n        Start loading the KV cache from the connector to vLLM's paged\n        KV buffer. This is called from the forward context before the\n        forward pass to enable async loading during model execution.\n\n        Args:\n            forward_context (ForwardContext): the forward context.\n            **kwargs: additional arguments for the load operation\n\n        Note:\n            The number of elements in kv_caches and layer_names should be\n            the same.\n\n        \"\"\"\n        metadata = self._get_connector_metadata()\n        assert isinstance(metadata, LMCacheMPConnectorMetadata)\n\n        request_ids = []\n        ops = []\n\n        for meta in metadata.requests:\n            if meta.direction != \"RETRIEVE\":\n                continue\n            request_ids.append(meta.request_id)\n            ops.append(meta.op)\n\n        if len(request_ids) == 0:\n            return\n\n        with torch.cuda.stream(torch.cuda.current_stream()):\n            event = torch.cuda.Event(interprocess=True)\n            event.record()\n\n        self.worker_adapter.batched_submit_retrieve_requests(request_ids, ops, event)\n\n    def wait_for_layer_load(self, layer_name: str) -> None:\n        \"\"\"\n        Block until the KV for a specific layer is loaded into vLLM's\n        paged buffer. This is called from within attention layer to ensure\n        async copying from start_load_kv is complete.\n\n        This interface will be useful for layer-by-layer pipelining.\n\n        Args:\n            layer_name: the name of that layer\n        \"\"\"\n        return\n\n    def save_kv_layer(\n        self,\n        layer_name: str,\n        kv_layer: torch.Tensor,\n        attn_metadata: AttentionMetadata,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"\n        Start saving a layer of KV cache from vLLM's paged buffer\n        to the connector. This is called from within attention layer to\n        enable async copying during execution.\n\n        Args:\n            layer_name (str): the name of the layer.\n            kv_layer (torch.Tensor): the paged KV buffer of the current\n                layer in vLLM.\n            attn_metadata (AttentionMetadata): the attention metadata.\n            **kwargs: additional arguments for the save operation.\n        \"\"\"\n        return\n\n    def wait_for_save(self):\n        \"\"\"\n        Block until all the save operations is done. This is called\n        as the forward context exits to ensure that the async saving\n        from save_kv_layer is complete before finishing the forward.\n\n        This prevents overwrites of paged KV buffer before saving done.\n        \"\"\"\n        metadata = self._get_connector_metadata()\n        assert isinstance(metadata, LMCacheMPConnectorMetadata)\n\n        request_ids = []\n        ops = []\n        for meta in metadata.requests:\n            if meta.direction != \"STORE\":\n                continue\n            request_ids.append(meta.request_id)\n            ops.append(meta.op)\n\n        if len(request_ids) == 0:\n            return\n\n        with torch.cuda.stream(torch.cuda.current_stream()):\n            event = torch.cuda.Event(interprocess=True)\n            event.record()\n\n        self.worker_adapter.batched_submit_store_requests(request_ids, ops, event)\n\n    def get_finished(\n        self, finished_req_ids: set[str]\n    ) -> tuple[set[str] | None, set[str] | None]:\n        \"\"\"\n        Notifies worker-side connector ids of requests that have\n        finished generating tokens on the worker.\n        The scheduler process (via the Executors) will use this output\n        to track which workers are done.\n\n        Returns:\n            ids of requests that have finished asynchronous transfer\n            (requests that previously returned True from request_finished()),\n            tuple of (sending/saving ids, recving/loading ids).\n            The finished saves/sends req ids must belong to a set provided in a\n            call to this method (this call or a prior one).\n        \"\"\"\n        val = self.worker_adapter.get_finished(finished_req_ids)\n        # logger.error(\"Finished req ids: %s, %s\", val[0], val[1])\n        return val\n\n    def get_block_ids_with_load_errors(self) -> set[int]:\n        \"\"\"\n        Get the set of block IDs that failed to load.\n\n        Returns:\n            Set of block IDs that encountered load errors.\n            Empty set if no load errors occurred.\n\n        Notes:\n            - Applies to both sync- and async-loading requests.\n            - Async loading: failed blocks may be reported in any forward pass\n              up to and including the pass where the request ID is returned by\n              `get_finished()`. Even if failures occur, the request must still\n              be reported via `get_finished()`, and the failed block IDs must\n              appear here no later than that same pass.\n            - Sync loading: failed blocks should be reported in the forward\n              pass in which they are detected.\n        \"\"\"\n        # TODO: add error tracking\n        return set()\n\n    def shutdown(self):\n        \"\"\"\n        Shutdown the connector. This is called when the worker process\n        is shutting down to ensure that all the async operations are\n        completed and the connector is cleaned up properly.\n        \"\"\"\n        if hasattr(self, \"worker_adapter\"):\n            self.worker_adapter.shutdown()\n        return None\n\n    def get_kv_connector_stats(self) -> Optional[\"KVConnectorStats\"]:\n        \"\"\"\n        Get the KV connector stats collected during the last interval.\n        \"\"\"\n        return None\n\n    # ==============================\n    # Scheduler-side methods\n    # ==============================\n\n    def get_num_new_matched_tokens(\n        self,\n        request: \"Request\",\n        num_computed_tokens: int,\n    ) -> tuple[int | None, bool]:\n        \"\"\"\n        Get number of new tokens that can be loaded from the\n        external KV cache beyond the num_computed_tokens.\n\n        Args:\n            request (Request): the request object.\n            num_computed_tokens (int): the number of locally\n                computed tokens for this request\n\n        Returns:\n            A tuple with the following elements:\n                - An optional number of tokens that can be loaded from the\n                  external KV cache beyond what is already computed.\n                  If None, it means that the connector needs more time to\n                  determine the number of matched tokens, and the scheduler\n                  should query for this request again later.\n                - `True` if external KV cache tokens will be loaded\n                  asynchronously (between scheduler steps). Must be\n                  'False' if the first element is 0.\n\n        Notes:\n            The connector should only consider the largest prefix of prompt-\n            tokens for which KV cache is actually available at the time of the\n            call. If the cache cannot be loaded for some tokens (e.g., due to\n            connectivity issues or eviction), those tokens must not be taken\n            into account.\n        \"\"\"\n        tracker = self._get_or_create_request_tracker(request)\n        # TODO: support loading KV for preempted requests in the future\n        if request.status == RequestStatus.PREEMPTED:\n            return 0, False\n\n        self.scheduler_adapter.maybe_submit_lookup_request(\n            request.request_id, convert_block_hashes_to_bytes(request.block_hashes)\n        )\n\n        ret = self.scheduler_adapter.check_lookup_result(request.request_id)\n        if ret is None:\n            return None, True\n\n        if ret == 0:\n            return 0, False\n\n        assert (\n            ret % (self.scheduler_adapter.num_blocks_per_chunk() * self.vllm_block_size)\n            == 0\n        )\n\n        # Update num stored blocks for the tracker\n        num_vllm_blocks = num_computed_tokens // self.vllm_block_size\n        num_lmcache_blocks = ret // self.vllm_block_size\n        tracker.increase_num_stored_blocks(num_lmcache_blocks)\n\n        # Save the vllm and lmcache hit tokens\n        tracker.num_vllm_hit_blocks = num_vllm_blocks\n        tracker.num_lmcache_hit_blocks = num_lmcache_blocks\n\n        need_to_load = max(0, ret - num_computed_tokens)\n        logger.debug(\n            \"vLLM hit is: %d, Need to load is %d\", num_computed_tokens, need_to_load\n        )\n        return need_to_load, need_to_load > 0\n\n    def update_state_after_alloc(\n        self, request: \"Request\", blocks: \"KVCacheBlocks\", num_external_tokens: int\n    ):\n        \"\"\"\n        Update KVConnector state after block allocation.\n\n        If get_num_new_matched_tokens previously returned True for a\n        request, this function may be called twice for that same request -\n        first when blocks are allocated for the connector tokens to be\n        asynchronously loaded into, and second when any additional blocks\n        are allocated, after the load/transfer is complete.\n\n        Args:\n            request (Request): the request object.\n            blocks (KVCacheBlocks): the blocks allocated for the request.\n            num_external_tokens (int): the number of tokens that will be\n                loaded from the external KV cache.\n        \"\"\"\n        # NOTE: the `blocks` are NEW BLOCKS allocated for this request.\n        tracker = self._get_request_tracker(request.request_id)\n        block_ids = reformat_block_ids(blocks.get_block_ids())\n\n        # No matter we need to retrieve or not, we need to update\n        # the block ids into the tracker\n        tracker.append_block_ids(block_ids)\n\n        # Update the state of the tracker\n        condition = tracker.needs_retrieve()\n        if tracker.state == LMCacheMPRequestState.PREFETCHING:\n            # If need to retrieve, change to WAITING_FOR_LOAD\n            # Otherwise, change to READY\n            tracker.state = (\n                LMCacheMPRequestState.WAITING_FOR_LOAD\n                if condition\n                else LMCacheMPRequestState.READY\n            )\n            # Clean up lookup future in scheduler adapter\n            self.scheduler_adapter._cleanup_lookup_result(request.request_id)\n\n    def build_connector_meta(\n        self, scheduler_output: SchedulerOutput\n    ) -> KVConnectorMetadata:\n        \"\"\"\n        Build the connector metadata for this step.\n\n        This function should NOT modify fields in the scheduler_output.\n        Also, calling this function will reset the state of the connector.\n\n        Args:\n            scheduler_output (SchedulerOutput): the scheduler output object.\n        \"\"\"\n        metadata = LMCacheMPConnectorMetadata()\n\n        self._process_retrieve_requests(metadata)\n        self._process_new_requests(scheduler_output, metadata)\n        self._process_cached_requests(scheduler_output, metadata)\n\n        if len(metadata) > 0:\n            logger.debug(\"Final connector metadata: %s\", metadata)\n\n        return metadata\n\n    def update_connector_output(self, connector_output: KVConnectorOutput):\n        \"\"\"\n        Update KVConnector state from worker-side connectors output.\n\n        Args:\n            connector_output (KVConnectorOutput): the worker-side\n                connectors output.\n        \"\"\"\n        return\n\n    def request_finished(\n        self,\n        request: \"Request\",\n        block_ids: list[int],\n    ) -> tuple[bool, dict[str, Any] | None]:\n        \"\"\"\n        Called exactly once when a request has finished, before its blocks are\n        freed.\n\n        The connector may assumes responsibility for freeing the blocks\n        asynchronously by returning True.\n\n        Returns:\n            True if the request is being saved/sent asynchronously and blocks\n            should not be freed until the request_id is returned from\n            get_finished().\n            Optional KVTransferParams to be included in the request outputs\n            returned by the engine.\n        \"\"\"\n        # Clean up request tracker to prevent memory leak\n        self._cleanup_request_tracker(request.request_id)\n        return True, None\n\n    def take_events(self) -> Iterable[\"KVCacheEvent\"]:\n        \"\"\"\n        Take the KV cache events from the connector.\n\n        Yields:\n            New KV cache events since the last call.\n        \"\"\"\n        return ()\n\n    @classmethod\n    def get_required_kvcache_layout(cls, vllm_config: \"VllmConfig\") -> str | None:\n        \"\"\"\n        Get the required KV cache layout for this connector.\n        Args:\n            vllm_config (VllmConfig): the vllm config.\n\n        Returns:\n            str: the required KV cache layout. e.g. HND, or NHD.\n            None if the connector does not require a specific layout.\n        \"\"\"\n\n        if cls is KVConnectorBase_V1:\n            raise TypeError(\n                \"get_required_kvcache_layout should not be called \"\n                \"on the abstract base class\"\n            )\n        return None\n\n    def get_finished_count(self) -> int | None:\n        \"\"\"\n        Get the count of requests expected to complete send/receive operations\n        via this connector. This method is used to initialize the\n        KVOutputAggregator, overwriting the default world_size.\n\n        Returns:\n            int: expected sending or receiving completion count.\n        \"\"\"\n        return None\n\n    @classmethod\n    def build_kv_connector_stats(\n        cls, data: dict[str, Any] | None = None\n    ) -> Optional[\"KVConnectorStats\"]:\n        \"\"\"\n        KVConnectorStats resolution method. This method allows dynamically\n        registered connectors to return their own KVConnectorStats object,\n        which can implement custom aggregation logic on the data dict.\n        \"\"\"\n        return None\n\n    @classmethod\n    def build_prom_metrics(\n        cls,\n        vllm_config: \"VllmConfig\",\n        metric_types: dict[type[\"PromMetric\"], type[\"PromMetricT\"]],\n        labelnames: list[str],\n        per_engine_labelvalues: dict[int, list[object]],\n    ) -> Optional[\"KVConnectorPromMetrics\"]:\n        \"\"\"\n        Create a KVConnectorPromMetrics subclass which should register\n        per-connector Prometheus metrics and implement observe() to\n        expose connector transfer stats via Prometheus.\n        \"\"\"\n        return None\n\n    ##############################\n    # Helper functions\n    ##############################\n    def _process_retrieve_requests(\n        self,\n        metadata: LMCacheMPConnectorMetadata,\n    ) -> None:\n        blocks_per_chunk = self.scheduler_adapter.num_blocks_per_chunk()\n\n        for request_tracker in self.request_trackers.values():\n            if request_tracker.state != LMCacheMPRequestState.WAITING_FOR_LOAD:\n                continue\n            r_metadata = LMCacheMPRequestMetadata.GetRetrieveMetadata(\n                request_tracker, blocks_per_chunk\n            )\n            if r_metadata is not None:\n                metadata.add_request_metadata(r_metadata)\n            request_tracker.state = LMCacheMPRequestState.READY\n\n    def _process_new_requests(\n        self,\n        scheduler_output: SchedulerOutput,\n        metadata: LMCacheMPConnectorMetadata,\n    ) -> None:\n        blocks_per_chunk = self.scheduler_adapter.num_blocks_per_chunk()\n\n        for new_request in scheduler_output.scheduled_new_reqs:\n            request_tracker = self._get_request_tracker(new_request.req_id)\n\n            num_new_tokens = scheduler_output.num_scheduled_tokens[new_request.req_id]\n            request_tracker.increase_num_scheduled_tokens(num_new_tokens)\n\n            r_meta = LMCacheMPRequestMetadata.GetStoreMetadata(\n                request_tracker, blocks_per_chunk, self.vllm_block_size\n            )\n            if r_meta is not None:\n                metadata.add_request_metadata(r_meta)\n\n    def _process_cached_requests(\n        self,\n        scheduler_output: SchedulerOutput,\n        metadata: LMCacheMPConnectorMetadata,\n    ) -> None:\n        blocks_per_chunk = self.scheduler_adapter.num_blocks_per_chunk()\n\n        cached_reqs = scheduler_output.scheduled_cached_reqs\n        for idx, request_id in enumerate(cached_reqs.req_ids):\n            request_tracker = self._get_request_tracker(request_id)\n\n            # Update block ids\n            new_block_ids = reformat_block_ids(cached_reqs.new_block_ids[idx])\n            if request_id not in cached_reqs.resumed_req_ids:\n                request_tracker.append_block_ids(new_block_ids)\n\n            # Update new scheduled tokens\n            num_new_tokens = cached_reqs.num_computed_tokens[idx]\n            request_tracker.increase_num_scheduled_tokens(num_new_tokens)\n\n            r_meta = LMCacheMPRequestMetadata.GetStoreMetadata(\n                request_tracker, blocks_per_chunk, self.vllm_block_size\n            )\n\n            if r_meta is not None:\n                metadata.add_request_metadata(r_meta)\n\n    def _get_request_tracker(self, request_id: str) -> LMCacheMPRequestTracker:\n        assert request_id in self.request_trackers, (\n            f\"Request tracker for request_id {request_id} not found. \"\n        )\n        return self.request_trackers[request_id]\n\n    def _get_or_create_request_tracker(\n        self, request: \"Request\"\n    ) -> LMCacheMPRequestTracker:\n        request_id = request.request_id\n        # Remove the old trackers that is created before the preemption\n        if (\n            request.status == RequestStatus.PREEMPTED\n            and request_id in self.request_trackers\n        ):\n            tracker = self.request_trackers[request_id]\n\n            # NOTE: since this function may be called multiple times\n            # for a single request (because get_num_new_matched_tokens\n            # may be called multiple times) for the same request, we\n            # will only do the remove if the tracker is not in the \"fresh\"\n            # state, i.e., PREFETCHING\n            if tracker.state != LMCacheMPRequestState.PREFETCHING:\n                self.request_trackers.pop(request_id)\n\n        if request_id not in self.request_trackers:\n            new_tracker = LMCacheMPRequestTracker(request)\n            self.request_trackers[request_id] = new_tracker\n        return self.request_trackers[request_id]\n\n    def _cleanup_request_tracker(self, request_id: str) -> None:\n        \"\"\"\n        Clean up request tracker and associated lookup future for a request.\n        This should be called when a request is finished to prevent memory leak.\n        \"\"\"\n        # Clean up request tracker\n        if self.request_trackers.pop(request_id, None):\n            logger.debug(\n                \"[KVConnector] Cleaned up request_tracker for request %s\",\n                request_id,\n            )",
      "language": "python"
    },
    {
      "code": "class LMCacheMPConnector(KVConnectorBase_V1):\n    \"\"\"\n    The connector for LMCache multi-process mode.\n\n    Extra configs (kv_transfer_config.extra_config):\n    - lmcache.mp.host: the host of the LMCache server.\n    - lmcache.mp.port: the port of the LMCache server.\n    \"\"\"\n\n    def __init__(\n        self,\n        vllm_config: \"VllmConfig\",\n        role: KVConnectorRole,\n        kv_cache_config: Optional[\"KVCacheConfig\"] = None,\n    ):\n        super().__init__(vllm_config, role, kv_cache_config)\n\n        assert vllm_config.kv_transfer_config is not None\n        server_host = vllm_config.kv_transfer_config.get_from_extra_config(\n            \"lmcache.mp.host\", \"tcp://localhost\"\n        )\n        server_port = vllm_config.kv_transfer_config.get_from_extra_config(\n            \"lmcache.mp.port\", 5555\n        )\n\n        server_url = f\"{server_host}:{server_port}\"\n        zmq_context = zmq.Context.instance()\n        if self.role == KVConnectorRole.SCHEDULER:\n            self.scheduler_adapter = create_scheduler_adapter(\n                server_url, zmq_context, vllm_config\n            )\n            self.request_trackers: dict[str, LMCacheMPRequestTracker] = {}\n        elif self.role == KVConnectorRole.WORKER:\n            self.worker_adapter = create_worker_adapter(\n                server_url, zmq_context, vllm_config\n            )\n        else:\n            raise ValueError(f\"Unknown KVConnectorRole: {self.role}\")\n\n        self.vllm_block_size = vllm_config.cache_config.block_size\n\n    @property\n    def role(self) -> KVConnectorRole:\n        return self._role\n\n    # ==============================\n    # Worker-side methods\n    # ==============================\n\n    def _get_connector_metadata(self) -> KVConnectorMetadata:\n        \"\"\"Get the connector metadata.\n\n        This function should only be called inside the connector.\n\n        Returns:\n            ConnectorMetadata: the connector metadata.\n        \"\"\"\n\n        # Should only be called while set to valid metadata.\n        assert self._connector_metadata is not None\n        return self._connector_metadata\n\n    def register_kv_caches(self, kv_caches: dict[str, torch.Tensor]):\n        \"\"\"\n        Initialize with the KV caches. Useful for pre-registering the\n        KV Caches in the KVConnector (e.g. for NIXL).\n\n        Args:\n            kv_caches: dictionary of layer names, kv cache\n        \"\"\"\n        logger.info(\"Registering kv caches!\")\n        self.worker_adapter.register_kv_caches(kv_caches)\n        return\n\n    def start_load_kv(self, forward_context: \"ForwardContext\", **kwargs: Any) -> None:\n        \"\"\"\n        Start loading the KV cache from the connector to vLLM's paged\n        KV buffer. This is called from the forward context before the\n        forward pass to enable async loading during model execution.\n\n        Args:\n            forward_context (ForwardContext): the forward context.\n            **kwargs: additional arguments for the load operation\n\n        Note:\n            The number of elements in kv_caches and layer_names should be\n            the same.\n\n        \"\"\"\n        metadata = self._get_connector_metadata()\n        assert isinstance(metadata, LMCacheMPConnectorMetadata)\n\n        request_ids = []\n        ops = []\n\n        for meta in metadata.requests:\n            if meta.direction != \"RETRIEVE\":\n                continue\n            request_ids.append(meta.request_id)\n            ops.append(meta.op)\n\n        if len(request_ids) == 0:\n            return\n\n        with torch.cuda.stream(torch.cuda.current_stream()):\n            event = torch.cuda.Event(interprocess=True)\n            event.record()\n\n        self.worker_adapter.batched_submit_retrieve_requests(request_ids, ops, event)\n\n    def wait_for_layer_load(self, layer_name: str) -> None:\n        \"\"\"\n        Block until the KV for a specific layer is loaded into vLLM's\n        paged buffer. This is called from within attention layer to ensure\n        async copying from start_load_kv is complete.\n\n        This interface will be useful for layer-by-layer pipelining.\n\n        Args:\n            layer_name: the name of that layer\n        \"\"\"\n        return\n\n    def save_kv_layer(\n        self,\n        layer_name: str,\n        kv_layer: torch.Tensor,\n        attn_metadata: AttentionMetadata,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"\n        Start saving a layer of KV cache from vLLM's paged buffer\n        to the connector. This is called from within attention layer to\n        enable async copying during execution.\n\n        Args:\n            layer_name (str): the name of the layer.\n            kv_layer (torch.Tensor): the paged KV buffer of the current\n                layer in vLLM.\n            attn_metadata (AttentionMetadata): the attention metadata.\n            **kwargs: additional arguments for the save operation.\n        \"\"\"\n        return\n\n    def wait_for_save(self):\n        \"\"\"\n        Block until all the save operations is done. This is called\n        as the forward context exits to ensure that the async saving\n        from save_kv_layer is complete before finishing the forward.\n\n        This prevents overwrites of paged KV buffer before saving done.\n        \"\"\"\n        metadata = self._get_connector_metadata()\n        assert isinstance(metadata, LMCacheMPConnectorMetadata)\n\n        request_ids = []\n        ops = []\n        for meta in metadata.requests:\n            if meta.direction != \"STORE\":\n                continue\n            request_ids.append(meta.request_id)\n            ops.append(meta.op)\n\n        if len(request_ids) == 0:\n            return\n\n        with torch.cuda.stream(torch.cuda.current_stream()):\n            event = torch.cuda.Event(interprocess=True)\n            event.record()\n\n        self.worker_adapter.batched_submit_store_requests(request_ids, ops, event)\n\n    def get_finished(\n        self, finished_req_ids: set[str]\n    ) -> tuple[set[str] | None, set[str] | None]:\n        \"\"\"\n        Notifies worker-side connector ids of requests that have\n        finished generating tokens on the worker.\n        The scheduler process (via the Executors) will use this output\n        to track which workers are done.\n\n        Returns:\n            ids of requests that have finished asynchronous transfer\n            (requests that previously returned True from request_finished()),\n            tuple of (sending/saving ids, recving/loading ids).\n            The finished saves/sends req ids must belong to a set provided in a\n            call to this method (this call or a prior one).\n        \"\"\"\n        val = self.worker_adapter.get_finished(finished_req_ids)\n        # logger.error(\"Finished req ids: %s, %s\", val[0], val[1])\n        return val\n\n    def get_block_ids_with_load_errors(self) -> set[int]:\n        \"\"\"\n        Get the set of block IDs that failed to load.\n\n        Returns:\n            Set of block IDs that encountered load errors.\n            Empty set if no load errors occurred.\n\n        Notes:\n            - Applies to both sync- and async-loading requests.\n            - Async loading: failed blocks may be reported in any forward pass\n              up to and including the pass where the request ID is returned by\n              `get_finished()`. Even if failures occur, the request must still\n              be reported via `get_finished()`, and the failed block IDs must\n              appear here no later than that same pass.\n            - Sync loading: failed blocks should be reported in the forward\n              pass in which they are detected.\n        \"\"\"\n        # TODO: add error tracking\n        return set()\n\n    def shutdown(self):\n        \"\"\"\n        Shutdown the connector. This is called when the worker process\n        is shutting down to ensure that all the async operations are\n        completed and the connector is cleaned up properly.\n        \"\"\"\n        if hasattr(self, \"worker_adapter\"):\n            self.worker_adapter.shutdown()\n        return None\n\n    def get_kv_connector_stats(self) -> Optional[\"KVConnectorStats\"]:\n        \"\"\"\n        Get the KV connector stats collected during the last interval.\n        \"\"\"\n        return None\n\n    # ==============================\n    # Scheduler-side methods\n    # ==============================\n\n    def get_num_new_matched_tokens(\n        self,\n        request: \"Request\",\n        num_computed_tokens: int,\n    ) -> tuple[int | None, bool]:\n        \"\"\"\n        Get number of new tokens that can be loaded from the\n        external KV cache beyond the num_computed_tokens.\n\n        Args:\n            request (Request): the request object.\n            num_computed_tokens (int): the number of locally\n                computed tokens for this request\n\n        Returns:\n            A tuple with the following elements:\n                - An optional number of tokens that can be loaded from the\n                  external KV cache beyond what is already computed.\n                  If None, it means that the connector needs more time to\n                  determine the number of matched tokens, and the scheduler\n                  should query for this request again later.\n                - `True` if external KV cache tokens will be loaded\n                  asynchronously (between scheduler steps). Must be\n                  'False' if the first element is 0.\n\n        Notes:\n            The connector should only consider the largest prefix of prompt-\n            tokens for which KV cache is actually available at the time of the\n            call. If the cache cannot be loaded for some tokens (e.g., due to\n            connectivity issues or eviction), those tokens must not be taken\n            into account.\n        \"\"\"\n        tracker = self._get_or_create_request_tracker(request)\n        # TODO: support loading KV for preempted requests in the future\n        if request.status == RequestStatus.PREEMPTED:\n            return 0, False\n\n        self.scheduler_adapter.maybe_submit_lookup_request(\n            request.request_id, convert_block_hashes_to_bytes(request.block_hashes)\n        )\n\n        ret = self.scheduler_adapter.check_lookup_result(request.request_id)\n        if ret is None:\n            return None, True\n\n        if ret == 0:\n            return 0, False\n\n        assert (\n            ret % (self.scheduler_adapter.num_blocks_per_chunk() * self.vllm_block_size)\n            == 0\n        )\n\n        # Update num stored blocks for the tracker\n        num_vllm_blocks = num_computed_tokens // self.vllm_block_size\n        num_lmcache_blocks = ret // self.vllm_block_size\n        tracker.increase_num_stored_blocks(num_lmcache_blocks)\n\n        # Save the vllm and lmcache hit tokens\n        tracker.num_vllm_hit_blocks = num_vllm_blocks\n        tracker.num_lmcache_hit_blocks = num_lmcache_blocks\n\n        need_to_load = max(0, ret - num_computed_tokens)\n        logger.debug(\n            \"vLLM hit is: %d, Need to load is %d\", num_computed_tokens, need_to_load\n        )\n        return need_to_load, need_to_load > 0\n\n    def update_state_after_alloc(\n        self, request: \"Request\", blocks: \"KVCacheBlocks\", num_external_tokens: int\n    ):\n        \"\"\"\n        Update KVConnector state after block allocation.\n\n        If get_num_new_matched_tokens previously returned True for a\n        request, this function may be called twice for that same request -\n        first when blocks are allocated for the connector tokens to be\n        asynchronously loaded into, and second when any additional blocks\n        are allocated, after the load/transfer is complete.\n\n        Args:\n            request (Request): the request object.\n            blocks (KVCacheBlocks): the blocks allocated for the request.\n            num_external_tokens (int): the number of tokens that will be\n                loaded from the external KV cache.\n        \"\"\"\n        # NOTE: the `blocks` are NEW BLOCKS allocated for this request.\n        tracker = self._get_request_tracker(request.request_id)\n        block_ids = reformat_block_ids(blocks.get_block_ids())\n\n        # No matter we need to retrieve or not, we need to update\n        # the block ids into the tracker\n        tracker.append_block_ids(block_ids)\n\n        # Update the state of the tracker\n        condition = tracker.needs_retrieve()\n        if tracker.state == LMCacheMPRequestState.PREFETCHING:\n            # If need to retrieve, change to WAITING_FOR_LOAD\n            # Otherwise, change to READY\n            tracker.state = (\n                LMCacheMPRequestState.WAITING_FOR_LOAD\n                if condition\n                else LMCacheMPRequestState.READY\n            )\n            # Clean up lookup future in scheduler adapter\n            self.scheduler_adapter._cleanup_lookup_result(request.request_id)\n\n    def build_connector_meta(\n        self, scheduler_output: SchedulerOutput\n    ) -> KVConnectorMetadata:\n        \"\"\"\n        Build the connector metadata for this step.\n\n        This function should NOT modify fields in the scheduler_output.\n        Also, calling this function will reset the state of the connector.\n\n        Args:\n            scheduler_output (SchedulerOutput): the scheduler output object.\n        \"\"\"\n        metadata = LMCacheMPConnectorMetadata()\n\n        self._process_retrieve_requests(metadata)\n        self._process_new_requests(scheduler_output, metadata)\n        self._process_cached_requests(scheduler_output, metadata)\n\n        if len(metadata) > 0:\n            logger.debug(\"Final connector metadata: %s\", metadata)\n\n        return metadata\n\n    def update_connector_output(self, connector_output: KVConnectorOutput):\n        \"\"\"\n        Update KVConnector state from worker-side connectors output.\n\n        Args:\n            connector_output (KVConnectorOutput): the worker-side\n                connectors output.\n        \"\"\"\n        return\n\n    def request_finished(\n        self,\n        request: \"Request\",\n        block_ids: list[int],\n    ) -> tuple[bool, dict[str, Any] | None]:\n        \"\"\"\n        Called exactly once when a request has finished, before its blocks are\n        freed.\n\n        The connector may assumes responsibility for freeing the blocks\n        asynchronously by returning True.\n\n        Returns:\n            True if the request is being saved/sent asynchronously and blocks\n            should not be freed until the request_id is returned from\n            get_finished().\n            Optional KVTransferParams to be included in the request outputs\n            returned by the engine.\n        \"\"\"\n        # Clean up request tracker to prevent memory leak\n        self._cleanup_request_tracker(request.request_id)\n        return True, None\n\n    def take_events(self) -> Iterable[\"KVCacheEvent\"]:\n        \"\"\"\n        Take the KV cache events from the connector.\n\n        Yields:\n            New KV cache events since the last call.\n        \"\"\"\n        return ()\n\n    @classmethod\n    def get_required_kvcache_layout(cls, vllm_config: \"VllmConfig\") -> str | None:\n        \"\"\"\n        Get the required KV cache layout for this connector.\n        Args:\n            vllm_config (VllmConfig): the vllm config.\n\n        Returns:\n            str: the required KV cache layout. e.g. HND, or NHD.\n            None if the connector does not require a specific layout.\n        \"\"\"\n\n        if cls is KVConnectorBase_V1:\n            raise TypeError(\n                \"get_required_kvcache_layout should not be called \"\n                \"on the abstract base class\"\n            )\n        return None\n\n    def get_finished_count(self) -> int | None:\n        \"\"\"\n        Get the count of requests expected to complete send/receive operations\n        via this connector. This method is used to initialize the\n        KVOutputAggregator, overwriting the default world_size.\n\n        Returns:\n            int: expected sending or receiving completion count.\n        \"\"\"\n        return None\n\n    @classmethod\n    def build_kv_connector_stats(\n        cls, data: dict[str, Any] | None = None\n    ) -> Optional[\"KVConnectorStats\"]:\n        \"\"\"\n        KVConnectorStats resolution method. This method allows dynamically\n        registered connectors to return their own KVConnectorStats object,\n        which can implement custom aggregation logic on the data dict.\n        \"\"\"\n        return None\n\n    @classmethod\n    def build_prom_metrics(\n        cls,\n        vllm_config: \"VllmConfig\",\n        metric_types: dict[type[\"PromMetric\"], type[\"PromMetricT\"]],\n        labelnames: list[str],\n        per_engine_labelvalues: dict[int, list[object]],\n    ) -> Optional[\"KVConnectorPromMetrics\"]:\n        \"\"\"\n        Create a KVConnectorPromMetrics subclass which should register\n        per-connector Prometheus metrics and implement observe() to\n        expose connector transfer stats via Prometheus.\n        \"\"\"\n        return None\n\n    ##############################\n    # Helper functions\n    ##############################\n    def _process_retrieve_requests(\n        self,\n        metadata: LMCacheMPConnectorMetadata,\n    ) -> None:\n        blocks_per_chunk = self.scheduler_adapter.num_blocks_per_chunk()\n\n        for request_tracker in self.request_trackers.values():\n            if request_tracker.state != LMCacheMPRequestState.WAITING_FOR_LOAD:\n                continue\n            r_metadata = LMCacheMPRequestMetadata.GetRetrieveMetadata(\n                request_tracker, blocks_per_chunk\n            )\n            if r_metadata is not None:\n                metadata.add_request_metadata(r_metadata)\n            request_tracker.state = LMCacheMPRequestState.READY\n\n    def _process_new_requests(\n        self,\n        scheduler_output: SchedulerOutput,\n        metadata: LMCacheMPConnectorMetadata,\n    ) -> None:\n        blocks_per_chunk = self.scheduler_adapter.num_blocks_per_chunk()\n\n        for new_request in scheduler_output.scheduled_new_reqs:\n            request_tracker = self._get_request_tracker(new_request.req_id)\n\n            num_new_tokens = scheduler_output.num_scheduled_tokens[new_request.req_id]\n            request_tracker.increase_num_scheduled_tokens(num_new_tokens)\n\n            r_meta = LMCacheMPRequestMetadata.GetStoreMetadata(\n                request_tracker, blocks_per_chunk, self.vllm_block_size\n            )\n            if r_meta is not None:\n                metadata.add_request_metadata(r_meta)\n\n    def _process_cached_requests(\n        self,\n        scheduler_output: SchedulerOutput,\n        metadata: LMCacheMPConnectorMetadata,\n    ) -> None:\n        blocks_per_chunk = self.scheduler_adapter.num_blocks_per_chunk()\n\n        cached_reqs = scheduler_output.scheduled_cached_reqs\n        for idx, request_id in enumerate(cached_reqs.req_ids):\n            request_tracker = self._get_request_tracker(request_id)\n\n            # Update block ids\n            new_block_ids = reformat_block_ids(cached_reqs.new_block_ids[idx])\n            if request_id not in cached_reqs.resumed_req_ids:\n                request_tracker.append_block_ids(new_block_ids)\n\n            # Update new scheduled tokens\n            num_new_tokens = cached_reqs.num_computed_tokens[idx]\n            request_tracker.increase_num_scheduled_tokens(num_new_tokens)\n\n            r_meta = LMCacheMPRequestMetadata.GetStoreMetadata(\n                request_tracker, blocks_per_chunk, self.vllm_block_size\n            )\n\n            if r_meta is not None:\n                metadata.add_request_metadata(r_meta)\n\n    def _get_request_tracker(self, request_id: str) -> LMCacheMPRequestTracker:\n        assert request_id in self.request_trackers, (\n            f\"Request tracker for request_id {request_id} not found. \"\n        )\n        return self.request_trackers[request_id]\n\n    def _get_or_create_request_tracker(\n        self, request: \"Request\"\n    ) -> LMCacheMPRequestTracker:\n        request_id = request.request_id\n        # Remove the old trackers that is created before the preemption\n        if (\n            request.status == RequestStatus.PREEMPTED\n            and request_id in self.request_trackers\n        ):\n            tracker = self.request_trackers[request_id]\n\n            # NOTE: since this function may be called multiple times\n            # for a single request (because get_num_new_matched_tokens\n            # may be called multiple times) for the same request, we\n            # will only do the remove if the tracker is not in the \"fresh\"\n            # state, i.e., PREFETCHING\n            if tracker.state != LMCacheMPRequestState.PREFETCHING:\n                self.request_trackers.pop(request_id)\n\n        if request_id not in self.request_trackers:\n            new_tracker = LMCacheMPRequestTracker(request)\n            self.request_trackers[request_id] = new_tracker\n        return self.request_trackers[request_id]\n\n    def _cleanup_request_tracker(self, request_id: str) -> None:\n        \"\"\"\n        Clean up request tracker and associated lookup future for a request.\n        This should be called when a request is finished to prevent memory leak.\n        \"\"\"\n        # Clean up request tracker\n        if self.request_trackers.pop(request_id, None):\n            logger.debug(\n                \"[KVConnector] Cleaned up request_tracker for request %s\",\n                request_id,\n            )",
      "language": "python"
    },
    {
      "code": "request_trackers: dict[str, LMCacheMPRequestTracker] = {}",
      "language": "yaml"
    },
    {
      "code": "request_trackers: dict[str, LMCacheMPRequestTracker] = {}",
      "language": "yaml"
    },
    {
      "code": "role: KVConnectorRole",
      "language": "yaml"
    },
    {
      "code": "role: KVConnectorRole",
      "language": "yaml"
    },
    {
      "code": "scheduler_adapter = create_scheduler_adapter(\n    server_url, zmq_context, vllm_config\n)",
      "language": "unknown"
    },
    {
      "code": "scheduler_adapter = create_scheduler_adapter(\n    server_url, zmq_context, vllm_config\n)",
      "language": "unknown"
    },
    {
      "code": "vllm_block_size = block_size",
      "language": "unknown"
    },
    {
      "code": "vllm_block_size = block_size",
      "language": "unknown"
    },
    {
      "code": "worker_adapter = create_worker_adapter(\n    server_url, zmq_context, vllm_config\n)",
      "language": "unknown"
    },
    {
      "code": "worker_adapter = create_worker_adapter(\n    server_url, zmq_context, vllm_config\n)",
      "language": "unknown"
    },
    {
      "code": "__init__(\n    vllm_config: VllmConfig,\n    role: KVConnectorRole,\n    kv_cache_config: Optional[KVCacheConfig] = None,\n)",
      "language": "python"
    },
    {
      "code": "__init__(\n    vllm_config: VllmConfig,\n    role: KVConnectorRole,\n    kv_cache_config: Optional[KVCacheConfig] = None,\n)",
      "language": "python"
    },
    {
      "code": "376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406",
      "language": "unknown"
    },
    {
      "code": "def __init__(\n    self,\n    vllm_config: \"VllmConfig\",\n    role: KVConnectorRole,\n    kv_cache_config: Optional[\"KVCacheConfig\"] = None,\n):\n    super().__init__(vllm_config, role, kv_cache_config)\n\n    assert vllm_config.kv_transfer_config is not None\n    server_host = vllm_config.kv_transfer_config.get_from_extra_config(\n        \"lmcache.mp.host\", \"tcp://localhost\"\n    )\n    server_port = vllm_config.kv_transfer_config.get_from_extra_config(\n        \"lmcache.mp.port\", 5555\n    )\n\n    server_url = f\"{server_host}:{server_port}\"\n    zmq_context = zmq.Context.instance()\n    if self.role == KVConnectorRole.SCHEDULER:\n        self.scheduler_adapter = create_scheduler_adapter(\n            server_url, zmq_context, vllm_config\n        )\n        self.request_trackers: dict[str, LMCacheMPRequestTracker] = {}\n    elif self.role == KVConnectorRole.WORKER:\n        self.worker_adapter = create_worker_adapter(\n            server_url, zmq_context, vllm_config\n        )\n    else:\n        raise ValueError(f\"Unknown KVConnectorRole: {self.role}\")\n\n    self.vllm_block_size = vllm_config.cache_config.block_size",
      "language": "python"
    },
    {
      "code": "def __init__(\n    self,\n    vllm_config: \"VllmConfig\",\n    role: KVConnectorRole,\n    kv_cache_config: Optional[\"KVCacheConfig\"] = None,\n):\n    super().__init__(vllm_config, role, kv_cache_config)\n\n    assert vllm_config.kv_transfer_config is not None\n    server_host = vllm_config.kv_transfer_config.get_from_extra_config(\n        \"lmcache.mp.host\", \"tcp://localhost\"\n    )\n    server_port = vllm_config.kv_transfer_config.get_from_extra_config(\n        \"lmcache.mp.port\", 5555\n    )\n\n    server_url = f\"{server_host}:{server_port}\"\n    zmq_context = zmq.Context.instance()\n    if self.role == KVConnectorRole.SCHEDULER:\n        self.scheduler_adapter = create_scheduler_adapter(\n            server_url, zmq_context, vllm_config\n        )\n        self.request_trackers: dict[str, LMCacheMPRequestTracker] = {}\n    elif self.role == KVConnectorRole.WORKER:\n        self.worker_adapter = create_worker_adapter(\n            server_url, zmq_context, vllm_config\n        )\n    else:\n        raise ValueError(f\"Unknown KVConnectorRole: {self.role}\")\n\n    self.vllm_block_size = vllm_config.cache_config.block_size",
      "language": "python"
    },
    {
      "code": "_cleanup_request_tracker(request_id: str) -> None",
      "language": "rust"
    },
    {
      "code": "_cleanup_request_tracker(request_id: str) -> None",
      "language": "rust"
    },
    {
      "code": "923\n924\n925\n926\n927\n928\n929\n930\n931\n932\n933",
      "language": "unknown"
    },
    {
      "code": "def _cleanup_request_tracker(self, request_id: str) -> None:\n    \"\"\"\n    Clean up request tracker and associated lookup future for a request.\n    This should be called when a request is finished to prevent memory leak.\n    \"\"\"\n    # Clean up request tracker\n    if self.request_trackers.pop(request_id, None):\n        logger.debug(\n            \"[KVConnector] Cleaned up request_tracker for request %s\",\n            request_id,\n        )",
      "language": "python"
    },
    {
      "code": "def _cleanup_request_tracker(self, request_id: str) -> None:\n    \"\"\"\n    Clean up request tracker and associated lookup future for a request.\n    This should be called when a request is finished to prevent memory leak.\n    \"\"\"\n    # Clean up request tracker\n    if self.request_trackers.pop(request_id, None):\n        logger.debug(\n            \"[KVConnector] Cleaned up request_tracker for request %s\",\n            request_id,\n        )",
      "language": "python"
    },
    {
      "code": "_get_connector_metadata() -> KVConnectorMetadata",
      "language": "php"
    },
    {
      "code": "_get_connector_metadata() -> KVConnectorMetadata",
      "language": "php"
    },
    {
      "code": "416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427",
      "language": "unknown"
    },
    {
      "code": "def _get_connector_metadata(self) -> KVConnectorMetadata:\n    \"\"\"Get the connector metadata.\n\n    This function should only be called inside the connector.\n\n    Returns:\n        ConnectorMetadata: the connector metadata.\n    \"\"\"\n\n    # Should only be called while set to valid metadata.\n    assert self._connector_metadata is not None\n    return self._connector_metadata",
      "language": "python"
    },
    {
      "code": "def _get_connector_metadata(self) -> KVConnectorMetadata:\n    \"\"\"Get the connector metadata.\n\n    This function should only be called inside the connector.\n\n    Returns:\n        ConnectorMetadata: the connector metadata.\n    \"\"\"\n\n    # Should only be called while set to valid metadata.\n    assert self._connector_metadata is not None\n    return self._connector_metadata",
      "language": "python"
    },
    {
      "code": "_get_or_create_request_tracker(\n    request: Request,\n) -> LMCacheMPRequestTracker",
      "language": "php"
    },
    {
      "code": "_get_or_create_request_tracker(\n    request: Request,\n) -> LMCacheMPRequestTracker",
      "language": "php"
    },
    {
      "code": "899\n900\n901\n902\n903\n904\n905\n906\n907\n908\n909\n910\n911\n912\n913\n914\n915\n916\n917\n918\n919\n920\n921",
      "language": "unknown"
    },
    {
      "code": "def _get_or_create_request_tracker(\n    self, request: \"Request\"\n) -> LMCacheMPRequestTracker:\n    request_id = request.request_id\n    # Remove the old trackers that is created before the preemption\n    if (\n        request.status == RequestStatus.PREEMPTED\n        and request_id in self.request_trackers\n    ):\n        tracker = self.request_trackers[request_id]\n\n        # NOTE: since this function may be called multiple times\n        # for a single request (because get_num_new_matched_tokens\n        # may be called multiple times) for the same request, we\n        # will only do the remove if the tracker is not in the \"fresh\"\n        # state, i.e., PREFETCHING\n        if tracker.state != LMCacheMPRequestState.PREFETCHING:\n            self.request_trackers.pop(request_id)\n\n    if request_id not in self.request_trackers:\n        new_tracker = LMCacheMPRequestTracker(request)\n        self.request_trackers[request_id] = new_tracker\n    return self.request_trackers[request_id]",
      "language": "python"
    },
    {
      "code": "def _get_or_create_request_tracker(\n    self, request: \"Request\"\n) -> LMCacheMPRequestTracker:\n    request_id = request.request_id\n    # Remove the old trackers that is created before the preemption\n    if (\n        request.status == RequestStatus.PREEMPTED\n        and request_id in self.request_trackers\n    ):\n        tracker = self.request_trackers[request_id]\n\n        # NOTE: since this function may be called multiple times\n        # for a single request (because get_num_new_matched_tokens\n        # may be called multiple times) for the same request, we\n        # will only do the remove if the tracker is not in the \"fresh\"\n        # state, i.e., PREFETCHING\n        if tracker.state != LMCacheMPRequestState.PREFETCHING:\n            self.request_trackers.pop(request_id)\n\n    if request_id not in self.request_trackers:\n        new_tracker = LMCacheMPRequestTracker(request)\n        self.request_trackers[request_id] = new_tracker\n    return self.request_trackers[request_id]",
      "language": "python"
    },
    {
      "code": "_get_request_tracker(\n    request_id: str,\n) -> LMCacheMPRequestTracker",
      "language": "php"
    },
    {
      "code": "_get_request_tracker(\n    request_id: str,\n) -> LMCacheMPRequestTracker",
      "language": "php"
    },
    {
      "code": "893\n894\n895\n896\n897",
      "language": "unknown"
    },
    {
      "code": "def _get_request_tracker(self, request_id: str) -> LMCacheMPRequestTracker:\n    assert request_id in self.request_trackers, (\n        f\"Request tracker for request_id {request_id} not found. \"\n    )\n    return self.request_trackers[request_id]",
      "language": "python"
    },
    {
      "code": "def _get_request_tracker(self, request_id: str) -> LMCacheMPRequestTracker:\n    assert request_id in self.request_trackers, (\n        f\"Request tracker for request_id {request_id} not found. \"\n    )\n    return self.request_trackers[request_id]",
      "language": "python"
    },
    {
      "code": "_process_cached_requests(\n    scheduler_output: SchedulerOutput,\n    metadata: LMCacheMPConnectorMetadata,\n) -> None",
      "language": "rust"
    },
    {
      "code": "_process_cached_requests(\n    scheduler_output: SchedulerOutput,\n    metadata: LMCacheMPConnectorMetadata,\n) -> None",
      "language": "rust"
    },
    {
      "code": "866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891",
      "language": "unknown"
    },
    {
      "code": "def _process_cached_requests(\n    self,\n    scheduler_output: SchedulerOutput,\n    metadata: LMCacheMPConnectorMetadata,\n) -> None:\n    blocks_per_chunk = self.scheduler_adapter.num_blocks_per_chunk()\n\n    cached_reqs = scheduler_output.scheduled_cached_reqs\n    for idx, request_id in enumerate(cached_reqs.req_ids):\n        request_tracker = self._get_request_tracker(request_id)\n\n        # Update block ids\n        new_block_ids = reformat_block_ids(cached_reqs.new_block_ids[idx])\n        if request_id not in cached_reqs.resumed_req_ids:\n            request_tracker.append_block_ids(new_block_ids)\n\n        # Update new scheduled tokens\n        num_new_tokens = cached_reqs.num_computed_tokens[idx]\n        request_tracker.increase_num_scheduled_tokens(num_new_tokens)\n\n        r_meta = LMCacheMPRequestMetadata.GetStoreMetadata(\n            request_tracker, blocks_per_chunk, self.vllm_block_size\n        )\n\n        if r_meta is not None:\n            metadata.add_request_metadata(r_meta)",
      "language": "python"
    },
    {
      "code": "def _process_cached_requests(\n    self,\n    scheduler_output: SchedulerOutput,\n    metadata: LMCacheMPConnectorMetadata,\n) -> None:\n    blocks_per_chunk = self.scheduler_adapter.num_blocks_per_chunk()\n\n    cached_reqs = scheduler_output.scheduled_cached_reqs\n    for idx, request_id in enumerate(cached_reqs.req_ids):\n        request_tracker = self._get_request_tracker(request_id)\n\n        # Update block ids\n        new_block_ids = reformat_block_ids(cached_reqs.new_block_ids[idx])\n        if request_id not in cached_reqs.resumed_req_ids:\n            request_tracker.append_block_ids(new_block_ids)\n\n        # Update new scheduled tokens\n        num_new_tokens = cached_reqs.num_computed_tokens[idx]\n        request_tracker.increase_num_scheduled_tokens(num_new_tokens)\n\n        r_meta = LMCacheMPRequestMetadata.GetStoreMetadata(\n            request_tracker, blocks_per_chunk, self.vllm_block_size\n        )\n\n        if r_meta is not None:\n            metadata.add_request_metadata(r_meta)",
      "language": "python"
    },
    {
      "code": "_process_new_requests(\n    scheduler_output: SchedulerOutput,\n    metadata: LMCacheMPConnectorMetadata,\n) -> None",
      "language": "rust"
    },
    {
      "code": "_process_new_requests(\n    scheduler_output: SchedulerOutput,\n    metadata: LMCacheMPConnectorMetadata,\n) -> None",
      "language": "rust"
    },
    {
      "code": "847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864",
      "language": "unknown"
    },
    {
      "code": "def _process_new_requests(\n    self,\n    scheduler_output: SchedulerOutput,\n    metadata: LMCacheMPConnectorMetadata,\n) -> None:\n    blocks_per_chunk = self.scheduler_adapter.num_blocks_per_chunk()\n\n    for new_request in scheduler_output.scheduled_new_reqs:\n        request_tracker = self._get_request_tracker(new_request.req_id)\n\n        num_new_tokens = scheduler_output.num_scheduled_tokens[new_request.req_id]\n        request_tracker.increase_num_scheduled_tokens(num_new_tokens)\n\n        r_meta = LMCacheMPRequestMetadata.GetStoreMetadata(\n            request_tracker, blocks_per_chunk, self.vllm_block_size\n        )\n        if r_meta is not None:\n            metadata.add_request_metadata(r_meta)",
      "language": "python"
    },
    {
      "code": "def _process_new_requests(\n    self,\n    scheduler_output: SchedulerOutput,\n    metadata: LMCacheMPConnectorMetadata,\n) -> None:\n    blocks_per_chunk = self.scheduler_adapter.num_blocks_per_chunk()\n\n    for new_request in scheduler_output.scheduled_new_reqs:\n        request_tracker = self._get_request_tracker(new_request.req_id)\n\n        num_new_tokens = scheduler_output.num_scheduled_tokens[new_request.req_id]\n        request_tracker.increase_num_scheduled_tokens(num_new_tokens)\n\n        r_meta = LMCacheMPRequestMetadata.GetStoreMetadata(\n            request_tracker, blocks_per_chunk, self.vllm_block_size\n        )\n        if r_meta is not None:\n            metadata.add_request_metadata(r_meta)",
      "language": "python"
    },
    {
      "code": "_process_retrieve_requests(\n    metadata: LMCacheMPConnectorMetadata,\n) -> None",
      "language": "rust"
    },
    {
      "code": "_process_retrieve_requests(\n    metadata: LMCacheMPConnectorMetadata,\n) -> None",
      "language": "rust"
    },
    {
      "code": "831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845",
      "language": "unknown"
    },
    {
      "code": "def _process_retrieve_requests(\n    self,\n    metadata: LMCacheMPConnectorMetadata,\n) -> None:\n    blocks_per_chunk = self.scheduler_adapter.num_blocks_per_chunk()\n\n    for request_tracker in self.request_trackers.values():\n        if request_tracker.state != LMCacheMPRequestState.WAITING_FOR_LOAD:\n            continue\n        r_metadata = LMCacheMPRequestMetadata.GetRetrieveMetadata(\n            request_tracker, blocks_per_chunk\n        )\n        if r_metadata is not None:\n            metadata.add_request_metadata(r_metadata)\n        request_tracker.state = LMCacheMPRequestState.READY",
      "language": "python"
    },
    {
      "code": "def _process_retrieve_requests(\n    self,\n    metadata: LMCacheMPConnectorMetadata,\n) -> None:\n    blocks_per_chunk = self.scheduler_adapter.num_blocks_per_chunk()\n\n    for request_tracker in self.request_trackers.values():\n        if request_tracker.state != LMCacheMPRequestState.WAITING_FOR_LOAD:\n            continue\n        r_metadata = LMCacheMPRequestMetadata.GetRetrieveMetadata(\n            request_tracker, blocks_per_chunk\n        )\n        if r_metadata is not None:\n            metadata.add_request_metadata(r_metadata)\n        request_tracker.state = LMCacheMPRequestState.READY",
      "language": "python"
    },
    {
      "code": "build_connector_meta(\n    scheduler_output: SchedulerOutput,\n) -> KVConnectorMetadata",
      "language": "php"
    },
    {
      "code": "build_connector_meta(\n    scheduler_output: SchedulerOutput,\n) -> KVConnectorMetadata",
      "language": "php"
    },
    {
      "code": "707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728",
      "language": "unknown"
    },
    {
      "code": "def build_connector_meta(\n    self, scheduler_output: SchedulerOutput\n) -> KVConnectorMetadata:\n    \"\"\"\n    Build the connector metadata for this step.\n\n    This function should NOT modify fields in the scheduler_output.\n    Also, calling this function will reset the state of the connector.\n\n    Args:\n        scheduler_output (SchedulerOutput): the scheduler output object.\n    \"\"\"\n    metadata = LMCacheMPConnectorMetadata()\n\n    self._process_retrieve_requests(metadata)\n    self._process_new_requests(scheduler_output, metadata)\n    self._process_cached_requests(scheduler_output, metadata)\n\n    if len(metadata) > 0:\n        logger.debug(\"Final connector metadata: %s\", metadata)\n\n    return metadata",
      "language": "python"
    },
    {
      "code": "def build_connector_meta(\n    self, scheduler_output: SchedulerOutput\n) -> KVConnectorMetadata:\n    \"\"\"\n    Build the connector metadata for this step.\n\n    This function should NOT modify fields in the scheduler_output.\n    Also, calling this function will reset the state of the connector.\n\n    Args:\n        scheduler_output (SchedulerOutput): the scheduler output object.\n    \"\"\"\n    metadata = LMCacheMPConnectorMetadata()\n\n    self._process_retrieve_requests(metadata)\n    self._process_new_requests(scheduler_output, metadata)\n    self._process_cached_requests(scheduler_output, metadata)\n\n    if len(metadata) > 0:\n        logger.debug(\"Final connector metadata: %s\", metadata)\n\n    return metadata",
      "language": "python"
    },
    {
      "code": "build_kv_connector_stats(\n    data: dict[str, Any] | None = None,\n) -> Optional[KVConnectorStats]",
      "language": "rust"
    },
    {
      "code": "build_kv_connector_stats(\n    data: dict[str, Any] | None = None,\n) -> Optional[KVConnectorStats]",
      "language": "rust"
    },
    {
      "code": "802\n803\n804\n805\n806\n807\n808\n809\n810\n811",
      "language": "unknown"
    },
    {
      "code": "@classmethod\ndef build_kv_connector_stats(\n    cls, data: dict[str, Any] | None = None\n) -> Optional[\"KVConnectorStats\"]:\n    \"\"\"\n    KVConnectorStats resolution method. This method allows dynamically\n    registered connectors to return their own KVConnectorStats object,\n    which can implement custom aggregation logic on the data dict.\n    \"\"\"\n    return None",
      "language": "python"
    },
    {
      "code": "@classmethod\ndef build_kv_connector_stats(\n    cls, data: dict[str, Any] | None = None\n) -> Optional[\"KVConnectorStats\"]:\n    \"\"\"\n    KVConnectorStats resolution method. This method allows dynamically\n    registered connectors to return their own KVConnectorStats object,\n    which can implement custom aggregation logic on the data dict.\n    \"\"\"\n    return None",
      "language": "python"
    },
    {
      "code": "build_prom_metrics(\n    vllm_config: VllmConfig,\n    metric_types: dict[type[PromMetric], type[PromMetricT]],\n    labelnames: list[str],\n    per_engine_labelvalues: dict[int, list[object]],\n) -> Optional[KVConnectorPromMetrics]",
      "language": "php"
    },
    {
      "code": "build_prom_metrics(\n    vllm_config: VllmConfig,\n    metric_types: dict[type[PromMetric], type[PromMetricT]],\n    labelnames: list[str],\n    per_engine_labelvalues: dict[int, list[object]],\n) -> Optional[KVConnectorPromMetrics]",
      "language": "php"
    },
    {
      "code": "813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826",
      "language": "unknown"
    },
    {
      "code": "@classmethod\ndef build_prom_metrics(\n    cls,\n    vllm_config: \"VllmConfig\",\n    metric_types: dict[type[\"PromMetric\"], type[\"PromMetricT\"]],\n    labelnames: list[str],\n    per_engine_labelvalues: dict[int, list[object]],\n) -> Optional[\"KVConnectorPromMetrics\"]:\n    \"\"\"\n    Create a KVConnectorPromMetrics subclass which should register\n    per-connector Prometheus metrics and implement observe() to\n    expose connector transfer stats via Prometheus.\n    \"\"\"\n    return None",
      "language": "python"
    },
    {
      "code": "@classmethod\ndef build_prom_metrics(\n    cls,\n    vllm_config: \"VllmConfig\",\n    metric_types: dict[type[\"PromMetric\"], type[\"PromMetricT\"]],\n    labelnames: list[str],\n    per_engine_labelvalues: dict[int, list[object]],\n) -> Optional[\"KVConnectorPromMetrics\"]:\n    \"\"\"\n    Create a KVConnectorPromMetrics subclass which should register\n    per-connector Prometheus metrics and implement observe() to\n    expose connector transfer stats via Prometheus.\n    \"\"\"\n    return None",
      "language": "python"
    },
    {
      "code": "get_block_ids_with_load_errors() -> set[int]",
      "language": "php"
    },
    {
      "code": "get_block_ids_with_load_errors() -> set[int]",
      "language": "php"
    },
    {
      "code": "559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578",
      "language": "unknown"
    },
    {
      "code": "def get_block_ids_with_load_errors(self) -> set[int]:\n    \"\"\"\n    Get the set of block IDs that failed to load.\n\n    Returns:\n        Set of block IDs that encountered load errors.\n        Empty set if no load errors occurred.\n\n    Notes:\n        - Applies to both sync- and async-loading requests.\n        - Async loading: failed blocks may be reported in any forward pass\n          up to and including the pass where the request ID is returned by\n          `get_finished()`. Even if failures occur, the request must still\n          be reported via `get_finished()`, and the failed block IDs must\n          appear here no later than that same pass.\n        - Sync loading: failed blocks should be reported in the forward\n          pass in which they are detected.\n    \"\"\"\n    # TODO: add error tracking\n    return set()",
      "language": "python"
    },
    {
      "code": "def get_block_ids_with_load_errors(self) -> set[int]:\n    \"\"\"\n    Get the set of block IDs that failed to load.\n\n    Returns:\n        Set of block IDs that encountered load errors.\n        Empty set if no load errors occurred.\n\n    Notes:\n        - Applies to both sync- and async-loading requests.\n        - Async loading: failed blocks may be reported in any forward pass\n          up to and including the pass where the request ID is returned by\n          `get_finished()`. Even if failures occur, the request must still\n          be reported via `get_finished()`, and the failed block IDs must\n          appear here no later than that same pass.\n        - Sync loading: failed blocks should be reported in the forward\n          pass in which they are detected.\n    \"\"\"\n    # TODO: add error tracking\n    return set()",
      "language": "python"
    },
    {
      "code": "get_finished(\n    finished_req_ids: set[str],\n) -> tuple[set[str] | None, set[str] | None]",
      "language": "rust"
    },
    {
      "code": "get_finished(\n    finished_req_ids: set[str],\n) -> tuple[set[str] | None, set[str] | None]",
      "language": "rust"
    },
    {
      "code": "539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557",
      "language": "unknown"
    },
    {
      "code": "def get_finished(\n    self, finished_req_ids: set[str]\n) -> tuple[set[str] | None, set[str] | None]:\n    \"\"\"\n    Notifies worker-side connector ids of requests that have\n    finished generating tokens on the worker.\n    The scheduler process (via the Executors) will use this output\n    to track which workers are done.\n\n    Returns:\n        ids of requests that have finished asynchronous transfer\n        (requests that previously returned True from request_finished()),\n        tuple of (sending/saving ids, recving/loading ids).\n        The finished saves/sends req ids must belong to a set provided in a\n        call to this method (this call or a prior one).\n    \"\"\"\n    val = self.worker_adapter.get_finished(finished_req_ids)\n    # logger.error(\"Finished req ids: %s, %s\", val[0], val[1])\n    return val",
      "language": "python"
    },
    {
      "code": "def get_finished(\n    self, finished_req_ids: set[str]\n) -> tuple[set[str] | None, set[str] | None]:\n    \"\"\"\n    Notifies worker-side connector ids of requests that have\n    finished generating tokens on the worker.\n    The scheduler process (via the Executors) will use this output\n    to track which workers are done.\n\n    Returns:\n        ids of requests that have finished asynchronous transfer\n        (requests that previously returned True from request_finished()),\n        tuple of (sending/saving ids, recving/loading ids).\n        The finished saves/sends req ids must belong to a set provided in a\n        call to this method (this call or a prior one).\n    \"\"\"\n    val = self.worker_adapter.get_finished(finished_req_ids)\n    # logger.error(\"Finished req ids: %s, %s\", val[0], val[1])\n    return val",
      "language": "python"
    },
    {
      "code": "get_finished_count() -> int | None",
      "language": "rust"
    },
    {
      "code": "get_finished_count() -> int | None",
      "language": "rust"
    },
    {
      "code": "791\n792\n793\n794\n795\n796\n797\n798\n799\n800",
      "language": "unknown"
    },
    {
      "code": "def get_finished_count(self) -> int | None:\n    \"\"\"\n    Get the count of requests expected to complete send/receive operations\n    via this connector. This method is used to initialize the\n    KVOutputAggregator, overwriting the default world_size.\n\n    Returns:\n        int: expected sending or receiving completion count.\n    \"\"\"\n    return None",
      "language": "python"
    },
    {
      "code": "def get_finished_count(self) -> int | None:\n    \"\"\"\n    Get the count of requests expected to complete send/receive operations\n    via this connector. This method is used to initialize the\n    KVOutputAggregator, overwriting the default world_size.\n\n    Returns:\n        int: expected sending or receiving completion count.\n    \"\"\"\n    return None",
      "language": "python"
    },
    {
      "code": "get_kv_connector_stats() -> Optional[KVConnectorStats]",
      "language": "php"
    },
    {
      "code": "get_kv_connector_stats() -> Optional[KVConnectorStats]",
      "language": "php"
    },
    {
      "code": "590\n591\n592\n593\n594",
      "language": "unknown"
    },
    {
      "code": "def get_kv_connector_stats(self) -> Optional[\"KVConnectorStats\"]:\n    \"\"\"\n    Get the KV connector stats collected during the last interval.\n    \"\"\"\n    return None",
      "language": "python"
    },
    {
      "code": "def get_kv_connector_stats(self) -> Optional[\"KVConnectorStats\"]:\n    \"\"\"\n    Get the KV connector stats collected during the last interval.\n    \"\"\"\n    return None",
      "language": "python"
    },
    {
      "code": "get_num_new_matched_tokens(\n    request: Request, num_computed_tokens: int\n) -> tuple[int | None, bool]",
      "language": "rust"
    },
    {
      "code": "get_num_new_matched_tokens(\n    request: Request, num_computed_tokens: int\n) -> tuple[int | None, bool]",
      "language": "rust"
    },
    {
      "code": "600\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666",
      "language": "unknown"
    },
    {
      "code": "def get_num_new_matched_tokens(\n    self,\n    request: \"Request\",\n    num_computed_tokens: int,\n) -> tuple[int | None, bool]:\n    \"\"\"\n    Get number of new tokens that can be loaded from the\n    external KV cache beyond the num_computed_tokens.\n\n    Args:\n        request (Request): the request object.\n        num_computed_tokens (int): the number of locally\n            computed tokens for this request\n\n    Returns:\n        A tuple with the following elements:\n            - An optional number of tokens that can be loaded from the\n              external KV cache beyond what is already computed.\n              If None, it means that the connector needs more time to\n              determine the number of matched tokens, and the scheduler\n              should query for this request again later.\n            - `True` if external KV cache tokens will be loaded\n              asynchronously (between scheduler steps). Must be\n              'False' if the first element is 0.\n\n    Notes:\n        The connector should only consider the largest prefix of prompt-\n        tokens for which KV cache is actually available at the time of the\n        call. If the cache cannot be loaded for some tokens (e.g., due to\n        connectivity issues or eviction), those tokens must not be taken\n        into account.\n    \"\"\"\n    tracker = self._get_or_create_request_tracker(request)\n    # TODO: support loading KV for preempted requests in the future\n    if request.status == RequestStatus.PREEMPTED:\n        return 0, False\n\n    self.scheduler_adapter.maybe_submit_lookup_request(\n        request.request_id, convert_block_hashes_to_bytes(request.block_hashes)\n    )\n\n    ret = self.scheduler_adapter.check_lookup_result(request.request_id)\n    if ret is None:\n        return None, True\n\n    if ret == 0:\n        return 0, False\n\n    assert (\n        ret % (self.scheduler_adapter.num_blocks_per_chunk() * self.vllm_block_size)\n        == 0\n    )\n\n    # Update num stored blocks for the tracker\n    num_vllm_blocks = num_computed_tokens // self.vllm_block_size\n    num_lmcache_blocks = ret // self.vllm_block_size\n    tracker.increase_num_stored_blocks(num_lmcache_blocks)\n\n    # Save the vllm and lmcache hit tokens\n    tracker.num_vllm_hit_blocks = num_vllm_blocks\n    tracker.num_lmcache_hit_blocks = num_lmcache_blocks\n\n    need_to_load = max(0, ret - num_computed_tokens)\n    logger.debug(\n        \"vLLM hit is: %d, Need to load is %d\", num_computed_tokens, need_to_load\n    )\n    return need_to_load, need_to_load > 0",
      "language": "python"
    },
    {
      "code": "def get_num_new_matched_tokens(\n    self,\n    request: \"Request\",\n    num_computed_tokens: int,\n) -> tuple[int | None, bool]:\n    \"\"\"\n    Get number of new tokens that can be loaded from the\n    external KV cache beyond the num_computed_tokens.\n\n    Args:\n        request (Request): the request object.\n        num_computed_tokens (int): the number of locally\n            computed tokens for this request\n\n    Returns:\n        A tuple with the following elements:\n            - An optional number of tokens that can be loaded from the\n              external KV cache beyond what is already computed.\n              If None, it means that the connector needs more time to\n              determine the number of matched tokens, and the scheduler\n              should query for this request again later.\n            - `True` if external KV cache tokens will be loaded\n              asynchronously (between scheduler steps). Must be\n              'False' if the first element is 0.\n\n    Notes:\n        The connector should only consider the largest prefix of prompt-\n        tokens for which KV cache is actually available at the time of the\n        call. If the cache cannot be loaded for some tokens (e.g., due to\n        connectivity issues or eviction), those tokens must not be taken\n        into account.\n    \"\"\"\n    tracker = self._get_or_create_request_tracker(request)\n    # TODO: support loading KV for preempted requests in the future\n    if request.status == RequestStatus.PREEMPTED:\n        return 0, False\n\n    self.scheduler_adapter.maybe_submit_lookup_request(\n        request.request_id, convert_block_hashes_to_bytes(request.block_hashes)\n    )\n\n    ret = self.scheduler_adapter.check_lookup_result(request.request_id)\n    if ret is None:\n        return None, True\n\n    if ret == 0:\n        return 0, False\n\n    assert (\n        ret % (self.scheduler_adapter.num_blocks_per_chunk() * self.vllm_block_size)\n        == 0\n    )\n\n    # Update num stored blocks for the tracker\n    num_vllm_blocks = num_computed_tokens // self.vllm_block_size\n    num_lmcache_blocks = ret // self.vllm_block_size\n    tracker.increase_num_stored_blocks(num_lmcache_blocks)\n\n    # Save the vllm and lmcache hit tokens\n    tracker.num_vllm_hit_blocks = num_vllm_blocks\n    tracker.num_lmcache_hit_blocks = num_lmcache_blocks\n\n    need_to_load = max(0, ret - num_computed_tokens)\n    logger.debug(\n        \"vLLM hit is: %d, Need to load is %d\", num_computed_tokens, need_to_load\n    )\n    return need_to_load, need_to_load > 0",
      "language": "python"
    },
    {
      "code": "get_required_kvcache_layout(\n    vllm_config: VllmConfig,\n) -> str | None",
      "language": "rust"
    },
    {
      "code": "get_required_kvcache_layout(\n    vllm_config: VllmConfig,\n) -> str | None",
      "language": "rust"
    },
    {
      "code": "772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789",
      "language": "unknown"
    },
    {
      "code": "@classmethod\ndef get_required_kvcache_layout(cls, vllm_config: \"VllmConfig\") -> str | None:\n    \"\"\"\n    Get the required KV cache layout for this connector.\n    Args:\n        vllm_config (VllmConfig): the vllm config.\n\n    Returns:\n        str: the required KV cache layout. e.g. HND, or NHD.\n        None if the connector does not require a specific layout.\n    \"\"\"\n\n    if cls is KVConnectorBase_V1:\n        raise TypeError(\n            \"get_required_kvcache_layout should not be called \"\n            \"on the abstract base class\"\n        )\n    return None",
      "language": "python"
    },
    {
      "code": "@classmethod\ndef get_required_kvcache_layout(cls, vllm_config: \"VllmConfig\") -> str | None:\n    \"\"\"\n    Get the required KV cache layout for this connector.\n    Args:\n        vllm_config (VllmConfig): the vllm config.\n\n    Returns:\n        str: the required KV cache layout. e.g. HND, or NHD.\n        None if the connector does not require a specific layout.\n    \"\"\"\n\n    if cls is KVConnectorBase_V1:\n        raise TypeError(\n            \"get_required_kvcache_layout should not be called \"\n            \"on the abstract base class\"\n        )\n    return None",
      "language": "python"
    },
    {
      "code": "register_kv_caches(kv_caches: dict[str, Tensor])",
      "language": "unknown"
    },
    {
      "code": "register_kv_caches(kv_caches: dict[str, Tensor])",
      "language": "unknown"
    },
    {
      "code": "429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439",
      "language": "unknown"
    },
    {
      "code": "def register_kv_caches(self, kv_caches: dict[str, torch.Tensor]):\n    \"\"\"\n    Initialize with the KV caches. Useful for pre-registering the\n    KV Caches in the KVConnector (e.g. for NIXL).\n\n    Args:\n        kv_caches: dictionary of layer names, kv cache\n    \"\"\"\n    logger.info(\"Registering kv caches!\")\n    self.worker_adapter.register_kv_caches(kv_caches)\n    return",
      "language": "python"
    },
    {
      "code": "def register_kv_caches(self, kv_caches: dict[str, torch.Tensor]):\n    \"\"\"\n    Initialize with the KV caches. Useful for pre-registering the\n    KV Caches in the KVConnector (e.g. for NIXL).\n\n    Args:\n        kv_caches: dictionary of layer names, kv cache\n    \"\"\"\n    logger.info(\"Registering kv caches!\")\n    self.worker_adapter.register_kv_caches(kv_caches)\n    return",
      "language": "python"
    },
    {
      "code": "request_finished(\n    request: Request, block_ids: list[int]\n) -> tuple[bool, dict[str, Any] | None]",
      "language": "rust"
    },
    {
      "code": "request_finished(\n    request: Request, block_ids: list[int]\n) -> tuple[bool, dict[str, Any] | None]",
      "language": "rust"
    },
    {
      "code": "740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761",
      "language": "unknown"
    },
    {
      "code": "def request_finished(\n    self,\n    request: \"Request\",\n    block_ids: list[int],\n) -> tuple[bool, dict[str, Any] | None]:\n    \"\"\"\n    Called exactly once when a request has finished, before its blocks are\n    freed.\n\n    The connector may assumes responsibility for freeing the blocks\n    asynchronously by returning True.\n\n    Returns:\n        True if the request is being saved/sent asynchronously and blocks\n        should not be freed until the request_id is returned from\n        get_finished().\n        Optional KVTransferParams to be included in the request outputs\n        returned by the engine.\n    \"\"\"\n    # Clean up request tracker to prevent memory leak\n    self._cleanup_request_tracker(request.request_id)\n    return True, None",
      "language": "python"
    },
    {
      "code": "def request_finished(\n    self,\n    request: \"Request\",\n    block_ids: list[int],\n) -> tuple[bool, dict[str, Any] | None]:\n    \"\"\"\n    Called exactly once when a request has finished, before its blocks are\n    freed.\n\n    The connector may assumes responsibility for freeing the blocks\n    asynchronously by returning True.\n\n    Returns:\n        True if the request is being saved/sent asynchronously and blocks\n        should not be freed until the request_id is returned from\n        get_finished().\n        Optional KVTransferParams to be included in the request outputs\n        returned by the engine.\n    \"\"\"\n    # Clean up request tracker to prevent memory leak\n    self._cleanup_request_tracker(request.request_id)\n    return True, None",
      "language": "python"
    },
    {
      "code": "save_kv_layer(\n    layer_name: str,\n    kv_layer: Tensor,\n    attn_metadata: AttentionMetadata,\n    **kwargs: Any,\n) -> None",
      "language": "rust"
    },
    {
      "code": "save_kv_layer(\n    layer_name: str,\n    kv_layer: Tensor,\n    attn_metadata: AttentionMetadata,\n    **kwargs: Any,\n) -> None",
      "language": "rust"
    },
    {
      "code": "490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n500\n501\n502\n503\n504\n505\n506\n507\n508\n509",
      "language": "unknown"
    },
    {
      "code": "def save_kv_layer(\n    self,\n    layer_name: str,\n    kv_layer: torch.Tensor,\n    attn_metadata: AttentionMetadata,\n    **kwargs: Any,\n) -> None:\n    \"\"\"\n    Start saving a layer of KV cache from vLLM's paged buffer\n    to the connector. This is called from within attention layer to\n    enable async copying during execution.\n\n    Args:\n        layer_name (str): the name of the layer.\n        kv_layer (torch.Tensor): the paged KV buffer of the current\n            layer in vLLM.\n        attn_metadata (AttentionMetadata): the attention metadata.\n        **kwargs: additional arguments for the save operation.\n    \"\"\"\n    return",
      "language": "python"
    },
    {
      "code": "def save_kv_layer(\n    self,\n    layer_name: str,\n    kv_layer: torch.Tensor,\n    attn_metadata: AttentionMetadata,\n    **kwargs: Any,\n) -> None:\n    \"\"\"\n    Start saving a layer of KV cache from vLLM's paged buffer\n    to the connector. This is called from within attention layer to\n    enable async copying during execution.\n\n    Args:\n        layer_name (str): the name of the layer.\n        kv_layer (torch.Tensor): the paged KV buffer of the current\n            layer in vLLM.\n        attn_metadata (AttentionMetadata): the attention metadata.\n        **kwargs: additional arguments for the save operation.\n    \"\"\"\n    return",
      "language": "python"
    },
    {
      "code": "580\n581\n582\n583\n584\n585\n586\n587\n588",
      "language": "unknown"
    },
    {
      "code": "def shutdown(self):\n    \"\"\"\n    Shutdown the connector. This is called when the worker process\n    is shutting down to ensure that all the async operations are\n    completed and the connector is cleaned up properly.\n    \"\"\"\n    if hasattr(self, \"worker_adapter\"):\n        self.worker_adapter.shutdown()\n    return None",
      "language": "python"
    },
    {
      "code": "def shutdown(self):\n    \"\"\"\n    Shutdown the connector. This is called when the worker process\n    is shutting down to ensure that all the async operations are\n    completed and the connector is cleaned up properly.\n    \"\"\"\n    if hasattr(self, \"worker_adapter\"):\n        self.worker_adapter.shutdown()\n    return None",
      "language": "python"
    },
    {
      "code": "start_load_kv(\n    forward_context: ForwardContext, **kwargs: Any\n) -> None",
      "language": "rust"
    },
    {
      "code": "start_load_kv(\n    forward_context: ForwardContext, **kwargs: Any\n) -> None",
      "language": "rust"
    },
    {
      "code": "441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475",
      "language": "unknown"
    },
    {
      "code": "def start_load_kv(self, forward_context: \"ForwardContext\", **kwargs: Any) -> None:\n    \"\"\"\n    Start loading the KV cache from the connector to vLLM's paged\n    KV buffer. This is called from the forward context before the\n    forward pass to enable async loading during model execution.\n\n    Args:\n        forward_context (ForwardContext): the forward context.\n        **kwargs: additional arguments for the load operation\n\n    Note:\n        The number of elements in kv_caches and layer_names should be\n        the same.\n\n    \"\"\"\n    metadata = self._get_connector_metadata()\n    assert isinstance(metadata, LMCacheMPConnectorMetadata)\n\n    request_ids = []\n    ops = []\n\n    for meta in metadata.requests:\n        if meta.direction != \"RETRIEVE\":\n            continue\n        request_ids.append(meta.request_id)\n        ops.append(meta.op)\n\n    if len(request_ids) == 0:\n        return\n\n    with torch.cuda.stream(torch.cuda.current_stream()):\n        event = torch.cuda.Event(interprocess=True)\n        event.record()\n\n    self.worker_adapter.batched_submit_retrieve_requests(request_ids, ops, event)",
      "language": "python"
    },
    {
      "code": "def start_load_kv(self, forward_context: \"ForwardContext\", **kwargs: Any) -> None:\n    \"\"\"\n    Start loading the KV cache from the connector to vLLM's paged\n    KV buffer. This is called from the forward context before the\n    forward pass to enable async loading during model execution.\n\n    Args:\n        forward_context (ForwardContext): the forward context.\n        **kwargs: additional arguments for the load operation\n\n    Note:\n        The number of elements in kv_caches and layer_names should be\n        the same.\n\n    \"\"\"\n    metadata = self._get_connector_metadata()\n    assert isinstance(metadata, LMCacheMPConnectorMetadata)\n\n    request_ids = []\n    ops = []\n\n    for meta in metadata.requests:\n        if meta.direction != \"RETRIEVE\":\n            continue\n        request_ids.append(meta.request_id)\n        ops.append(meta.op)\n\n    if len(request_ids) == 0:\n        return\n\n    with torch.cuda.stream(torch.cuda.current_stream()):\n        event = torch.cuda.Event(interprocess=True)\n        event.record()\n\n    self.worker_adapter.batched_submit_retrieve_requests(request_ids, ops, event)",
      "language": "python"
    },
    {
      "code": "take_events() -> Iterable[KVCacheEvent]",
      "language": "php"
    },
    {
      "code": "take_events() -> Iterable[KVCacheEvent]",
      "language": "php"
    },
    {
      "code": "763\n764\n765\n766\n767\n768\n769\n770",
      "language": "unknown"
    },
    {
      "code": "def take_events(self) -> Iterable[\"KVCacheEvent\"]:\n    \"\"\"\n    Take the KV cache events from the connector.\n\n    Yields:\n        New KV cache events since the last call.\n    \"\"\"\n    return ()",
      "language": "python"
    },
    {
      "code": "def take_events(self) -> Iterable[\"KVCacheEvent\"]:\n    \"\"\"\n    Take the KV cache events from the connector.\n\n    Yields:\n        New KV cache events since the last call.\n    \"\"\"\n    return ()",
      "language": "python"
    },
    {
      "code": "update_connector_output(\n    connector_output: KVConnectorOutput,\n)",
      "language": "yaml"
    },
    {
      "code": "update_connector_output(\n    connector_output: KVConnectorOutput,\n)",
      "language": "yaml"
    },
    {
      "code": "730\n731\n732\n733\n734\n735\n736\n737\n738",
      "language": "unknown"
    },
    {
      "code": "def update_connector_output(self, connector_output: KVConnectorOutput):\n    \"\"\"\n    Update KVConnector state from worker-side connectors output.\n\n    Args:\n        connector_output (KVConnectorOutput): the worker-side\n            connectors output.\n    \"\"\"\n    return",
      "language": "sql"
    },
    {
      "code": "def update_connector_output(self, connector_output: KVConnectorOutput):\n    \"\"\"\n    Update KVConnector state from worker-side connectors output.\n\n    Args:\n        connector_output (KVConnectorOutput): the worker-side\n            connectors output.\n    \"\"\"\n    return",
      "language": "sql"
    },
    {
      "code": "update_state_after_alloc(\n    request: Request,\n    blocks: KVCacheBlocks,\n    num_external_tokens: int,\n)",
      "language": "yaml"
    },
    {
      "code": "update_state_after_alloc(\n    request: Request,\n    blocks: KVCacheBlocks,\n    num_external_tokens: int,\n)",
      "language": "yaml"
    },
    {
      "code": "668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\n700\n701\n702\n703\n704\n705",
      "language": "unknown"
    },
    {
      "code": "def update_state_after_alloc(\n    self, request: \"Request\", blocks: \"KVCacheBlocks\", num_external_tokens: int\n):\n    \"\"\"\n    Update KVConnector state after block allocation.\n\n    If get_num_new_matched_tokens previously returned True for a\n    request, this function may be called twice for that same request -\n    first when blocks are allocated for the connector tokens to be\n    asynchronously loaded into, and second when any additional blocks\n    are allocated, after the load/transfer is complete.\n\n    Args:\n        request (Request): the request object.\n        blocks (KVCacheBlocks): the blocks allocated for the request.\n        num_external_tokens (int): the number of tokens that will be\n            loaded from the external KV cache.\n    \"\"\"\n    # NOTE: the `blocks` are NEW BLOCKS allocated for this request.\n    tracker = self._get_request_tracker(request.request_id)\n    block_ids = reformat_block_ids(blocks.get_block_ids())\n\n    # No matter we need to retrieve or not, we need to update\n    # the block ids into the tracker\n    tracker.append_block_ids(block_ids)\n\n    # Update the state of the tracker\n    condition = tracker.needs_retrieve()\n    if tracker.state == LMCacheMPRequestState.PREFETCHING:\n        # If need to retrieve, change to WAITING_FOR_LOAD\n        # Otherwise, change to READY\n        tracker.state = (\n            LMCacheMPRequestState.WAITING_FOR_LOAD\n            if condition\n            else LMCacheMPRequestState.READY\n        )\n        # Clean up lookup future in scheduler adapter\n        self.scheduler_adapter._cleanup_lookup_result(request.request_id)",
      "language": "python"
    },
    {
      "code": "def update_state_after_alloc(\n    self, request: \"Request\", blocks: \"KVCacheBlocks\", num_external_tokens: int\n):\n    \"\"\"\n    Update KVConnector state after block allocation.\n\n    If get_num_new_matched_tokens previously returned True for a\n    request, this function may be called twice for that same request -\n    first when blocks are allocated for the connector tokens to be\n    asynchronously loaded into, and second when any additional blocks\n    are allocated, after the load/transfer is complete.\n\n    Args:\n        request (Request): the request object.\n        blocks (KVCacheBlocks): the blocks allocated for the request.\n        num_external_tokens (int): the number of tokens that will be\n            loaded from the external KV cache.\n    \"\"\"\n    # NOTE: the `blocks` are NEW BLOCKS allocated for this request.\n    tracker = self._get_request_tracker(request.request_id)\n    block_ids = reformat_block_ids(blocks.get_block_ids())\n\n    # No matter we need to retrieve or not, we need to update\n    # the block ids into the tracker\n    tracker.append_block_ids(block_ids)\n\n    # Update the state of the tracker\n    condition = tracker.needs_retrieve()\n    if tracker.state == LMCacheMPRequestState.PREFETCHING:\n        # If need to retrieve, change to WAITING_FOR_LOAD\n        # Otherwise, change to READY\n        tracker.state = (\n            LMCacheMPRequestState.WAITING_FOR_LOAD\n            if condition\n            else LMCacheMPRequestState.READY\n        )\n        # Clean up lookup future in scheduler adapter\n        self.scheduler_adapter._cleanup_lookup_result(request.request_id)",
      "language": "python"
    },
    {
      "code": "wait_for_layer_load(layer_name: str) -> None",
      "language": "rust"
    },
    {
      "code": "wait_for_layer_load(layer_name: str) -> None",
      "language": "rust"
    },
    {
      "code": "477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488",
      "language": "unknown"
    },
    {
      "code": "def wait_for_layer_load(self, layer_name: str) -> None:\n    \"\"\"\n    Block until the KV for a specific layer is loaded into vLLM's\n    paged buffer. This is called from within attention layer to ensure\n    async copying from start_load_kv is complete.\n\n    This interface will be useful for layer-by-layer pipelining.\n\n    Args:\n        layer_name: the name of that layer\n    \"\"\"\n    return",
      "language": "python"
    },
    {
      "code": "def wait_for_layer_load(self, layer_name: str) -> None:\n    \"\"\"\n    Block until the KV for a specific layer is loaded into vLLM's\n    paged buffer. This is called from within attention layer to ensure\n    async copying from start_load_kv is complete.\n\n    This interface will be useful for layer-by-layer pipelining.\n\n    Args:\n        layer_name: the name of that layer\n    \"\"\"\n    return",
      "language": "python"
    },
    {
      "code": "wait_for_save()",
      "language": "unknown"
    },
    {
      "code": "wait_for_save()",
      "language": "unknown"
    },
    {
      "code": "511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537",
      "language": "unknown"
    },
    {
      "code": "def wait_for_save(self):\n    \"\"\"\n    Block until all the save operations is done. This is called\n    as the forward context exits to ensure that the async saving\n    from save_kv_layer is complete before finishing the forward.\n\n    This prevents overwrites of paged KV buffer before saving done.\n    \"\"\"\n    metadata = self._get_connector_metadata()\n    assert isinstance(metadata, LMCacheMPConnectorMetadata)\n\n    request_ids = []\n    ops = []\n    for meta in metadata.requests:\n        if meta.direction != \"STORE\":\n            continue\n        request_ids.append(meta.request_id)\n        ops.append(meta.op)\n\n    if len(request_ids) == 0:\n        return\n\n    with torch.cuda.stream(torch.cuda.current_stream()):\n        event = torch.cuda.Event(interprocess=True)\n        event.record()\n\n    self.worker_adapter.batched_submit_store_requests(request_ids, ops, event)",
      "language": "python"
    },
    {
      "code": "def wait_for_save(self):\n    \"\"\"\n    Block until all the save operations is done. This is called\n    as the forward context exits to ensure that the async saving\n    from save_kv_layer is complete before finishing the forward.\n\n    This prevents overwrites of paged KV buffer before saving done.\n    \"\"\"\n    metadata = self._get_connector_metadata()\n    assert isinstance(metadata, LMCacheMPConnectorMetadata)\n\n    request_ids = []\n    ops = []\n    for meta in metadata.requests:\n        if meta.direction != \"STORE\":\n            continue\n        request_ids.append(meta.request_id)\n        ops.append(meta.op)\n\n    if len(request_ids) == 0:\n        return\n\n    with torch.cuda.stream(torch.cuda.current_stream()):\n        event = torch.cuda.Event(interprocess=True)\n        event.record()\n\n    self.worker_adapter.batched_submit_store_requests(request_ids, ops, event)",
      "language": "python"
    },
    {
      "code": "340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364",
      "language": "unknown"
    },
    {
      "code": "class LMCacheMPConnectorMetadata(KVConnectorMetadata):\n    def __init__(self):\n        super().__init__()\n        self.requests: list[LMCacheMPRequestMetadata] = []\n\n    def add_request_metadata(self, request_metadata: LMCacheMPRequestMetadata):\n        self.requests.append(request_metadata)\n\n    def __len__(self):\n        return len(self.requests)\n\n    # For debugging\n    def __str__(self):\n        request_strs = []\n        for req_meta in self.requests:\n            request_strs.append(\n                f\"RequestMetadata(request_id={req_meta.request_id}, \"\n                f\"direction={req_meta.direction}, \"\n                f\"num_blocks={len(req_meta.op)}, \"\n                f\"block_ids={req_meta.op.block_ids})\"\n            )\n        return \"[\" + \"\\n\".join(request_strs) + \"]\"\n\n    def __repr__(self):\n        return self.__str__()",
      "language": "python"
    },
    {
      "code": "class LMCacheMPConnectorMetadata(KVConnectorMetadata):\n    def __init__(self):\n        super().__init__()\n        self.requests: list[LMCacheMPRequestMetadata] = []\n\n    def add_request_metadata(self, request_metadata: LMCacheMPRequestMetadata):\n        self.requests.append(request_metadata)\n\n    def __len__(self):\n        return len(self.requests)\n\n    # For debugging\n    def __str__(self):\n        request_strs = []\n        for req_meta in self.requests:\n            request_strs.append(\n                f\"RequestMetadata(request_id={req_meta.request_id}, \"\n                f\"direction={req_meta.direction}, \"\n                f\"num_blocks={len(req_meta.op)}, \"\n                f\"block_ids={req_meta.op.block_ids})\"\n            )\n        return \"[\" + \"\\n\".join(request_strs) + \"]\"\n\n    def __repr__(self):\n        return self.__str__()",
      "language": "python"
    },
    {
      "code": "requests: list[LMCacheMPRequestMetadata] = []",
      "language": "yaml"
    },
    {
      "code": "requests: list[LMCacheMPRequestMetadata] = []",
      "language": "yaml"
    },
    {
      "code": "341\n342\n343",
      "language": "unknown"
    },
    {
      "code": "def __init__(self):\n    super().__init__()\n    self.requests: list[LMCacheMPRequestMetadata] = []",
      "language": "python"
    },
    {
      "code": "def __init__(self):\n    super().__init__()\n    self.requests: list[LMCacheMPRequestMetadata] = []",
      "language": "python"
    },
    {
      "code": "def __len__(self):\n    return len(self.requests)",
      "language": "python"
    },
    {
      "code": "def __len__(self):\n    return len(self.requests)",
      "language": "python"
    },
    {
      "code": "def __repr__(self):\n    return self.__str__()",
      "language": "python"
    },
    {
      "code": "def __repr__(self):\n    return self.__str__()",
      "language": "python"
    },
    {
      "code": "352\n353\n354\n355\n356\n357\n358\n359\n360\n361",
      "language": "unknown"
    },
    {
      "code": "def __str__(self):\n    request_strs = []\n    for req_meta in self.requests:\n        request_strs.append(\n            f\"RequestMetadata(request_id={req_meta.request_id}, \"\n            f\"direction={req_meta.direction}, \"\n            f\"num_blocks={len(req_meta.op)}, \"\n            f\"block_ids={req_meta.op.block_ids})\"\n        )\n    return \"[\" + \"\\n\".join(request_strs) + \"]\"",
      "language": "python"
    },
    {
      "code": "def __str__(self):\n    request_strs = []\n    for req_meta in self.requests:\n        request_strs.append(\n            f\"RequestMetadata(request_id={req_meta.request_id}, \"\n            f\"direction={req_meta.direction}, \"\n            f\"num_blocks={len(req_meta.op)}, \"\n            f\"block_ids={req_meta.op.block_ids})\"\n        )\n    return \"[\" + \"\\n\".join(request_strs) + \"]\"",
      "language": "python"
    },
    {
      "code": "add_request_metadata(\n    request_metadata: LMCacheMPRequestMetadata,\n)",
      "language": "yaml"
    },
    {
      "code": "add_request_metadata(\n    request_metadata: LMCacheMPRequestMetadata,\n)",
      "language": "yaml"
    },
    {
      "code": "def add_request_metadata(self, request_metadata: LMCacheMPRequestMetadata):\n    self.requests.append(request_metadata)",
      "language": "python"
    },
    {
      "code": "def add_request_metadata(self, request_metadata: LMCacheMPRequestMetadata):\n    self.requests.append(request_metadata)",
      "language": "python"
    },
    {
      "code": "243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337",
      "language": "unknown"
    },
    {
      "code": "@dataclass\nclass LMCacheMPRequestMetadata:\n    request_id: str\n    direction: Literal[\"STORE\", \"RETRIEVE\"]\n    op: LoadStoreOp\n\n    @staticmethod\n    def GetStoreMetadata(\n        tracker: LMCacheMPRequestTracker,\n        blocks_in_chunk: int,\n        vllm_block_size: int,\n    ) -> \"LMCacheMPRequestMetadata | None\":\n        \"\"\"\n        Generate the store metadata for the current request tracker.\n\n        Args:\n            tracker: The request tracker to generate the metadata from.\n            blocks_in_chunk: the number of blocks in a LMCache data chunk\n        \"\"\"\n        # Store the blocks that has block hashes\n        # NOTE: the invariant here is that `num_stored_blocks` should\n        # always be a multiple of `blocks_in_chunk`\n        # TODO: This should be checked everytime we update the num_stored_blocks\n        min_available_blocks = min(\n            len(tracker.block_hashes),\n            len(tracker.allocated_block_ids),\n            tracker.num_scheduled_tokens // vllm_block_size,\n        )\n        num_staging_blocks = min_available_blocks - tracker.num_stored_blocks\n        num_chunks = num_staging_blocks // blocks_in_chunk\n\n        if num_chunks >= 1:\n            start = tracker.num_stored_blocks\n            end = start + num_chunks * blocks_in_chunk\n            block_hashes = convert_block_hashes_to_bytes(\n                tracker.block_hashes[start:end]\n            )\n            block_ids = tracker.allocated_block_ids[start:end]\n\n            ret = LMCacheMPRequestMetadata(\n                request_id=tracker.request_id,\n                direction=\"STORE\",\n                op=LoadStoreOp(block_hashes=block_hashes, block_ids=block_ids),\n            )\n\n            # Update the request tracker\n            tracker.increase_num_stored_blocks(end - start)\n            return ret\n\n        return None\n\n    @staticmethod\n    def GetRetrieveMetadata(\n        tracker: LMCacheMPRequestTracker,\n        blocks_in_chunk: int,\n    ) -> \"LMCacheMPRequestMetadata | None\":\n        \"\"\"\n        Generate the retrieve metadata for the current request tracker.\n\n        Args:\n            tracker: The request tracker to generate the metadata from.\n            blocks_in_chunk: the number of blocks in a LMCache data chunk\n        \"\"\"\n        if not tracker.is_ready_for_retrieving():\n            return None\n\n        # |---------------------|-----------------|----------------|\n        # | num_vllm_hit_blocks |\n        # | lmcache chunk 1   | lmcache chunk 2   |\n        #                     |  need to retrieve |\n\n        start = tracker.num_vllm_hit_blocks // blocks_in_chunk * blocks_in_chunk\n        end = tracker.num_lmcache_hit_blocks\n        assert end % blocks_in_chunk == 0, (\n            \"The number of LMCache hit blocks should be a multiple of the \"\n            \"number of blocks in a lmcache chunk. \"\n        )\n        assert len(tracker.block_hashes) >= end, (\n            \"The number of block hashes should be greater than or equal to the \"\n            \"number of LMCache hit blocks. \"\n        )\n        if end > start:\n            block_hashes = convert_block_hashes_to_bytes(\n                tracker.block_hashes[start:end]\n            )\n            block_ids = tracker.allocated_block_ids[start:end]\n\n            ret = LMCacheMPRequestMetadata(\n                request_id=tracker.request_id,\n                direction=\"RETRIEVE\",\n                op=LoadStoreOp(block_hashes=block_hashes, block_ids=block_ids),\n            )\n            return ret\n\n        return None",
      "language": "python"
    },
    {
      "code": "@dataclass\nclass LMCacheMPRequestMetadata:\n    request_id: str\n    direction: Literal[\"STORE\", \"RETRIEVE\"]\n    op: LoadStoreOp\n\n    @staticmethod\n    def GetStoreMetadata(\n        tracker: LMCacheMPRequestTracker,\n        blocks_in_chunk: int,\n        vllm_block_size: int,\n    ) -> \"LMCacheMPRequestMetadata | None\":\n        \"\"\"\n        Generate the store metadata for the current request tracker.\n\n        Args:\n            tracker: The request tracker to generate the metadata from.\n            blocks_in_chunk: the number of blocks in a LMCache data chunk\n        \"\"\"\n        # Store the blocks that has block hashes\n        # NOTE: the invariant here is that `num_stored_blocks` should\n        # always be a multiple of `blocks_in_chunk`\n        # TODO: This should be checked everytime we update the num_stored_blocks\n        min_available_blocks = min(\n            len(tracker.block_hashes),\n            len(tracker.allocated_block_ids),\n            tracker.num_scheduled_tokens // vllm_block_size,\n        )\n        num_staging_blocks = min_available_blocks - tracker.num_stored_blocks\n        num_chunks = num_staging_blocks // blocks_in_chunk\n\n        if num_chunks >= 1:\n            start = tracker.num_stored_blocks\n            end = start + num_chunks * blocks_in_chunk\n            block_hashes = convert_block_hashes_to_bytes(\n                tracker.block_hashes[start:end]\n            )\n            block_ids = tracker.allocated_block_ids[start:end]\n\n            ret = LMCacheMPRequestMetadata(\n                request_id=tracker.request_id,\n                direction=\"STORE\",\n                op=LoadStoreOp(block_hashes=block_hashes, block_ids=block_ids),\n            )\n\n            # Update the request tracker\n            tracker.increase_num_stored_blocks(end - start)\n            return ret\n\n        return None\n\n    @staticmethod\n    def GetRetrieveMetadata(\n        tracker: LMCacheMPRequestTracker,\n        blocks_in_chunk: int,\n    ) -> \"LMCacheMPRequestMetadata | None\":\n        \"\"\"\n        Generate the retrieve metadata for the current request tracker.\n\n        Args:\n            tracker: The request tracker to generate the metadata from.\n            blocks_in_chunk: the number of blocks in a LMCache data chunk\n        \"\"\"\n        if not tracker.is_ready_for_retrieving():\n            return None\n\n        # |---------------------|-----------------|----------------|\n        # | num_vllm_hit_blocks |\n        # | lmcache chunk 1   | lmcache chunk 2   |\n        #                     |  need to retrieve |\n\n        start = tracker.num_vllm_hit_blocks // blocks_in_chunk * blocks_in_chunk\n        end = tracker.num_lmcache_hit_blocks\n        assert end % blocks_in_chunk == 0, (\n            \"The number of LMCache hit blocks should be a multiple of the \"\n            \"number of blocks in a lmcache chunk. \"\n        )\n        assert len(tracker.block_hashes) >= end, (\n            \"The number of block hashes should be greater than or equal to the \"\n            \"number of LMCache hit blocks. \"\n        )\n        if end > start:\n            block_hashes = convert_block_hashes_to_bytes(\n                tracker.block_hashes[start:end]\n            )\n            block_ids = tracker.allocated_block_ids[start:end]\n\n            ret = LMCacheMPRequestMetadata(\n                request_id=tracker.request_id,\n                direction=\"RETRIEVE\",\n                op=LoadStoreOp(block_hashes=block_hashes, block_ids=block_ids),\n            )\n            return ret\n\n        return None",
      "language": "python"
    },
    {
      "code": "direction: Literal['STORE', 'RETRIEVE']",
      "language": "yaml"
    },
    {
      "code": "direction: Literal['STORE', 'RETRIEVE']",
      "language": "yaml"
    },
    {
      "code": "op: LoadStoreOp",
      "language": "yaml"
    },
    {
      "code": "op: LoadStoreOp",
      "language": "yaml"
    },
    {
      "code": "request_id: str",
      "language": "yaml"
    },
    {
      "code": "request_id: str",
      "language": "yaml"
    },
    {
      "code": "GetRetrieveMetadata(\n    tracker: LMCacheMPRequestTracker, blocks_in_chunk: int\n) -> LMCacheMPRequestMetadata | None",
      "language": "rust"
    },
    {
      "code": "GetRetrieveMetadata(\n    tracker: LMCacheMPRequestTracker, blocks_in_chunk: int\n) -> LMCacheMPRequestMetadata | None",
      "language": "rust"
    },
    {
      "code": "294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337",
      "language": "unknown"
    },
    {
      "code": "@staticmethod\ndef GetRetrieveMetadata(\n    tracker: LMCacheMPRequestTracker,\n    blocks_in_chunk: int,\n) -> \"LMCacheMPRequestMetadata | None\":\n    \"\"\"\n    Generate the retrieve metadata for the current request tracker.\n\n    Args:\n        tracker: The request tracker to generate the metadata from.\n        blocks_in_chunk: the number of blocks in a LMCache data chunk\n    \"\"\"\n    if not tracker.is_ready_for_retrieving():\n        return None\n\n    # |---------------------|-----------------|----------------|\n    # | num_vllm_hit_blocks |\n    # | lmcache chunk 1   | lmcache chunk 2   |\n    #                     |  need to retrieve |\n\n    start = tracker.num_vllm_hit_blocks // blocks_in_chunk * blocks_in_chunk\n    end = tracker.num_lmcache_hit_blocks\n    assert end % blocks_in_chunk == 0, (\n        \"The number of LMCache hit blocks should be a multiple of the \"\n        \"number of blocks in a lmcache chunk. \"\n    )\n    assert len(tracker.block_hashes) >= end, (\n        \"The number of block hashes should be greater than or equal to the \"\n        \"number of LMCache hit blocks. \"\n    )\n    if end > start:\n        block_hashes = convert_block_hashes_to_bytes(\n            tracker.block_hashes[start:end]\n        )\n        block_ids = tracker.allocated_block_ids[start:end]\n\n        ret = LMCacheMPRequestMetadata(\n            request_id=tracker.request_id,\n            direction=\"RETRIEVE\",\n            op=LoadStoreOp(block_hashes=block_hashes, block_ids=block_ids),\n        )\n        return ret\n\n    return None",
      "language": "python"
    },
    {
      "code": "@staticmethod\ndef GetRetrieveMetadata(\n    tracker: LMCacheMPRequestTracker,\n    blocks_in_chunk: int,\n) -> \"LMCacheMPRequestMetadata | None\":\n    \"\"\"\n    Generate the retrieve metadata for the current request tracker.\n\n    Args:\n        tracker: The request tracker to generate the metadata from.\n        blocks_in_chunk: the number of blocks in a LMCache data chunk\n    \"\"\"\n    if not tracker.is_ready_for_retrieving():\n        return None\n\n    # |---------------------|-----------------|----------------|\n    # | num_vllm_hit_blocks |\n    # | lmcache chunk 1   | lmcache chunk 2   |\n    #                     |  need to retrieve |\n\n    start = tracker.num_vllm_hit_blocks // blocks_in_chunk * blocks_in_chunk\n    end = tracker.num_lmcache_hit_blocks\n    assert end % blocks_in_chunk == 0, (\n        \"The number of LMCache hit blocks should be a multiple of the \"\n        \"number of blocks in a lmcache chunk. \"\n    )\n    assert len(tracker.block_hashes) >= end, (\n        \"The number of block hashes should be greater than or equal to the \"\n        \"number of LMCache hit blocks. \"\n    )\n    if end > start:\n        block_hashes = convert_block_hashes_to_bytes(\n            tracker.block_hashes[start:end]\n        )\n        block_ids = tracker.allocated_block_ids[start:end]\n\n        ret = LMCacheMPRequestMetadata(\n            request_id=tracker.request_id,\n            direction=\"RETRIEVE\",\n            op=LoadStoreOp(block_hashes=block_hashes, block_ids=block_ids),\n        )\n        return ret\n\n    return None",
      "language": "python"
    },
    {
      "code": "GetStoreMetadata(\n    tracker: LMCacheMPRequestTracker,\n    blocks_in_chunk: int,\n    vllm_block_size: int,\n) -> LMCacheMPRequestMetadata | None",
      "language": "rust"
    },
    {
      "code": "GetStoreMetadata(\n    tracker: LMCacheMPRequestTracker,\n    blocks_in_chunk: int,\n    vllm_block_size: int,\n) -> LMCacheMPRequestMetadata | None",
      "language": "rust"
    },
    {
      "code": "249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292",
      "language": "unknown"
    },
    {
      "code": "@staticmethod\ndef GetStoreMetadata(\n    tracker: LMCacheMPRequestTracker,\n    blocks_in_chunk: int,\n    vllm_block_size: int,\n) -> \"LMCacheMPRequestMetadata | None\":\n    \"\"\"\n    Generate the store metadata for the current request tracker.\n\n    Args:\n        tracker: The request tracker to generate the metadata from.\n        blocks_in_chunk: the number of blocks in a LMCache data chunk\n    \"\"\"\n    # Store the blocks that has block hashes\n    # NOTE: the invariant here is that `num_stored_blocks` should\n    # always be a multiple of `blocks_in_chunk`\n    # TODO: This should be checked everytime we update the num_stored_blocks\n    min_available_blocks = min(\n        len(tracker.block_hashes),\n        len(tracker.allocated_block_ids),\n        tracker.num_scheduled_tokens // vllm_block_size,\n    )\n    num_staging_blocks = min_available_blocks - tracker.num_stored_blocks\n    num_chunks = num_staging_blocks // blocks_in_chunk\n\n    if num_chunks >= 1:\n        start = tracker.num_stored_blocks\n        end = start + num_chunks * blocks_in_chunk\n        block_hashes = convert_block_hashes_to_bytes(\n            tracker.block_hashes[start:end]\n        )\n        block_ids = tracker.allocated_block_ids[start:end]\n\n        ret = LMCacheMPRequestMetadata(\n            request_id=tracker.request_id,\n            direction=\"STORE\",\n            op=LoadStoreOp(block_hashes=block_hashes, block_ids=block_ids),\n        )\n\n        # Update the request tracker\n        tracker.increase_num_stored_blocks(end - start)\n        return ret\n\n    return None",
      "language": "python"
    },
    {
      "code": "@staticmethod\ndef GetStoreMetadata(\n    tracker: LMCacheMPRequestTracker,\n    blocks_in_chunk: int,\n    vllm_block_size: int,\n) -> \"LMCacheMPRequestMetadata | None\":\n    \"\"\"\n    Generate the store metadata for the current request tracker.\n\n    Args:\n        tracker: The request tracker to generate the metadata from.\n        blocks_in_chunk: the number of blocks in a LMCache data chunk\n    \"\"\"\n    # Store the blocks that has block hashes\n    # NOTE: the invariant here is that `num_stored_blocks` should\n    # always be a multiple of `blocks_in_chunk`\n    # TODO: This should be checked everytime we update the num_stored_blocks\n    min_available_blocks = min(\n        len(tracker.block_hashes),\n        len(tracker.allocated_block_ids),\n        tracker.num_scheduled_tokens // vllm_block_size,\n    )\n    num_staging_blocks = min_available_blocks - tracker.num_stored_blocks\n    num_chunks = num_staging_blocks // blocks_in_chunk\n\n    if num_chunks >= 1:\n        start = tracker.num_stored_blocks\n        end = start + num_chunks * blocks_in_chunk\n        block_hashes = convert_block_hashes_to_bytes(\n            tracker.block_hashes[start:end]\n        )\n        block_ids = tracker.allocated_block_ids[start:end]\n\n        ret = LMCacheMPRequestMetadata(\n            request_id=tracker.request_id,\n            direction=\"STORE\",\n            op=LoadStoreOp(block_hashes=block_hashes, block_ids=block_ids),\n        )\n\n        # Update the request tracker\n        tracker.increase_num_stored_blocks(end - start)\n        return ret\n\n    return None",
      "language": "python"
    },
    {
      "code": "__init__(\n    request_id: str,\n    direction: Literal[\"STORE\", \"RETRIEVE\"],\n    op: LoadStoreOp,\n) -> None",
      "language": "python"
    },
    {
      "code": "__init__(\n    request_id: str,\n    direction: Literal[\"STORE\", \"RETRIEVE\"],\n    op: LoadStoreOp,\n) -> None",
      "language": "python"
    },
    {
      "code": "131\n132\n133\n134\n135\n136\n137\n138\n139\n140",
      "language": "unknown"
    },
    {
      "code": "class LMCacheMPRequestState(enum.Enum):\n    \"\"\"\n    State machine:\n    PREFETCHING -- update_state_after_alloc --> WAITING_FOR_LOAD\n    WAITING_FOR_LOAD -- process_loading_requests --> READY\n    \"\"\"\n\n    PREFETCHING = enum.auto()\n    WAITING_FOR_LOAD = enum.auto()\n    READY = enum.auto()",
      "language": "php"
    },
    {
      "code": "class LMCacheMPRequestState(enum.Enum):\n    \"\"\"\n    State machine:\n    PREFETCHING -- update_state_after_alloc --> WAITING_FOR_LOAD\n    WAITING_FOR_LOAD -- process_loading_requests --> READY\n    \"\"\"\n\n    PREFETCHING = enum.auto()\n    WAITING_FOR_LOAD = enum.auto()\n    READY = enum.auto()",
      "language": "php"
    },
    {
      "code": "PREFETCHING = auto()",
      "language": "unknown"
    },
    {
      "code": "PREFETCHING = auto()",
      "language": "unknown"
    },
    {
      "code": "READY = auto()",
      "language": "unknown"
    },
    {
      "code": "READY = auto()",
      "language": "unknown"
    },
    {
      "code": "WAITING_FOR_LOAD = auto()",
      "language": "unknown"
    },
    {
      "code": "WAITING_FOR_LOAD = auto()",
      "language": "unknown"
    },
    {
      "code": "143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240",
      "language": "unknown"
    },
    {
      "code": "@dataclass\nclass LMCacheMPRequestTracker:\n    # NOTE: this class used vLLM data structures, should be part of\n    # vLLM integration code\n\n    request_id: str\n\n    # Read-only lists to track the token ids and block hashes\n    all_token_ids: ConstantList[int]\n    block_hashes: ConstantList[\"BlockHash\"]\n\n    # Block ids and hashes will be updated at update_states_after_alloc and\n    # during the generation\n    allocated_block_ids: list[int] = field(default_factory=list)\n\n    # Number of scheduled tokens in this request. We keep tracking this to\n    # avoid saving half-full blocks.\n    num_scheduled_tokens: int = 0\n\n    # Number of blocks stored will be initialized when lookup the external\n    # hit tokens and will be updated when processing new requests and cached\n    # requests.\n    num_stored_blocks: int = 0\n\n    # Staging load operation -- save vllm and lmcache hit tokens during lookup\n    num_vllm_hit_blocks: int = 0\n    num_lmcache_hit_blocks: int = 0\n\n    # Main state\n    state: LMCacheMPRequestState = LMCacheMPRequestState.PREFETCHING\n\n    def __init__(self, request: \"Request\"):\n        self.request_id = request.request_id\n        self.all_token_ids = request.all_token_ids\n        self.block_hashes = ConstantList(request.block_hashes)\n        self.allocated_block_ids = []\n        self.num_stored_blocks = 0\n        self.num_vllm_hit_blocks = 0\n        self.num_lmcache_hit_blocks = 0\n        self.state = LMCacheMPRequestState.PREFETCHING\n\n    ####\n    # Check the state of the request\n    ####\n    def needs_retrieve(self) -> bool:\n        \"\"\"Check whether the current request needs retrieve, will be used\n        update_stage_after_alloc\"\"\"\n        return (\n            self.num_lmcache_hit_blocks > self.num_vllm_hit_blocks\n            and self.state != LMCacheMPRequestState.READY\n        )\n\n    def is_ready_for_retrieving(self) -> bool:\n        \"\"\"Check whether the current request is ready for retrieving,\n        will be used in process_loading_requests\"\"\"\n        return (\n            self.state == LMCacheMPRequestState.WAITING_FOR_LOAD\n            and self.needs_retrieve()\n        )\n\n    ####\n    # Update internal states\n    ####\n    def increase_num_scheduled_tokens(self, num_new_tokens: int):\n        self.num_scheduled_tokens += num_new_tokens\n\n    def increase_num_stored_blocks(self, num_new_blocks: int):\n        \"\"\"Increase the number of stored blocks for the current request\n        This function will be called when processing the cached requests.\n        \"\"\"\n        self.num_stored_blocks += num_new_blocks\n\n    def append_block_ids(\n        self,\n        new_block_ids: list[int],\n    ):\n        \"\"\"Update the block ids for the current request\n        This function will be called when processing the cached requests.\n        \"\"\"\n        self.allocated_block_ids.extend(new_block_ids)\n\n    ####\n    # For debugging\n    ####\n    def __repr__(self) -> str:\n        return (\n            f\"LMCacheMPRequestTracker(request_id={self.request_id}, \"\n            f\"num_tokens={len(self.all_token_ids)}, \"\n            f\"num_block_hashes={len(self.block_hashes)}, \"\n            f\"num_allocated_blocks={len(self.allocated_block_ids)}, \"\n            f\"num_stored_blocks={self.num_stored_blocks}, \"\n            f\"vllm_hit_blocks={self.num_vllm_hit_blocks}, \"\n            f\"lmcache_hit_blocks={self.num_lmcache_hit_blocks}, \"\n            f\"state={self.state})\"\n        )\n\n    def __str__(self) -> str:\n        return self.__repr__()",
      "language": "python"
    },
    {
      "code": "@dataclass\nclass LMCacheMPRequestTracker:\n    # NOTE: this class used vLLM data structures, should be part of\n    # vLLM integration code\n\n    request_id: str\n\n    # Read-only lists to track the token ids and block hashes\n    all_token_ids: ConstantList[int]\n    block_hashes: ConstantList[\"BlockHash\"]\n\n    # Block ids and hashes will be updated at update_states_after_alloc and\n    # during the generation\n    allocated_block_ids: list[int] = field(default_factory=list)\n\n    # Number of scheduled tokens in this request. We keep tracking this to\n    # avoid saving half-full blocks.\n    num_scheduled_tokens: int = 0\n\n    # Number of blocks stored will be initialized when lookup the external\n    # hit tokens and will be updated when processing new requests and cached\n    # requests.\n    num_stored_blocks: int = 0\n\n    # Staging load operation -- save vllm and lmcache hit tokens during lookup\n    num_vllm_hit_blocks: int = 0\n    num_lmcache_hit_blocks: int = 0\n\n    # Main state\n    state: LMCacheMPRequestState = LMCacheMPRequestState.PREFETCHING\n\n    def __init__(self, request: \"Request\"):\n        self.request_id = request.request_id\n        self.all_token_ids = request.all_token_ids\n        self.block_hashes = ConstantList(request.block_hashes)\n        self.allocated_block_ids = []\n        self.num_stored_blocks = 0\n        self.num_vllm_hit_blocks = 0\n        self.num_lmcache_hit_blocks = 0\n        self.state = LMCacheMPRequestState.PREFETCHING\n\n    ####\n    # Check the state of the request\n    ####\n    def needs_retrieve(self) -> bool:\n        \"\"\"Check whether the current request needs retrieve, will be used\n        update_stage_after_alloc\"\"\"\n        return (\n            self.num_lmcache_hit_blocks > self.num_vllm_hit_blocks\n            and self.state != LMCacheMPRequestState.READY\n        )\n\n    def is_ready_for_retrieving(self) -> bool:\n        \"\"\"Check whether the current request is ready for retrieving,\n        will be used in process_loading_requests\"\"\"\n        return (\n            self.state == LMCacheMPRequestState.WAITING_FOR_LOAD\n            and self.needs_retrieve()\n        )\n\n    ####\n    # Update internal states\n    ####\n    def increase_num_scheduled_tokens(self, num_new_tokens: int):\n        self.num_scheduled_tokens += num_new_tokens\n\n    def increase_num_stored_blocks(self, num_new_blocks: int):\n        \"\"\"Increase the number of stored blocks for the current request\n        This function will be called when processing the cached requests.\n        \"\"\"\n        self.num_stored_blocks += num_new_blocks\n\n    def append_block_ids(\n        self,\n        new_block_ids: list[int],\n    ):\n        \"\"\"Update the block ids for the current request\n        This function will be called when processing the cached requests.\n        \"\"\"\n        self.allocated_block_ids.extend(new_block_ids)\n\n    ####\n    # For debugging\n    ####\n    def __repr__(self) -> str:\n        return (\n            f\"LMCacheMPRequestTracker(request_id={self.request_id}, \"\n            f\"num_tokens={len(self.all_token_ids)}, \"\n            f\"num_block_hashes={len(self.block_hashes)}, \"\n            f\"num_allocated_blocks={len(self.allocated_block_ids)}, \"\n            f\"num_stored_blocks={self.num_stored_blocks}, \"\n            f\"vllm_hit_blocks={self.num_vllm_hit_blocks}, \"\n            f\"lmcache_hit_blocks={self.num_lmcache_hit_blocks}, \"\n            f\"state={self.state})\"\n        )\n\n    def __str__(self) -> str:\n        return self.__repr__()",
      "language": "python"
    },
    {
      "code": "all_token_ids: ConstantList[int] = all_token_ids",
      "language": "yaml"
    },
    {
      "code": "all_token_ids: ConstantList[int] = all_token_ids",
      "language": "yaml"
    },
    {
      "code": "allocated_block_ids: list[int] = []",
      "language": "yaml"
    },
    {
      "code": "allocated_block_ids: list[int] = []",
      "language": "yaml"
    },
    {
      "code": "block_hashes: ConstantList[BlockHash] = ConstantList(\n    block_hashes\n)",
      "language": "yaml"
    },
    {
      "code": "block_hashes: ConstantList[BlockHash] = ConstantList(\n    block_hashes\n)",
      "language": "yaml"
    },
    {
      "code": "num_lmcache_hit_blocks: int = 0",
      "language": "typescript"
    },
    {
      "code": "num_lmcache_hit_blocks: int = 0",
      "language": "typescript"
    },
    {
      "code": "num_scheduled_tokens: int = 0",
      "language": "typescript"
    },
    {
      "code": "num_scheduled_tokens: int = 0",
      "language": "typescript"
    },
    {
      "code": "num_stored_blocks: int = 0",
      "language": "typescript"
    },
    {
      "code": "num_stored_blocks: int = 0",
      "language": "typescript"
    },
    {
      "code": "num_vllm_hit_blocks: int = 0",
      "language": "typescript"
    },
    {
      "code": "num_vllm_hit_blocks: int = 0",
      "language": "typescript"
    },
    {
      "code": "request_id: str = request_id",
      "language": "typescript"
    },
    {
      "code": "request_id: str = request_id",
      "language": "typescript"
    },
    {
      "code": "state: LMCacheMPRequestState = PREFETCHING",
      "language": "typescript"
    },
    {
      "code": "state: LMCacheMPRequestState = PREFETCHING",
      "language": "typescript"
    },
    {
      "code": "__init__(request: Request)",
      "language": "python"
    },
    {
      "code": "__init__(request: Request)",
      "language": "python"
    },
    {
      "code": "174\n175\n176\n177\n178\n179\n180\n181\n182",
      "language": "unknown"
    },
    {
      "code": "def __init__(self, request: \"Request\"):\n    self.request_id = request.request_id\n    self.all_token_ids = request.all_token_ids\n    self.block_hashes = ConstantList(request.block_hashes)\n    self.allocated_block_ids = []\n    self.num_stored_blocks = 0\n    self.num_vllm_hit_blocks = 0\n    self.num_lmcache_hit_blocks = 0\n    self.state = LMCacheMPRequestState.PREFETCHING",
      "language": "python"
    },
    {
      "code": "def __init__(self, request: \"Request\"):\n    self.request_id = request.request_id\n    self.all_token_ids = request.all_token_ids\n    self.block_hashes = ConstantList(request.block_hashes)\n    self.allocated_block_ids = []\n    self.num_stored_blocks = 0\n    self.num_vllm_hit_blocks = 0\n    self.num_lmcache_hit_blocks = 0\n    self.state = LMCacheMPRequestState.PREFETCHING",
      "language": "python"
    },
    {
      "code": "__repr__() -> str",
      "language": "php"
    },
    {
      "code": "__repr__() -> str",
      "language": "php"
    },
    {
      "code": "227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237",
      "language": "unknown"
    },
    {
      "code": "def __repr__(self) -> str:\n    return (\n        f\"LMCacheMPRequestTracker(request_id={self.request_id}, \"\n        f\"num_tokens={len(self.all_token_ids)}, \"\n        f\"num_block_hashes={len(self.block_hashes)}, \"\n        f\"num_allocated_blocks={len(self.allocated_block_ids)}, \"\n        f\"num_stored_blocks={self.num_stored_blocks}, \"\n        f\"vllm_hit_blocks={self.num_vllm_hit_blocks}, \"\n        f\"lmcache_hit_blocks={self.num_lmcache_hit_blocks}, \"\n        f\"state={self.state})\"\n    )",
      "language": "python"
    },
    {
      "code": "def __repr__(self) -> str:\n    return (\n        f\"LMCacheMPRequestTracker(request_id={self.request_id}, \"\n        f\"num_tokens={len(self.all_token_ids)}, \"\n        f\"num_block_hashes={len(self.block_hashes)}, \"\n        f\"num_allocated_blocks={len(self.allocated_block_ids)}, \"\n        f\"num_stored_blocks={self.num_stored_blocks}, \"\n        f\"vllm_hit_blocks={self.num_vllm_hit_blocks}, \"\n        f\"lmcache_hit_blocks={self.num_lmcache_hit_blocks}, \"\n        f\"state={self.state})\"\n    )",
      "language": "python"
    },
    {
      "code": "__str__() -> str",
      "language": "php"
    },
    {
      "code": "__str__() -> str",
      "language": "php"
    },
    {
      "code": "def __str__(self) -> str:\n    return self.__repr__()",
      "language": "python"
    },
    {
      "code": "def __str__(self) -> str:\n    return self.__repr__()",
      "language": "python"
    },
    {
      "code": "append_block_ids(new_block_ids: list[int])",
      "language": "unknown"
    },
    {
      "code": "append_block_ids(new_block_ids: list[int])",
      "language": "unknown"
    },
    {
      "code": "215\n216\n217\n218\n219\n220\n221\n222",
      "language": "unknown"
    },
    {
      "code": "def append_block_ids(\n    self,\n    new_block_ids: list[int],\n):\n    \"\"\"Update the block ids for the current request\n    This function will be called when processing the cached requests.\n    \"\"\"\n    self.allocated_block_ids.extend(new_block_ids)",
      "language": "python"
    },
    {
      "code": "def append_block_ids(\n    self,\n    new_block_ids: list[int],\n):\n    \"\"\"Update the block ids for the current request\n    This function will be called when processing the cached requests.\n    \"\"\"\n    self.allocated_block_ids.extend(new_block_ids)",
      "language": "python"
    },
    {
      "code": "increase_num_scheduled_tokens(num_new_tokens: int)",
      "language": "unknown"
    },
    {
      "code": "increase_num_scheduled_tokens(num_new_tokens: int)",
      "language": "unknown"
    },
    {
      "code": "def increase_num_scheduled_tokens(self, num_new_tokens: int):\n    self.num_scheduled_tokens += num_new_tokens",
      "language": "python"
    },
    {
      "code": "def increase_num_scheduled_tokens(self, num_new_tokens: int):\n    self.num_scheduled_tokens += num_new_tokens",
      "language": "python"
    },
    {
      "code": "increase_num_stored_blocks(num_new_blocks: int)",
      "language": "unknown"
    },
    {
      "code": "increase_num_stored_blocks(num_new_blocks: int)",
      "language": "unknown"
    },
    {
      "code": "209\n210\n211\n212\n213",
      "language": "unknown"
    },
    {
      "code": "def increase_num_stored_blocks(self, num_new_blocks: int):\n    \"\"\"Increase the number of stored blocks for the current request\n    This function will be called when processing the cached requests.\n    \"\"\"\n    self.num_stored_blocks += num_new_blocks",
      "language": "python"
    },
    {
      "code": "def increase_num_stored_blocks(self, num_new_blocks: int):\n    \"\"\"Increase the number of stored blocks for the current request\n    This function will be called when processing the cached requests.\n    \"\"\"\n    self.num_stored_blocks += num_new_blocks",
      "language": "python"
    },
    {
      "code": "is_ready_for_retrieving() -> bool",
      "language": "php"
    },
    {
      "code": "is_ready_for_retrieving() -> bool",
      "language": "php"
    },
    {
      "code": "195\n196\n197\n198\n199\n200\n201",
      "language": "unknown"
    },
    {
      "code": "def is_ready_for_retrieving(self) -> bool:\n    \"\"\"Check whether the current request is ready for retrieving,\n    will be used in process_loading_requests\"\"\"\n    return (\n        self.state == LMCacheMPRequestState.WAITING_FOR_LOAD\n        and self.needs_retrieve()\n    )",
      "language": "python"
    },
    {
      "code": "def is_ready_for_retrieving(self) -> bool:\n    \"\"\"Check whether the current request is ready for retrieving,\n    will be used in process_loading_requests\"\"\"\n    return (\n        self.state == LMCacheMPRequestState.WAITING_FOR_LOAD\n        and self.needs_retrieve()\n    )",
      "language": "python"
    },
    {
      "code": "needs_retrieve() -> bool",
      "language": "php"
    },
    {
      "code": "needs_retrieve() -> bool",
      "language": "php"
    },
    {
      "code": "187\n188\n189\n190\n191\n192\n193",
      "language": "unknown"
    },
    {
      "code": "def needs_retrieve(self) -> bool:\n    \"\"\"Check whether the current request needs retrieve, will be used\n    update_stage_after_alloc\"\"\"\n    return (\n        self.num_lmcache_hit_blocks > self.num_vllm_hit_blocks\n        and self.state != LMCacheMPRequestState.READY\n    )",
      "language": "python"
    },
    {
      "code": "def needs_retrieve(self) -> bool:\n    \"\"\"Check whether the current request needs retrieve, will be used\n    update_stage_after_alloc\"\"\"\n    return (\n        self.num_lmcache_hit_blocks > self.num_vllm_hit_blocks\n        and self.state != LMCacheMPRequestState.READY\n    )",
      "language": "python"
    },
    {
      "code": "convert_block_hashes_to_bytes(\n    block_hashes: list[BlockHash],\n) -> list[bytes]",
      "language": "php"
    },
    {
      "code": "convert_block_hashes_to_bytes(\n    block_hashes: list[BlockHash],\n) -> list[bytes]",
      "language": "php"
    },
    {
      "code": "125\n126\n127\n128",
      "language": "unknown"
    },
    {
      "code": "def convert_block_hashes_to_bytes(\n    block_hashes: list[\"BlockHash\"],\n) -> list[bytes]:\n    return cast(list[bytes], block_hashes)",
      "language": "python"
    },
    {
      "code": "def convert_block_hashes_to_bytes(\n    block_hashes: list[\"BlockHash\"],\n) -> list[bytes]:\n    return cast(list[bytes], block_hashes)",
      "language": "python"
    },
    {
      "code": "create_scheduler_adapter(\n    server_url: str,\n    zmq_context: Context,\n    vllm_config: VllmConfig,\n) -> LMCacheMPSchedulerAdapter",
      "language": "php"
    },
    {
      "code": "create_scheduler_adapter(\n    server_url: str,\n    zmq_context: Context,\n    vllm_config: VllmConfig,\n) -> LMCacheMPSchedulerAdapter",
      "language": "php"
    },
    {
      "code": "89\n 90\n 91\n 92\n 93\n 94\n 95\n 96\n 97\n 98\n 99\n100\n101\n102\n103\n104",
      "language": "unknown"
    },
    {
      "code": "def create_scheduler_adapter(\n    server_url: str, zmq_context: zmq.Context, vllm_config: VllmConfig\n) -> LMCacheMPSchedulerAdapter:\n    world_size, kv_rank = extract_world_size_and_kv_rank(\n        vllm_config.parallel_config.world_size,\n        vllm_config.parallel_config.rank,\n        vllm_config,\n    )\n    return LMCacheMPSchedulerAdapter(\n        server_url,\n        zmq_context,\n        vllm_config.model_config.model,\n        world_size,\n        kv_rank,\n        vllm_config.cache_config.block_size,\n    )",
      "language": "python"
    },
    {
      "code": "def create_scheduler_adapter(\n    server_url: str, zmq_context: zmq.Context, vllm_config: VllmConfig\n) -> LMCacheMPSchedulerAdapter:\n    world_size, kv_rank = extract_world_size_and_kv_rank(\n        vllm_config.parallel_config.world_size,\n        vllm_config.parallel_config.rank,\n        vllm_config,\n    )\n    return LMCacheMPSchedulerAdapter(\n        server_url,\n        zmq_context,\n        vllm_config.model_config.model,\n        world_size,\n        kv_rank,\n        vllm_config.cache_config.block_size,\n    )",
      "language": "python"
    },
    {
      "code": "create_worker_adapter(\n    server_url: str,\n    zmq_context: Context,\n    vllm_config: VllmConfig,\n) -> LMCacheMPWorkerAdapter",
      "language": "php"
    },
    {
      "code": "create_worker_adapter(\n    server_url: str,\n    zmq_context: Context,\n    vllm_config: VllmConfig,\n) -> LMCacheMPWorkerAdapter",
      "language": "php"
    },
    {
      "code": "107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122",
      "language": "unknown"
    },
    {
      "code": "def create_worker_adapter(\n    server_url: str, zmq_context: zmq.Context, vllm_config: VllmConfig\n) -> LMCacheMPWorkerAdapter:\n    world_size, kv_rank = extract_world_size_and_kv_rank(\n        vllm_config.parallel_config.world_size,\n        vllm_config.parallel_config.rank,\n        vllm_config,\n    )\n    return LMCacheMPWorkerAdapter(\n        server_url,\n        zmq_context,\n        vllm_config.model_config.model,\n        world_size,\n        kv_rank,\n        vllm_config.cache_config.block_size,\n    )",
      "language": "python"
    },
    {
      "code": "def create_worker_adapter(\n    server_url: str, zmq_context: zmq.Context, vllm_config: VllmConfig\n) -> LMCacheMPWorkerAdapter:\n    world_size, kv_rank = extract_world_size_and_kv_rank(\n        vllm_config.parallel_config.world_size,\n        vllm_config.parallel_config.rank,\n        vllm_config,\n    )\n    return LMCacheMPWorkerAdapter(\n        server_url,\n        zmq_context,\n        vllm_config.model_config.model,\n        world_size,\n        kv_rank,\n        vllm_config.cache_config.block_size,\n    )",
      "language": "python"
    },
    {
      "code": "extract_world_size_and_kv_rank(\n    world_size: int, rank: int, vllm_config: VllmConfig\n) -> tuple[int, int]",
      "language": "php"
    },
    {
      "code": "extract_world_size_and_kv_rank(\n    world_size: int, rank: int, vllm_config: VllmConfig\n) -> tuple[int, int]",
      "language": "php"
    },
    {
      "code": "65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86",
      "language": "unknown"
    },
    {
      "code": "def extract_world_size_and_kv_rank(\n    world_size: int,\n    rank: int,\n    vllm_config: VllmConfig,\n) -> tuple[int, int]:\n    \"\"\"\n    Convert the rank for the MLA.\n    \"\"\"\n    use_mla = mla_enabled(vllm_config.model_config)\n    if not use_mla:\n        return world_size, rank\n    else:\n        # Tensor parallel does not change the KV caches for MLA models.\n        # So we need to \"exclude\" the effect of TP on rank and world size\n        tp_size = vllm_config.parallel_config.tensor_parallel_size\n        # vLLM constructs TP groups first, and then construct other\n        # parallel groups on top of TP groups.\n        # for example, TP=4, PP=2,\n        # TP group: [0, 1, 2, 3], [4, 5, 6, 7]\n        # PP group: [0, 4], [1, 5], [2, 6], [3, 7]\n        # So we can \"exclude\" the effect of TP by rank // tp_size.\n        return world_size // tp_size, rank // tp_size",
      "language": "python"
    },
    {
      "code": "def extract_world_size_and_kv_rank(\n    world_size: int,\n    rank: int,\n    vllm_config: VllmConfig,\n) -> tuple[int, int]:\n    \"\"\"\n    Convert the rank for the MLA.\n    \"\"\"\n    use_mla = mla_enabled(vllm_config.model_config)\n    if not use_mla:\n        return world_size, rank\n    else:\n        # Tensor parallel does not change the KV caches for MLA models.\n        # So we need to \"exclude\" the effect of TP on rank and world size\n        tp_size = vllm_config.parallel_config.tensor_parallel_size\n        # vLLM constructs TP groups first, and then construct other\n        # parallel groups on top of TP groups.\n        # for example, TP=4, PP=2,\n        # TP group: [0, 1, 2, 3], [4, 5, 6, 7]\n        # PP group: [0, 4], [1, 5], [2, 6], [3, 7]\n        # So we can \"exclude\" the effect of TP by rank // tp_size.\n        return world_size // tp_size, rank // tp_size",
      "language": "python"
    },
    {
      "code": "reformat_block_ids(\n    block_ids: tuple[list[int], ...] | None,\n) -> list[int]",
      "language": "rust"
    },
    {
      "code": "reformat_block_ids(\n    block_ids: tuple[list[int], ...] | None,\n) -> list[int]",
      "language": "rust"
    },
    {
      "code": "49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62",
      "language": "unknown"
    },
    {
      "code": "def reformat_block_ids(block_ids: tuple[list[int], ...] | None) -> list[int]:\n    if block_ids is None:\n        return []\n    assert isinstance(block_ids, tuple), (\n        f\"Expected block_ids to be a tuple of lists, but got {type(block_ids)}\"\n    )\n\n    if len(block_ids) > 1:\n        raise RuntimeError(\n            \"LMCacheMPConnector only works without hybrid kv cache manager. \"\n            \"Please pass --disable-hybrid-kv-cache-manager when starting vllm\"\n        )\n\n    return block_ids[0]",
      "language": "python"
    },
    {
      "code": "def reformat_block_ids(block_ids: tuple[list[int], ...] | None) -> list[int]:\n    if block_ids is None:\n        return []\n    assert isinstance(block_ids, tuple), (\n        f\"Expected block_ids to be a tuple of lists, but got {type(block_ids)}\"\n    )\n\n    if len(block_ids) > 1:\n        raise RuntimeError(\n            \"LMCacheMPConnector only works without hybrid kv cache manager. \"\n            \"Please pass --disable-hybrid-kv-cache-manager when starting vllm\"\n        )\n\n    return block_ids[0]",
      "language": "python"
    }
  ],
  "patterns": [],
  "links": [
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_mp_connector/",
    "https://docs.vllm.ai/en/latest/",
    "https://docs.vllm.ai/en/latest/usage/",
    "https://docs.vllm.ai/en/latest/contributing/",
    "https://docs.vllm.ai/en/latest/benchmarking/",
    "https://docs.vllm.ai/en/latest/api/",
    "https://docs.vllm.ai/en/latest/cli/",
    "https://docs.vllm.ai/en/latest/community/contact_us/",
    "https://docs.vllm.ai/en/latest/getting_started/quickstart/",
    "https://docs.vllm.ai/en/latest/getting_started/installation/",
    "https://docs.vllm.ai/en/latest/getting_started/installation/gpu/",
    "https://docs.vllm.ai/en/latest/getting_started/installation/cpu/",
    "https://docs.vllm.ai/en/latest/examples/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/async_llm_streaming/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/audio_language/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/automatic_prefix_caching/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/basic/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/batch_llm_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/chat_with_tools/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/context_extension/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/data_parallel/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/disaggregated-prefill-v1/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/disaggregated_prefill/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/encoder_decoder_multimodal/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/kv_load_failure_recovery/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/llm_engine_example/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/llm_engine_reset_kv/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/load_sharded_state/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/logits_processor/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/lora_with_quantization_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/metrics/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/mistral-small/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/mlpspeculator/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/multilora_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/openai_batch/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/prefix_caching/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/prompt_embed_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/qwen2_5_omni/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/qwen3_omni/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/qwen_1m/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/reproducibility/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf_colocate/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf_online_quant/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf_utils/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/save_sharded_state/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/simple_profiling/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/skip_loading_weights_in_engine_init/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/spec_decode/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/structured_outputs/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/torchrun_dp_example/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/torchrun_example/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/vision_language/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/vision_language_multi_image/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/api_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/chart-helm/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/dashboards/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_encoder/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_prefill/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_serving/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/elastic_ep/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/gradio_openai_chatbot_webserver/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/gradio_webserver/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/kv_events_subscriber/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/multi-node-serving/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/multi_instance_data_parallel/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_for_multimodal/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools_required/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools_xlam/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools_xlam_streaming/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_tool_calls_with_reasoning/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_with_reasoning/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_with_reasoning_streaming/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_completion_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_responses_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_responses_client_with_mcp_tools/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_responses_client_with_tools/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_transcription_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_translation_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/opentelemetry/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/prometheus_grafana/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/prompt_embed_inference_with_openai_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/ray_serve_deepseek/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/retrieval_augmented_generation_with_langchain/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/retrieval_augmented_generation_with_llamaindex/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/run_cluster/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/sagemaker-entrypoint/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/streamlit_openai_chatbot_webserver/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/structured_outputs/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/token_generation_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/utils/",
    "https://docs.vllm.ai/en/latest/examples/others/lmcache/",
    "https://docs.vllm.ai/en/latest/examples/others/logging_configuration/",
    "https://docs.vllm.ai/en/latest/examples/others/tensorize_vllm_model/",
    "https://docs.vllm.ai/en/latest/examples/pooling/classify/",
    "https://docs.vllm.ai/en/latest/examples/pooling/embed/",
    "https://docs.vllm.ai/en/latest/examples/pooling/plugin/",
    "https://docs.vllm.ai/en/latest/examples/pooling/pooling/",
    "https://docs.vllm.ai/en/latest/examples/pooling/score/",
    "https://docs.vllm.ai/en/latest/examples/pooling/token_classify/",
    "https://docs.vllm.ai/en/latest/examples/pooling/token_embed/",
    "https://docs.vllm.ai/en/latest/usage/v1_guide/",
    "https://docs.vllm.ai/en/latest/usage/faq/",
    "https://docs.vllm.ai/en/latest/usage/metrics/",
    "https://docs.vllm.ai/en/latest/usage/reproducibility/",
    "https://docs.vllm.ai/en/latest/usage/security/",
    "https://docs.vllm.ai/en/latest/usage/troubleshooting/",
    "https://docs.vllm.ai/en/latest/usage/usage_stats/",
    "https://docs.vllm.ai/en/latest/serving/offline_inference/",
    "https://docs.vllm.ai/en/latest/serving/openai_compatible_server/",
    "https://docs.vllm.ai/en/latest/serving/context_parallel_deployment/",
    "https://docs.vllm.ai/en/latest/serving/data_parallel_deployment/",
    "https://docs.vllm.ai/en/latest/serving/distributed_troubleshooting/",
    "https://docs.vllm.ai/en/latest/serving/expert_parallel_deployment/",
    "https://docs.vllm.ai/en/latest/serving/parallelism_scaling/",
    "https://docs.vllm.ai/en/latest/serving/integrations/langchain/",
    "https://docs.vllm.ai/en/latest/serving/integrations/llamaindex/",
    "https://docs.vllm.ai/en/latest/deployment/docker/",
    "https://docs.vllm.ai/en/latest/deployment/k8s/",
    "https://docs.vllm.ai/en/latest/deployment/nginx/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/anyscale/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/anything-llm/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/autogen/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/bentoml/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/cerebrium/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/chatbox/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/dify/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/dstack/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/haystack/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/helm/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/hf_inference_endpoints/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/litellm/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/lobe-chat/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/lws/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/modal/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/open-webui/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/retrieval_augmented_generation/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/skypilot/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/streamlit/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/triton/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kaito/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kserve/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kthena/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kubeai/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kuberay/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/llamastack/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/llm-d/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/llmaz/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/production-stack/",
    "https://docs.vllm.ai/en/latest/training/rlhf/",
    "https://docs.vllm.ai/en/latest/training/trl/",
    "https://docs.vllm.ai/en/latest/configuration/",
    "https://docs.vllm.ai/en/latest/configuration/conserving_memory/",
    "https://docs.vllm.ai/en/latest/configuration/engine_args/",
    "https://docs.vllm.ai/en/latest/configuration/env_vars/",
    "https://docs.vllm.ai/en/latest/configuration/model_resolution/",
    "https://docs.vllm.ai/en/latest/configuration/optimization/",
    "https://docs.vllm.ai/en/latest/configuration/serve_args/",
    "https://docs.vllm.ai/en/latest/models/supported_models/",
    "https://docs.vllm.ai/en/latest/models/generative_models/",
    "https://docs.vllm.ai/en/latest/models/pooling_models/",
    "https://docs.vllm.ai/en/latest/models/extensions/fastsafetensor/",
    "https://docs.vllm.ai/en/latest/models/extensions/runai_model_streamer/",
    "https://docs.vllm.ai/en/latest/models/extensions/tensorizer/",
    "https://docs.vllm.ai/en/latest/models/hardware_supported_models/cpu/",
    "https://docs.vllm.ai/en/latest/models/hardware_supported_models/xpu/",
    "https://docs.vllm.ai/en/latest/features/",
    "https://docs.vllm.ai/en/latest/features/automatic_prefix_caching/",
    "https://docs.vllm.ai/en/latest/features/batch_invariance/",
    "https://docs.vllm.ai/en/latest/features/custom_arguments/",
    "https://docs.vllm.ai/en/latest/features/custom_logitsprocs/",
    "https://docs.vllm.ai/en/latest/features/disagg_encoder/",
    "https://docs.vllm.ai/en/latest/features/disagg_prefill/",
    "https://docs.vllm.ai/en/latest/features/interleaved_thinking/",
    "https://docs.vllm.ai/en/latest/features/lora/",
    "https://docs.vllm.ai/en/latest/features/mooncake_connector_usage/",
    "https://docs.vllm.ai/en/latest/features/multimodal_inputs/",
    "https://docs.vllm.ai/en/latest/features/nixl_connector_usage/",
    "https://docs.vllm.ai/en/latest/features/prompt_embeds/",
    "https://docs.vllm.ai/en/latest/features/reasoning_outputs/",
    "https://docs.vllm.ai/en/latest/features/sleep_mode/",
    "https://docs.vllm.ai/en/latest/features/spec_decode/",
    "https://docs.vllm.ai/en/latest/features/structured_outputs/",
    "https://docs.vllm.ai/en/latest/features/tool_calling/",
    "https://docs.vllm.ai/en/latest/features/quantization/",
    "https://docs.vllm.ai/en/latest/features/quantization/auto_awq/",
    "https://docs.vllm.ai/en/latest/features/quantization/auto_round/",
    "https://docs.vllm.ai/en/latest/features/quantization/bitblas/",
    "https://docs.vllm.ai/en/latest/features/quantization/bnb/",
    "https://docs.vllm.ai/en/latest/features/quantization/fp8/",
    "https://docs.vllm.ai/en/latest/features/quantization/gguf/",
    "https://docs.vllm.ai/en/latest/features/quantization/gptqmodel/",
    "https://docs.vllm.ai/en/latest/features/quantization/inc/",
    "https://docs.vllm.ai/en/latest/features/quantization/int4/",
    "https://docs.vllm.ai/en/latest/features/quantization/int8/",
    "https://docs.vllm.ai/en/latest/features/quantization/modelopt/",
    "https://docs.vllm.ai/en/latest/features/quantization/quantized_kvcache/",
    "https://docs.vllm.ai/en/latest/features/quantization/quark/",
    "https://docs.vllm.ai/en/latest/features/quantization/torchao/",
    "https://docs.vllm.ai/en/latest/contributing/deprecation_policy/",
    "https://docs.vllm.ai/en/latest/contributing/dockerfile/dockerfile/",
    "https://docs.vllm.ai/en/latest/contributing/incremental_build/",
    "https://docs.vllm.ai/en/latest/contributing/profiling/",
    "https://docs.vllm.ai/en/latest/contributing/vulnerability_management/",
    "https://docs.vllm.ai/en/latest/contributing/model/",
    "https://docs.vllm.ai/en/latest/contributing/model/basic/",
    "https://docs.vllm.ai/en/latest/contributing/model/registration/",
    "https://docs.vllm.ai/en/latest/contributing/model/tests/",
    "https://docs.vllm.ai/en/latest/contributing/model/multimodal/",
    "https://docs.vllm.ai/en/latest/contributing/model/transcription/",
    "https://docs.vllm.ai/en/latest/contributing/ci/failures/",
    "https://docs.vllm.ai/en/latest/contributing/ci/nightly_builds/",
    "https://docs.vllm.ai/en/latest/contributing/ci/update_pytorch_version/",
    "https://docs.vllm.ai/en/latest/design/io_processor_plugins/",
    "https://docs.vllm.ai/en/latest/design/lora_resolver_plugins/",
    "https://docs.vllm.ai/en/latest/design/plugin_system/",
    "https://docs.vllm.ai/en/latest/design/arch_overview/",
    "https://docs.vllm.ai/en/latest/design/cuda_graphs/",
    "https://docs.vllm.ai/en/latest/design/dbo/",
    "https://docs.vllm.ai/en/latest/design/debug_vllm_compile/",
    "https://docs.vllm.ai/en/latest/design/fused_moe_modular_kernel/",
    "https://docs.vllm.ai/en/latest/design/huggingface_integration/",
    "https://docs.vllm.ai/en/latest/design/hybrid_kv_cache_manager/",
    "https://docs.vllm.ai/en/latest/design/logits_processors/",
    "https://docs.vllm.ai/en/latest/design/metrics/",
    "https://docs.vllm.ai/en/latest/design/mm_processing/",
    "https://docs.vllm.ai/en/latest/design/moe_kernel_features/",
    "https://docs.vllm.ai/en/latest/design/multiprocessing/",
    "https://docs.vllm.ai/en/latest/design/optimization_levels/",
    "https://docs.vllm.ai/en/latest/design/p2p_nccl_connector/",
    "https://docs.vllm.ai/en/latest/design/paged_attention/",
    "https://docs.vllm.ai/en/latest/design/prefix_caching/",
    "https://docs.vllm.ai/en/latest/design/torch_compile/",
    "https://docs.vllm.ai/en/latest/benchmarking/cli/",
    "https://docs.vllm.ai/en/latest/benchmarking/sweeps/",
    "https://docs.vllm.ai/en/latest/benchmarking/dashboard/",
    "https://docs.vllm.ai/en/latest/api/vllm/",
    "https://docs.vllm.ai/en/latest/api/vllm/beam_search/",
    "https://docs.vllm.ai/en/latest/api/vllm/collect_env/",
    "https://docs.vllm.ai/en/latest/api/vllm/connections/",
    "https://docs.vllm.ai/en/latest/api/vllm/env_override/",
    "https://docs.vllm.ai/en/latest/api/vllm/envs/",
    "https://docs.vllm.ai/en/latest/api/vllm/forward_context/",
    "https://docs.vllm.ai/en/latest/api/vllm/logger/",
    "https://docs.vllm.ai/en/latest/api/vllm/logits_process/",
    "https://docs.vllm.ai/en/latest/api/vllm/logprobs/",
    "https://docs.vllm.ai/en/latest/api/vllm/outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/pooling_params/",
    "https://docs.vllm.ai/en/latest/api/vllm/sampling_params/",
    "https://docs.vllm.ai/en/latest/api/vllm/scalar_type/",
    "https://docs.vllm.ai/en/latest/api/vllm/scripts/",
    "https://docs.vllm.ai/en/latest/api/vllm/sequence/",
    "https://docs.vllm.ai/en/latest/api/vllm/tasks/",
    "https://docs.vllm.ai/en/latest/api/vllm/tracing/",
    "https://docs.vllm.ai/en/latest/api/vllm/version/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/image/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/video/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layer/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/selector/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/chunked_local_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/cross_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/encoder_only_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/mm_encoder_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/chunked_prefill_paged_decode/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/common/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/flashmla/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/merge_attn_states/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/paged_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/pallas_kv_cache_update/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/prefix_prefill/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/rocm_aiter_mla_sparse/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_decode_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_merge_attn_states/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_reshape_and_cache_flash/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_unified_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/vit_attn_wrappers/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/fa_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/kv_sharing_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/kv_transfer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/datasets/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/latency/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/startup/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/throughput/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/endpoint_request_func/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/ready_checker/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/cli/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/param_sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/plot/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/plot_pareto/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/serve_sla/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/server/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/sla_sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/activation_quant_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/base_static_graph/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/caching/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/collective_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/compiler_interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/counter/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/cuda_graph/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/decorators/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fix_functionalization/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fusion_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fx_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/inductor_pass/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/matcher_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/monitor/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/noop_elimination/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/partition_rules/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/pass_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/piecewise_backend/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/post_cleanup/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/qk_norm_rope_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/rocm_aiter_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/sequence_parallelism/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/torch25_custom_graph_pass/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/vllm_inductor_pass/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/compilation/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/device/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/ec_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/kv_events/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/kv_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/load/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/lora/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/model/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/multimodal/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/observability/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/parallel/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/pooler/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/profiler/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/scheduler/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/speculative/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/speech_to_text/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/structured_outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/vllm/",
    "https://docs.vllm.ai/en/latest/api/vllm/device_allocator/",
    "https://docs.vllm.ai/en/latest/api/vllm/device_allocator/cumem/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/communication_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_events/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/parallel_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/tpu_distributed_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/all2all/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/all_reduce_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/base_device_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/cpu_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/cuda_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/cuda_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/custom_all_reduce/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/mnnvl_compat/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/pynccl/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/pynccl_allocator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/pynccl_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/quick_all_reduce/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/ray_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/shm_broadcast/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/shm_object_storage/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/symm_mem/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/tpu_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/xpu_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_transfer_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/example_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/factory/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/async_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/eplb_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/rebalance_execute/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/policy/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/policy/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/policy/default/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_transfer_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/factory/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/decode_bench_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/example_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/mooncake_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/multi_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/offloading_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/multi_process_adapter/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/vllm_v1_adapter/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/tensor_memory_pool/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/arg_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/async_llm_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/llm_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/api_server/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/chat_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/constants/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/context/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/launcher/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/llm/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/logger/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/renderer/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/responses_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/score_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/ssl/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/tool/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/tool_server/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/anthropic/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/anthropic/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/anthropic/serving_messages/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/collect_env/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/main/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/openai/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/run_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/types/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/latency/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/main/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/startup/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/throughput/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/api_server/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/cli_args/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/orca_metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/run_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_chat/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_chat_stream_harmony/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_completion/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_models/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_responses/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_transcription/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/speech_to_text/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/parser/harmony_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/parser/responses_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/conftest/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/sagemaker/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/sagemaker/routes/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/cache/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/elastic_ep/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/elastic_ep/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/elastic_ep/middleware/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/health/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/server_info/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/lora/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/lora/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/profile/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/profile/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rlhf/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rlhf/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rpc/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rpc/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/sleep/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/sleep/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/tokenize/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/tokenize/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/tokenize/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/data/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/parse/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/preprocess/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/dump_input/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/formatter/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/lazy/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/log_time/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/lora_model/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/lora_weights/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/model_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/peft_helper/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/request/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/resolver/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/worker_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/base_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/column_parallel_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/logits_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/replicated_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/row_parallel_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/vocal_parallel_embedding/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/ipex_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/ipex_ops/lora_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/torch_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/torch_ops/lora_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/fused_moe_lora_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/kernel_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/lora_expand_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/lora_kernel_metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/lora_shrink_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/xla_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/xla_ops/lora_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_gpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_selector/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_xpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/custom_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/parameter/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/activation/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/attention_layer_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/batch_invariant/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/conv/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/kda/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/layernorm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/lightning_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/logits_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/pooler/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/resampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/vocab_parallel_embedding/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk_delta_h/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk_o/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk_scaled_dot_kkt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/cumsum/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/fused_recurrent/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/index_py/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/kda/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/l2norm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/layernorm_guard/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/op/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/solve_tril/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/wy_fast/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/all2all_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/cpu_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/cutlass_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deep_gemm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deep_gemm_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deepep_ll_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_cutedsl_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_cutlass_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_trtllm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_batched_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_marlin_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_moe_method_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_moe_modular_method/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/gpt_oss_triton_kernels_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/layer/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/modular_kernel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_align_block_size/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_pallas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_permute_unpermute/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_torch_iterative/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/pplx_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/routing_simulator/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/shared_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/topk_weight_and_reduce/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/trtllm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/zero_expert_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/linear_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/mamba_mixer/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/mamba_mixer2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/mamba_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/short_conv/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/causal_conv1d/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/layernorm_gated/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/mamba_ssm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_bmm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_chunk_scan/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_chunk_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_combined/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_state_passing/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/auto_round/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/awq/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/awq_marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/awq_triton/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/base_config/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/bitblas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/bitsandbytes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/cpu_wna16/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/deepspeedfp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/experts_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/fbgemm_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/fp_quant/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gguf/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq_bitblas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq_marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq_marlin_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/hqq_marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/inc/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/input_quant_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/ipex_quant/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kv_cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/modelopt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/moe_wna16/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/mxfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/petit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/ptpc_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/qutlass_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/rtn/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/schema/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/torchao/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/tpu_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/triton_scaled_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_scheme/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a4_nvfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a8_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a8_int/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_nvfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a16_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_wNa16/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/module/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/schemes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/schemes/linear_qutlass_nvfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/allspark/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/bitblas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/conch/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/cutlass/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/dynamic_4bit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/exllama/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/MPLinearKernel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/machete/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/xpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/aiter/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/cutlass/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/ScaledMMLinearKernel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/triton/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/xla/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/quark/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/quark_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_ocp_mx/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_scheme/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/allspark_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/bitblas_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/flashinfer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/fp8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/gptq_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/int8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/layer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/machete_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_fp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_test/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_test_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/mxfp4_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/mxfp6_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/mxfp8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/nvfp4_emulation_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/nvfp4_moe_support/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/ocp_mx_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/petit_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/quant_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/w8a8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/common/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/deepseek_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/dual_chunk_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/dynamic_ntk_alpha_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/dynamic_ntk_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/ernie45_vl_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/linear_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/llama3_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/llama4_vision_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/mrope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/ntk_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/phi3_long_rope_scaled_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/xdrope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/yarn_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/base_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/bitsandbytes_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/default_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/dummy_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/gguf_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/online_quantization/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/runai_streamer_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/sharded_state_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/tensorizer/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/tensorizer_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/weight_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/adapters/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/afmoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/aimv2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/apertus/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/arcee/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/arctic/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/aria/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/audioflamingo3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/aya_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bagel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/baichuan/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bailing_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bee/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bert/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bert_with_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/blip/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/blip2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bloom/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/chameleon/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/chatglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/clip/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/cohere2_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/commandr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/dbrx/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepencoder/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_ocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_v2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_vl2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/dots1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/dots_ocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45_vl_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/exaone/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/exaone4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/fairseq2_llama/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/falcon/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/falcon_h1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/flex_olmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/fuyu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3n/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3n_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4_1v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4_moe_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_bigcode/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_j/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_neox/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_oss/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granite/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granite_speech/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granitemoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granitemoehybrid/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granitemoeshared/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gritlm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/grok1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/h2ovl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/hunyuan_v1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/hunyuan_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/hyperclovax_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/idefics2_vision_model/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/idefics3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interfaces/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interfaces_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/intern_vit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/internlm2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/internlm2_ve/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interns1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interns1_vit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/internvl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jais/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jais2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jina_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/keye/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/keye_vl1_5/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/kimi_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/kimi_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/lfm2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/lfm2_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/lightonocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama4_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama_eagle3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava_next/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava_next_video/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava_onevision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/longcat_flash/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/longcat_flash_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mamba2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/medusa/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/midashenglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mimo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mimo_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mimo_v2_flash/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpm3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpm_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpmv/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minimax_m2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minimax_text_01/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minimax_vl_01/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mistral3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mistral_large_3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mistral_large_3_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mixtral/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mllama4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mlp_speculator/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/modernbert/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/module_mapping/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/molmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/moonvit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mpt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nano_nemotron_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron_h/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron_nas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nvlm_d/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/olmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/olmo2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/olmoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/opencua/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/openpangu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/openpangu_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/opt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/orion/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ouro/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ovis/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ovis2_5/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/paddleocr_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/paligemma/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/persimmon/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi3v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi4mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi4mm_audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi4mm_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phimoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/pixtral/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/plamo2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/plamo3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_5_omni_thinker/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_5_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_rm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_next/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_next_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_omni_moe_thinker/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_vl_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/radio/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/roberta/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/rvl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/seed_oss/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/siglip/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/siglip2navit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/skyworkr1v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/smolvlm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/solar/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/stablelm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/starcoder2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/step3_text/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/step3_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/swin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/tarsier/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/telechat2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/teleflm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/terratorch/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ultravox/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/voxtral/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/voxtral_streaming/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/whisper/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/whisper_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/zamba2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/causal/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/legacy/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/multimodal/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/pooling/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/warmup/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/warmup/deep_gemm_warmup/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/warmup/kernel_warmup/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/evs/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/hasher/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/image/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/inputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/parse/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/processing/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/profiling/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/video/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/cuda/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/rocm/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/xpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/io_processors/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/io_processors/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/lora_resolvers/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/lora_resolvers/filesystem_resolver/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/layerwise_profile/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/ray/",
    "https://docs.vllm.ai/en/latest/api/vllm/ray/lazy_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/ray/ray_env/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/abs_reasoning_parsers/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/basic_parsers/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/deepseek_r1_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/deepseek_v3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/ernie45_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/glm4_moe_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/gptoss_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/granite_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/holo2_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/hunyuan_a13b_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/identity_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/minimax_m2_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/mistral_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/olmo3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/qwen3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/seedoss_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/step3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/deepseek_v32/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/deepseek_v32_encoding/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/detokenizer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/hf/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/mistral/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/abstract_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/deepseekv3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/deepseekv31_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/deepseekv32_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/ernie45_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/functiongemma_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/gigachat3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/glm4_moe_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/glm47_moe_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/granite_20b_fc_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/granite_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/hermes_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/hunyuan_a13b_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/internlm2_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/jamba_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/kimi_k2_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/llama4_pythonic_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/llama_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/longcat_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/minimax_m2_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/minimax_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/mistral_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/olmo3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/openai_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/phi4mini_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/pythonic_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/qwen3coder_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/qwen3xml_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/seed_oss_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/step3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/xlam_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/config_parser_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/dynamic_module/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/gguf_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/repo_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/runai_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/s3_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/tokenizer/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/chat_templates/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/chat_templates/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/afmoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/arctic/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/bagel/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/chatglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/deepseek_vl2/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/dotsocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/falcon/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/flex_olmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/hunyuan_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/jais/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/kimi_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/kimi_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/lfm2_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/medusa/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/midashenglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/mistral/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/mlp_speculator/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/moonvit/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/nemotron/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/nemotron_h/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/olmo3/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/ovis/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/qwen3_next/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/radio/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/step3_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/tarsier2/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/ultravox/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/speculators/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/speculators/algos/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/speculators/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/bagel/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/deepseek_ocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/deepseek_vl2/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/hunyuan_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/hunyuan_vl_image/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/ovis/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/ovis2_5/",
    "https://docs.vllm.ai/en/latest/api/vllm/triton_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/triton_utils/importing/",
    "https://docs.vllm.ai/en/latest/api/vllm/usage/",
    "https://docs.vllm.ai/en/latest/api/vllm/usage/usage_lib/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/argparse_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/async_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/collection_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/counter/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/deep_gemm/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/flashinfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/func_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/gc_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/hashing/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/import_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/jsontree/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/math_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/mem_constants/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/mem_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/nccl/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/network_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/nvtx_pytorch_hooks/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/platform_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/profiling/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/serial_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/system_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/tensor_schema/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/torch_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/cudagraph_dispatcher/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_cache_interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/request/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/serial_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/cpu_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/flash_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/flashinfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/flex_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/gdn_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/linear_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mamba1_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mamba2_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mamba_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/pallas/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/rocm_aiter_fa/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/rocm_aiter_unified_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/rocm_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/short_conv_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/tree_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/triton_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/aiter_triton_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/common/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/cutlass_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashattn_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashinfer_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashmla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashmla_sparse/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/indexer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/rocm_aiter_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/rocm_aiter_mla_sparse/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/triton_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/block_pool/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/encoder_cache_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_coordinator/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/single_type_kv_cache_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/async_scheduler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/output/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/request_queue/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/scheduler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/async_llm/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/coordinator/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/core/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/core_client/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/detokenizer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/exceptions/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/input_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/llm_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/logprobs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/output_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/parallel_sampling/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/multiproc_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/ray_distributed_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/ray_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/ray_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/uniproc_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/arc_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/backend/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/factory/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/lru_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/mediums/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/spec/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/backends/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/worker/cpu_gpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/worker/worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/loggers/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/perf/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/prometheus/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/ray_wrappers/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/reader/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/stats/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/pool/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/pool/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/rejection_sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/builtin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/state/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/bad_words/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/logprobs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/penalties/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/topk_topp_sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/tpu/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/tpu/sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/medusa/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/ngram_proposer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/suffix_decoding/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_guidance/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_lm_format_enforcer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_outlines/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_types/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_xgrammar/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/request/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/block_table/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/cp_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/cpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/cpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/dp_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/ec_connector_model_runner_mixin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_input_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_ubatch_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/kv_connector_model_runner_mixin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/lora_model_runner_mixin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/tpu_input_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/tpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/tpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/ubatch_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/ubatching/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/worker_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/workspace/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/xpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/xpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/async_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/attn_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/block_table/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/cudagraph_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/dp_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/input_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/states/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/structured_outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/metrics/logits/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/gumbel/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/logprob/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/min_p/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/output/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/penalties/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/eagle_cudagraph/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/rejection_sample/",
    "https://docs.vllm.ai/en/latest/cli/serve/",
    "https://docs.vllm.ai/en/latest/cli/chat/",
    "https://docs.vllm.ai/en/latest/cli/complete/",
    "https://docs.vllm.ai/en/latest/cli/run-batch/",
    "https://docs.vllm.ai/en/latest/cli/bench/latency/",
    "https://docs.vllm.ai/en/latest/cli/bench/serve/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/plot/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/plot_pareto/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/serve/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/serve_sla/",
    "https://docs.vllm.ai/en/latest/cli/bench/throughput/",
    "https://docs.vllm.ai/en/latest/community/meetups/",
    "https://docs.vllm.ai/en/latest/community/sponsors/",
    "https://docs.vllm.ai/en/latest/governance/collaboration/",
    "https://docs.vllm.ai/en/latest/governance/committers/",
    "https://docs.vllm.ai/en/latest/governance/process/"
  ]
}