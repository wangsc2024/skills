{
  "url": "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.memory.canvas.html",
  "title": "autogen_ext.memory.canvas — AutoGen",
  "content": "An in‑memory canvas that stores text files with full revision history.\n\nThis is an experimental API and may change in the future.\n\nBesides the original CRUD‑like operations, this enhanced implementation adds:\n\napply_patch – applies patches using the unidiff library for accurate hunk application and context line validation.\n\nget_revision_content – random access to any historical revision.\n\nget_revision_diffs – obtain the list of diffs applied between every consecutive pair of revisions so that a caller can replay or audit the full change history.\n\nReturn the exact content stored in revision.\n\nIf the revision does not exist an empty string is returned so that downstream code can handle the “not found” case without exceptions.\n\nReturn a chronological list of unified‑diffs for filename.\n\nEach element in the returned list represents the diff that transformed revision n into revision n+1 (starting at revision 1 → 2).\n\nReturn a mapping of filename → latest revision number.\n\nReturn the most recent content or an empty string if the file is new.\n\nCreate filename or append a new revision containing new_content.\n\nReturn a unified diff between from_revision and to_revision.\n\nApply patch_text (unified diff) to the latest revision and save a new revision.\n\nUses the unidiff library to accurately apply hunks and validate context lines.\n\nReturn a summarised view of every file and its latest revision.\n\nA memory implementation that uses a Canvas for storing file-like content. Inserts the current state of the canvas into the ChatCompletionContext on each turn.\n\nThis is an experimental API and may change in the future.\n\nThe TextCanvasMemory provides a persistent, file-like storage mechanism that can be used by agents to read and write content. It automatically injects the current state of all files in the canvas into the model context before each inference.\n\nThis is particularly useful for: - Allowing agents to create and modify documents over multiple turns - Enabling collaborative document editing between multiple agents - Maintaining persistent state across conversation turns - Working with content too large to fit in a single message\n\nThe canvas provides tools for: - Creating or updating files with new content - Applying patches (unified diff format) to existing files\n\nExample: Using TextCanvasMemory with an AssistantAgent\n\nThe following example demonstrates how to create a TextCanvasMemory and use it with an AssistantAgent to write and update a story file.\n\nExample: Using TextCanvasMemory with multiple agents\n\nThe following example shows how to use TextCanvasMemory with multiple agents collaborating on the same document.\n\nInject the entire canvas summary (or a selected subset) as reference data. Here, we just put it into a system message, but you could customize.\n\nPotentially search for matching filenames or file content. This example returns empty.\n\nExample usage: Possibly interpret content as a patch or direct file update. Could also be done by a specialized “CanvasTool” instead.\n\nClear the entire canvas by replacing it with a new empty instance.\n\nClean up any resources used by the memory implementation.\n\nReturns an UpdateFileTool instance that works with this memory’s canvas.\n\nReturns an ApplyPatchTool instance that works with this memory’s canvas.\n\nautogen_ext.experimental.task_centric_memory\n\nautogen_ext.memory.chromadb",
  "headings": [
    {
      "level": "h1",
      "text": "autogen_ext.memory.canvas#",
      "id": ""
    }
  ],
  "code_samples": [
    {
      "code": "import asyncio\nfrom autogen_core import CancellationToken\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.messages import TextMessage\nfrom autogen_ext.memory.canvas import TextCanvasMemory\n\n\nasync def main():\n    # Create a model client\n    model_client = OpenAIChatCompletionClient(\n        model=\"gpt-4o\",\n        # api_key = \"your_openai_api_key\"\n    )\n\n    # Create the canvas memory\n    text_canvas_memory = TextCanvasMemory()\n\n    # Get tools for working with the canvas\n    update_file_tool = text_canvas_memory.get_update_file_tool()\n    apply_patch_tool = text_canvas_memory.get_apply_patch_tool()\n\n    # Create an agent with the canvas memory and tools\n    writer_agent = AssistantAgent(\n        name=\"Writer\",\n        model_client=model_client,\n        description=\"A writer agent that creates and updates stories.\",\n        system_message='''\n        You are a Writer Agent. Your focus is to generate a story based on the user's request.\n\n        Instructions for using the canvas:\n\n        - The story should be stored on the canvas in a file named \"story.md\".\n        - If \"story.md\" does not exist, create it by calling the 'update_file' tool.\n        - If \"story.md\" already exists, generate a unified diff (patch) from the current\n          content to the new version, and call the 'apply_patch' tool to apply the changes.\n\n        IMPORTANT: Do not include the full story text in your chat messages.\n        Only write the story content to the canvas using the tools.\n        ''',\n        tools=[update_file_tool, apply_patch_tool],\n        memory=[text_canvas_memory],\n    )\n\n    # Send a message to the agent\n    await writer_agent.on_messages(\n        [TextMessage(content=\"Write a short story about a bunny and a sunflower.\", source=\"user\")],\n        CancellationToken(),\n    )\n\n    # Retrieve the content from the canvas\n    story_content = text_canvas_memory.canvas.get_latest_content(\"story.md\")\n    print(\"Story content from canvas:\")\n    print(story_content)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom autogen_core import CancellationToken\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.messages import TextMessage\nfrom autogen_ext.memory.canvas import TextCanvasMemory\n\n\nasync def main():\n    # Create a model client\n    model_client = OpenAIChatCompletionClient(\n        model=\"gpt-4o\",\n        # api_key = \"your_openai_api_key\"\n    )\n\n    # Create the canvas memory\n    text_canvas_memory = TextCanvasMemory()\n\n    # Get tools for working with the canvas\n    update_file_tool = text_canvas_memory.get_update_file_tool()\n    apply_patch_tool = text_canvas_memory.get_apply_patch_tool()\n\n    # Create an agent with the canvas memory and tools\n    writer_agent = AssistantAgent(\n        name=\"Writer\",\n        model_client=model_client,\n        description=\"A writer agent that creates and updates stories.\",\n        system_message='''\n        You are a Writer Agent. Your focus is to generate a story based on the user's request.\n\n        Instructions for using the canvas:\n\n        - The story should be stored on the canvas in a file named \"story.md\".\n        - If \"story.md\" does not exist, create it by calling the 'update_file' tool.\n        - If \"story.md\" already exists, generate a unified diff (patch) from the current\n          content to the new version, and call the 'apply_patch' tool to apply the changes.\n\n        IMPORTANT: Do not include the full story text in your chat messages.\n        Only write the story content to the canvas using the tools.\n        ''',\n        tools=[update_file_tool, apply_patch_tool],\n        memory=[text_canvas_memory],\n    )\n\n    # Send a message to the agent\n    await writer_agent.on_messages(\n        [TextMessage(content=\"Write a short story about a bunny and a sunflower.\", source=\"user\")],\n        CancellationToken(),\n    )\n\n    # Retrieve the content from the canvas\n    story_content = text_canvas_memory.canvas.get_latest_content(\"story.md\")\n    print(\"Story content from canvas:\")\n    print(story_content)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.conditions import TextMentionTermination\nfrom autogen_ext.memory.canvas import TextCanvasMemory\n\n\nasync def main():\n    # Create a model client\n    model_client = OpenAIChatCompletionClient(\n        model=\"gpt-4o\",\n        # api_key = \"your_openai_api_key\"\n    )\n\n    # Create the shared canvas memory\n    text_canvas_memory = TextCanvasMemory()\n    update_file_tool = text_canvas_memory.get_update_file_tool()\n    apply_patch_tool = text_canvas_memory.get_apply_patch_tool()\n\n    # Create a writer agent\n    writer_agent = AssistantAgent(\n        name=\"Writer\",\n        model_client=model_client,\n        description=\"A writer agent that creates stories.\",\n        system_message=\"You write children's stories on the canvas in story.md.\",\n        tools=[update_file_tool, apply_patch_tool],\n        memory=[text_canvas_memory],\n    )\n\n    # Create a critique agent\n    critique_agent = AssistantAgent(\n        name=\"Critique\",\n        model_client=model_client,\n        description=\"A critique agent that provides feedback on stories.\",\n        system_message=\"You review the story.md file and provide constructive feedback.\",\n        memory=[text_canvas_memory],\n    )\n\n    # Create a team with both agents\n    team = RoundRobinGroupChat(\n        participants=[writer_agent, critique_agent],\n        termination_condition=TextMentionTermination(\"TERMINATE\"),\n        max_turns=10,\n    )\n\n    # Run the team on a task\n    await team.run(task=\"Create a children's book about a bunny and a sunflower\")\n\n    # Get the final story\n    story = text_canvas_memory.canvas.get_latest_content(\"story.md\")\n    print(story)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.conditions import TextMentionTermination\nfrom autogen_ext.memory.canvas import TextCanvasMemory\n\n\nasync def main():\n    # Create a model client\n    model_client = OpenAIChatCompletionClient(\n        model=\"gpt-4o\",\n        # api_key = \"your_openai_api_key\"\n    )\n\n    # Create the shared canvas memory\n    text_canvas_memory = TextCanvasMemory()\n    update_file_tool = text_canvas_memory.get_update_file_tool()\n    apply_patch_tool = text_canvas_memory.get_apply_patch_tool()\n\n    # Create a writer agent\n    writer_agent = AssistantAgent(\n        name=\"Writer\",\n        model_client=model_client,\n        description=\"A writer agent that creates stories.\",\n        system_message=\"You write children's stories on the canvas in story.md.\",\n        tools=[update_file_tool, apply_patch_tool],\n        memory=[text_canvas_memory],\n    )\n\n    # Create a critique agent\n    critique_agent = AssistantAgent(\n        name=\"Critique\",\n        model_client=model_client,\n        description=\"A critique agent that provides feedback on stories.\",\n        system_message=\"You review the story.md file and provide constructive feedback.\",\n        memory=[text_canvas_memory],\n    )\n\n    # Create a team with both agents\n    team = RoundRobinGroupChat(\n        participants=[writer_agent, critique_agent],\n        termination_condition=TextMentionTermination(\"TERMINATE\"),\n        max_turns=10,\n    )\n\n    # Run the team on a task\n    await team.run(task=\"Create a children's book about a bunny and a sunflower\")\n\n    # Get the final story\n    story = text_canvas_memory.canvas.get_latest_content(\"story.md\")\n    print(story)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())",
      "language": "python"
    }
  ],
  "patterns": [
    {
      "description": "API Reference autogen_ext.memory.canvas autogen_ext.memory.canvas# class TextCanvas[source]# Bases: BaseCanvas An in‑memory canvas that stores text files with full revision history. Warning This is an experimental API and may change in the future. Besides the original CRUD‑like operations, this enhanced implementation adds: apply_patch – applies patches using the unidiff library for accurate hunk application and context line validation. get_revision_content – random access to any historical revision. get_revision_diffs – obtain the list of diffs applied between every consecutive pair of revisions so that a caller can replay or audit the full change history. get_revision_content(filename: str, revision: int) → str[source]# Return the exact content stored in revision. If the revision does not exist an empty string is returned so that downstream code can handle the “not found” case without exceptions. get_revision_diffs(filename: str) → List[str][source]# Return a chronological list of unified‑diffs for filename. Each element in the returned list represents the diff that transformed revision n into revision n+1 (starting at revision 1 → 2). list_files() → Dict[str, int][source]# Return a mapping of filename → latest revision number. get_latest_content(filename: str) → str[source]# Return the most recent content or an empty string if the file is new. add_or_update_file(filename: str, new_content: str | bytes | Any) → None[source]# Create filename or append a new revision containing new_content. get_diff(filename: str, from_revision: int, to_revision: int) → str[source]# Return a unified diff between from_revision and to_revision. apply_patch(filename: str, patch_data: str | bytes | Any) → None[source]# Apply patch_text (unified diff) to the latest revision and save a new revision. Uses the unidiff library to accurately apply hunks and validate context lines. get_all_contents_for_context() → str[source]# Return a summarised view of every file and its latest revision. class TextCanvasMemory(canvas: TextCanvas | None = None)[source]# Bases: Memory A memory implementation that uses a Canvas for storing file-like content. Inserts the current state of the canvas into the ChatCompletionContext on each turn. Warning This is an experimental API and may change in the future. The TextCanvasMemory provides a persistent, file-like storage mechanism that can be used by agents to read and write content. It automatically injects the current state of all files in the canvas into the model context before each inference. This is particularly useful for: - Allowing agents to create and modify documents over multiple turns - Enabling collaborative document editing between multiple agents - Maintaining persistent state across conversation turns - Working with content too large to fit in a single message The canvas provides tools for: - Creating or updating files with new content - Applying patches (unified diff format) to existing files Examples Example: Using TextCanvasMemory with an AssistantAgent The following example demonstrates how to create a TextCanvasMemory and use it with an AssistantAgent to write and update a story file. import asyncio from autogen_core import CancellationToken from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.messages import TextMessage from autogen_ext.memory.canvas import TextCanvasMemory async def main(): # Create a model client model_client = OpenAIChatCompletionClient( model=\"gpt-4o\", # api_key = \"your_openai_api_key\" ) # Create the canvas memory text_canvas_memory = TextCanvasMemory() # Get tools for working with the canvas update_file_tool = text_canvas_memory.get_update_file_tool() apply_patch_tool = text_canvas_memory.get_apply_patch_tool() # Create an agent with the canvas memory and tools writer_agent = AssistantAgent( name=\"Writer\", model_client=model_client, description=\"A writer agent that creates and updates stories.\", system_message=''' You are a Writer Agent. Your focus is to generate a story based on the user's request. Instructions for using the canvas: - The story should be stored on the canvas in a file named \"story.md\". - If \"story.md\" does not exist, create it by calling the 'update_file' tool. - If \"story.md\" already exists, generate a unified diff (patch) from the current content to the new version, and call the 'apply_patch' tool to apply the changes. IMPORTANT: Do not include the full story text in your chat messages. Only write the story content to the canvas using the tools. ''', tools=[update_file_tool, apply_patch_tool], memory=[text_canvas_memory], ) # Send a message to the agent await writer_agent.on_messages( [TextMessage(content=\"Write a short story about a bunny and a sunflower.\", source=\"user\")], CancellationToken(), ) # Retrieve the content from the canvas story_content = text_canvas_memory.canvas.get_latest_content(\"story.md\") print(\"Story content from canvas:\") print(story_content) if __name__ == \"__main__\": asyncio.run(main()) Example: Using TextCanvasMemory with multiple agents The following example shows how to use TextCanvasMemory with multiple agents collaborating on the same document. import asyncio from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.teams import RoundRobinGroupChat from autogen_agentchat.conditions import TextMentionTermination from autogen_ext.memory.canvas import TextCanvasMemory async def main(): # Create a model client model_client = OpenAIChatCompletionClient( model=\"gpt-4o\", # api_key = \"your_openai_api_key\" ) # Create the shared canvas memory text_canvas_memory = TextCanvasMemory() update_file_tool = text_canvas_memory.get_update_file_tool() apply_patch_tool = text_canvas_memory.get_apply_patch_tool() # Create a writer agent writer_agent = AssistantAgent( name=\"Writer\", model_client=model_client, description=\"A writer agent that creates stories.\", system_message=\"You write children's stories on the canvas in story.md.\", tools=[update_file_tool, apply_patch_tool], memory=[text_canvas_memory], ) # Create a critique agent critique_agent = AssistantAgent( name=\"Critique\", model_client=model_client, description=\"A critique agent that provides feedback on stories.\", system_message=\"You review the story.md file and provide constructive feedback.\", memory=[text_canvas_memory], ) # Create a team with both agents team = RoundRobinGroupChat( participants=[writer_agent, critique_agent], termination_condition=TextMentionTermination(\"TERMINATE\"), max_turns=10, ) # Run the team on a task await team.run(task=\"Create a children's book about a bunny and a sunflower\") # Get the final story story = text_canvas_memory.canvas.get_latest_content(\"story.md\") print(story) if __name__ == \"__main__\": asyncio.run(main()) async update_context(model_context: ChatCompletionContext) → UpdateContextResult[source]# Inject the entire canvas summary (or a selected subset) as reference data. Here, we just put it into a system message, but you could customize. async query(query: str | MemoryContent, cancellation_token: CancellationToken | None = None, **kwargs: Any) → MemoryQueryResult[source]# Potentially search for matching filenames or file content. This example returns empty. async add(content: MemoryContent, cancellation_token: CancellationToken | None = None) → None[source]# Example usage: Possibly interpret content as a patch or direct file update. Could also be done by a specialized “CanvasTool” instead. async clear() → None[source]# Clear the entire canvas by replacing it with a new empty instance. async close() → None[source]# Clean up any resources used by the memory implementation. get_update_file_tool() → UpdateFileTool[source]# Returns an UpdateFileTool instance that works with this memory’s canvas. get_apply_patch_tool() → ApplyPatchTool[source]# Returns an ApplyPatchTool instance that works with this memory’s canvas. previous autogen_ext.experimental.task_centric_memory next autogen_ext.memory.chromadb On this page TextCanvas TextCanvas.get_revision_content() TextCanvas.get_revision_diffs() TextCanvas.list_files() TextCanvas.get_latest_content() TextCanvas.add_or_update_file() TextCanvas.get_diff() TextCanvas.apply_patch() TextCanvas.get_all_contents_for_context() TextCanvasMemory TextCanvasMemory.update_context() TextCanvasMemory.query() TextCanvasMemory.add() TextCanvasMemory.clear() TextCanvasMemory.close() TextCanvasMemory.get_update_file_tool() TextCanvasMemory.get_apply_patch_tool() Edit on GitHub Show Source",
      "code": "BaseCanvas"
    },
    {
      "description": "API Reference autogen_ext.memory.canvas autogen_ext.memory.canvas# class TextCanvas[source]# Bases: BaseCanvas An in‑memory canvas that stores text files with full revision history. Warning This is an experimental API and may change in the future. Besides the original CRUD‑like operations, this enhanced implementation adds: apply_patch – applies patches using the unidiff library for accurate hunk application and context line validation. get_revision_content – random access to any historical revision. get_revision_diffs – obtain the list of diffs applied between every consecutive pair of revisions so that a caller can replay or audit the full change history. get_revision_content(filename: str, revision: int) → str[source]# Return the exact content stored in revision. If the revision does not exist an empty string is returned so that downstream code can handle the “not found” case without exceptions. get_revision_diffs(filename: str) → List[str][source]# Return a chronological list of unified‑diffs for filename. Each element in the returned list represents the diff that transformed revision n into revision n+1 (starting at revision 1 → 2). list_files() → Dict[str, int][source]# Return a mapping of filename → latest revision number. get_latest_content(filename: str) → str[source]# Return the most recent content or an empty string if the file is new. add_or_update_file(filename: str, new_content: str | bytes | Any) → None[source]# Create filename or append a new revision containing new_content. get_diff(filename: str, from_revision: int, to_revision: int) → str[source]# Return a unified diff between from_revision and to_revision. apply_patch(filename: str, patch_data: str | bytes | Any) → None[source]# Apply patch_text (unified diff) to the latest revision and save a new revision. Uses the unidiff library to accurately apply hunks and validate context lines. get_all_contents_for_context() → str[source]# Return a summarised view of every file and its latest revision. class TextCanvasMemory(canvas: TextCanvas | None = None)[source]# Bases: Memory A memory implementation that uses a Canvas for storing file-like content. Inserts the current state of the canvas into the ChatCompletionContext on each turn. Warning This is an experimental API and may change in the future. The TextCanvasMemory provides a persistent, file-like storage mechanism that can be used by agents to read and write content. It automatically injects the current state of all files in the canvas into the model context before each inference. This is particularly useful for: - Allowing agents to create and modify documents over multiple turns - Enabling collaborative document editing between multiple agents - Maintaining persistent state across conversation turns - Working with content too large to fit in a single message The canvas provides tools for: - Creating or updating files with new content - Applying patches (unified diff format) to existing files Examples Example: Using TextCanvasMemory with an AssistantAgent The following example demonstrates how to create a TextCanvasMemory and use it with an AssistantAgent to write and update a story file. import asyncio from autogen_core import CancellationToken from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.messages import TextMessage from autogen_ext.memory.canvas import TextCanvasMemory async def main(): # Create a model client model_client = OpenAIChatCompletionClient( model=\"gpt-4o\", # api_key = \"your_openai_api_key\" ) # Create the canvas memory text_canvas_memory = TextCanvasMemory() # Get tools for working with the canvas update_file_tool = text_canvas_memory.get_update_file_tool() apply_patch_tool = text_canvas_memory.get_apply_patch_tool() # Create an agent with the canvas memory and tools writer_agent = AssistantAgent( name=\"Writer\", model_client=model_client, description=\"A writer agent that creates and updates stories.\", system_message=''' You are a Writer Agent. Your focus is to generate a story based on the user's request. Instructions for using the canvas: - The story should be stored on the canvas in a file named \"story.md\". - If \"story.md\" does not exist, create it by calling the 'update_file' tool. - If \"story.md\" already exists, generate a unified diff (patch) from the current content to the new version, and call the 'apply_patch' tool to apply the changes. IMPORTANT: Do not include the full story text in your chat messages. Only write the story content to the canvas using the tools. ''', tools=[update_file_tool, apply_patch_tool], memory=[text_canvas_memory], ) # Send a message to the agent await writer_agent.on_messages( [TextMessage(content=\"Write a short story about a bunny and a sunflower.\", source=\"user\")], CancellationToken(), ) # Retrieve the content from the canvas story_content = text_canvas_memory.canvas.get_latest_content(\"story.md\") print(\"Story content from canvas:\") print(story_content) if __name__ == \"__main__\": asyncio.run(main()) Example: Using TextCanvasMemory with multiple agents The following example shows how to use TextCanvasMemory with multiple agents collaborating on the same document. import asyncio from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.teams import RoundRobinGroupChat from autogen_agentchat.conditions import TextMentionTermination from autogen_ext.memory.canvas import TextCanvasMemory async def main(): # Create a model client model_client = OpenAIChatCompletionClient( model=\"gpt-4o\", # api_key = \"your_openai_api_key\" ) # Create the shared canvas memory text_canvas_memory = TextCanvasMemory() update_file_tool = text_canvas_memory.get_update_file_tool() apply_patch_tool = text_canvas_memory.get_apply_patch_tool() # Create a writer agent writer_agent = AssistantAgent( name=\"Writer\", model_client=model_client, description=\"A writer agent that creates stories.\", system_message=\"You write children's stories on the canvas in story.md.\", tools=[update_file_tool, apply_patch_tool], memory=[text_canvas_memory], ) # Create a critique agent critique_agent = AssistantAgent( name=\"Critique\", model_client=model_client, description=\"A critique agent that provides feedback on stories.\", system_message=\"You review the story.md file and provide constructive feedback.\", memory=[text_canvas_memory], ) # Create a team with both agents team = RoundRobinGroupChat( participants=[writer_agent, critique_agent], termination_condition=TextMentionTermination(\"TERMINATE\"), max_turns=10, ) # Run the team on a task await team.run(task=\"Create a children's book about a bunny and a sunflower\") # Get the final story story = text_canvas_memory.canvas.get_latest_content(\"story.md\") print(story) if __name__ == \"__main__\": asyncio.run(main()) async update_context(model_context: ChatCompletionContext) → UpdateContextResult[source]# Inject the entire canvas summary (or a selected subset) as reference data. Here, we just put it into a system message, but you could customize. async query(query: str | MemoryContent, cancellation_token: CancellationToken | None = None, **kwargs: Any) → MemoryQueryResult[source]# Potentially search for matching filenames or file content. This example returns empty. async add(content: MemoryContent, cancellation_token: CancellationToken | None = None) → None[source]# Example usage: Possibly interpret content as a patch or direct file update. Could also be done by a specialized “CanvasTool” instead. async clear() → None[source]# Clear the entire canvas by replacing it with a new empty instance. async close() → None[source]# Clean up any resources used by the memory implementation. get_update_file_tool() → UpdateFileTool[source]# Returns an UpdateFileTool instance that works with this memory’s canvas. get_apply_patch_tool() → ApplyPatchTool[source]# Returns an ApplyPatchTool instance that works with this memory’s canvas. previous autogen_ext.experimental.task_centric_memory next autogen_ext.memory.chromadb",
      "code": "BaseCanvas"
    },
    {
      "description": "Example: Using TextCanvasMemory with an AssistantAgent",
      "code": "import asyncio\nfrom autogen_core import CancellationToken\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.messages import TextMessage\nfrom autogen_ext.memory.canvas import TextCanvasMemory\n\n\nasync def main():\n    # Create a model client\n    model_client = OpenAIChatCompletionClient(\n        model=\"gpt-4o\",\n        # api_key = \"your_openai_api_key\"\n    )\n\n    # Create the canvas memory\n    text_canvas_memory = TextCanvasMemory()\n\n    # Get tools for working with the canvas\n    update_file_tool = text_canvas_memory.get_update_file_tool()\n    apply_patch_tool = text_canvas_memory.get_apply_patch_tool()\n\n    # Create an agent with the canvas memory and tools\n    writer_agent = AssistantAgent(\n        name=\"Writer\",\n        model_client=model_client,\n        description=\"A writer agent that creates and updates stories.\",\n        system_message='''\n        You are a Writer Agent. Your focus is to generate a story based on the user's request.\n\n        Instructions for using the canvas:\n\n        - The story should be stored on the canvas in a file named \"story.md\".\n        - If \"story.md\" does not exist, create it by calling the 'update_file' tool.\n        - If \"story.md\" already exists, generate a unified diff (patch) from the current\n          content to the new version, and call the 'apply_patch' tool to apply the changes.\n\n        IMPORTANT: Do not include the full story text in your chat messages.\n        Only write the story content to the canvas using the tools.\n        ''',\n        tools=[update_file_tool, apply_patch_tool],\n        memory=[text_canvas_memory],\n    )\n\n    # Send a message to the agent\n    await writer_agent.on_messages(\n        [TextMessage(content=\"Write a short story about a bunny and a sunflower.\", source=\"user\")],\n        CancellationToken(),\n    )\n\n    # Retrieve the content from the canvas\n    story_content = text_canvas_memory.canvas.get_latest_content(\"story.md\")\n    print(\"Story content from canvas:\")\n    print(story_content)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
    },
    {
      "description": "Example: Using TextCanvasMemory with multiple agents",
      "code": "import asyncio\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.conditions import TextMentionTermination\nfrom autogen_ext.memory.canvas import TextCanvasMemory\n\n\nasync def main():\n    # Create a model client\n    model_client = OpenAIChatCompletionClient(\n        model=\"gpt-4o\",\n        # api_key = \"your_openai_api_key\"\n    )\n\n    # Create the shared canvas memory\n    text_canvas_memory = TextCanvasMemory()\n    update_file_tool = text_canvas_memory.get_update_file_tool()\n    apply_patch_tool = text_canvas_memory.get_apply_patch_tool()\n\n    # Create a writer agent\n    writer_agent = AssistantAgent(\n        name=\"Writer\",\n        model_client=model_client,\n        description=\"A writer agent that creates stories.\",\n        system_message=\"You write children's stories on the canvas in story.md.\",\n        tools=[update_file_tool, apply_patch_tool],\n        memory=[text_canvas_memory],\n    )\n\n    # Create a critique agent\n    critique_agent = AssistantAgent(\n        name=\"Critique\",\n        model_client=model_client,\n        description=\"A critique agent that provides feedback on stories.\",\n        system_message=\"You review the story.md file and provide constructive feedback.\",\n        memory=[text_canvas_memory],\n    )\n\n    # Create a team with both agents\n    team = RoundRobinGroupChat(\n        participants=[writer_agent, critique_agent],\n        termination_condition=TextMentionTermination(\"TERMINATE\"),\n        max_turns=10,\n    )\n\n    # Run the team on a task\n    await team.run(task=\"Create a children's book about a bunny and a sunflower\")\n\n    # Get the final story\n    story = text_canvas_memory.canvas.get_latest_content(\"story.md\")\n    print(story)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
    },
    {
      "description": "Example usage: Possibly interpret content as a patch or direct file update. Could also be done by a specialized “CanvasTool” instead.",
      "code": "TextCanvas"
    }
  ],
  "links": [
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.memory.canvas.html",
    "https://microsoft.github.io/autogen/stable/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/extensions-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/reference/index.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.agents.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.base.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.conditions.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.messages.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.state.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.teams.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.tools.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.ui.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.utils.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.code_executor.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.exceptions.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.logging.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.memory.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.model_context.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.models.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.tool_agent.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.tools.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.utils.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.cache_store.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.experimental.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.memory.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.teams.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.ui.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.azure.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.file_surfer.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.magentic_one.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.openai.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.video_surfer.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.web_surfer.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.auth.azure.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.cache_store.diskcache.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.cache_store.redis.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.azure.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.docker.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.docker_jupyter.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.jupyter.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.local.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.experimental.task_centric_memory.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.memory.chromadb.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.memory.mem0.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.memory.redis.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.anthropic.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.azure.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.cache.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.llama_cpp.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.ollama.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.openai.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.replay.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.semantic_kernel.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.teams.magentic_one.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.azure.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.code_execution.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.graphrag.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.http.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.langchain.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.mcp.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.semantic_kernel.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.video_surfer.tools.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.web_surfer.playwright_controller.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.experimental.task_centric_memory.utils.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.anthropic.config.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.azure.config.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.ollama.config.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.openai.config.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.protos.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.protos.agent_worker_pb2.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.protos.agent_worker_pb2_grpc.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.protos.cloudevent_pb2.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.protos.cloudevent_pb2_grpc.html"
  ]
}