{
  "url": "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.experimental.task_centric_memory.utils.html",
  "title": "autogen_ext.experimental.task_centric_memory.utils — AutoGen",
  "content": "A minimal wrapper combining task-centric memory with an agent or team. Applications may use the Apprentice class, or they may directly instantiate and call the Memory Controller using this class as an example.\n\nclient – The client to call the model.\n\nconfig – An optional dict that can be used to override the following values: name_of_agent_or_team: The name of the target agent or team for assigning tasks to. disable_prefix_caching: True to disable prefix caching by prepending random ints to the first message. MemoryController: A config dict passed to MemoryController.\n\nAn optional dict that can be used to override the following values:\n\nname_of_agent_or_team: The name of the target agent or team for assigning tasks to.\n\ndisable_prefix_caching: True to disable prefix caching by prepending random ints to the first message.\n\nMemoryController: A config dict passed to MemoryController.\n\nlogger – An optional logger. If None, a default logger will be created.\n\nResets the memory bank.\n\nHandles a user message, extracting any advice and assigning a task to the agent.\n\nAdds a task-solution pair to the memory bank, to be retrieved together later as a combined insight. This is useful when the insight is a demonstration of how to solve a given type of task.\n\nAssigns a task to the agent, along with any relevant insights/memories.\n\nRepeatedly assigns a task to the completion agent, and tries to learn from failures by creating useful insights as memories.\n\nPasses the given task to the target agent or team.\n\nBases: ChatCompletionClient\n\nA chat completion client that supports fast, large-scale tests of code calling LLM clients.\n\nTwo modes are supported:\n\n“record”: delegates to the underlying client while also recording the input messages and responses, which are saved to disk when finalize() is called.\n\n“replay”: loads previously recorded message and responses from disk, then on each call checks that its message matches the recorded message, and returns the recorded response.\n\nThe recorded data is stored as a JSON list of records. Each record is a dictionary with a “mode” field (either “create” or “create_stream”), a serialized list of messages, and either a “response” (for create calls) or a “stream” (a list of streamed outputs for create_stream calls).\n\nReplayChatCompletionClient and ChatCompletionCache do similar things, but with significant differences:\n\nReplayChatCompletionClient replays pre-defined responses in a specified order without recording anything or checking the messages sent to the client.\n\nChatCompletionCache caches responses and replays them for messages that have been seen before, regardless of order, and calls the base client for any uncached messages.\n\nCreates a single response from the model.\n\nmessages (Sequence[LLMMessage]) – The messages to send to the model.\n\ntools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to [].\n\ntool_choice (Tool | Literal[\"auto\", \"required\", \"none\"], optional) – A single Tool object to force the model to use, “auto” to let the model choose any available tool, “required” to force tool usage, or “none” to disable tool usage. Defaults to “auto”.\n\njson_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither. Defaults to None. If set to a Pydantic BaseModel type, it will be used as the output type for structured output. If set to a boolean, it will be used to determine whether to use JSON mode or not. If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.\n\nextra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}.\n\ncancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None.\n\nCreateResult – The result of the model call.\n\nCreates a stream of string chunks from the model ending with a CreateResult.\n\nmessages (Sequence[LLMMessage]) – The messages to send to the model.\n\ntools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to [].\n\ntool_choice (Tool | Literal[\"auto\", \"required\", \"none\"], optional) – A single Tool object to force the model to use, “auto” to let the model choose any available tool, “required” to force tool usage, or “none” to disable tool usage. Defaults to “auto”.\n\njson_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither. Defaults to None. If set to a Pydantic BaseModel type, it will be used as the output type for structured output. If set to a boolean, it will be used to determine whether to use JSON mode or not. If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.\n\nWhether to use JSON mode, structured output, or neither. Defaults to None. If set to a Pydantic BaseModel type, it will be used as the output type for structured output. If set to a boolean, it will be used to determine whether to use JSON mode or not. If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.\n\nextra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}.\n\ncancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None.\n\nAsyncGenerator[Union[str, CreateResult], None] – A generator that yields string chunks and ends with a CreateResult.\n\nIn record mode, saves the accumulated records to disk. In replay mode, makes sure all the records were checked.\n\nRuns basic tests, and determines task success without limitation to string matches.\n\nclient – The client to call the model.\n\nlogger – An optional logger. If None, no logging will be performed.\n\nCalls the model client with the given input and returns the response.\n\nDetermines whether the response is equivalent to the task’s correct answer.\n\nLogs text and images to a set of HTML pages, one per function/method, linked to each other in a call tree.\n\nconfig – An optional dict that can be used to override the following values: level: The logging level, one of DEBUG, INFO, WARNING, ERROR, CRITICAL, or NONE. path: The path to the directory where the log files will be written.\n\nAn optional dict that can be used to override the following values:\n\nlevel: The logging level, one of DEBUG, INFO, WARNING, ERROR, CRITICAL, or NONE.\n\npath: The path to the directory where the log files will be written.\n\nAdds DEBUG text to the current page if debugging level <= DEBUG.\n\nAdds INFO text to the current page if debugging level <= INFO.\n\nAdds WARNING text to the current page if debugging level <= WARNING.\n\nAdds ERROR text to the current page if debugging level <= ERROR.\n\nAdds CRITICAL text to the current page if debugging level <= CRITICAL.\n\nAdds a page containing the message’s content, including any images.\n\nAdds a page containing a list of dicts.\n\nLogs messages sent to a model and the TaskResult response to a new page.\n\nLogs messages sent to a model and the TaskResult response to a new page.\n\nReturns a link to a local file in the log.\n\nInserts a thumbnail link to an image to the page.\n\nWrites the current state of the log to disk.\n\nAdds a new page corresponding to the current function call.\n\nFinishes the page corresponding to the current function call.\n\nGives an AssistantAgent the ability to learn quickly from user teachings, hints, and advice.\n\nInstantiate MemoryController.\n\nInstantiate Teachability, passing the memory controller as a parameter.\n\nInstantiate an AssistantAgent, passing the teachability instance (wrapped in a list) as the memory parameter.\n\nUse the AssistantAgent as usual, such as for chatting with the user.\n\nGet the memory instance identifier.\n\nExtracts any advice from the last user turn to be stored in memory, and adds any relevant memories to the model context.\n\nTries to extract any advice from the passed content and add it to memory.\n\nReturns any memories that seem relevant to the query.\n\nClear all entries from memory.\n\nClean up memory resources.\n\nautogen_ext.agents.web_surfer.playwright_controller\n\nautogen_ext.models.anthropic.config",
  "headings": [
    {
      "level": "h1",
      "text": "autogen_ext.experimental.task_centric_memory.utils#",
      "id": ""
    }
  ],
  "code_samples": [],
  "patterns": [
    {
      "description": "API Reference autogen_ext.experimental.task_centric_memory.utils autogen_ext.experimental.task_centric_memory.utils# class Apprentice(client: ChatCompletionClient, config: ApprenticeConfig | None = None, logger: PageLogger | None = None)[source]# Bases: object A minimal wrapper combining task-centric memory with an agent or team. Applications may use the Apprentice class, or they may directly instantiate and call the Memory Controller using this class as an example. Parameters: client – The client to call the model. config – An optional dict that can be used to override the following values: name_of_agent_or_team: The name of the target agent or team for assigning tasks to. disable_prefix_caching: True to disable prefix caching by prepending random ints to the first message. MemoryController: A config dict passed to MemoryController. logger – An optional logger. If None, a default logger will be created. reset_memory() → None[source]# Resets the memory bank. async handle_user_message(text: str, should_await: bool = True) → str[source]# Handles a user message, extracting any advice and assigning a task to the agent. async add_task_solution_pair_to_memory(task: str, solution: str) → None[source]# Adds a task-solution pair to the memory bank, to be retrieved together later as a combined insight. This is useful when the insight is a demonstration of how to solve a given type of task. async assign_task(task: str, use_memory: bool = True, should_await: bool = True) → str[source]# Assigns a task to the agent, along with any relevant insights/memories. async train_on_task(task: str, expected_answer: str) → None[source]# Repeatedly assigns a task to the completion agent, and tries to learn from failures by creating useful insights as memories. async assign_task_to_agent_or_team(task: str) → Tuple[str, str][source]# Passes the given task to the target agent or team. class ChatCompletionClientRecorder(client: ChatCompletionClient, mode: Literal['record', 'replay'], session_file_path: str, logger: PageLogger | None = None)[source]# Bases: ChatCompletionClient A chat completion client that supports fast, large-scale tests of code calling LLM clients. Two modes are supported: “record”: delegates to the underlying client while also recording the input messages and responses, which are saved to disk when finalize() is called. “replay”: loads previously recorded message and responses from disk, then on each call checks that its message matches the recorded message, and returns the recorded response. The recorded data is stored as a JSON list of records. Each record is a dictionary with a “mode” field (either “create” or “create_stream”), a serialized list of messages, and either a “response” (for create calls) or a “stream” (a list of streamed outputs for create_stream calls). ReplayChatCompletionClient and ChatCompletionCache do similar things, but with significant differences: ReplayChatCompletionClient replays pre-defined responses in a specified order without recording anything or checking the messages sent to the client. ChatCompletionCache caches responses and replays them for messages that have been seen before, regardless of order, and calls the base client for any uncached messages. async create(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None, tool_choice: Tool | Literal['auto', 'required', 'none'] = 'auto') → CreateResult[source]# Creates a single response from the model. Parameters: messages (Sequence[LLMMessage]) – The messages to send to the model. tools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to []. tool_choice (Tool | Literal[\"auto\", \"required\", \"none\"], optional) – A single Tool object to force the model to use, “auto” to let the model choose any available tool, “required” to force tool usage, or “none” to disable tool usage. Defaults to “auto”. json_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither. Defaults to None. If set to a Pydantic BaseModel type, it will be used as the output type for structured output. If set to a boolean, it will be used to determine whether to use JSON mode or not. If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt. extra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}. cancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None. Returns: CreateResult – The result of the model call. create_stream(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None, tool_choice: Tool | Literal['auto', 'required', 'none'] = 'auto') → AsyncGenerator[str | CreateResult, None][source]# Creates a stream of string chunks from the model ending with a CreateResult. Parameters: messages (Sequence[LLMMessage]) – The messages to send to the model. tools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to []. tool_choice (Tool | Literal[\"auto\", \"required\", \"none\"], optional) – A single Tool object to force the model to use, “auto” to let the model choose any available tool, “required” to force tool usage, or “none” to disable tool usage. Defaults to “auto”. json_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither. Defaults to None. If set to a Pydantic BaseModel type, it will be used as the output type for structured output. If set to a boolean, it will be used to determine whether to use JSON mode or not. If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt. extra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}. cancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None. Returns: AsyncGenerator[Union[str, CreateResult], None] – A generator that yields string chunks and ends with a CreateResult. async close() → None[source]# actual_usage() → RequestUsage[source]# total_usage() → RequestUsage[source]# count_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]# remaining_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]# property capabilities: ModelCapabilities# property model_info: ModelInfo# finalize() → None[source]# In record mode, saves the accumulated records to disk. In replay mode, makes sure all the records were checked. class Grader(client: ChatCompletionClient, logger: PageLogger | None = None)[source]# Bases: object Runs basic tests, and determines task success without limitation to string matches. Parameters: client – The client to call the model. logger – An optional logger. If None, no logging will be performed. async test_apprentice(apprentice: Apprentice, task_description: str, expected_answer: str, num_trials: int, use_memory: bool, client: ChatCompletionClient) → Tuple[int, int][source]# async call_model(summary: str, user_content: str | List[str | Image], system_message_content: str | None = None, keep_these_messages: bool = True) → str[source]# Calls the model client with the given input and returns the response. async is_response_correct(task_description: str, response_to_be_graded: str, correct_answer: str) → Tuple[bool, str][source]# Determines whether the response is equivalent to the task’s correct answer. class PageLogger(config: PageLoggerConfig | None = None)[source]# Bases: object Logs text and images to a set of HTML pages, one per function/method, linked to each other in a call tree. Parameters: config – An optional dict that can be used to override the following values: level: The logging level, one of DEBUG, INFO, WARNING, ERROR, CRITICAL, or NONE. path: The path to the directory where the log files will be written. finalize() → None[source]# debug(line: str) → None[source]# Adds DEBUG text to the current page if debugging level <= DEBUG. info(line: str) → None[source]# Adds INFO text to the current page if debugging level <= INFO. warning(line: str) → None[source]# Adds WARNING text to the current page if debugging level <= WARNING. error(line: str) → None[source]# Adds ERROR text to the current page if debugging level <= ERROR. critical(line: str) → None[source]# Adds CRITICAL text to the current page if debugging level <= CRITICAL. log_message_content(message_content: str | List[str | Image] | List[FunctionCall] | List[FunctionExecutionResult], summary: str) → None[source]# Adds a page containing the message’s content, including any images. log_dict_list(content: List[Mapping[str, Any]], summary: str) → None[source]# Adds a page containing a list of dicts. log_model_call(summary: str, input_messages: List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], response: CreateResult) → Page | None[source]# Logs messages sent to a model and the TaskResult response to a new page. log_model_task(summary: str, input_messages: List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], task_result: TaskResult) → Page | None[source]# Logs messages sent to a model and the TaskResult response to a new page. log_link_to_local_file(file_path: str) → str[source]# Returns a link to a local file in the log. add_link_to_image(description: str, source_image_path: str) → None[source]# Inserts a thumbnail link to an image to the page. flush(finished: bool = False) → None[source]# Writes the current state of the log to disk. enter_function() → Page | None[source]# Adds a new page corresponding to the current function call. leave_function() → None[source]# Finishes the page corresponding to the current function call. class Teachability(memory_controller: MemoryController, name: str | None = None)[source]# Bases: Memory Gives an AssistantAgent the ability to learn quickly from user teachings, hints, and advice. Steps for usage: Instantiate MemoryController. Instantiate Teachability, passing the memory controller as a parameter. Instantiate an AssistantAgent, passing the teachability instance (wrapped in a list) as the memory parameter. Use the AssistantAgent as usual, such as for chatting with the user. property name: str# Get the memory instance identifier. async update_context(model_context: ChatCompletionContext) → UpdateContextResult[source]# Extracts any advice from the last user turn to be stored in memory, and adds any relevant memories to the model context. async add(content: MemoryContent, cancellation_token: CancellationToken | None = None) → None[source]# Tries to extract any advice from the passed content and add it to memory. async query(query: str | MemoryContent, cancellation_token: CancellationToken | None = None, **kwargs: Any) → MemoryQueryResult[source]# Returns any memories that seem relevant to the query. async clear() → None[source]# Clear all entries from memory. async close() → None[source]# Clean up memory resources. class ApprenticeConfig[source]# Bases: TypedDict name_of_agent_or_team: str# disable_prefix_caching: bool# MemoryController: MemoryControllerConfig# class PageLoggerConfig[source]# Bases: TypedDict level: str# path: str# previous autogen_ext.agents.web_surfer.playwright_controller next autogen_ext.models.anthropic.config On this page Apprentice Apprentice.reset_memory() Apprentice.handle_user_message() Apprentice.add_task_solution_pair_to_memory() Apprentice.assign_task() Apprentice.train_on_task() Apprentice.assign_task_to_agent_or_team() ChatCompletionClientRecorder ChatCompletionClientRecorder.create() ChatCompletionClientRecorder.create_stream() ChatCompletionClientRecorder.close() ChatCompletionClientRecorder.actual_usage() ChatCompletionClientRecorder.total_usage() ChatCompletionClientRecorder.count_tokens() ChatCompletionClientRecorder.remaining_tokens() ChatCompletionClientRecorder.capabilities ChatCompletionClientRecorder.model_info ChatCompletionClientRecorder.finalize() Grader Grader.test_apprentice() Grader.call_model() Grader.is_response_correct() PageLogger PageLogger.finalize() PageLogger.debug() PageLogger.info() PageLogger.warning() PageLogger.error() PageLogger.critical() PageLogger.log_message_content() PageLogger.log_dict_list() PageLogger.log_model_call() PageLogger.log_model_task() PageLogger.log_link_to_local_file() PageLogger.add_link_to_image() PageLogger.flush() PageLogger.enter_function() PageLogger.leave_function() Teachability Teachability.name Teachability.update_context() Teachability.add() Teachability.query() Teachability.clear() Teachability.close() ApprenticeConfig ApprenticeConfig.name_of_agent_or_team ApprenticeConfig.disable_prefix_caching ApprenticeConfig.MemoryController PageLoggerConfig PageLoggerConfig.level PageLoggerConfig.path Edit on GitHub Show Source",
      "code": "object"
    },
    {
      "description": "API Reference autogen_ext.experimental.task_centric_memory.utils autogen_ext.experimental.task_centric_memory.utils# class Apprentice(client: ChatCompletionClient, config: ApprenticeConfig | None = None, logger: PageLogger | None = None)[source]# Bases: object A minimal wrapper combining task-centric memory with an agent or team. Applications may use the Apprentice class, or they may directly instantiate and call the Memory Controller using this class as an example. Parameters: client – The client to call the model. config – An optional dict that can be used to override the following values: name_of_agent_or_team: The name of the target agent or team for assigning tasks to. disable_prefix_caching: True to disable prefix caching by prepending random ints to the first message. MemoryController: A config dict passed to MemoryController. logger – An optional logger. If None, a default logger will be created. reset_memory() → None[source]# Resets the memory bank. async handle_user_message(text: str, should_await: bool = True) → str[source]# Handles a user message, extracting any advice and assigning a task to the agent. async add_task_solution_pair_to_memory(task: str, solution: str) → None[source]# Adds a task-solution pair to the memory bank, to be retrieved together later as a combined insight. This is useful when the insight is a demonstration of how to solve a given type of task. async assign_task(task: str, use_memory: bool = True, should_await: bool = True) → str[source]# Assigns a task to the agent, along with any relevant insights/memories. async train_on_task(task: str, expected_answer: str) → None[source]# Repeatedly assigns a task to the completion agent, and tries to learn from failures by creating useful insights as memories. async assign_task_to_agent_or_team(task: str) → Tuple[str, str][source]# Passes the given task to the target agent or team. class ChatCompletionClientRecorder(client: ChatCompletionClient, mode: Literal['record', 'replay'], session_file_path: str, logger: PageLogger | None = None)[source]# Bases: ChatCompletionClient A chat completion client that supports fast, large-scale tests of code calling LLM clients. Two modes are supported: “record”: delegates to the underlying client while also recording the input messages and responses, which are saved to disk when finalize() is called. “replay”: loads previously recorded message and responses from disk, then on each call checks that its message matches the recorded message, and returns the recorded response. The recorded data is stored as a JSON list of records. Each record is a dictionary with a “mode” field (either “create” or “create_stream”), a serialized list of messages, and either a “response” (for create calls) or a “stream” (a list of streamed outputs for create_stream calls). ReplayChatCompletionClient and ChatCompletionCache do similar things, but with significant differences: ReplayChatCompletionClient replays pre-defined responses in a specified order without recording anything or checking the messages sent to the client. ChatCompletionCache caches responses and replays them for messages that have been seen before, regardless of order, and calls the base client for any uncached messages. async create(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None, tool_choice: Tool | Literal['auto', 'required', 'none'] = 'auto') → CreateResult[source]# Creates a single response from the model. Parameters: messages (Sequence[LLMMessage]) – The messages to send to the model. tools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to []. tool_choice (Tool | Literal[\"auto\", \"required\", \"none\"], optional) – A single Tool object to force the model to use, “auto” to let the model choose any available tool, “required” to force tool usage, or “none” to disable tool usage. Defaults to “auto”. json_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither. Defaults to None. If set to a Pydantic BaseModel type, it will be used as the output type for structured output. If set to a boolean, it will be used to determine whether to use JSON mode or not. If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt. extra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}. cancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None. Returns: CreateResult – The result of the model call. create_stream(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None, tool_choice: Tool | Literal['auto', 'required', 'none'] = 'auto') → AsyncGenerator[str | CreateResult, None][source]# Creates a stream of string chunks from the model ending with a CreateResult. Parameters: messages (Sequence[LLMMessage]) – The messages to send to the model. tools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to []. tool_choice (Tool | Literal[\"auto\", \"required\", \"none\"], optional) – A single Tool object to force the model to use, “auto” to let the model choose any available tool, “required” to force tool usage, or “none” to disable tool usage. Defaults to “auto”. json_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither. Defaults to None. If set to a Pydantic BaseModel type, it will be used as the output type for structured output. If set to a boolean, it will be used to determine whether to use JSON mode or not. If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt. extra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}. cancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None. Returns: AsyncGenerator[Union[str, CreateResult], None] – A generator that yields string chunks and ends with a CreateResult. async close() → None[source]# actual_usage() → RequestUsage[source]# total_usage() → RequestUsage[source]# count_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]# remaining_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]# property capabilities: ModelCapabilities# property model_info: ModelInfo# finalize() → None[source]# In record mode, saves the accumulated records to disk. In replay mode, makes sure all the records were checked. class Grader(client: ChatCompletionClient, logger: PageLogger | None = None)[source]# Bases: object Runs basic tests, and determines task success without limitation to string matches. Parameters: client – The client to call the model. logger – An optional logger. If None, no logging will be performed. async test_apprentice(apprentice: Apprentice, task_description: str, expected_answer: str, num_trials: int, use_memory: bool, client: ChatCompletionClient) → Tuple[int, int][source]# async call_model(summary: str, user_content: str | List[str | Image], system_message_content: str | None = None, keep_these_messages: bool = True) → str[source]# Calls the model client with the given input and returns the response. async is_response_correct(task_description: str, response_to_be_graded: str, correct_answer: str) → Tuple[bool, str][source]# Determines whether the response is equivalent to the task’s correct answer. class PageLogger(config: PageLoggerConfig | None = None)[source]# Bases: object Logs text and images to a set of HTML pages, one per function/method, linked to each other in a call tree. Parameters: config – An optional dict that can be used to override the following values: level: The logging level, one of DEBUG, INFO, WARNING, ERROR, CRITICAL, or NONE. path: The path to the directory where the log files will be written. finalize() → None[source]# debug(line: str) → None[source]# Adds DEBUG text to the current page if debugging level <= DEBUG. info(line: str) → None[source]# Adds INFO text to the current page if debugging level <= INFO. warning(line: str) → None[source]# Adds WARNING text to the current page if debugging level <= WARNING. error(line: str) → None[source]# Adds ERROR text to the current page if debugging level <= ERROR. critical(line: str) → None[source]# Adds CRITICAL text to the current page if debugging level <= CRITICAL. log_message_content(message_content: str | List[str | Image] | List[FunctionCall] | List[FunctionExecutionResult], summary: str) → None[source]# Adds a page containing the message’s content, including any images. log_dict_list(content: List[Mapping[str, Any]], summary: str) → None[source]# Adds a page containing a list of dicts. log_model_call(summary: str, input_messages: List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], response: CreateResult) → Page | None[source]# Logs messages sent to a model and the TaskResult response to a new page. log_model_task(summary: str, input_messages: List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], task_result: TaskResult) → Page | None[source]# Logs messages sent to a model and the TaskResult response to a new page. log_link_to_local_file(file_path: str) → str[source]# Returns a link to a local file in the log. add_link_to_image(description: str, source_image_path: str) → None[source]# Inserts a thumbnail link to an image to the page. flush(finished: bool = False) → None[source]# Writes the current state of the log to disk. enter_function() → Page | None[source]# Adds a new page corresponding to the current function call. leave_function() → None[source]# Finishes the page corresponding to the current function call. class Teachability(memory_controller: MemoryController, name: str | None = None)[source]# Bases: Memory Gives an AssistantAgent the ability to learn quickly from user teachings, hints, and advice. Steps for usage: Instantiate MemoryController. Instantiate Teachability, passing the memory controller as a parameter. Instantiate an AssistantAgent, passing the teachability instance (wrapped in a list) as the memory parameter. Use the AssistantAgent as usual, such as for chatting with the user. property name: str# Get the memory instance identifier. async update_context(model_context: ChatCompletionContext) → UpdateContextResult[source]# Extracts any advice from the last user turn to be stored in memory, and adds any relevant memories to the model context. async add(content: MemoryContent, cancellation_token: CancellationToken | None = None) → None[source]# Tries to extract any advice from the passed content and add it to memory. async query(query: str | MemoryContent, cancellation_token: CancellationToken | None = None, **kwargs: Any) → MemoryQueryResult[source]# Returns any memories that seem relevant to the query. async clear() → None[source]# Clear all entries from memory. async close() → None[source]# Clean up memory resources. class ApprenticeConfig[source]# Bases: TypedDict name_of_agent_or_team: str# disable_prefix_caching: bool# MemoryController: MemoryControllerConfig# class PageLoggerConfig[source]# Bases: TypedDict level: str# path: str# previous autogen_ext.agents.web_surfer.playwright_controller next autogen_ext.models.anthropic.config",
      "code": "object"
    },
    {
      "description": "Steps for usage:",
      "code": "TypedDict"
    }
  ],
  "links": [
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.experimental.task_centric_memory.utils.html",
    "https://microsoft.github.io/autogen/stable/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/extensions-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/reference/index.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.agents.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.base.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.conditions.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.messages.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.state.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.teams.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.tools.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.ui.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.utils.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.code_executor.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.exceptions.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.logging.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.memory.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.model_context.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.models.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.tool_agent.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.tools.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.utils.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.cache_store.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.experimental.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.memory.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.teams.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.ui.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.azure.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.file_surfer.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.magentic_one.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.openai.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.video_surfer.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.web_surfer.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.auth.azure.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.cache_store.diskcache.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.cache_store.redis.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.azure.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.docker.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.docker_jupyter.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.jupyter.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.local.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.experimental.task_centric_memory.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.memory.canvas.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.memory.chromadb.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.memory.mem0.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.memory.redis.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.anthropic.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.azure.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.cache.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.llama_cpp.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.ollama.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.openai.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.replay.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.semantic_kernel.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.teams.magentic_one.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.azure.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.code_execution.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.graphrag.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.http.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.langchain.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.mcp.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.semantic_kernel.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.video_surfer.tools.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.web_surfer.playwright_controller.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.anthropic.config.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.azure.config.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.ollama.config.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.openai.config.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.protos.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.protos.agent_worker_pb2.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.protos.agent_worker_pb2_grpc.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.protos.cloudevent_pb2.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.protos.cloudevent_pb2_grpc.html"
  ]
}