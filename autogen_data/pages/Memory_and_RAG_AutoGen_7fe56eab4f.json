{
  "url": "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/memory.html",
  "title": "Memory and RAG — AutoGen",
  "content": "There are several use cases where it is valuable to maintain a store of useful facts that can be intelligently added to the context of the agent just before a specific step. The typically use case here is a RAG pattern where a query is used to retrieve relevant information from a database that is then added to the agent’s context.\n\nAgentChat provides a Memory protocol that can be extended to provide this functionality. The key methods are query, update_context, add, clear, and close.\n\nadd: add new entries to the memory store\n\nquery: retrieve relevant information from the memory store\n\nupdate_context: mutate an agent’s internal model_context by adding the retrieved information (used in the AssistantAgent class)\n\nclear: clear all entries from the memory store\n\nclose: clean up any resources used by the memory store\n\nListMemory is provided as an example implementation of the Memory protocol. It is a simple list-based memory implementation that maintains memories in chronological order, appending the most recent memories to the model’s context. The implementation is designed to be straightforward and predictable, making it easy to understand and debug. In the following example, we will use ListMemory to maintain a memory bank of user preferences and demonstrate how it can be used to provide consistent context for agent responses over time.\n\nWe can inspect that the assistant_agent model_context is actually updated with the retrieved memory entries. The transform method is used to format the retrieved memory entries into a string that can be used by the agent. In this case, we simply concatenate the content of each memory entry into a single string.\n\nWe see above that the weather is returned in Centigrade as stated in the user preferences.\n\nSimilarly, assuming we ask a separate question about generating a meal plan, the agent is able to retrieve relevant information from the memory store and provide a personalized (vegan) response.\n\nYou can build on the Memory protocol to implement more complex memory stores. For example, you could implement a custom memory store that uses a vector database to store and retrieve information, or a memory store that uses a machine learning model to generate personalized responses based on the user’s preferences etc.\n\nSpecifically, you will need to overload the add, query and update_context methods to implement the desired functionality and pass the memory store to your agent.\n\nCurrently the following example memory stores are available as part of the autogen_ext extensions package.\n\nautogen_ext.memory.chromadb.ChromaDBVectorMemory: A memory store that uses a vector database to store and retrieve information.\n\nautogen_ext.memory.chromadb.SentenceTransformerEmbeddingFunctionConfig: A configuration class for the SentenceTransformer embedding function used by the ChromaDBVectorMemory store. Note that other embedding functions such as autogen_ext.memory.openai.OpenAIEmbeddingFunctionConfig can also be used with the ChromaDBVectorMemory store.\n\nautogen_ext.memory.redis.RedisMemory: A memory store that uses a Redis vector database to store and retrieve information.\n\nNote that you can also serialize the ChromaDBVectorMemory and save it to disk.\n\nYou can perform the same persistent memory storage using Redis. Note, you will need to have a running Redis instance to connect to.\n\nSee RedisMemory for instructions to run Redis locally or via Docker.\n\nThe RAG (Retrieval Augmented Generation) pattern which is common in building AI systems encompasses two distinct phases:\n\nIndexing: Loading documents, chunking them, and storing them in a vector database\n\nRetrieval: Finding and using relevant chunks during conversation runtime\n\nIn our previous examples, we manually added items to memory and passed them to our agents. In practice, the indexing process is usually automated and based on much larger document sources like product documentation, internal files, or knowledge bases.\n\nNote: The quality of a RAG system is dependent on the quality of the chunking and retrieval process (models, embeddings, etc.). You may need to experiement with more advanced chunking and retrieval models to get the best results.\n\nTo begin, let’s create a simple document indexer that we will used to load documents, chunk them, and store them in a ChromaDBVectorMemory memory store.\n\nNow let’s use our indexer with ChromaDBVectorMemory to build a complete RAG agent:\n\nThis implementation provides a RAG agent that can answer questions based on AutoGen documentation. When a question is asked, the Memory system retrieves relevant chunks and adds them to the context, enabling the assistant to generate informed responses.\n\nFor production systems, you might want to:\n\nImplement more sophisticated chunking strategies\n\nAdd metadata filtering capabilities\n\nCustomize the retrieval scoring\n\nOptimize embedding models for your specific domain\n\nautogen_ext.memory.mem0.Mem0Memory provides integration with Mem0.ai’s memory system. It supports both cloud-based and local backends, offering advanced memory capabilities for agents. The implementation handles proper retrieval and context updating, making it suitable for production environments.\n\nIn the following example, we’ll demonstrate how to use Mem0Memory to maintain persistent memories across conversations:\n\nThe example above demonstrates how Mem0Memory can be used with an assistant agent. The memory integration ensures that:\n\nAll agent interactions are stored in Mem0 for future reference\n\nRelevant memories (like user preferences) are automatically retrieved and added to the context\n\nThe agent can maintain consistent behavior based on stored memories\n\nMem0Memory is particularly useful for:\n\nLong-running agent deployments that need persistent memory\n\nApplications requiring enhanced privacy controls\n\nTeams wanting unified memory management across agents\n\nUse cases needing advanced memory filtering and analytics\n\nJust like ChromaDBVectorMemory, you can serialize Mem0Memory configurations:\n\nGraphFlow (Workflows)",
  "headings": [
    {
      "level": "h1",
      "text": "Memory and RAG#",
      "id": ""
    },
    {
      "level": "h1",
      "text": "ListMemory Example#",
      "id": ""
    },
    {
      "level": "h1",
      "text": "Custom Memory Stores (Vector DBs, etc.)#",
      "id": ""
    },
    {
      "level": "h2",
      "text": "Redis Memory#",
      "id": ""
    },
    {
      "level": "h1",
      "text": "RAG Agent: Putting It All Together#",
      "id": ""
    },
    {
      "level": "h2",
      "text": "Building a Simple RAG Agent#",
      "id": ""
    },
    {
      "level": "h1",
      "text": "Mem0Memory Example#",
      "id": ""
    }
  ],
  "code_samples": [
    {
      "code": "from autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.ui import Console\nfrom autogen_core.memory import ListMemory, MemoryContent, MemoryMimeType\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient",
      "language": "sql"
    },
    {
      "code": "from autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.ui import Console\nfrom autogen_core.memory import ListMemory, MemoryContent, MemoryMimeType\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient",
      "language": "sql"
    },
    {
      "code": "# Initialize user memory\nuser_memory = ListMemory()\n\n# Add user preferences to memory\nawait user_memory.add(MemoryContent(content=\"The weather should be in metric units\", mime_type=MemoryMimeType.TEXT))\n\nawait user_memory.add(MemoryContent(content=\"Meal recipe must be vegan\", mime_type=MemoryMimeType.TEXT))\n\n\nasync def get_weather(city: str, units: str = \"imperial\") -> str:\n    if units == \"imperial\":\n        return f\"The weather in {city} is 73 °F and Sunny.\"\n    elif units == \"metric\":\n        return f\"The weather in {city} is 23 °C and Sunny.\"\n    else:\n        return f\"Sorry, I don't know the weather in {city}.\"\n\n\nassistant_agent = AssistantAgent(\n    name=\"assistant_agent\",\n    model_client=OpenAIChatCompletionClient(\n        model=\"gpt-4o-2024-08-06\",\n    ),\n    tools=[get_weather],\n    memory=[user_memory],\n)",
      "language": "python"
    },
    {
      "code": "# Initialize user memory\nuser_memory = ListMemory()\n\n# Add user preferences to memory\nawait user_memory.add(MemoryContent(content=\"The weather should be in metric units\", mime_type=MemoryMimeType.TEXT))\n\nawait user_memory.add(MemoryContent(content=\"Meal recipe must be vegan\", mime_type=MemoryMimeType.TEXT))\n\n\nasync def get_weather(city: str, units: str = \"imperial\") -> str:\n    if units == \"imperial\":\n        return f\"The weather in {city} is 73 °F and Sunny.\"\n    elif units == \"metric\":\n        return f\"The weather in {city} is 23 °C and Sunny.\"\n    else:\n        return f\"Sorry, I don't know the weather in {city}.\"\n\n\nassistant_agent = AssistantAgent(\n    name=\"assistant_agent\",\n    model_client=OpenAIChatCompletionClient(\n        model=\"gpt-4o-2024-08-06\",\n    ),\n    tools=[get_weather],\n    memory=[user_memory],\n)",
      "language": "python"
    },
    {
      "code": "# Run the agent with a task.\nstream = assistant_agent.run_stream(task=\"What is the weather in New York?\")\nawait Console(stream)",
      "language": "markdown"
    },
    {
      "code": "# Run the agent with a task.\nstream = assistant_agent.run_stream(task=\"What is the weather in New York?\")\nawait Console(stream)",
      "language": "markdown"
    },
    {
      "code": "---------- TextMessage (user) ----------\nWhat is the weather in New York?\n---------- MemoryQueryEvent (assistant_agent) ----------\n[MemoryContent(content='The weather should be in metric units', mime_type=<MemoryMimeType.TEXT: 'text/plain'>, metadata=None), MemoryContent(content='Meal recipe must be vegan', mime_type=<MemoryMimeType.TEXT: 'text/plain'>, metadata=None)]\n---------- ToolCallRequestEvent (assistant_agent) ----------\n[FunctionCall(id='call_33uMqZO6hwOfEpJavP9GW9LI', arguments='{\"city\":\"New York\",\"units\":\"metric\"}', name='get_weather')]\n---------- ToolCallExecutionEvent (assistant_agent) ----------\n[FunctionExecutionResult(content='The weather in New York is 23 °C and Sunny.', name='get_weather', call_id='call_33uMqZO6hwOfEpJavP9GW9LI', is_error=False)]\n---------- ToolCallSummaryMessage (assistant_agent) ----------\nThe weather in New York is 23 °C and Sunny.",
      "language": "json"
    },
    {
      "code": "---------- TextMessage (user) ----------\nWhat is the weather in New York?\n---------- MemoryQueryEvent (assistant_agent) ----------\n[MemoryContent(content='The weather should be in metric units', mime_type=<MemoryMimeType.TEXT: 'text/plain'>, metadata=None), MemoryContent(content='Meal recipe must be vegan', mime_type=<MemoryMimeType.TEXT: 'text/plain'>, metadata=None)]\n---------- ToolCallRequestEvent (assistant_agent) ----------\n[FunctionCall(id='call_33uMqZO6hwOfEpJavP9GW9LI', arguments='{\"city\":\"New York\",\"units\":\"metric\"}', name='get_weather')]\n---------- ToolCallExecutionEvent (assistant_agent) ----------\n[FunctionExecutionResult(content='The weather in New York is 23 °C and Sunny.', name='get_weather', call_id='call_33uMqZO6hwOfEpJavP9GW9LI', is_error=False)]\n---------- ToolCallSummaryMessage (assistant_agent) ----------\nThe weather in New York is 23 °C and Sunny.",
      "language": "json"
    },
    {
      "code": "TaskResult(messages=[TextMessage(source='user', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 7, 1, 23, 53, 8, 867845, tzinfo=datetime.timezone.utc), content='What is the weather in New York?', type='TextMessage'), MemoryQueryEvent(source='assistant_agent', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 7, 1, 23, 53, 8, 869589, tzinfo=datetime.timezone.utc), content=[MemoryContent(content='The weather should be in metric units', mime_type=<MemoryMimeType.TEXT: 'text/plain'>, metadata=None), MemoryContent(content='Meal recipe must be vegan', mime_type=<MemoryMimeType.TEXT: 'text/plain'>, metadata=None)], type='MemoryQueryEvent'), ToolCallRequestEvent(source='assistant_agent', models_usage=RequestUsage(prompt_tokens=123, completion_tokens=19), metadata={}, created_at=datetime.datetime(2025, 7, 1, 23, 53, 10, 240626, tzinfo=datetime.timezone.utc), content=[FunctionCall(id='call_33uMqZO6hwOfEpJavP9GW9LI', arguments='{\"city\":\"New York\",\"units\":\"metric\"}', name='get_weather')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='assistant_agent', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 7, 1, 23, 53, 10, 242633, tzinfo=datetime.timezone.utc), content=[FunctionExecutionResult(content='The weather in New York is 23 °C and Sunny.', name='get_weather', call_id='call_33uMqZO6hwOfEpJavP9GW9LI', is_error=False)], type='ToolCallExecutionEvent'), ToolCallSummaryMessage(source='assistant_agent', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 7, 1, 23, 53, 10, 243722, tzinfo=datetime.timezone.utc), content='The weather in New York is 23 °C and Sunny.', type='ToolCallSummaryMessage')], stop_reason=None)",
      "language": "json"
    },
    {
      "code": "TaskResult(messages=[TextMessage(source='user', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 7, 1, 23, 53, 8, 867845, tzinfo=datetime.timezone.utc), content='What is the weather in New York?', type='TextMessage'), MemoryQueryEvent(source='assistant_agent', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 7, 1, 23, 53, 8, 869589, tzinfo=datetime.timezone.utc), content=[MemoryContent(content='The weather should be in metric units', mime_type=<MemoryMimeType.TEXT: 'text/plain'>, metadata=None), MemoryContent(content='Meal recipe must be vegan', mime_type=<MemoryMimeType.TEXT: 'text/plain'>, metadata=None)], type='MemoryQueryEvent'), ToolCallRequestEvent(source='assistant_agent', models_usage=RequestUsage(prompt_tokens=123, completion_tokens=19), metadata={}, created_at=datetime.datetime(2025, 7, 1, 23, 53, 10, 240626, tzinfo=datetime.timezone.utc), content=[FunctionCall(id='call_33uMqZO6hwOfEpJavP9GW9LI', arguments='{\"city\":\"New York\",\"units\":\"metric\"}', name='get_weather')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='assistant_agent', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 7, 1, 23, 53, 10, 242633, tzinfo=datetime.timezone.utc), content=[FunctionExecutionResult(content='The weather in New York is 23 °C and Sunny.', name='get_weather', call_id='call_33uMqZO6hwOfEpJavP9GW9LI', is_error=False)], type='ToolCallExecutionEvent'), ToolCallSummaryMessage(source='assistant_agent', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 7, 1, 23, 53, 10, 243722, tzinfo=datetime.timezone.utc), content='The weather in New York is 23 °C and Sunny.', type='ToolCallSummaryMessage')], stop_reason=None)",
      "language": "json"
    },
    {
      "code": "await assistant_agent._model_context.get_messages()",
      "language": "csharp"
    },
    {
      "code": "await assistant_agent._model_context.get_messages()",
      "language": "csharp"
    },
    {
      "code": "[UserMessage(content='What is the weather in New York?', source='user', type='UserMessage'),\n SystemMessage(content='\\nRelevant memory content (in chronological order):\\n1. The weather should be in metric units\\n2. Meal recipe must be vegan\\n', type='SystemMessage'),\n AssistantMessage(content=[FunctionCall(id='call_33uMqZO6hwOfEpJavP9GW9LI', arguments='{\"city\":\"New York\",\"units\":\"metric\"}', name='get_weather')], thought=None, source='assistant_agent', type='AssistantMessage'),\n FunctionExecutionResultMessage(content=[FunctionExecutionResult(content='The weather in New York is 23 °C and Sunny.', name='get_weather', call_id='call_33uMqZO6hwOfEpJavP9GW9LI', is_error=False)], type='FunctionExecutionResultMessage')]",
      "language": "json"
    },
    {
      "code": "[UserMessage(content='What is the weather in New York?', source='user', type='UserMessage'),\n SystemMessage(content='\\nRelevant memory content (in chronological order):\\n1. The weather should be in metric units\\n2. Meal recipe must be vegan\\n', type='SystemMessage'),\n AssistantMessage(content=[FunctionCall(id='call_33uMqZO6hwOfEpJavP9GW9LI', arguments='{\"city\":\"New York\",\"units\":\"metric\"}', name='get_weather')], thought=None, source='assistant_agent', type='AssistantMessage'),\n FunctionExecutionResultMessage(content=[FunctionExecutionResult(content='The weather in New York is 23 °C and Sunny.', name='get_weather', call_id='call_33uMqZO6hwOfEpJavP9GW9LI', is_error=False)], type='FunctionExecutionResultMessage')]",
      "language": "json"
    },
    {
      "code": "stream = assistant_agent.run_stream(task=\"Write brief meal recipe with broth\")\nawait Console(stream)",
      "language": "csharp"
    },
    {
      "code": "stream = assistant_agent.run_stream(task=\"Write brief meal recipe with broth\")\nawait Console(stream)",
      "language": "csharp"
    },
    {
      "code": "---------- TextMessage (user) ----------\nWrite brief meal recipe with broth\n---------- MemoryQueryEvent (assistant_agent) ----------\n[MemoryContent(content='The weather should be in metric units', mime_type=<MemoryMimeType.TEXT: 'text/plain'>, metadata=None), MemoryContent(content='Meal recipe must be vegan', mime_type=<MemoryMimeType.TEXT: 'text/plain'>, metadata=None)]\n---------- TextMessage (assistant_agent) ----------\nHere's a brief vegan meal recipe using broth:\n\n**Vegan Vegetable Broth Soup**\n\n**Ingredients:**\n- 1 tablespoon olive oil\n- 1 onion, chopped\n- 3 cloves garlic, minced\n- 2 carrots, sliced\n- 2 celery stalks, sliced\n- 1 zucchini, chopped\n- 1 cup mushrooms, sliced\n- 1 cup kale or spinach, chopped\n- 1 can (400g) diced tomatoes\n- 4 cups vegetable broth\n- 1 teaspoon dried thyme\n- Salt and pepper to taste\n- Fresh parsley, chopped (for garnish)\n\n**Instructions:**\n1. Heat olive oil in a large pot over medium heat. Add the onion and garlic, and sauté until soft.\n2. Add the carrots, celery, zucchini, and mushrooms. Cook for about 5 minutes until the vegetables begin to soften.\n3. Add the diced tomatoes, vegetable broth, and dried thyme. Bring to a boil.\n4. Reduce heat and let it simmer for about 20 minutes, or until the vegetables are tender.\n5. Stir in the chopped kale or spinach and cook for another 5 minutes.\n6. Season with salt and pepper to taste.\n7. Serve hot, garnished with fresh parsley.\n\nEnjoy your comforting vegan vegetable broth soup!",
      "language": "julia"
    },
    {
      "code": "---------- TextMessage (user) ----------\nWrite brief meal recipe with broth\n---------- MemoryQueryEvent (assistant_agent) ----------\n[MemoryContent(content='The weather should be in metric units', mime_type=<MemoryMimeType.TEXT: 'text/plain'>, metadata=None), MemoryContent(content='Meal recipe must be vegan', mime_type=<MemoryMimeType.TEXT: 'text/plain'>, metadata=None)]\n---------- TextMessage (assistant_agent) ----------\nHere's a brief vegan meal recipe using broth:\n\n**Vegan Vegetable Broth Soup**\n\n**Ingredients:**\n- 1 tablespoon olive oil\n- 1 onion, chopped\n- 3 cloves garlic, minced\n- 2 carrots, sliced\n- 2 celery stalks, sliced\n- 1 zucchini, chopped\n- 1 cup mushrooms, sliced\n- 1 cup kale or spinach, chopped\n- 1 can (400g) diced tomatoes\n- 4 cups vegetable broth\n- 1 teaspoon dried thyme\n- Salt and pepper to taste\n- Fresh parsley, chopped (for garnish)\n\n**Instructions:**\n1. Heat olive oil in a large pot over medium heat. Add the onion and garlic, and sauté until soft.\n2. Add the carrots, celery, zucchini, and mushrooms. Cook for about 5 minutes until the vegetables begin to soften.\n3. Add the diced tomatoes, vegetable broth, and dried thyme. Bring to a boil.\n4. Reduce heat and let it simmer for about 20 minutes, or until the vegetables are tender.\n5. Stir in the chopped kale or spinach and cook for another 5 minutes.\n6. Season with salt and pepper to taste.\n7. Serve hot, garnished with fresh parsley.\n\nEnjoy your comforting vegan vegetable broth soup!",
      "language": "julia"
    },
    {
      "code": "TaskResult(messages=[TextMessage(source='user', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 7, 1, 23, 53, 10, 256897, tzinfo=datetime.timezone.utc), content='Write brief meal recipe with broth', type='TextMessage'), MemoryQueryEvent(source='assistant_agent', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 7, 1, 23, 53, 10, 258468, tzinfo=datetime.timezone.utc), content=[MemoryContent(content='The weather should be in metric units', mime_type=<MemoryMimeType.TEXT: 'text/plain'>, metadata=None), MemoryContent(content='Meal recipe must be vegan', mime_type=<MemoryMimeType.TEXT: 'text/plain'>, metadata=None)], type='MemoryQueryEvent'), TextMessage(source='assistant_agent', models_usage=RequestUsage(prompt_tokens=205, completion_tokens=266), metadata={}, created_at=datetime.datetime(2025, 7, 1, 23, 53, 14, 67151, tzinfo=datetime.timezone.utc), content=\"Here's a brief vegan meal recipe using broth:\\n\\n**Vegan Vegetable Broth Soup**\\n\\n**Ingredients:**\\n- 1 tablespoon olive oil\\n- 1 onion, chopped\\n- 3 cloves garlic, minced\\n- 2 carrots, sliced\\n- 2 celery stalks, sliced\\n- 1 zucchini, chopped\\n- 1 cup mushrooms, sliced\\n- 1 cup kale or spinach, chopped\\n- 1 can (400g) diced tomatoes\\n- 4 cups vegetable broth\\n- 1 teaspoon dried thyme\\n- Salt and pepper to taste\\n- Fresh parsley, chopped (for garnish)\\n\\n**Instructions:**\\n1. Heat olive oil in a large pot over medium heat. Add the onion and garlic, and sauté until soft.\\n2. Add the carrots, celery, zucchini, and mushrooms. Cook for about 5 minutes until the vegetables begin to soften.\\n3. Add the diced tomatoes, vegetable broth, and dried thyme. Bring to a boil.\\n4. Reduce heat and let it simmer for about 20 minutes, or until the vegetables are tender.\\n5. Stir in the chopped kale or spinach and cook for another 5 minutes.\\n6. Season with salt and pepper to taste.\\n7. Serve hot, garnished with fresh parsley.\\n\\nEnjoy your comforting vegan vegetable broth soup!\", type='TextMessage')], stop_reason=None)",
      "language": "julia"
    },
    {
      "code": "TaskResult(messages=[TextMessage(source='user', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 7, 1, 23, 53, 10, 256897, tzinfo=datetime.timezone.utc), content='Write brief meal recipe with broth', type='TextMessage'), MemoryQueryEvent(source='assistant_agent', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 7, 1, 23, 53, 10, 258468, tzinfo=datetime.timezone.utc), content=[MemoryContent(content='The weather should be in metric units', mime_type=<MemoryMimeType.TEXT: 'text/plain'>, metadata=None), MemoryContent(content='Meal recipe must be vegan', mime_type=<MemoryMimeType.TEXT: 'text/plain'>, metadata=None)], type='MemoryQueryEvent'), TextMessage(source='assistant_agent', models_usage=RequestUsage(prompt_tokens=205, completion_tokens=266), metadata={}, created_at=datetime.datetime(2025, 7, 1, 23, 53, 14, 67151, tzinfo=datetime.timezone.utc), content=\"Here's a brief vegan meal recipe using broth:\\n\\n**Vegan Vegetable Broth Soup**\\n\\n**Ingredients:**\\n- 1 tablespoon olive oil\\n- 1 onion, chopped\\n- 3 cloves garlic, minced\\n- 2 carrots, sliced\\n- 2 celery stalks, sliced\\n- 1 zucchini, chopped\\n- 1 cup mushrooms, sliced\\n- 1 cup kale or spinach, chopped\\n- 1 can (400g) diced tomatoes\\n- 4 cups vegetable broth\\n- 1 teaspoon dried thyme\\n- Salt and pepper to taste\\n- Fresh parsley, chopped (for garnish)\\n\\n**Instructions:**\\n1. Heat olive oil in a large pot over medium heat. Add the onion and garlic, and sauté until soft.\\n2. Add the carrots, celery, zucchini, and mushrooms. Cook for about 5 minutes until the vegetables begin to soften.\\n3. Add the diced tomatoes, vegetable broth, and dried thyme. Bring to a boil.\\n4. Reduce heat and let it simmer for about 20 minutes, or until the vegetables are tender.\\n5. Stir in the chopped kale or spinach and cook for another 5 minutes.\\n6. Season with salt and pepper to taste.\\n7. Serve hot, garnished with fresh parsley.\\n\\nEnjoy your comforting vegan vegetable broth soup!\", type='TextMessage')], stop_reason=None)",
      "language": "julia"
    },
    {
      "code": "import tempfile\n\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.ui import Console\nfrom autogen_core.memory import MemoryContent, MemoryMimeType\nfrom autogen_ext.memory.chromadb import (\n    ChromaDBVectorMemory,\n    PersistentChromaDBVectorMemoryConfig,\n    SentenceTransformerEmbeddingFunctionConfig,\n)\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n# Use a temporary directory for ChromaDB persistence\nwith tempfile.TemporaryDirectory() as tmpdir:\n    chroma_user_memory = ChromaDBVectorMemory(\n        config=PersistentChromaDBVectorMemoryConfig(\n            collection_name=\"preferences\",\n            persistence_path=tmpdir,  # Use the temp directory here\n            k=2,  # Return top k results\n            score_threshold=0.4,  # Minimum similarity score\n            embedding_function_config=SentenceTransformerEmbeddingFunctionConfig(\n                model_name=\"all-MiniLM-L6-v2\"  # Use default model for testing\n            ),\n        )\n    )\n    # Add user preferences to memory\n    await chroma_user_memory.add(\n        MemoryContent(\n            content=\"The weather should be in metric units\",\n            mime_type=MemoryMimeType.TEXT,\n            metadata={\"category\": \"preferences\", \"type\": \"units\"},\n        )\n    )\n\n    await chroma_user_memory.add(\n        MemoryContent(\n            content=\"Meal recipe must be vegan\",\n            mime_type=MemoryMimeType.TEXT,\n            metadata={\"category\": \"preferences\", \"type\": \"dietary\"},\n        )\n    )\n\n    model_client = OpenAIChatCompletionClient(\n        model=\"gpt-4o\",\n    )\n\n    # Create assistant agent with ChromaDB memory\n    assistant_agent = AssistantAgent(\n        name=\"assistant_agent\",\n        model_client=model_client,\n        tools=[get_weather],\n        memory=[chroma_user_memory],\n    )\n\n    stream = assistant_agent.run_stream(task=\"What is the weather in New York?\")\n    await Console(stream)\n\n    await model_client.close()\n    await chroma_user_memory.close()",
      "language": "typescript"
    },
    {
      "code": "import tempfile\n\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.ui import Console\nfrom autogen_core.memory import MemoryContent, MemoryMimeType\nfrom autogen_ext.memory.chromadb import (\n    ChromaDBVectorMemory,\n    PersistentChromaDBVectorMemoryConfig,\n    SentenceTransformerEmbeddingFunctionConfig,\n)\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n# Use a temporary directory for ChromaDB persistence\nwith tempfile.TemporaryDirectory() as tmpdir:\n    chroma_user_memory = ChromaDBVectorMemory(\n        config=PersistentChromaDBVectorMemoryConfig(\n            collection_name=\"preferences\",\n            persistence_path=tmpdir,  # Use the temp directory here\n            k=2,  # Return top k results\n            score_threshold=0.4,  # Minimum similarity score\n            embedding_function_config=SentenceTransformerEmbeddingFunctionConfig(\n                model_name=\"all-MiniLM-L6-v2\"  # Use default model for testing\n            ),\n        )\n    )\n    # Add user preferences to memory\n    await chroma_user_memory.add(\n        MemoryContent(\n            content=\"The weather should be in metric units\",\n            mime_type=MemoryMimeType.TEXT,\n            metadata={\"category\": \"preferences\", \"type\": \"units\"},\n        )\n    )\n\n    await chroma_user_memory.add(\n        MemoryContent(\n            content=\"Meal recipe must be vegan\",\n            mime_type=MemoryMimeType.TEXT,\n            metadata={\"category\": \"preferences\", \"type\": \"dietary\"},\n        )\n    )\n\n    model_client = OpenAIChatCompletionClient(\n        model=\"gpt-4o\",\n    )\n\n    # Create assistant agent with ChromaDB memory\n    assistant_agent = AssistantAgent(\n        name=\"assistant_agent\",\n        model_client=model_client,\n        tools=[get_weather],\n        memory=[chroma_user_memory],\n    )\n\n    stream = assistant_agent.run_stream(task=\"What is the weather in New York?\")\n    await Console(stream)\n\n    await model_client.close()\n    await chroma_user_memory.close()",
      "language": "typescript"
    },
    {
      "code": "---------- TextMessage (user) ----------\nWhat is the weather in New York?\n---------- MemoryQueryEvent (assistant_agent) ----------\n[MemoryContent(content='The weather should be in metric units', mime_type='MemoryMimeType.TEXT', metadata={'category': 'preferences', 'mime_type': 'MemoryMimeType.TEXT', 'type': 'units', 'score': 0.4342913031578064, 'id': 'b8a70e90-a39f-47ed-ab7b-5a274009d9f0'}), MemoryContent(content='The weather should be in metric units', mime_type='MemoryMimeType.TEXT', metadata={'mime_type': 'MemoryMimeType.TEXT', 'type': 'units', 'category': 'preferences', 'score': 0.4342913031578064, 'id': 'b240f12a-1440-42d1-8f5e-3d8a388363f2'})]\n---------- ToolCallRequestEvent (assistant_agent) ----------\n[FunctionCall(id='call_YmKqq1nWXgAkAAyXWWk9YpFW', arguments='{\"city\":\"New York\",\"units\":\"metric\"}', name='get_weather')]\n---------- ToolCallExecutionEvent (assistant_agent) ----------\n[FunctionExecutionResult(content='The weather in New York is 23 °C and Sunny.', name='get_weather', call_id='call_YmKqq1nWXgAkAAyXWWk9YpFW', is_error=False)]\n---------- ToolCallSummaryMessage (assistant_agent) ----------\nThe weather in New York is 23 °C and Sunny.",
      "language": "json"
    },
    {
      "code": "---------- TextMessage (user) ----------\nWhat is the weather in New York?\n---------- MemoryQueryEvent (assistant_agent) ----------\n[MemoryContent(content='The weather should be in metric units', mime_type='MemoryMimeType.TEXT', metadata={'category': 'preferences', 'mime_type': 'MemoryMimeType.TEXT', 'type': 'units', 'score': 0.4342913031578064, 'id': 'b8a70e90-a39f-47ed-ab7b-5a274009d9f0'}), MemoryContent(content='The weather should be in metric units', mime_type='MemoryMimeType.TEXT', metadata={'mime_type': 'MemoryMimeType.TEXT', 'type': 'units', 'category': 'preferences', 'score': 0.4342913031578064, 'id': 'b240f12a-1440-42d1-8f5e-3d8a388363f2'})]\n---------- ToolCallRequestEvent (assistant_agent) ----------\n[FunctionCall(id='call_YmKqq1nWXgAkAAyXWWk9YpFW', arguments='{\"city\":\"New York\",\"units\":\"metric\"}', name='get_weather')]\n---------- ToolCallExecutionEvent (assistant_agent) ----------\n[FunctionExecutionResult(content='The weather in New York is 23 °C and Sunny.', name='get_weather', call_id='call_YmKqq1nWXgAkAAyXWWk9YpFW', is_error=False)]\n---------- ToolCallSummaryMessage (assistant_agent) ----------\nThe weather in New York is 23 °C and Sunny.",
      "language": "json"
    },
    {
      "code": "chroma_user_memory.dump_component().model_dump_json()",
      "language": "unknown"
    },
    {
      "code": "chroma_user_memory.dump_component().model_dump_json()",
      "language": "unknown"
    },
    {
      "code": "'{\"provider\":\"autogen_ext.memory.chromadb.ChromaDBVectorMemory\",\"component_type\":\"memory\",\"version\":1,\"component_version\":1,\"description\":\"Store and retrieve memory using vector similarity search powered by ChromaDB.\",\"label\":\"ChromaDBVectorMemory\",\"config\":{\"client_type\":\"persistent\",\"collection_name\":\"preferences\",\"distance_metric\":\"cosine\",\"k\":2,\"score_threshold\":0.4,\"allow_reset\":false,\"tenant\":\"default_tenant\",\"database\":\"default_database\",\"persistence_path\":\"/Users/justin.cechmanek/.chromadb_autogen\"}}'",
      "language": "json"
    },
    {
      "code": "'{\"provider\":\"autogen_ext.memory.chromadb.ChromaDBVectorMemory\",\"component_type\":\"memory\",\"version\":1,\"component_version\":1,\"description\":\"Store and retrieve memory using vector similarity search powered by ChromaDB.\",\"label\":\"ChromaDBVectorMemory\",\"config\":{\"client_type\":\"persistent\",\"collection_name\":\"preferences\",\"distance_metric\":\"cosine\",\"k\":2,\"score_threshold\":0.4,\"allow_reset\":false,\"tenant\":\"default_tenant\",\"database\":\"default_database\",\"persistence_path\":\"/Users/justin.cechmanek/.chromadb_autogen\"}}'",
      "language": "json"
    },
    {
      "code": "from logging import WARNING, getLogger\n\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.ui import Console\nfrom autogen_core.memory import MemoryContent, MemoryMimeType\nfrom autogen_ext.memory.redis import RedisMemory, RedisMemoryConfig\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nlogger = getLogger()\nlogger.setLevel(WARNING)\n\n# Initailize Redis memory\nredis_memory = RedisMemory(\n    config=RedisMemoryConfig(\n        redis_url=\"redis://localhost:6379\",\n        index_name=\"chat_history\",\n        prefix=\"memory\",\n    )\n)\n\n# Add user preferences to memory\nawait redis_memory.add(\n    MemoryContent(\n        content=\"The weather should be in metric units\",\n        mime_type=MemoryMimeType.TEXT,\n        metadata={\"category\": \"preferences\", \"type\": \"units\"},\n    )\n)\n\nawait redis_memory.add(\n    MemoryContent(\n        content=\"Meal recipe must be vegan\",\n        mime_type=MemoryMimeType.TEXT,\n        metadata={\"category\": \"preferences\", \"type\": \"dietary\"},\n    )\n)\n\nmodel_client = OpenAIChatCompletionClient(\n    model=\"gpt-4o\",\n)\n\n# Create assistant agent with ChromaDB memory\nassistant_agent = AssistantAgent(\n    name=\"assistant_agent\",\n    model_client=model_client,\n    tools=[get_weather],\n    memory=[redis_memory],\n)\n\nstream = assistant_agent.run_stream(task=\"What is the weather in New York?\")\nawait Console(stream)\n\nawait model_client.close()\nawait redis_memory.close()",
      "language": "python"
    },
    {
      "code": "from logging import WARNING, getLogger\n\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.ui import Console\nfrom autogen_core.memory import MemoryContent, MemoryMimeType\nfrom autogen_ext.memory.redis import RedisMemory, RedisMemoryConfig\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nlogger = getLogger()\nlogger.setLevel(WARNING)\n\n# Initailize Redis memory\nredis_memory = RedisMemory(\n    config=RedisMemoryConfig(\n        redis_url=\"redis://localhost:6379\",\n        index_name=\"chat_history\",\n        prefix=\"memory\",\n    )\n)\n\n# Add user preferences to memory\nawait redis_memory.add(\n    MemoryContent(\n        content=\"The weather should be in metric units\",\n        mime_type=MemoryMimeType.TEXT,\n        metadata={\"category\": \"preferences\", \"type\": \"units\"},\n    )\n)\n\nawait redis_memory.add(\n    MemoryContent(\n        content=\"Meal recipe must be vegan\",\n        mime_type=MemoryMimeType.TEXT,\n        metadata={\"category\": \"preferences\", \"type\": \"dietary\"},\n    )\n)\n\nmodel_client = OpenAIChatCompletionClient(\n    model=\"gpt-4o\",\n)\n\n# Create assistant agent with ChromaDB memory\nassistant_agent = AssistantAgent(\n    name=\"assistant_agent\",\n    model_client=model_client,\n    tools=[get_weather],\n    memory=[redis_memory],\n)\n\nstream = assistant_agent.run_stream(task=\"What is the weather in New York?\")\nawait Console(stream)\n\nawait model_client.close()\nawait redis_memory.close()",
      "language": "python"
    },
    {
      "code": "---------- TextMessage (user) ----------\nWhat is the weather in New York?\n---------- MemoryQueryEvent (assistant_agent) ----------\n[MemoryContent(content='The weather should be in metric units', mime_type=<MemoryMimeType.TEXT: 'text/plain'>, metadata={'category': 'preferences', 'type': 'units'})]\n---------- ToolCallRequestEvent (assistant_agent) ----------\n[FunctionCall(id='call_1R6wV3uDOK8mGK2Vh2t0h4ld', arguments='{\"city\":\"New York\",\"units\":\"metric\"}', name='get_weather')]\n---------- ToolCallExecutionEvent (assistant_agent) ----------\n[FunctionExecutionResult(content='The weather in New York is 23 °C and Sunny.', name='get_weather', call_id='call_1R6wV3uDOK8mGK2Vh2t0h4ld', is_error=False)]\n---------- ToolCallSummaryMessage (assistant_agent) ----------\nThe weather in New York is 23 °C and Sunny.",
      "language": "json"
    },
    {
      "code": "---------- TextMessage (user) ----------\nWhat is the weather in New York?\n---------- MemoryQueryEvent (assistant_agent) ----------\n[MemoryContent(content='The weather should be in metric units', mime_type=<MemoryMimeType.TEXT: 'text/plain'>, metadata={'category': 'preferences', 'type': 'units'})]\n---------- ToolCallRequestEvent (assistant_agent) ----------\n[FunctionCall(id='call_1R6wV3uDOK8mGK2Vh2t0h4ld', arguments='{\"city\":\"New York\",\"units\":\"metric\"}', name='get_weather')]\n---------- ToolCallExecutionEvent (assistant_agent) ----------\n[FunctionExecutionResult(content='The weather in New York is 23 °C and Sunny.', name='get_weather', call_id='call_1R6wV3uDOK8mGK2Vh2t0h4ld', is_error=False)]\n---------- ToolCallSummaryMessage (assistant_agent) ----------\nThe weather in New York is 23 °C and Sunny.",
      "language": "json"
    },
    {
      "code": "import re\nfrom typing import List\n\nimport aiofiles\nimport aiohttp\nfrom autogen_core.memory import Memory, MemoryContent, MemoryMimeType\n\n\nclass SimpleDocumentIndexer:\n    \"\"\"Basic document indexer for AutoGen Memory.\"\"\"\n\n    def __init__(self, memory: Memory, chunk_size: int = 1500) -> None:\n        self.memory = memory\n        self.chunk_size = chunk_size\n\n    async def _fetch_content(self, source: str) -> str:\n        \"\"\"Fetch content from URL or file.\"\"\"\n        if source.startswith((\"http://\", \"https://\")):\n            async with aiohttp.ClientSession() as session:\n                async with session.get(source) as response:\n                    return await response.text()\n        else:\n            async with aiofiles.open(source, \"r\", encoding=\"utf-8\") as f:\n                return await f.read()\n\n    def _strip_html(self, text: str) -> str:\n        \"\"\"Remove HTML tags and normalize whitespace.\"\"\"\n        text = re.sub(r\"<[^>]*>\", \" \", text)\n        text = re.sub(r\"\\s+\", \" \", text)\n        return text.strip()\n\n    def _split_text(self, text: str) -> List[str]:\n        \"\"\"Split text into fixed-size chunks.\"\"\"\n        chunks: list[str] = []\n        # Just split text into fixed-size chunks\n        for i in range(0, len(text), self.chunk_size):\n            chunk = text[i : i + self.chunk_size]\n            chunks.append(chunk.strip())\n        return chunks\n\n    async def index_documents(self, sources: List[str]) -> int:\n        \"\"\"Index documents into memory.\"\"\"\n        total_chunks = 0\n\n        for source in sources:\n            try:\n                content = await self._fetch_content(source)\n\n                # Strip HTML if content appears to be HTML\n                if \"<\" in content and \">\" in content:\n                    content = self._strip_html(content)\n\n                chunks = self._split_text(content)\n\n                for i, chunk in enumerate(chunks):\n                    await self.memory.add(\n                        MemoryContent(\n                            content=chunk, mime_type=MemoryMimeType.TEXT, metadata={\"source\": source, \"chunk_index\": i}\n                        )\n                    )\n\n                total_chunks += len(chunks)\n\n            except Exception as e:\n                print(f\"Error indexing {source}: {str(e)}\")\n\n        return total_chunks",
      "language": "python"
    },
    {
      "code": "import re\nfrom typing import List\n\nimport aiofiles\nimport aiohttp\nfrom autogen_core.memory import Memory, MemoryContent, MemoryMimeType\n\n\nclass SimpleDocumentIndexer:\n    \"\"\"Basic document indexer for AutoGen Memory.\"\"\"\n\n    def __init__(self, memory: Memory, chunk_size: int = 1500) -> None:\n        self.memory = memory\n        self.chunk_size = chunk_size\n\n    async def _fetch_content(self, source: str) -> str:\n        \"\"\"Fetch content from URL or file.\"\"\"\n        if source.startswith((\"http://\", \"https://\")):\n            async with aiohttp.ClientSession() as session:\n                async with session.get(source) as response:\n                    return await response.text()\n        else:\n            async with aiofiles.open(source, \"r\", encoding=\"utf-8\") as f:\n                return await f.read()\n\n    def _strip_html(self, text: str) -> str:\n        \"\"\"Remove HTML tags and normalize whitespace.\"\"\"\n        text = re.sub(r\"<[^>]*>\", \" \", text)\n        text = re.sub(r\"\\s+\", \" \", text)\n        return text.strip()\n\n    def _split_text(self, text: str) -> List[str]:\n        \"\"\"Split text into fixed-size chunks.\"\"\"\n        chunks: list[str] = []\n        # Just split text into fixed-size chunks\n        for i in range(0, len(text), self.chunk_size):\n            chunk = text[i : i + self.chunk_size]\n            chunks.append(chunk.strip())\n        return chunks\n\n    async def index_documents(self, sources: List[str]) -> int:\n        \"\"\"Index documents into memory.\"\"\"\n        total_chunks = 0\n\n        for source in sources:\n            try:\n                content = await self._fetch_content(source)\n\n                # Strip HTML if content appears to be HTML\n                if \"<\" in content and \">\" in content:\n                    content = self._strip_html(content)\n\n                chunks = self._split_text(content)\n\n                for i, chunk in enumerate(chunks):\n                    await self.memory.add(\n                        MemoryContent(\n                            content=chunk, mime_type=MemoryMimeType.TEXT, metadata={\"source\": source, \"chunk_index\": i}\n                        )\n                    )\n\n                total_chunks += len(chunks)\n\n            except Exception as e:\n                print(f\"Error indexing {source}: {str(e)}\")\n\n        return total_chunks",
      "language": "python"
    },
    {
      "code": "import os\nfrom pathlib import Path\n\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.memory.chromadb import ChromaDBVectorMemory, PersistentChromaDBVectorMemoryConfig\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n# Initialize vector memory\n\nrag_memory = ChromaDBVectorMemory(\n    config=PersistentChromaDBVectorMemoryConfig(\n        collection_name=\"autogen_docs\",\n        persistence_path=os.path.join(str(Path.home()), \".chromadb_autogen\"),\n        k=3,  # Return top 3 results\n        score_threshold=0.4,  # Minimum similarity score\n    )\n)\n\nawait rag_memory.clear()  # Clear existing memory\n\n\n# Index AutoGen documentation\nasync def index_autogen_docs() -> None:\n    indexer = SimpleDocumentIndexer(memory=rag_memory)\n    sources = [\n        \"https://raw.githubusercontent.com/microsoft/autogen/main/README.md\",\n        \"https://microsoft.github.io/autogen/dev/user-guide/agentchat-user-guide/tutorial/agents.html\",\n        \"https://microsoft.github.io/autogen/dev/user-guide/agentchat-user-guide/tutorial/teams.html\",\n        \"https://microsoft.github.io/autogen/dev/user-guide/agentchat-user-guide/tutorial/termination.html\",\n    ]\n    chunks: int = await indexer.index_documents(sources)\n    print(f\"Indexed {chunks} chunks from {len(sources)} AutoGen documents\")\n\n\nawait index_autogen_docs()",
      "language": "python"
    },
    {
      "code": "import os\nfrom pathlib import Path\n\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.memory.chromadb import ChromaDBVectorMemory, PersistentChromaDBVectorMemoryConfig\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n# Initialize vector memory\n\nrag_memory = ChromaDBVectorMemory(\n    config=PersistentChromaDBVectorMemoryConfig(\n        collection_name=\"autogen_docs\",\n        persistence_path=os.path.join(str(Path.home()), \".chromadb_autogen\"),\n        k=3,  # Return top 3 results\n        score_threshold=0.4,  # Minimum similarity score\n    )\n)\n\nawait rag_memory.clear()  # Clear existing memory\n\n\n# Index AutoGen documentation\nasync def index_autogen_docs() -> None:\n    indexer = SimpleDocumentIndexer(memory=rag_memory)\n    sources = [\n        \"https://raw.githubusercontent.com/microsoft/autogen/main/README.md\",\n        \"https://microsoft.github.io/autogen/dev/user-guide/agentchat-user-guide/tutorial/agents.html\",\n        \"https://microsoft.github.io/autogen/dev/user-guide/agentchat-user-guide/tutorial/teams.html\",\n        \"https://microsoft.github.io/autogen/dev/user-guide/agentchat-user-guide/tutorial/termination.html\",\n    ]\n    chunks: int = await indexer.index_documents(sources)\n    print(f\"Indexed {chunks} chunks from {len(sources)} AutoGen documents\")\n\n\nawait index_autogen_docs()",
      "language": "python"
    },
    {
      "code": "Indexed 70 chunks from 4 AutoGen documents",
      "language": "sql"
    },
    {
      "code": "Indexed 70 chunks from 4 AutoGen documents",
      "language": "sql"
    },
    {
      "code": "# Create our RAG assistant agent\nrag_assistant = AssistantAgent(\n    name=\"rag_assistant\", model_client=OpenAIChatCompletionClient(model=\"gpt-4o\"), memory=[rag_memory]\n)\n\n# Ask questions about AutoGen\nstream = rag_assistant.run_stream(task=\"What is AgentChat?\")\nawait Console(stream)\n\n# Remember to close the memory when done\nawait rag_memory.close()",
      "language": "markdown"
    },
    {
      "code": "# Create our RAG assistant agent\nrag_assistant = AssistantAgent(\n    name=\"rag_assistant\", model_client=OpenAIChatCompletionClient(model=\"gpt-4o\"), memory=[rag_memory]\n)\n\n# Ask questions about AutoGen\nstream = rag_assistant.run_stream(task=\"What is AgentChat?\")\nawait Console(stream)\n\n# Remember to close the memory when done\nawait rag_memory.close()",
      "language": "markdown"
    },
    {
      "code": "---------- TextMessage (user) ----------\nWhat is AgentChat?\n---------- MemoryQueryEvent (rag_assistant) ----------\n[MemoryContent(content='e of the AssistantAgent , we can now proceed to the next section to learn about the teams feature in AgentChat. previous Messages next Teams On this page Assistant Agent Getting Result Multi-Modal Input Streaming Messages Using Tools and Workbench Built-in Tools and Workbench Function Tool Model Context Protocol (MCP) Workbench Agent as a Tool Parallel Tool Calls Tool Iterations Structured Output Streaming Tokens Using Model Context Other Preset Agents Next Step Edit on GitHub Show Source so the DOM is not blocked --> © Copyright 2024, Microsoft. Privacy Policy | Consumer Health Privacy Built with the PyData Sphinx Theme 0.16.0.', mime_type='MemoryMimeType.TEXT', metadata={'chunk_index': 16, 'mime_type': 'MemoryMimeType.TEXT', 'source': 'https://microsoft.github.io/autogen/dev/user-guide/agentchat-user-guide/tutorial/agents.html', 'score': 0.6237251460552216, 'id': '6457da13-1c25-44f0-bea3-158e5c0c5bb4'}), MemoryContent(content='h Literature Review API Reference PyPi Source AgentChat Agents Agents # AutoGen AgentChat provides a set of preset Agents, each with variations in how an agent might respond to messages. All agents share the following attributes and methods: name : The unique name of the agent. description : The description of the agent in text. run : The method that runs the agent given a task as a string or a list of messages, and returns a TaskResult . Agents are expected to be stateful and this method is expected to be called with new messages, not complete history . run_stream : Same as run() but returns an iterator of messages that subclass BaseAgentEvent or BaseChatMessage followed by a TaskResult as the last item. See autogen_agentchat.messages for more information on AgentChat message types. Assistant Agent # AssistantAgent is a built-in agent that uses a language model and has the ability to use tools. Warning AssistantAgent is a “kitchen sink” agent for prototyping and educational purpose – it is very general. Make sure you read the documentation and implementation to understand the design choices. Once you fully understand the design, you may want to implement your own agent. See Custom Agent . from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.messages import StructuredMessage from autogen_agentchat.ui import Console from autogen_ext.models.openai import OpenAIChatCompletionClient # Define a tool that searches the web for information. # For simplicity, we', mime_type='MemoryMimeType.TEXT', metadata={'chunk_index': 1, 'mime_type': 'MemoryMimeType.TEXT', 'source': 'https://microsoft.github.io/autogen/dev/user-guide/agentchat-user-guide/tutorial/agents.html', 'score': 0.6212755441665649, 'id': 'ab3a553f-bb69-41ff-b6a9-8397b4cb3cb1'}), MemoryContent(content='Literature Review API Reference PyPi Source AgentChat Teams Teams # In this section you’ll learn how to create a multi-agent team (or simply team) using AutoGen. A team is a group of agents that work together to achieve a common goal. We’ll first show you how to create and run a team. We’ll then explain how to observe the team’s behavior, which is crucial for debugging and understanding the team’s performance, and common operations to control the team’s behavior. AgentChat supports several team presets: RoundRobinGroupChat : A team that runs a group chat with participants taking turns in a round-robin fashion (covered on this page). Tutorial SelectorGroupChat : A team that selects the next speaker using a ChatCompletion model after each message. Tutorial MagenticOneGroupChat : A generalist multi-agent system for solving open-ended web and file-based tasks across a variety of domains. Tutorial Swarm : A team that uses HandoffMessage to signal transitions between agents. Tutorial Note When should you use a team? Teams are for complex tasks that require collaboration and diverse expertise. However, they also demand more scaffolding to steer compared to single agents. While AutoGen simplifies the process of working with teams, start with a single agent for simpler tasks, and transition to a multi-agent team when a single agent proves inadequate. Ensure that you have optimized your single agent with the appropriate tools and instructions before moving to a team-based approach. Cre', mime_type='MemoryMimeType.TEXT', metadata={'mime_type': 'MemoryMimeType.TEXT', 'chunk_index': 1, 'source': 'https://microsoft.github.io/autogen/dev/user-guide/agentchat-user-guide/tutorial/teams.html', 'score': 0.5267025232315063, 'id': '554b20a9-e041-4ac6-b2f1-11261336861c'})]\n---------- TextMessage (rag_assistant) ----------\nAgentChat is a framework that provides a set of preset agents designed to handle conversations and tasks using a variety of response strategies. It includes features for managing individual agents as well as creating teams of agents that can work collaboratively on complex goals. These agents are stateful, meaning they can manage and track ongoing conversations. AgentChat also includes agents that can utilize tools to enhance their capabilities.\n\nKey features of AgentChat include:\n- **Preset Agents**: These agents are pre-configured with specific behavior patterns for handling tasks and messages.\n- **Agent Attributes and Methods**: Each agent has a unique name and description, and methods like `run` and `run_stream` to execute tasks and handle messages.\n- **AssistantAgent**: A built-in general-purpose agent used primarily for prototyping and educational purposes.\n- **Team Configurations**: AgentChat allows for the creation of multi-agent teams for tasks that are too complex for a single agent. Teams run in preset formats like RoundRobinGroupChat or Swarm, providing structured interaction among agents.\n\nOverall, AgentChat is designed for flexible deployment of conversational agents, either singly or in groups, across a variety of tasks. \n\nTERMINATE",
      "language": "json"
    },
    {
      "code": "---------- TextMessage (user) ----------\nWhat is AgentChat?\n---------- MemoryQueryEvent (rag_assistant) ----------\n[MemoryContent(content='e of the AssistantAgent , we can now proceed to the next section to learn about the teams feature in AgentChat. previous Messages next Teams On this page Assistant Agent Getting Result Multi-Modal Input Streaming Messages Using Tools and Workbench Built-in Tools and Workbench Function Tool Model Context Protocol (MCP) Workbench Agent as a Tool Parallel Tool Calls Tool Iterations Structured Output Streaming Tokens Using Model Context Other Preset Agents Next Step Edit on GitHub Show Source so the DOM is not blocked --> © Copyright 2024, Microsoft. Privacy Policy | Consumer Health Privacy Built with the PyData Sphinx Theme 0.16.0.', mime_type='MemoryMimeType.TEXT', metadata={'chunk_index': 16, 'mime_type': 'MemoryMimeType.TEXT', 'source': 'https://microsoft.github.io/autogen/dev/user-guide/agentchat-user-guide/tutorial/agents.html', 'score': 0.6237251460552216, 'id': '6457da13-1c25-44f0-bea3-158e5c0c5bb4'}), MemoryContent(content='h Literature Review API Reference PyPi Source AgentChat Agents Agents # AutoGen AgentChat provides a set of preset Agents, each with variations in how an agent might respond to messages. All agents share the following attributes and methods: name : The unique name of the agent. description : The description of the agent in text. run : The method that runs the agent given a task as a string or a list of messages, and returns a TaskResult . Agents are expected to be stateful and this method is expected to be called with new messages, not complete history . run_stream : Same as run() but returns an iterator of messages that subclass BaseAgentEvent or BaseChatMessage followed by a TaskResult as the last item. See autogen_agentchat.messages for more information on AgentChat message types. Assistant Agent # AssistantAgent is a built-in agent that uses a language model and has the ability to use tools. Warning AssistantAgent is a “kitchen sink” agent for prototyping and educational purpose – it is very general. Make sure you read the documentation and implementation to understand the design choices. Once you fully understand the design, you may want to implement your own agent. See Custom Agent . from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.messages import StructuredMessage from autogen_agentchat.ui import Console from autogen_ext.models.openai import OpenAIChatCompletionClient # Define a tool that searches the web for information. # For simplicity, we', mime_type='MemoryMimeType.TEXT', metadata={'chunk_index': 1, 'mime_type': 'MemoryMimeType.TEXT', 'source': 'https://microsoft.github.io/autogen/dev/user-guide/agentchat-user-guide/tutorial/agents.html', 'score': 0.6212755441665649, 'id': 'ab3a553f-bb69-41ff-b6a9-8397b4cb3cb1'}), MemoryContent(content='Literature Review API Reference PyPi Source AgentChat Teams Teams # In this section you’ll learn how to create a multi-agent team (or simply team) using AutoGen. A team is a group of agents that work together to achieve a common goal. We’ll first show you how to create and run a team. We’ll then explain how to observe the team’s behavior, which is crucial for debugging and understanding the team’s performance, and common operations to control the team’s behavior. AgentChat supports several team presets: RoundRobinGroupChat : A team that runs a group chat with participants taking turns in a round-robin fashion (covered on this page). Tutorial SelectorGroupChat : A team that selects the next speaker using a ChatCompletion model after each message. Tutorial MagenticOneGroupChat : A generalist multi-agent system for solving open-ended web and file-based tasks across a variety of domains. Tutorial Swarm : A team that uses HandoffMessage to signal transitions between agents. Tutorial Note When should you use a team? Teams are for complex tasks that require collaboration and diverse expertise. However, they also demand more scaffolding to steer compared to single agents. While AutoGen simplifies the process of working with teams, start with a single agent for simpler tasks, and transition to a multi-agent team when a single agent proves inadequate. Ensure that you have optimized your single agent with the appropriate tools and instructions before moving to a team-based approach. Cre', mime_type='MemoryMimeType.TEXT', metadata={'mime_type': 'MemoryMimeType.TEXT', 'chunk_index': 1, 'source': 'https://microsoft.github.io/autogen/dev/user-guide/agentchat-user-guide/tutorial/teams.html', 'score': 0.5267025232315063, 'id': '554b20a9-e041-4ac6-b2f1-11261336861c'})]\n---------- TextMessage (rag_assistant) ----------\nAgentChat is a framework that provides a set of preset agents designed to handle conversations and tasks using a variety of response strategies. It includes features for managing individual agents as well as creating teams of agents that can work collaboratively on complex goals. These agents are stateful, meaning they can manage and track ongoing conversations. AgentChat also includes agents that can utilize tools to enhance their capabilities.\n\nKey features of AgentChat include:\n- **Preset Agents**: These agents are pre-configured with specific behavior patterns for handling tasks and messages.\n- **Agent Attributes and Methods**: Each agent has a unique name and description, and methods like `run` and `run_stream` to execute tasks and handle messages.\n- **AssistantAgent**: A built-in general-purpose agent used primarily for prototyping and educational purposes.\n- **Team Configurations**: AgentChat allows for the creation of multi-agent teams for tasks that are too complex for a single agent. Teams run in preset formats like RoundRobinGroupChat or Swarm, providing structured interaction among agents.\n\nOverall, AgentChat is designed for flexible deployment of conversational agents, either singly or in groups, across a variety of tasks. \n\nTERMINATE",
      "language": "json"
    },
    {
      "code": "from autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.ui import Console\nfrom autogen_core.memory import MemoryContent, MemoryMimeType\nfrom autogen_ext.memory.mem0 import Mem0Memory\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n# Initialize Mem0 cloud memory (requires API key)\n# For local deployment, use is_cloud=False with appropriate config\nmem0_memory = Mem0Memory(\n    is_cloud=True,\n    limit=5,  # Maximum number of memories to retrieve\n)\n\n# Add user preferences to memory\nawait mem0_memory.add(\n    MemoryContent(\n        content=\"The weather should be in metric units\",\n        mime_type=MemoryMimeType.TEXT,\n        metadata={\"category\": \"preferences\", \"type\": \"units\"},\n    )\n)\n\nawait mem0_memory.add(\n    MemoryContent(\n        content=\"Meal recipe must be vegan\",\n        mime_type=MemoryMimeType.TEXT,\n        metadata={\"category\": \"preferences\", \"type\": \"dietary\"},\n    )\n)\n\n# Create assistant with mem0 memory\nassistant_agent = AssistantAgent(\n    name=\"assistant_agent\",\n    model_client=OpenAIChatCompletionClient(\n        model=\"gpt-4o-2024-08-06\",\n    ),\n    tools=[get_weather],\n    memory=[mem0_memory],\n)\n\n# Ask about the weather\nstream = assistant_agent.run_stream(task=\"What are my dietary preferences?\")\nawait Console(stream)",
      "language": "json"
    },
    {
      "code": "from autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.ui import Console\nfrom autogen_core.memory import MemoryContent, MemoryMimeType\nfrom autogen_ext.memory.mem0 import Mem0Memory\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n# Initialize Mem0 cloud memory (requires API key)\n# For local deployment, use is_cloud=False with appropriate config\nmem0_memory = Mem0Memory(\n    is_cloud=True,\n    limit=5,  # Maximum number of memories to retrieve\n)\n\n# Add user preferences to memory\nawait mem0_memory.add(\n    MemoryContent(\n        content=\"The weather should be in metric units\",\n        mime_type=MemoryMimeType.TEXT,\n        metadata={\"category\": \"preferences\", \"type\": \"units\"},\n    )\n)\n\nawait mem0_memory.add(\n    MemoryContent(\n        content=\"Meal recipe must be vegan\",\n        mime_type=MemoryMimeType.TEXT,\n        metadata={\"category\": \"preferences\", \"type\": \"dietary\"},\n    )\n)\n\n# Create assistant with mem0 memory\nassistant_agent = AssistantAgent(\n    name=\"assistant_agent\",\n    model_client=OpenAIChatCompletionClient(\n        model=\"gpt-4o-2024-08-06\",\n    ),\n    tools=[get_weather],\n    memory=[mem0_memory],\n)\n\n# Ask about the weather\nstream = assistant_agent.run_stream(task=\"What are my dietary preferences?\")\nawait Console(stream)",
      "language": "json"
    },
    {
      "code": "# Serialize the memory configuration\nconfig_json = mem0_memory.dump_component().model_dump_json()\nprint(f\"Memory config JSON: {config_json[:100]}...\")",
      "language": "markdown"
    },
    {
      "code": "# Serialize the memory configuration\nconfig_json = mem0_memory.dump_component().model_dump_json()\nprint(f\"Memory config JSON: {config_json[:100]}...\")",
      "language": "markdown"
    }
  ],
  "patterns": [],
  "links": [
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/memory.html",
    "https://microsoft.github.io/autogen/stable/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/extensions-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/reference/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/installation.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/quickstart.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/migration-guide.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/models.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/messages.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/agents.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/teams.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/human-in-the-loop.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/termination.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/state.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/custom-agents.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/selector-group-chat.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/swarm.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/magentic-one.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/graph-flow.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/logging.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/serialize-components.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tracing.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/examples/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/examples/travel-planning.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/examples/company-research.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/examples/literature-review.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.memory.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.agents.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.memory.redis.html"
  ]
}