{
  "url": "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/migration-guide.html",
  "title": "Migration Guide for v0.2 to v0.4 — AutoGen",
  "content": "This is a migration guide for users of the v0.2.* versions of autogen-agentchat to the v0.4 version, which introduces a new set of APIs and features. The v0.4 version contains breaking changes. Please read this guide carefully. We still maintain the v0.2 version in the 0.2 branch; however, we highly recommend you upgrade to the v0.4 version.\n\nWe no longer have admin access to the pyautogen PyPI package, and the releases from that package are no longer from Microsoft since version 0.2.34. To continue use the v0.2 version of AutoGen, install it using autogen-agentchat~=0.2. Please read our clarification statement regarding forks.\n\nSince the release of AutoGen in 2023, we have intensively listened to our community and users from small startups and large enterprises, gathering much feedback. Based on that feedback, we built AutoGen v0.4, a from-the-ground-up rewrite adopting an asynchronous, event-driven architecture to address issues such as observability, flexibility, interactive control, and scale.\n\nThe v0.4 API is layered: the Core API is the foundation layer offering a scalable, event-driven actor framework for creating agentic workflows; the AgentChat API is built on Core, offering a task-driven, high-level framework for building interactive agentic applications. It is a replacement for AutoGen v0.2.\n\nMost of this guide focuses on v0.4’s AgentChat API; however, you can also build your own high-level framework using just the Core API.\n\nJump straight to the AgentChat Tutorial to get started with v0.4.\n\nWe provide a detailed guide on how to migrate your existing codebase from v0.2 to v0.4.\n\nSee each feature below for detailed information on how to migrate.\n\nMigration Guide for v0.2 to v0.4\n\nWhat’s in this guide?\n\nUse model client class directly\n\nModel Client for OpenAI-Compatible APIs\n\nConversable Agent and Register Reply\n\nSave and Load Agent State\n\nConversion between v0.2 and v0.4 Messages\n\nGroup Chat with Resume\n\nSave and Load Group Chat State\n\nGroup Chat with Tool Use\n\nGroup Chat with Custom Selector (Stateflow)\n\nLong Context Handling\n\nObservability and Control\n\nThe following features currently in v0.2 will be provided in the future releases of v0.4.* versions:\n\nModel Client Cost #4835\n\nWe will update this guide when the missing features become available.\n\nIn v0.2 you configure the model client as follows, and create the OpenAIWrapper object.\n\nNote: In AutoGen 0.2, the OpenAI client would try configs in the list until one worked. 0.4 instead expects a specfic model configuration to be chosen.\n\nIn v0.4, we offer two ways to create a model client.\n\nAutoGen 0.4 has a generic component configuration system. Model clients are a great use case for this. See below for how to create an OpenAI chat completion client.\n\nRead more on OpenAIChatCompletionClient.\n\nYou can use a the OpenAIChatCompletionClient to connect to an OpenAI-Compatible API, but you need to specify the base_url and model_info.\n\nNote: We don’t test all the OpenAI-Compatible APIs, and many of them works differently from the OpenAI API even though they may claim to suppor it. Please test them before using them.\n\nRead about Model Clients in AgentChat Tutorial and more detailed information on Core API Docs\n\nSupport for other hosted models will be added in the future.\n\nIn v0.2, you can set the cache seed through the cache_seed parameter in the LLM config. The cache is enabled by default.\n\nIn v0.4, the cache is not enabled by default, to use it you need to use a ChatCompletionCache wrapper around the model client.\n\nYou can use a DiskCacheStore or RedisStore to store the cache.\n\nHere’s an example of using diskcache for local caching:\n\nIn v0.2, you create an assistant agent as follows:\n\nIn v0.4, it is similar, but you need to specify model_client instead of llm_config.\n\nHowever, the usage is somewhat different. In v0.4, instead of calling assistant.send, you call assistant.on_messages or assistant.on_messages_stream to handle incoming messages. Furthermore, the on_messages and on_messages_stream methods are asynchronous, and the latter returns an async generator to stream the inner thoughts of the agent.\n\nHere is how you can call the assistant agent in v0.4 directly, continuing from the above example:\n\nThe CancellationToken can be used to cancel the request asynchronously when you call cancellation_token.cancel(), which will cause the await on the on_messages call to raise a CancelledError.\n\nRead more on Agent Tutorial and AssistantAgent.\n\nThe AssistantAgent in v0.4 supports multi-modal inputs if the model client supports it. The vision capability of the model client is used to determine if the agent supports multi-modal inputs.\n\nIn v0.2, you create a user proxy as follows:\n\nThis user proxy would take input from the user through console, and would terminate if the incoming message ends with “TERMINATE”.\n\nIn v0.4, a user proxy is simply an agent that takes user input only, there is no other special configuration needed. You can create a user proxy as follows:\n\nSee UserProxyAgent for more details and how to customize the input function with timeout.\n\nIn v0.2, there was the concept of teachable agents as well as a RAG agents that could take a database config.\n\nIn v0.4, you can implement a RAG agent using the Memory class. Specifically, you can define a memory store class, and pass that as a parameter to the assistant agent. See the Memory tutorial for more details.\n\nThis clear separation of concerns allows you to implement a memory store that uses any database or storage system you want (you have to inherit from the Memory class) and use it with an assistant agent. The example below shows how to use a ChromaDB vector memory store with the assistant agent. In addition, your application logic should determine how and when to add content to the memory store. For example, you may choose to call memory.add for every response from the assistant agent or use a separate LLM call to determine if the content should be added to the memory store.\n\nIn v0.2, you can create a conversable agent and register a reply function as follows:\n\nRather than guessing what the reply_func does, all its parameters, and what the position should be, in v0.4, we can simply create a custom agent and implement the on_messages, on_reset, and produced_message_types methods.\n\nYou can then use the custom agent in the same way as the AssistantAgent. See Custom Agent Tutorial for more details.\n\nIn v0.2 there is no built-in way to save and load an agent’s state: you need to implement it yourself by exporting the chat_messages attribute of ConversableAgent and importing it back through the chat_messages parameter.\n\nIn v0.4, you can call save_state and load_state methods on agents to save and load their state.\n\nYou can also call save_state and load_state on any teams, such as RoundRobinGroupChat to save and load the state of the entire team.\n\nIn v0.2, you can create a two-agent chat for code execution as follows:\n\nTo get the same behavior in v0.4, you can use the AssistantAgent and CodeExecutorAgent together in a RoundRobinGroupChat.\n\nIn v0.2, to create a tool use chatbot, you must have two agents, one for calling the tool and one for executing the tool. You need to initiate a two-agent chat for every user request.\n\nIn v0.4, you really just need one agent – the AssistantAgent – to handle both the tool calling and tool execution.\n\nWhen using tool-equipped agents inside a group chat such as RoundRobinGroupChat, you simply do the same as above to add tools to the agents, and create a group chat with the agents.\n\nIn v0.2, you get a ChatResult object from the initiate_chat method. For example:\n\nSee ChatResult Docs for more details.\n\nIn v0.4, you get a TaskResult object from a run or run_stream method. The TaskResult object contains the messages which is the message history of the chat, including both agents’ private (tool calls, etc.) and public messages.\n\nThere are some notable differences between TaskResult and ChatResult:\n\nThe messages list in TaskResult uses different message format than the ChatResult.chat_history list.\n\nThere is no summary field. It is up to the application to decide how to summarize the chat using the messages list.\n\nhuman_input is not provided in the TaskResult object, as the user input can be extracted from the messages list by filtering with the source field.\n\ncost is not provided in the TaskResult object, however, you can calculate the cost based on token usage. It would be a great community extension to add cost calculation. See community extensions.\n\nYou can use the following conversion functions to convert between a v0.4 message in autogen_agentchat.base.TaskResult.messages and a v0.2 message in ChatResult.chat_history.\n\nIn v0.2, you need to create a GroupChat class and pass it into a GroupChatManager, and have a participant that is a user proxy to initiate the chat. For a simple scenario of a writer and a critic, you can do the following:\n\nIn v0.4, you can use the RoundRobinGroupChat to achieve the same behavior.\n\nFor LLM-based speaker selection, you can use the SelectorGroupChat instead. See Selector Group Chat Tutorial and SelectorGroupChat for more details.\n\nNote: In v0.4, you do not need to register functions on a user proxy to use tools in a group chat. You can simply pass the tool functions to the AssistantAgent as shown in the Tool Use section. The agent will automatically call the tools when needed. If your tool doesn’t output well formed response, you can use the reflect_on_tool_use parameter to have the model reflect on the tool use.\n\nIn v0.2, group chat with resume is a bit complicated. You need to explicitly save the group chat messages and load them back when you want to resume the chat. See Resuming Group Chat in v0.2 for more details.\n\nIn v0.4, you can simply call run or run_stream again with the same group chat object to resume the chat. To export and load the state, you can use save_state and load_state methods.\n\nIn v0.2, you need to explicitly save the group chat messages and load them back when you want to resume the chat.\n\nIn v0.4, you can simply call save_state and load_state methods on the group chat object. See Group Chat with Resume for an example.\n\nIn v0.2 group chat, when tools are involved, you need to register the tool functions on a user proxy, and include the user proxy in the group chat. The tool calls made by other agents will be routed to the user proxy to execute.\n\nWe have observed numerous issues with this approach, such as the the tool call routing not working as expected, and the tool call request and result cannot be accepted by models without support for function calling.\n\nIn v0.4, there is no need to register the tool functions on a user proxy, as the tools are directly executed within the AssistantAgent, which publishes the response from the tool to the group chat. So the group chat manager does not need to be involved in routing tool calls.\n\nSee Selector Group Chat Tutorial for an example of using tools in a group chat.\n\nIn v0.2 group chat, when the speaker_selection_method is set to a custom function, it can override the default selection method. This is useful for implementing a state-based selection method. For more details, see Custom Sepaker Selection in v0.2.\n\nIn v0.4, you can use the SelectorGroupChat with selector_func to achieve the same behavior. The selector_func is a function that takes the current message thread of the group chat and returns the next speaker’s name. If None is returned, the LLM-based selection method will be used.\n\nHere is an example of using the state-based selection method to implement a web search/analysis scenario.\n\nNested chat allows you to nest a whole team or another agent inside an agent. This is useful for creating a hierarchical structure of agents or “information silos”, as the nested agents cannot communicate directly with other agents outside of the same group.\n\nIn v0.2, nested chat is supported by using the register_nested_chats method on the ConversableAgent class. You need to specify the nested sequence of agents using dictionaries, See Nested Chat in v0.2 for more details.\n\nIn v0.4, nested chat is an implementation detail of a custom agent. You can create a custom agent that takes a team or another agent as a parameter and implements the on_messages method to trigger the nested team or agent. It is up to the application to decide how to pass or transform the messages from and to the nested team or agent.\n\nThe following example shows a simple nested chat that counts numbers.\n\nYou should see the following output:\n\nYou can take a look at SocietyOfMindAgent for a more complex implementation.\n\nIn v0.2, sequential chat is supported by using the initiate_chats function. It takes input a list of dictionary configurations for each step of the sequence. See Sequential Chat in v0.2 for more details.\n\nBase on the feedback from the community, the initiate_chats function is too opinionated and not flexible enough to support the diverse set of scenarios that users want to implement. We often find users struggling to get the initiate_chats function to work when they can easily glue the steps together usign basic Python code. Therefore, in v0.4, we do not provide a built-in function for sequential chat in the AgentChat API.\n\nInstead, you can create an event-driven sequential workflow using the Core API, and use the other components provided the AgentChat API to implement each step of the workflow. See an example of sequential workflow in the Core API Tutorial.\n\nWe recognize that the concept of workflow is at the heart of many applications, and we will provide more built-in support for workflows in the future.\n\nIn v0.2, GPTAssistantAgent is a special agent class that is backed by the OpenAI Assistant API.\n\nIn v0.4, the equivalent is the OpenAIAssistantAgent class. It supports the same set of features as the GPTAssistantAgent in v0.2 with more such as customizable threads and file uploads. See OpenAIAssistantAgent for more details.\n\nIn v0.2, long context that overflows the model’s context window can be handled by using the transforms capability that is added to an ConversableAgent after which is contructed.\n\nThe feedbacks from our community has led us to believe this feature is essential and should be a built-in component of AssistantAgent, and can be used for every custom agent.\n\nIn v0.4, we introduce the ChatCompletionContext base class that manages message history and provides a virtual view of the history. Applications can use built-in implementations such as BufferedChatCompletionContext to limit the message history sent to the model, or provide their own implementations that creates different virtual views.\n\nTo use BufferedChatCompletionContext in an AssistantAgent in a chatbot scenario.\n\nIn this example, the chatbot can only read the last 10 messages in the history.\n\nIn v0.4 AgentChat, you can observe the agents by using the on_messages_stream method which returns an async generator to stream the inner thoughts and actions of the agent. For teams, you can use the run_stream method to stream the inner conversation among the agents in the team. Your application can use these streams to observe the agents and teams in real-time.\n\nBoth the on_messages_stream and run_stream methods takes a CancellationToken as a parameter which can be used to cancel the output stream asynchronously and stop the agent or team. For teams, you can also use termination conditions to stop the team when a certain condition is met. See Termination Condition Tutorial for more details.\n\nUnlike the v0.2 which comes with a special logging module, the v0.4 API simply uses Python’s logging module to log events such as model client calls. See Logging in the Core API documentation for more details.\n\nThe code executors in v0.2 and v0.4 are nearly identical except the v0.4 executors support async API. You can also use CancellationToken to cancel a code execution if it takes too long. See Command Line Code Executors Tutorial in the Core API documentation.\n\nWe also added ACADynamicSessionsCodeExecutor that can use Azure Container Apps (ACA) dynamic sessions for code execution. See ACA Dynamic Sessions Code Executor Docs.",
  "headings": [
    {
      "level": "h1",
      "text": "Migration Guide for v0.2 to v0.4#",
      "id": ""
    },
    {
      "level": "h2",
      "text": "What is v0.4?#",
      "id": ""
    },
    {
      "level": "h2",
      "text": "New to AutoGen?#",
      "id": ""
    },
    {
      "level": "h2",
      "text": "What’s in this guide?#",
      "id": ""
    },
    {
      "level": "h2",
      "text": "Model Client#",
      "id": ""
    },
    {
      "level": "h3",
      "text": "Use component config#",
      "id": ""
    },
    {
      "level": "h3",
      "text": "Use model client class directly#",
      "id": ""
    },
    {
      "level": "h2",
      "text": "Model Client for OpenAI-Compatible APIs#",
      "id": ""
    },
    {
      "level": "h2",
      "text": "Model Client Cache#",
      "id": ""
    },
    {
      "level": "h2",
      "text": "Assistant Agent#",
      "id": ""
    },
    {
      "level": "h2",
      "text": "Multi-Modal Agent#",
      "id": ""
    },
    {
      "level": "h2",
      "text": "User Proxy#",
      "id": ""
    },
    {
      "level": "h2",
      "text": "RAG Agent#",
      "id": ""
    },
    {
      "level": "h2",
      "text": "Conversable Agent and Register Reply#",
      "id": ""
    },
    {
      "level": "h2",
      "text": "Save and Load Agent State#",
      "id": ""
    },
    {
      "level": "h2",
      "text": "Two-Agent Chat#",
      "id": ""
    },
    {
      "level": "h2",
      "text": "Tool Use#",
      "id": ""
    },
    {
      "level": "h2",
      "text": "Chat Result#",
      "id": ""
    },
    {
      "level": "h2",
      "text": "Conversion between v0.2 and v0.4 Messages#",
      "id": ""
    },
    {
      "level": "h2",
      "text": "Group Chat#",
      "id": ""
    },
    {
      "level": "h2",
      "text": "Group Chat with Resume#",
      "id": ""
    },
    {
      "level": "h2",
      "text": "Save and Load Group Chat State#",
      "id": ""
    },
    {
      "level": "h2",
      "text": "Group Chat with Tool Use#",
      "id": ""
    },
    {
      "level": "h2",
      "text": "Group Chat with Custom Selector (Stateflow)#",
      "id": ""
    },
    {
      "level": "h2",
      "text": "Nested Chat#",
      "id": ""
    },
    {
      "level": "h2",
      "text": "Sequential Chat#",
      "id": ""
    },
    {
      "level": "h2",
      "text": "GPTAssistantAgent#",
      "id": ""
    },
    {
      "level": "h2",
      "text": "Long Context Handling#",
      "id": ""
    },
    {
      "level": "h2",
      "text": "Observability and Control#",
      "id": ""
    },
    {
      "level": "h2",
      "text": "Code Executors#",
      "id": ""
    }
  ],
  "code_samples": [
    {
      "code": "from autogen.oai import OpenAIWrapper\n\nconfig_list = [\n    {\"model\": \"gpt-4o\", \"api_key\": \"sk-xxx\"},\n    {\"model\": \"gpt-4o-mini\", \"api_key\": \"sk-xxx\"},\n]\n\nmodel_client = OpenAIWrapper(config_list=config_list)",
      "language": "json"
    },
    {
      "code": "from autogen.oai import OpenAIWrapper\n\nconfig_list = [\n    {\"model\": \"gpt-4o\", \"api_key\": \"sk-xxx\"},\n    {\"model\": \"gpt-4o-mini\", \"api_key\": \"sk-xxx\"},\n]\n\nmodel_client = OpenAIWrapper(config_list=config_list)",
      "language": "json"
    },
    {
      "code": "from autogen_core.models import ChatCompletionClient\n\nconfig = {\n    \"provider\": \"OpenAIChatCompletionClient\",\n    \"config\": {\n        \"model\": \"gpt-4o\",\n        \"api_key\": \"sk-xxx\" # os.environ[\"...']\n    }\n}\n\nmodel_client = ChatCompletionClient.load_component(config)",
      "language": "json"
    },
    {
      "code": "from autogen_core.models import ChatCompletionClient\n\nconfig = {\n    \"provider\": \"OpenAIChatCompletionClient\",\n    \"config\": {\n        \"model\": \"gpt-4o\",\n        \"api_key\": \"sk-xxx\" # os.environ[\"...']\n    }\n}\n\nmodel_client = ChatCompletionClient.load_component(config)",
      "language": "json"
    },
    {
      "code": "from autogen_ext.models.openai import OpenAIChatCompletionClient\n\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o\", api_key=\"sk-xxx\")",
      "language": "sql"
    },
    {
      "code": "from autogen_ext.models.openai import OpenAIChatCompletionClient\n\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o\", api_key=\"sk-xxx\")",
      "language": "sql"
    },
    {
      "code": "from autogen_ext.models.openai import AzureOpenAIChatCompletionClient\n\nmodel_client = AzureOpenAIChatCompletionClient(\n    azure_deployment=\"gpt-4o\",\n    azure_endpoint=\"https://<your-endpoint>.openai.azure.com/\",\n    model=\"gpt-4o\",\n    api_version=\"2024-09-01-preview\",\n    api_key=\"sk-xxx\",\n)",
      "language": "sql"
    },
    {
      "code": "from autogen_ext.models.openai import AzureOpenAIChatCompletionClient\n\nmodel_client = AzureOpenAIChatCompletionClient(\n    azure_deployment=\"gpt-4o\",\n    azure_endpoint=\"https://<your-endpoint>.openai.azure.com/\",\n    model=\"gpt-4o\",\n    api_version=\"2024-09-01-preview\",\n    api_key=\"sk-xxx\",\n)",
      "language": "sql"
    },
    {
      "code": "from autogen_ext.models.openai import OpenAIChatCompletionClient\n\ncustom_model_client = OpenAIChatCompletionClient(\n    model=\"custom-model-name\",\n    base_url=\"https://custom-model.com/reset/of/the/path\",\n    api_key=\"placeholder\",\n    model_info={\n        \"vision\": True,\n        \"function_calling\": True,\n        \"json_output\": True,\n        \"family\": \"unknown\",\n        \"structured_output\": True,\n    },\n)",
      "language": "json"
    },
    {
      "code": "from autogen_ext.models.openai import OpenAIChatCompletionClient\n\ncustom_model_client = OpenAIChatCompletionClient(\n    model=\"custom-model-name\",\n    base_url=\"https://custom-model.com/reset/of/the/path\",\n    api_key=\"placeholder\",\n    model_info={\n        \"vision\": True,\n        \"function_calling\": True,\n        \"json_output\": True,\n        \"family\": \"unknown\",\n        \"structured_output\": True,\n    },\n)",
      "language": "json"
    },
    {
      "code": "llm_config = {\n    \"config_list\": [{\"model\": \"gpt-4o\", \"api_key\": \"sk-xxx\"}],\n    \"seed\": 42,\n    \"temperature\": 0,\n    \"cache_seed\": 42,\n}",
      "language": "json"
    },
    {
      "code": "llm_config = {\n    \"config_list\": [{\"model\": \"gpt-4o\", \"api_key\": \"sk-xxx\"}],\n    \"seed\": 42,\n    \"temperature\": 0,\n    \"cache_seed\": 42,\n}",
      "language": "json"
    },
    {
      "code": "pip install -U \"autogen-ext[openai, diskcache, redis]\"",
      "language": "unknown"
    },
    {
      "code": "pip install -U \"autogen-ext[openai, diskcache, redis]\"",
      "language": "unknown"
    },
    {
      "code": "import asyncio\nimport tempfile\n\nfrom autogen_core.models import UserMessage\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_ext.models.cache import ChatCompletionCache, CHAT_CACHE_VALUE_TYPE\nfrom autogen_ext.cache_store.diskcache import DiskCacheStore\nfrom diskcache import Cache\n\n\nasync def main():\n    with tempfile.TemporaryDirectory() as tmpdirname:\n        # Initialize the original client\n        openai_model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n        # Then initialize the CacheStore, in this case with diskcache.Cache.\n        # You can also use redis like:\n        # from autogen_ext.cache_store.redis import RedisStore\n        # import redis\n        # redis_instance = redis.Redis()\n        # cache_store = RedisCacheStore[CHAT_CACHE_VALUE_TYPE](redis_instance)\n        cache_store = DiskCacheStore[CHAT_CACHE_VALUE_TYPE](Cache(tmpdirname))\n        cache_client = ChatCompletionCache(openai_model_client, cache_store)\n\n        response = await cache_client.create([UserMessage(content=\"Hello, how are you?\", source=\"user\")])\n        print(response)  # Should print response from OpenAI\n        response = await cache_client.create([UserMessage(content=\"Hello, how are you?\", source=\"user\")])\n        print(response)  # Should print cached response\n        await openai_model_client.close()\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nimport tempfile\n\nfrom autogen_core.models import UserMessage\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_ext.models.cache import ChatCompletionCache, CHAT_CACHE_VALUE_TYPE\nfrom autogen_ext.cache_store.diskcache import DiskCacheStore\nfrom diskcache import Cache\n\n\nasync def main():\n    with tempfile.TemporaryDirectory() as tmpdirname:\n        # Initialize the original client\n        openai_model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n        # Then initialize the CacheStore, in this case with diskcache.Cache.\n        # You can also use redis like:\n        # from autogen_ext.cache_store.redis import RedisStore\n        # import redis\n        # redis_instance = redis.Redis()\n        # cache_store = RedisCacheStore[CHAT_CACHE_VALUE_TYPE](redis_instance)\n        cache_store = DiskCacheStore[CHAT_CACHE_VALUE_TYPE](Cache(tmpdirname))\n        cache_client = ChatCompletionCache(openai_model_client, cache_store)\n\n        response = await cache_client.create([UserMessage(content=\"Hello, how are you?\", source=\"user\")])\n        print(response)  # Should print response from OpenAI\n        response = await cache_client.create([UserMessage(content=\"Hello, how are you?\", source=\"user\")])\n        print(response)  # Should print cached response\n        await openai_model_client.close()\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "from autogen.agentchat import AssistantAgent\n\nllm_config = {\n    \"config_list\": [{\"model\": \"gpt-4o\", \"api_key\": \"sk-xxx\"}],\n    \"seed\": 42,\n    \"temperature\": 0,\n}\n\nassistant = AssistantAgent(\n    name=\"assistant\",\n    system_message=\"You are a helpful assistant.\",\n    llm_config=llm_config,\n)",
      "language": "json"
    },
    {
      "code": "from autogen.agentchat import AssistantAgent\n\nllm_config = {\n    \"config_list\": [{\"model\": \"gpt-4o\", \"api_key\": \"sk-xxx\"}],\n    \"seed\": 42,\n    \"temperature\": 0,\n}\n\nassistant = AssistantAgent(\n    name=\"assistant\",\n    system_message=\"You are a helpful assistant.\",\n    llm_config=llm_config,\n)",
      "language": "json"
    },
    {
      "code": "from autogen_agentchat.agents import AssistantAgent\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o\", api_key=\"sk-xxx\", seed=42, temperature=0)\n\nassistant = AssistantAgent(\n    name=\"assistant\",\n    system_message=\"You are a helpful assistant.\",\n    model_client=model_client,\n)",
      "language": "sql"
    },
    {
      "code": "from autogen_agentchat.agents import AssistantAgent\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o\", api_key=\"sk-xxx\", seed=42, temperature=0)\n\nassistant = AssistantAgent(\n    name=\"assistant\",\n    system_message=\"You are a helpful assistant.\",\n    model_client=model_client,\n)",
      "language": "sql"
    },
    {
      "code": "import asyncio\nfrom autogen_agentchat.messages import TextMessage\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_core import CancellationToken\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", seed=42, temperature=0)\n\n    assistant = AssistantAgent(\n        name=\"assistant\",\n        system_message=\"You are a helpful assistant.\",\n        model_client=model_client,\n    )\n\n    cancellation_token = CancellationToken()\n    response = await assistant.on_messages([TextMessage(content=\"Hello!\", source=\"user\")], cancellation_token)\n    print(response)\n\n    await model_client.close()\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom autogen_agentchat.messages import TextMessage\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_core import CancellationToken\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", seed=42, temperature=0)\n\n    assistant = AssistantAgent(\n        name=\"assistant\",\n        system_message=\"You are a helpful assistant.\",\n        model_client=model_client,\n    )\n\n    cancellation_token = CancellationToken()\n    response = await assistant.on_messages([TextMessage(content=\"Hello!\", source=\"user\")], cancellation_token)\n    print(response)\n\n    await model_client.close()\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom pathlib import Path\nfrom autogen_agentchat.messages import MultiModalMessage\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_core import CancellationToken, Image\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", seed=42, temperature=0)\n\n    assistant = AssistantAgent(\n        name=\"assistant\",\n        system_message=\"You are a helpful assistant.\",\n        model_client=model_client,\n    )\n\n    cancellation_token = CancellationToken()\n    message = MultiModalMessage(\n        content=[\"Here is an image:\", Image.from_file(Path(\"test.png\"))],\n        source=\"user\",\n    )\n    response = await assistant.on_messages([message], cancellation_token)\n    print(response)\n\n    await model_client.close()\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom pathlib import Path\nfrom autogen_agentchat.messages import MultiModalMessage\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_core import CancellationToken, Image\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", seed=42, temperature=0)\n\n    assistant = AssistantAgent(\n        name=\"assistant\",\n        system_message=\"You are a helpful assistant.\",\n        model_client=model_client,\n    )\n\n    cancellation_token = CancellationToken()\n    message = MultiModalMessage(\n        content=[\"Here is an image:\", Image.from_file(Path(\"test.png\"))],\n        source=\"user\",\n    )\n    response = await assistant.on_messages([message], cancellation_token)\n    print(response)\n\n    await model_client.close()\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "from autogen.agentchat import UserProxyAgent\n\nuser_proxy = UserProxyAgent(\n    name=\"user_proxy\",\n    human_input_mode=\"NEVER\",\n    max_consecutive_auto_reply=10,\n    code_execution_config=False,\n    llm_config=False,\n)",
      "language": "sql"
    },
    {
      "code": "from autogen.agentchat import UserProxyAgent\n\nuser_proxy = UserProxyAgent(\n    name=\"user_proxy\",\n    human_input_mode=\"NEVER\",\n    max_consecutive_auto_reply=10,\n    code_execution_config=False,\n    llm_config=False,\n)",
      "language": "sql"
    },
    {
      "code": "from autogen_agentchat.agents import UserProxyAgent\n\nuser_proxy = UserProxyAgent(\"user_proxy\")",
      "language": "sql"
    },
    {
      "code": "from autogen_agentchat.agents import UserProxyAgent\n\nuser_proxy = UserProxyAgent(\"user_proxy\")",
      "language": "sql"
    },
    {
      "code": "teachable_agent = ConversableAgent(\n    name=\"teachable_agent\",\n    llm_config=llm_config\n)\n\n# Instantiate a Teachability object. Its parameters are all optional.\nteachability = Teachability(\n    reset_db=False,\n    path_to_db_dir=\"./tmp/interactive/teachability_db\"\n)\n\nteachability.add_to_agent(teachable_agent)",
      "language": "markdown"
    },
    {
      "code": "teachable_agent = ConversableAgent(\n    name=\"teachable_agent\",\n    llm_config=llm_config\n)\n\n# Instantiate a Teachability object. Its parameters are all optional.\nteachability = Teachability(\n    reset_db=False,\n    path_to_db_dir=\"./tmp/interactive/teachability_db\"\n)\n\nteachability.add_to_agent(teachable_agent)",
      "language": "markdown"
    },
    {
      "code": "# ...\n# example of a ChromaDBVectorMemory class\nchroma_user_memory = ChromaDBVectorMemory(\n    config=PersistentChromaDBVectorMemoryConfig(\n        collection_name=\"preferences\",\n        persistence_path=os.path.join(str(Path.home()), \".chromadb_autogen\"),\n        k=2,  # Return top  k results\n        score_threshold=0.4,  # Minimum similarity score\n    )\n)\n\n# you can add logic such as a document indexer that adds content to the memory store\n\nassistant_agent = AssistantAgent(\n    name=\"assistant_agent\",\n    model_client=OpenAIChatCompletionClient(\n        model=\"gpt-4o\",\n    ),\n    tools=[get_weather],\n    memory=[chroma_user_memory],\n)",
      "language": "php"
    },
    {
      "code": "# ...\n# example of a ChromaDBVectorMemory class\nchroma_user_memory = ChromaDBVectorMemory(\n    config=PersistentChromaDBVectorMemoryConfig(\n        collection_name=\"preferences\",\n        persistence_path=os.path.join(str(Path.home()), \".chromadb_autogen\"),\n        k=2,  # Return top  k results\n        score_threshold=0.4,  # Minimum similarity score\n    )\n)\n\n# you can add logic such as a document indexer that adds content to the memory store\n\nassistant_agent = AssistantAgent(\n    name=\"assistant_agent\",\n    model_client=OpenAIChatCompletionClient(\n        model=\"gpt-4o\",\n    ),\n    tools=[get_weather],\n    memory=[chroma_user_memory],\n)",
      "language": "php"
    },
    {
      "code": "from typing import Any, Dict, List, Optional, Tuple, Union\nfrom autogen.agentchat import ConversableAgent\n\nllm_config = {\n    \"config_list\": [{\"model\": \"gpt-4o\", \"api_key\": \"sk-xxx\"}],\n    \"seed\": 42,\n    \"temperature\": 0,\n}\n\nconversable_agent = ConversableAgent(\n    name=\"conversable_agent\",\n    system_message=\"You are a helpful assistant.\",\n    llm_config=llm_config,\n    code_execution_config={\"work_dir\": \"coding\"},\n    human_input_mode=\"NEVER\",\n    max_consecutive_auto_reply=10,\n)\n\ndef reply_func(\n    recipient: ConversableAgent,\n    messages: Optional[List[Dict]] = None,\n    sender: Optional[Agent] = None,\n    config: Optional[Any] = None,\n) -> Tuple[bool, Union[str, Dict, None]]:\n    # Custom reply logic here\n    return True, \"Custom reply\"\n\n# Register the reply function\nconversable_agent.register_reply([ConversableAgent], reply_func, position=0)\n\n# NOTE: An async reply function will only be invoked with async send.",
      "language": "python"
    },
    {
      "code": "from typing import Any, Dict, List, Optional, Tuple, Union\nfrom autogen.agentchat import ConversableAgent\n\nllm_config = {\n    \"config_list\": [{\"model\": \"gpt-4o\", \"api_key\": \"sk-xxx\"}],\n    \"seed\": 42,\n    \"temperature\": 0,\n}\n\nconversable_agent = ConversableAgent(\n    name=\"conversable_agent\",\n    system_message=\"You are a helpful assistant.\",\n    llm_config=llm_config,\n    code_execution_config={\"work_dir\": \"coding\"},\n    human_input_mode=\"NEVER\",\n    max_consecutive_auto_reply=10,\n)\n\ndef reply_func(\n    recipient: ConversableAgent,\n    messages: Optional[List[Dict]] = None,\n    sender: Optional[Agent] = None,\n    config: Optional[Any] = None,\n) -> Tuple[bool, Union[str, Dict, None]]:\n    # Custom reply logic here\n    return True, \"Custom reply\"\n\n# Register the reply function\nconversable_agent.register_reply([ConversableAgent], reply_func, position=0)\n\n# NOTE: An async reply function will only be invoked with async send.",
      "language": "python"
    },
    {
      "code": "from typing import Sequence\nfrom autogen_core import CancellationToken\nfrom autogen_agentchat.agents import BaseChatAgent\nfrom autogen_agentchat.messages import TextMessage, BaseChatMessage\nfrom autogen_agentchat.base import Response\n\nclass CustomAgent(BaseChatAgent):\n    async def on_messages(self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) -> Response:\n        return Response(chat_message=TextMessage(content=\"Custom reply\", source=self.name))\n\n    async def on_reset(self, cancellation_token: CancellationToken) -> None:\n        pass\n\n    @property\n    def produced_message_types(self) -> Sequence[type[BaseChatMessage]]:\n        return (TextMessage,)",
      "language": "python"
    },
    {
      "code": "from typing import Sequence\nfrom autogen_core import CancellationToken\nfrom autogen_agentchat.agents import BaseChatAgent\nfrom autogen_agentchat.messages import TextMessage, BaseChatMessage\nfrom autogen_agentchat.base import Response\n\nclass CustomAgent(BaseChatAgent):\n    async def on_messages(self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) -> Response:\n        return Response(chat_message=TextMessage(content=\"Custom reply\", source=self.name))\n\n    async def on_reset(self, cancellation_token: CancellationToken) -> None:\n        pass\n\n    @property\n    def produced_message_types(self) -> Sequence[type[BaseChatMessage]]:\n        return (TextMessage,)",
      "language": "python"
    },
    {
      "code": "import asyncio\nimport json\nfrom autogen_agentchat.messages import TextMessage\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_core import CancellationToken\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", seed=42, temperature=0)\n\n    assistant = AssistantAgent(\n        name=\"assistant\",\n        system_message=\"You are a helpful assistant.\",\n        model_client=model_client,\n    )\n\n    cancellation_token = CancellationToken()\n    response = await assistant.on_messages([TextMessage(content=\"Hello!\", source=\"user\")], cancellation_token)\n    print(response)\n\n    # Save the state.\n    state = await assistant.save_state()\n\n    # (Optional) Write state to disk.\n    with open(\"assistant_state.json\", \"w\") as f:\n        json.dump(state, f)\n\n    # (Optional) Load it back from disk.\n    with open(\"assistant_state.json\", \"r\") as f:\n        state = json.load(f)\n        print(state) # Inspect the state, which contains the chat history.\n\n    # Carry on the chat.\n    response = await assistant.on_messages([TextMessage(content=\"Tell me a joke.\", source=\"user\")], cancellation_token)\n    print(response)\n\n    # Load the state, resulting the agent to revert to the previous state before the last message.\n    await assistant.load_state(state)\n\n    # Carry on the same chat again.\n    response = await assistant.on_messages([TextMessage(content=\"Tell me a joke.\", source=\"user\")], cancellation_token)\n    # Close the connection to the model client.\n    await model_client.close()\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nimport json\nfrom autogen_agentchat.messages import TextMessage\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_core import CancellationToken\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", seed=42, temperature=0)\n\n    assistant = AssistantAgent(\n        name=\"assistant\",\n        system_message=\"You are a helpful assistant.\",\n        model_client=model_client,\n    )\n\n    cancellation_token = CancellationToken()\n    response = await assistant.on_messages([TextMessage(content=\"Hello!\", source=\"user\")], cancellation_token)\n    print(response)\n\n    # Save the state.\n    state = await assistant.save_state()\n\n    # (Optional) Write state to disk.\n    with open(\"assistant_state.json\", \"w\") as f:\n        json.dump(state, f)\n\n    # (Optional) Load it back from disk.\n    with open(\"assistant_state.json\", \"r\") as f:\n        state = json.load(f)\n        print(state) # Inspect the state, which contains the chat history.\n\n    # Carry on the chat.\n    response = await assistant.on_messages([TextMessage(content=\"Tell me a joke.\", source=\"user\")], cancellation_token)\n    print(response)\n\n    # Load the state, resulting the agent to revert to the previous state before the last message.\n    await assistant.load_state(state)\n\n    # Carry on the same chat again.\n    response = await assistant.on_messages([TextMessage(content=\"Tell me a joke.\", source=\"user\")], cancellation_token)\n    # Close the connection to the model client.\n    await model_client.close()\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "from autogen.coding import LocalCommandLineCodeExecutor\nfrom autogen.agentchat import AssistantAgent, UserProxyAgent\n\nllm_config = {\n    \"config_list\": [{\"model\": \"gpt-4o\", \"api_key\": \"sk-xxx\"}],\n    \"seed\": 42,\n    \"temperature\": 0,\n}\n\nassistant = AssistantAgent(\n    name=\"assistant\",\n    system_message=\"You are a helpful assistant. Write all code in python. Reply only 'TERMINATE' if the task is done.\",\n    llm_config=llm_config,\n    is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n)\n\nuser_proxy = UserProxyAgent(\n    name=\"user_proxy\",\n    human_input_mode=\"NEVER\",\n    max_consecutive_auto_reply=10,\n    code_execution_config={\"code_executor\": LocalCommandLineCodeExecutor(work_dir=\"coding\")},\n    llm_config=False,\n    is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n)\n\nchat_result = user_proxy.initiate_chat(assistant, message=\"Write a python script to print 'Hello, world!'\")\n# Intermediate messages are printed to the console directly.\nprint(chat_result)",
      "language": "json"
    },
    {
      "code": "from autogen.coding import LocalCommandLineCodeExecutor\nfrom autogen.agentchat import AssistantAgent, UserProxyAgent\n\nllm_config = {\n    \"config_list\": [{\"model\": \"gpt-4o\", \"api_key\": \"sk-xxx\"}],\n    \"seed\": 42,\n    \"temperature\": 0,\n}\n\nassistant = AssistantAgent(\n    name=\"assistant\",\n    system_message=\"You are a helpful assistant. Write all code in python. Reply only 'TERMINATE' if the task is done.\",\n    llm_config=llm_config,\n    is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n)\n\nuser_proxy = UserProxyAgent(\n    name=\"user_proxy\",\n    human_input_mode=\"NEVER\",\n    max_consecutive_auto_reply=10,\n    code_execution_config={\"code_executor\": LocalCommandLineCodeExecutor(work_dir=\"coding\")},\n    llm_config=False,\n    is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n)\n\nchat_result = user_proxy.initiate_chat(assistant, message=\"Write a python script to print 'Hello, world!'\")\n# Intermediate messages are printed to the console directly.\nprint(chat_result)",
      "language": "json"
    },
    {
      "code": "import asyncio\nfrom autogen_agentchat.agents import AssistantAgent, CodeExecutorAgent\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.conditions import TextMentionTermination, MaxMessageTermination\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.code_executors.local import LocalCommandLineCodeExecutor\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", seed=42, temperature=0)\n\n    assistant = AssistantAgent(\n        name=\"assistant\",\n        system_message=\"You are a helpful assistant. Write all code in python. Reply only 'TERMINATE' if the task is done.\",\n        model_client=model_client,\n    )\n\n    code_executor = CodeExecutorAgent(\n        name=\"code_executor\",\n        code_executor=LocalCommandLineCodeExecutor(work_dir=\"coding\"),\n    )\n\n    # The termination condition is a combination of text termination and max message termination, either of which will cause the chat to terminate.\n    termination = TextMentionTermination(\"TERMINATE\") | MaxMessageTermination(10)\n\n    # The group chat will alternate between the assistant and the code executor.\n    group_chat = RoundRobinGroupChat([assistant, code_executor], termination_condition=termination)\n\n    # `run_stream` returns an async generator to stream the intermediate messages.\n    stream = group_chat.run_stream(task=\"Write a python script to print 'Hello, world!'\")\n    # `Console` is a simple UI to display the stream.\n    await Console(stream)\n    \n    # Close the connection to the model client.\n    await model_client.close()\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom autogen_agentchat.agents import AssistantAgent, CodeExecutorAgent\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.conditions import TextMentionTermination, MaxMessageTermination\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.code_executors.local import LocalCommandLineCodeExecutor\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", seed=42, temperature=0)\n\n    assistant = AssistantAgent(\n        name=\"assistant\",\n        system_message=\"You are a helpful assistant. Write all code in python. Reply only 'TERMINATE' if the task is done.\",\n        model_client=model_client,\n    )\n\n    code_executor = CodeExecutorAgent(\n        name=\"code_executor\",\n        code_executor=LocalCommandLineCodeExecutor(work_dir=\"coding\"),\n    )\n\n    # The termination condition is a combination of text termination and max message termination, either of which will cause the chat to terminate.\n    termination = TextMentionTermination(\"TERMINATE\") | MaxMessageTermination(10)\n\n    # The group chat will alternate between the assistant and the code executor.\n    group_chat = RoundRobinGroupChat([assistant, code_executor], termination_condition=termination)\n\n    # `run_stream` returns an async generator to stream the intermediate messages.\n    stream = group_chat.run_stream(task=\"Write a python script to print 'Hello, world!'\")\n    # `Console` is a simple UI to display the stream.\n    await Console(stream)\n    \n    # Close the connection to the model client.\n    await model_client.close()\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "from autogen.agentchat import AssistantAgent, UserProxyAgent, register_function\n\nllm_config = {\n    \"config_list\": [{\"model\": \"gpt-4o\", \"api_key\": \"sk-xxx\"}],\n    \"seed\": 42,\n    \"temperature\": 0,\n}\n\ntool_caller = AssistantAgent(\n    name=\"tool_caller\",\n    system_message=\"You are a helpful assistant. You can call tools to help user.\",\n    llm_config=llm_config,\n    max_consecutive_auto_reply=1, # Set to 1 so that we return to the application after each assistant reply as we are building a chatbot.\n)\n\ntool_executor = UserProxyAgent(\n    name=\"tool_executor\",\n    human_input_mode=\"NEVER\",\n    code_execution_config=False,\n    llm_config=False,\n)\n\ndef get_weather(city: str) -> str:\n    return f\"The weather in {city} is 72 degree and sunny.\"\n\n# Register the tool function to the tool caller and executor.\nregister_function(get_weather, caller=tool_caller, executor=tool_executor)\n\nwhile True:\n    user_input = input(\"User: \")\n    if user_input == \"exit\":\n        break\n    chat_result = tool_executor.initiate_chat(\n        tool_caller,\n        message=user_input,\n        summary_method=\"reflection_with_llm\", # To let the model reflect on the tool use, set to \"last_msg\" to return the tool call result directly.\n    )\n    print(\"Assistant:\", chat_result.summary)",
      "language": "python"
    },
    {
      "code": "from autogen.agentchat import AssistantAgent, UserProxyAgent, register_function\n\nllm_config = {\n    \"config_list\": [{\"model\": \"gpt-4o\", \"api_key\": \"sk-xxx\"}],\n    \"seed\": 42,\n    \"temperature\": 0,\n}\n\ntool_caller = AssistantAgent(\n    name=\"tool_caller\",\n    system_message=\"You are a helpful assistant. You can call tools to help user.\",\n    llm_config=llm_config,\n    max_consecutive_auto_reply=1, # Set to 1 so that we return to the application after each assistant reply as we are building a chatbot.\n)\n\ntool_executor = UserProxyAgent(\n    name=\"tool_executor\",\n    human_input_mode=\"NEVER\",\n    code_execution_config=False,\n    llm_config=False,\n)\n\ndef get_weather(city: str) -> str:\n    return f\"The weather in {city} is 72 degree and sunny.\"\n\n# Register the tool function to the tool caller and executor.\nregister_function(get_weather, caller=tool_caller, executor=tool_executor)\n\nwhile True:\n    user_input = input(\"User: \")\n    if user_input == \"exit\":\n        break\n    chat_result = tool_executor.initiate_chat(\n        tool_caller,\n        message=user_input,\n        summary_method=\"reflection_with_llm\", # To let the model reflect on the tool use, set to \"last_msg\" to return the tool call result directly.\n    )\n    print(\"Assistant:\", chat_result.summary)",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom autogen_core import CancellationToken\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.messages import TextMessage\n\ndef get_weather(city: str) -> str: # Async tool is possible too.\n    return f\"The weather in {city} is 72 degree and sunny.\"\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", seed=42, temperature=0)\n    assistant = AssistantAgent(\n        name=\"assistant\",\n        system_message=\"You are a helpful assistant. You can call tools to help user.\",\n        model_client=model_client,\n        tools=[get_weather],\n        reflect_on_tool_use=True, # Set to True to have the model reflect on the tool use, set to False to return the tool call result directly.\n    )\n    while True:\n        user_input = input(\"User: \")\n        if user_input == \"exit\":\n            break\n        response = await assistant.on_messages([TextMessage(content=user_input, source=\"user\")], CancellationToken())\n        print(\"Assistant:\", response.chat_message.to_text())\n    await model_client.close()\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom autogen_core import CancellationToken\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.messages import TextMessage\n\ndef get_weather(city: str) -> str: # Async tool is possible too.\n    return f\"The weather in {city} is 72 degree and sunny.\"\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", seed=42, temperature=0)\n    assistant = AssistantAgent(\n        name=\"assistant\",\n        system_message=\"You are a helpful assistant. You can call tools to help user.\",\n        model_client=model_client,\n        tools=[get_weather],\n        reflect_on_tool_use=True, # Set to True to have the model reflect on the tool use, set to False to return the tool call result directly.\n    )\n    while True:\n        user_input = input(\"User: \")\n        if user_input == \"exit\":\n            break\n        response = await assistant.on_messages([TextMessage(content=user_input, source=\"user\")], CancellationToken())\n        print(\"Assistant:\", response.chat_message.to_text())\n    await model_client.close()\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "chat_result = tool_executor.initiate_chat(\n    tool_caller,\n    message=user_input,\n    summary_method=\"reflection_with_llm\",\n)\nprint(chat_result.summary) # Get LLM-reflected summary of the chat.\nprint(chat_result.chat_history) # Get the chat history.\nprint(chat_result.cost) # Get the cost of the chat.\nprint(chat_result.human_input) # Get the human input solicited by the chat.",
      "language": "unknown"
    },
    {
      "code": "chat_result = tool_executor.initiate_chat(\n    tool_caller,\n    message=user_input,\n    summary_method=\"reflection_with_llm\",\n)\nprint(chat_result.summary) # Get LLM-reflected summary of the chat.\nprint(chat_result.chat_history) # Get the chat history.\nprint(chat_result.cost) # Get the cost of the chat.\nprint(chat_result.human_input) # Get the human input solicited by the chat.",
      "language": "unknown"
    },
    {
      "code": "from typing import Any, Dict, List, Literal\n\nfrom autogen_agentchat.messages import (\n    BaseAgentEvent,\n    BaseChatMessage,\n    HandoffMessage,\n    MultiModalMessage,\n    StopMessage,\n    TextMessage,\n    ToolCallExecutionEvent,\n    ToolCallRequestEvent,\n    ToolCallSummaryMessage,\n)\nfrom autogen_core import FunctionCall, Image\nfrom autogen_core.models import FunctionExecutionResult\n\n\ndef convert_to_v02_message(\n    message: BaseAgentEvent | BaseChatMessage,\n    role: Literal[\"assistant\", \"user\", \"tool\"],\n    image_detail: Literal[\"auto\", \"high\", \"low\"] = \"auto\",\n) -> Dict[str, Any]:\n    \"\"\"Convert a v0.4 AgentChat message to a v0.2 message.\n\n    Args:\n        message (BaseAgentEvent | BaseChatMessage): The message to convert.\n        role (Literal[\"assistant\", \"user\", \"tool\"]): The role of the message.\n        image_detail (Literal[\"auto\", \"high\", \"low\"], optional): The detail level of image content in multi-modal message. Defaults to \"auto\".\n\n    Returns:\n        Dict[str, Any]: The converted AutoGen v0.2 message.\n    \"\"\"\n    v02_message: Dict[str, Any] = {}\n    if isinstance(message, TextMessage | StopMessage | HandoffMessage | ToolCallSummaryMessage):\n        v02_message = {\"content\": message.content, \"role\": role, \"name\": message.source}\n    elif isinstance(message, MultiModalMessage):\n        v02_message = {\"content\": [], \"role\": role, \"name\": message.source}\n        for modal in message.content:\n            if isinstance(modal, str):\n                v02_message[\"content\"].append({\"type\": \"text\", \"text\": modal})\n            elif isinstance(modal, Image):\n                v02_message[\"content\"].append(modal.to_openai_format(detail=image_detail))\n            else:\n                raise ValueError(f\"Invalid multimodal message content: {modal}\")\n    elif isinstance(message, ToolCallRequestEvent):\n        v02_message = {\"tool_calls\": [], \"role\": \"assistant\", \"content\": None, \"name\": message.source}\n        for tool_call in message.content:\n            v02_message[\"tool_calls\"].append(\n                {\n                    \"id\": tool_call.id,\n                    \"type\": \"function\",\n                    \"function\": {\"name\": tool_call.name, \"args\": tool_call.arguments},\n                }\n            )\n    elif isinstance(message, ToolCallExecutionEvent):\n        tool_responses: List[Dict[str, str]] = []\n        for tool_result in message.content:\n            tool_responses.append(\n                {\n                    \"tool_call_id\": tool_result.call_id,\n                    \"role\": \"tool\",\n                    \"content\": tool_result.content,\n                }\n            )\n        content = \"\\n\\n\".join([response[\"content\"] for response in tool_responses])\n        v02_message = {\"tool_responses\": tool_responses, \"role\": \"tool\", \"content\": content}\n    else:\n        raise ValueError(f\"Invalid message type: {type(message)}\")\n    return v02_message\n\n\ndef convert_to_v04_message(message: Dict[str, Any]) -> BaseAgentEvent | BaseChatMessage:\n    \"\"\"Convert a v0.2 message to a v0.4 AgentChat message.\"\"\"\n    if \"tool_calls\" in message:\n        tool_calls: List[FunctionCall] = []\n        for tool_call in message[\"tool_calls\"]:\n            tool_calls.append(\n                FunctionCall(\n                    id=tool_call[\"id\"],\n                    name=tool_call[\"function\"][\"name\"],\n                    arguments=tool_call[\"function\"][\"args\"],\n                )\n            )\n        return ToolCallRequestEvent(source=message[\"name\"], content=tool_calls)\n    elif \"tool_responses\" in message:\n        tool_results: List[FunctionExecutionResult] = []\n        for tool_response in message[\"tool_responses\"]:\n            tool_results.append(\n                FunctionExecutionResult(\n                    call_id=tool_response[\"tool_call_id\"],\n                    content=tool_response[\"content\"],\n                    is_error=False,\n                    name=tool_response[\"name\"],\n                )\n            )\n        return ToolCallExecutionEvent(source=\"tools\", content=tool_results)\n    elif isinstance(message[\"content\"], list):\n        content: List[str | Image] = []\n        for modal in message[\"content\"]:  # type: ignore\n            if modal[\"type\"] == \"text\":  # type: ignore\n                content.append(modal[\"text\"])  # type: ignore\n            else:\n                content.append(Image.from_uri(modal[\"image_url\"][\"url\"]))  # type: ignore\n        return MultiModalMessage(content=content, source=message[\"name\"])\n    elif isinstance(message[\"content\"], str):\n        return TextMessage(content=message[\"content\"], source=message[\"name\"])\n    else:\n        raise ValueError(f\"Unable to convert message: {message}\")",
      "language": "python"
    },
    {
      "code": "from typing import Any, Dict, List, Literal\n\nfrom autogen_agentchat.messages import (\n    BaseAgentEvent,\n    BaseChatMessage,\n    HandoffMessage,\n    MultiModalMessage,\n    StopMessage,\n    TextMessage,\n    ToolCallExecutionEvent,\n    ToolCallRequestEvent,\n    ToolCallSummaryMessage,\n)\nfrom autogen_core import FunctionCall, Image\nfrom autogen_core.models import FunctionExecutionResult\n\n\ndef convert_to_v02_message(\n    message: BaseAgentEvent | BaseChatMessage,\n    role: Literal[\"assistant\", \"user\", \"tool\"],\n    image_detail: Literal[\"auto\", \"high\", \"low\"] = \"auto\",\n) -> Dict[str, Any]:\n    \"\"\"Convert a v0.4 AgentChat message to a v0.2 message.\n\n    Args:\n        message (BaseAgentEvent | BaseChatMessage): The message to convert.\n        role (Literal[\"assistant\", \"user\", \"tool\"]): The role of the message.\n        image_detail (Literal[\"auto\", \"high\", \"low\"], optional): The detail level of image content in multi-modal message. Defaults to \"auto\".\n\n    Returns:\n        Dict[str, Any]: The converted AutoGen v0.2 message.\n    \"\"\"\n    v02_message: Dict[str, Any] = {}\n    if isinstance(message, TextMessage | StopMessage | HandoffMessage | ToolCallSummaryMessage):\n        v02_message = {\"content\": message.content, \"role\": role, \"name\": message.source}\n    elif isinstance(message, MultiModalMessage):\n        v02_message = {\"content\": [], \"role\": role, \"name\": message.source}\n        for modal in message.content:\n            if isinstance(modal, str):\n                v02_message[\"content\"].append({\"type\": \"text\", \"text\": modal})\n            elif isinstance(modal, Image):\n                v02_message[\"content\"].append(modal.to_openai_format(detail=image_detail))\n            else:\n                raise ValueError(f\"Invalid multimodal message content: {modal}\")\n    elif isinstance(message, ToolCallRequestEvent):\n        v02_message = {\"tool_calls\": [], \"role\": \"assistant\", \"content\": None, \"name\": message.source}\n        for tool_call in message.content:\n            v02_message[\"tool_calls\"].append(\n                {\n                    \"id\": tool_call.id,\n                    \"type\": \"function\",\n                    \"function\": {\"name\": tool_call.name, \"args\": tool_call.arguments},\n                }\n            )\n    elif isinstance(message, ToolCallExecutionEvent):\n        tool_responses: List[Dict[str, str]] = []\n        for tool_result in message.content:\n            tool_responses.append(\n                {\n                    \"tool_call_id\": tool_result.call_id,\n                    \"role\": \"tool\",\n                    \"content\": tool_result.content,\n                }\n            )\n        content = \"\\n\\n\".join([response[\"content\"] for response in tool_responses])\n        v02_message = {\"tool_responses\": tool_responses, \"role\": \"tool\", \"content\": content}\n    else:\n        raise ValueError(f\"Invalid message type: {type(message)}\")\n    return v02_message\n\n\ndef convert_to_v04_message(message: Dict[str, Any]) -> BaseAgentEvent | BaseChatMessage:\n    \"\"\"Convert a v0.2 message to a v0.4 AgentChat message.\"\"\"\n    if \"tool_calls\" in message:\n        tool_calls: List[FunctionCall] = []\n        for tool_call in message[\"tool_calls\"]:\n            tool_calls.append(\n                FunctionCall(\n                    id=tool_call[\"id\"],\n                    name=tool_call[\"function\"][\"name\"],\n                    arguments=tool_call[\"function\"][\"args\"],\n                )\n            )\n        return ToolCallRequestEvent(source=message[\"name\"], content=tool_calls)\n    elif \"tool_responses\" in message:\n        tool_results: List[FunctionExecutionResult] = []\n        for tool_response in message[\"tool_responses\"]:\n            tool_results.append(\n                FunctionExecutionResult(\n                    call_id=tool_response[\"tool_call_id\"],\n                    content=tool_response[\"content\"],\n                    is_error=False,\n                    name=tool_response[\"name\"],\n                )\n            )\n        return ToolCallExecutionEvent(source=\"tools\", content=tool_results)\n    elif isinstance(message[\"content\"], list):\n        content: List[str | Image] = []\n        for modal in message[\"content\"]:  # type: ignore\n            if modal[\"type\"] == \"text\":  # type: ignore\n                content.append(modal[\"text\"])  # type: ignore\n            else:\n                content.append(Image.from_uri(modal[\"image_url\"][\"url\"]))  # type: ignore\n        return MultiModalMessage(content=content, source=message[\"name\"])\n    elif isinstance(message[\"content\"], str):\n        return TextMessage(content=message[\"content\"], source=message[\"name\"])\n    else:\n        raise ValueError(f\"Unable to convert message: {message}\")",
      "language": "python"
    },
    {
      "code": "from autogen.agentchat import AssistantAgent, GroupChat, GroupChatManager\n\nllm_config = {\n    \"config_list\": [{\"model\": \"gpt-4o\", \"api_key\": \"sk-xxx\"}],\n    \"seed\": 42,\n    \"temperature\": 0,\n}\n\nwriter = AssistantAgent(\n    name=\"writer\",\n    description=\"A writer.\",\n    system_message=\"You are a writer.\",\n    llm_config=llm_config,\n    is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"APPROVE\"),\n)\n\ncritic = AssistantAgent(\n    name=\"critic\",\n    description=\"A critic.\",\n    system_message=\"You are a critic, provide feedback on the writing. Reply only 'APPROVE' if the task is done.\",\n    llm_config=llm_config,\n)\n\n# Create a group chat with the writer and critic.\ngroupchat = GroupChat(agents=[writer, critic], messages=[], max_round=12)\n\n# Create a group chat manager to manage the group chat, use round-robin selection method.\nmanager = GroupChatManager(groupchat=groupchat, llm_config=llm_config, speaker_selection_method=\"round_robin\")\n\n# Initiate the chat with the editor, intermediate messages are printed to the console directly.\nresult = editor.initiate_chat(\n    manager,\n    message=\"Write a short story about a robot that discovers it has feelings.\",\n)\nprint(result.summary)",
      "language": "json"
    },
    {
      "code": "from autogen.agentchat import AssistantAgent, GroupChat, GroupChatManager\n\nllm_config = {\n    \"config_list\": [{\"model\": \"gpt-4o\", \"api_key\": \"sk-xxx\"}],\n    \"seed\": 42,\n    \"temperature\": 0,\n}\n\nwriter = AssistantAgent(\n    name=\"writer\",\n    description=\"A writer.\",\n    system_message=\"You are a writer.\",\n    llm_config=llm_config,\n    is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"APPROVE\"),\n)\n\ncritic = AssistantAgent(\n    name=\"critic\",\n    description=\"A critic.\",\n    system_message=\"You are a critic, provide feedback on the writing. Reply only 'APPROVE' if the task is done.\",\n    llm_config=llm_config,\n)\n\n# Create a group chat with the writer and critic.\ngroupchat = GroupChat(agents=[writer, critic], messages=[], max_round=12)\n\n# Create a group chat manager to manage the group chat, use round-robin selection method.\nmanager = GroupChatManager(groupchat=groupchat, llm_config=llm_config, speaker_selection_method=\"round_robin\")\n\n# Initiate the chat with the editor, intermediate messages are printed to the console directly.\nresult = editor.initiate_chat(\n    manager,\n    message=\"Write a short story about a robot that discovers it has feelings.\",\n)\nprint(result.summary)",
      "language": "json"
    },
    {
      "code": "import asyncio\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.conditions import TextMentionTermination\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", seed=42, temperature=0)\n\n    writer = AssistantAgent(\n        name=\"writer\",\n        description=\"A writer.\",\n        system_message=\"You are a writer.\",\n        model_client=model_client,\n    )\n\n    critic = AssistantAgent(\n        name=\"critic\",\n        description=\"A critic.\",\n        system_message=\"You are a critic, provide feedback on the writing. Reply only 'APPROVE' if the task is done.\",\n        model_client=model_client,\n    )\n\n    # The termination condition is a text termination, which will cause the chat to terminate when the text \"APPROVE\" is received.\n    termination = TextMentionTermination(\"APPROVE\")\n\n    # The group chat will alternate between the writer and the critic.\n    group_chat = RoundRobinGroupChat([writer, critic], termination_condition=termination, max_turns=12)\n\n    # `run_stream` returns an async generator to stream the intermediate messages.\n    stream = group_chat.run_stream(task=\"Write a short story about a robot that discovers it has feelings.\")\n    # `Console` is a simple UI to display the stream.\n    await Console(stream)\n    # Close the connection to the model client.\n    await model_client.close()\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.conditions import TextMentionTermination\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", seed=42, temperature=0)\n\n    writer = AssistantAgent(\n        name=\"writer\",\n        description=\"A writer.\",\n        system_message=\"You are a writer.\",\n        model_client=model_client,\n    )\n\n    critic = AssistantAgent(\n        name=\"critic\",\n        description=\"A critic.\",\n        system_message=\"You are a critic, provide feedback on the writing. Reply only 'APPROVE' if the task is done.\",\n        model_client=model_client,\n    )\n\n    # The termination condition is a text termination, which will cause the chat to terminate when the text \"APPROVE\" is received.\n    termination = TextMentionTermination(\"APPROVE\")\n\n    # The group chat will alternate between the writer and the critic.\n    group_chat = RoundRobinGroupChat([writer, critic], termination_condition=termination, max_turns=12)\n\n    # `run_stream` returns an async generator to stream the intermediate messages.\n    stream = group_chat.run_stream(task=\"Write a short story about a robot that discovers it has feelings.\")\n    # `Console` is a simple UI to display the stream.\n    await Console(stream)\n    # Close the connection to the model client.\n    await model_client.close()\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nimport json\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.conditions import TextMentionTermination\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\ndef create_team(model_client : OpenAIChatCompletionClient) -> RoundRobinGroupChat:\n    writer = AssistantAgent(\n        name=\"writer\",\n        description=\"A writer.\",\n        system_message=\"You are a writer.\",\n        model_client=model_client,\n    )\n\n    critic = AssistantAgent(\n        name=\"critic\",\n        description=\"A critic.\",\n        system_message=\"You are a critic, provide feedback on the writing. Reply only 'APPROVE' if the task is done.\",\n        model_client=model_client,\n    )\n\n    # The termination condition is a text termination, which will cause the chat to terminate when the text \"APPROVE\" is received.\n    termination = TextMentionTermination(\"APPROVE\")\n\n    # The group chat will alternate between the writer and the critic.\n    group_chat = RoundRobinGroupChat([writer, critic], termination_condition=termination)\n\n    return group_chat\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", seed=42, temperature=0)\n    # Create team.\n    group_chat = create_team(model_client)\n\n    # `run_stream` returns an async generator to stream the intermediate messages.\n    stream = group_chat.run_stream(task=\"Write a short story about a robot that discovers it has feelings.\")\n    # `Console` is a simple UI to display the stream.\n    await Console(stream)\n\n    # Save the state of the group chat and all participants.\n    state = await group_chat.save_state()\n    with open(\"group_chat_state.json\", \"w\") as f:\n        json.dump(state, f)\n\n    # Create a new team with the same participants configuration.\n    group_chat = create_team(model_client)\n\n    # Load the state of the group chat and all participants.\n    with open(\"group_chat_state.json\", \"r\") as f:\n        state = json.load(f)\n    await group_chat.load_state(state)\n\n    # Resume the chat.\n    stream = group_chat.run_stream(task=\"Translate the story into Chinese.\")\n    await Console(stream)\n\n    # Close the connection to the model client.\n    await model_client.close()\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nimport json\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.conditions import TextMentionTermination\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\ndef create_team(model_client : OpenAIChatCompletionClient) -> RoundRobinGroupChat:\n    writer = AssistantAgent(\n        name=\"writer\",\n        description=\"A writer.\",\n        system_message=\"You are a writer.\",\n        model_client=model_client,\n    )\n\n    critic = AssistantAgent(\n        name=\"critic\",\n        description=\"A critic.\",\n        system_message=\"You are a critic, provide feedback on the writing. Reply only 'APPROVE' if the task is done.\",\n        model_client=model_client,\n    )\n\n    # The termination condition is a text termination, which will cause the chat to terminate when the text \"APPROVE\" is received.\n    termination = TextMentionTermination(\"APPROVE\")\n\n    # The group chat will alternate between the writer and the critic.\n    group_chat = RoundRobinGroupChat([writer, critic], termination_condition=termination)\n\n    return group_chat\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", seed=42, temperature=0)\n    # Create team.\n    group_chat = create_team(model_client)\n\n    # `run_stream` returns an async generator to stream the intermediate messages.\n    stream = group_chat.run_stream(task=\"Write a short story about a robot that discovers it has feelings.\")\n    # `Console` is a simple UI to display the stream.\n    await Console(stream)\n\n    # Save the state of the group chat and all participants.\n    state = await group_chat.save_state()\n    with open(\"group_chat_state.json\", \"w\") as f:\n        json.dump(state, f)\n\n    # Create a new team with the same participants configuration.\n    group_chat = create_team(model_client)\n\n    # Load the state of the group chat and all participants.\n    with open(\"group_chat_state.json\", \"r\") as f:\n        state = json.load(f)\n    await group_chat.load_state(state)\n\n    # Resume the chat.\n    stream = group_chat.run_stream(task=\"Translate the story into Chinese.\")\n    await Console(stream)\n\n    # Close the connection to the model client.\n    await model_client.close()\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom typing import Sequence\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination\nfrom autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage\nfrom autogen_agentchat.teams import SelectorGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n# Note: This example uses mock tools instead of real APIs for demonstration purposes\ndef search_web_tool(query: str) -> str:\n    if \"2006-2007\" in query:\n        return \"\"\"Here are the total points scored by Miami Heat players in the 2006-2007 season:\n        Udonis Haslem: 844 points\n        Dwayne Wade: 1397 points\n        James Posey: 550 points\n        ...\n        \"\"\"\n    elif \"2007-2008\" in query:\n        return \"The number of total rebounds for Dwayne Wade in the Miami Heat season 2007-2008 is 214.\"\n    elif \"2008-2009\" in query:\n        return \"The number of total rebounds for Dwayne Wade in the Miami Heat season 2008-2009 is 398.\"\n    return \"No data found.\"\n\n\ndef percentage_change_tool(start: float, end: float) -> float:\n    return ((end - start) / start) * 100\n\ndef create_team(model_client : OpenAIChatCompletionClient) -> SelectorGroupChat:\n    planning_agent = AssistantAgent(\n        \"PlanningAgent\",\n        description=\"An agent for planning tasks, this agent should be the first to engage when given a new task.\",\n        model_client=model_client,\n        system_message=\"\"\"\n        You are a planning agent.\n        Your job is to break down complex tasks into smaller, manageable subtasks.\n        Your team members are:\n            Web search agent: Searches for information\n            Data analyst: Performs calculations\n\n        You only plan and delegate tasks - you do not execute them yourself.\n\n        When assigning tasks, use this format:\n        1. <agent> : <task>\n\n        After all tasks are complete, summarize the findings and end with \"TERMINATE\".\n        \"\"\",\n    )\n\n    web_search_agent = AssistantAgent(\n        \"WebSearchAgent\",\n        description=\"A web search agent.\",\n        tools=[search_web_tool],\n        model_client=model_client,\n        system_message=\"\"\"\n        You are a web search agent.\n        Your only tool is search_tool - use it to find information.\n        You make only one search call at a time.\n        Once you have the results, you never do calculations based on them.\n        \"\"\",\n    )\n\n    data_analyst_agent = AssistantAgent(\n        \"DataAnalystAgent\",\n        description=\"A data analyst agent. Useful for performing calculations.\",\n        model_client=model_client,\n        tools=[percentage_change_tool],\n        system_message=\"\"\"\n        You are a data analyst.\n        Given the tasks you have been assigned, you should analyze the data and provide results using the tools provided.\n        \"\"\",\n    )\n\n    # The termination condition is a combination of text mention termination and max message termination.\n    text_mention_termination = TextMentionTermination(\"TERMINATE\")\n    max_messages_termination = MaxMessageTermination(max_messages=25)\n    termination = text_mention_termination | max_messages_termination\n\n    # The selector function is a function that takes the current message thread of the group chat\n    # and returns the next speaker's name. If None is returned, the LLM-based selection method will be used.\n    def selector_func(messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> str | None:\n        if messages[-1].source != planning_agent.name:\n            return planning_agent.name # Always return to the planning agent after the other agents have spoken.\n        return None\n\n    team = SelectorGroupChat(\n        [planning_agent, web_search_agent, data_analyst_agent],\n        model_client=OpenAIChatCompletionClient(model=\"gpt-4o-mini\"), # Use a smaller model for the selector.\n        termination_condition=termination,\n        selector_func=selector_func,\n    )\n    return team\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n    team = create_team(model_client)\n    task = \"Who was the Miami Heat player with the highest points in the 2006-2007 season, and what was the percentage change in his total rebounds between the 2007-2008 and 2008-2009 seasons?\"\n    await Console(team.run_stream(task=task))\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom typing import Sequence\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination\nfrom autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage\nfrom autogen_agentchat.teams import SelectorGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n# Note: This example uses mock tools instead of real APIs for demonstration purposes\ndef search_web_tool(query: str) -> str:\n    if \"2006-2007\" in query:\n        return \"\"\"Here are the total points scored by Miami Heat players in the 2006-2007 season:\n        Udonis Haslem: 844 points\n        Dwayne Wade: 1397 points\n        James Posey: 550 points\n        ...\n        \"\"\"\n    elif \"2007-2008\" in query:\n        return \"The number of total rebounds for Dwayne Wade in the Miami Heat season 2007-2008 is 214.\"\n    elif \"2008-2009\" in query:\n        return \"The number of total rebounds for Dwayne Wade in the Miami Heat season 2008-2009 is 398.\"\n    return \"No data found.\"\n\n\ndef percentage_change_tool(start: float, end: float) -> float:\n    return ((end - start) / start) * 100\n\ndef create_team(model_client : OpenAIChatCompletionClient) -> SelectorGroupChat:\n    planning_agent = AssistantAgent(\n        \"PlanningAgent\",\n        description=\"An agent for planning tasks, this agent should be the first to engage when given a new task.\",\n        model_client=model_client,\n        system_message=\"\"\"\n        You are a planning agent.\n        Your job is to break down complex tasks into smaller, manageable subtasks.\n        Your team members are:\n            Web search agent: Searches for information\n            Data analyst: Performs calculations\n\n        You only plan and delegate tasks - you do not execute them yourself.\n\n        When assigning tasks, use this format:\n        1. <agent> : <task>\n\n        After all tasks are complete, summarize the findings and end with \"TERMINATE\".\n        \"\"\",\n    )\n\n    web_search_agent = AssistantAgent(\n        \"WebSearchAgent\",\n        description=\"A web search agent.\",\n        tools=[search_web_tool],\n        model_client=model_client,\n        system_message=\"\"\"\n        You are a web search agent.\n        Your only tool is search_tool - use it to find information.\n        You make only one search call at a time.\n        Once you have the results, you never do calculations based on them.\n        \"\"\",\n    )\n\n    data_analyst_agent = AssistantAgent(\n        \"DataAnalystAgent\",\n        description=\"A data analyst agent. Useful for performing calculations.\",\n        model_client=model_client,\n        tools=[percentage_change_tool],\n        system_message=\"\"\"\n        You are a data analyst.\n        Given the tasks you have been assigned, you should analyze the data and provide results using the tools provided.\n        \"\"\",\n    )\n\n    # The termination condition is a combination of text mention termination and max message termination.\n    text_mention_termination = TextMentionTermination(\"TERMINATE\")\n    max_messages_termination = MaxMessageTermination(max_messages=25)\n    termination = text_mention_termination | max_messages_termination\n\n    # The selector function is a function that takes the current message thread of the group chat\n    # and returns the next speaker's name. If None is returned, the LLM-based selection method will be used.\n    def selector_func(messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> str | None:\n        if messages[-1].source != planning_agent.name:\n            return planning_agent.name # Always return to the planning agent after the other agents have spoken.\n        return None\n\n    team = SelectorGroupChat(\n        [planning_agent, web_search_agent, data_analyst_agent],\n        model_client=OpenAIChatCompletionClient(model=\"gpt-4o-mini\"), # Use a smaller model for the selector.\n        termination_condition=termination,\n        selector_func=selector_func,\n    )\n    return team\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n    team = create_team(model_client)\n    task = \"Who was the Miami Heat player with the highest points in the 2006-2007 season, and what was the percentage change in his total rebounds between the 2007-2008 and 2008-2009 seasons?\"\n    await Console(team.run_stream(task=task))\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom typing import Sequence\nfrom autogen_core import CancellationToken\nfrom autogen_agentchat.agents import BaseChatAgent\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.messages import TextMessage, BaseChatMessage\nfrom autogen_agentchat.base import Response\n\nclass CountingAgent(BaseChatAgent):\n    \"\"\"An agent that returns a new number by adding 1 to the last number in the input messages.\"\"\"\n    async def on_messages(self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) -> Response:\n        if len(messages) == 0:\n            last_number = 0 # Start from 0 if no messages are given.\n        else:\n            assert isinstance(messages[-1], TextMessage)\n            last_number = int(messages[-1].content) # Otherwise, start from the last number.\n        return Response(chat_message=TextMessage(content=str(last_number + 1), source=self.name))\n\n    async def on_reset(self, cancellation_token: CancellationToken) -> None:\n        pass\n\n    @property\n    def produced_message_types(self) -> Sequence[type[BaseChatMessage]]:\n        return (TextMessage,)\n\nclass NestedCountingAgent(BaseChatAgent):\n    \"\"\"An agent that increments the last number in the input messages\n    multiple times using a nested counting team.\"\"\"\n    def __init__(self, name: str, counting_team: RoundRobinGroupChat) -> None:\n        super().__init__(name, description=\"An agent that counts numbers.\")\n        self._counting_team = counting_team\n\n    async def on_messages(self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) -> Response:\n        # Run the inner team with the given messages and returns the last message produced by the team.\n        result = await self._counting_team.run(task=messages, cancellation_token=cancellation_token)\n        # To stream the inner messages, implement `on_messages_stream` and use that to implement `on_messages`.\n        assert isinstance(result.messages[-1], TextMessage)\n        return Response(chat_message=result.messages[-1], inner_messages=result.messages[len(messages):-1])\n\n    async def on_reset(self, cancellation_token: CancellationToken) -> None:\n        # Reset the inner team.\n        await self._counting_team.reset()\n\n    @property\n    def produced_message_types(self) -> Sequence[type[BaseChatMessage]]:\n        return (TextMessage,)\n\nasync def main() -> None:\n    # Create a team of two counting agents as the inner team.\n    counting_agent_1 = CountingAgent(\"counting_agent_1\", description=\"An agent that counts numbers.\")\n    counting_agent_2 = CountingAgent(\"counting_agent_2\", description=\"An agent that counts numbers.\")\n    counting_team = RoundRobinGroupChat([counting_agent_1, counting_agent_2], max_turns=5)\n    # Create a nested counting agent that takes the inner team as a parameter.\n    nested_counting_agent = NestedCountingAgent(\"nested_counting_agent\", counting_team)\n    # Run the nested counting agent with a message starting from 1.\n    response = await nested_counting_agent.on_messages([TextMessage(content=\"1\", source=\"user\")], CancellationToken())\n    assert response.inner_messages is not None\n    for message in response.inner_messages:\n        print(message)\n    print(response.chat_message)\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom typing import Sequence\nfrom autogen_core import CancellationToken\nfrom autogen_agentchat.agents import BaseChatAgent\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.messages import TextMessage, BaseChatMessage\nfrom autogen_agentchat.base import Response\n\nclass CountingAgent(BaseChatAgent):\n    \"\"\"An agent that returns a new number by adding 1 to the last number in the input messages.\"\"\"\n    async def on_messages(self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) -> Response:\n        if len(messages) == 0:\n            last_number = 0 # Start from 0 if no messages are given.\n        else:\n            assert isinstance(messages[-1], TextMessage)\n            last_number = int(messages[-1].content) # Otherwise, start from the last number.\n        return Response(chat_message=TextMessage(content=str(last_number + 1), source=self.name))\n\n    async def on_reset(self, cancellation_token: CancellationToken) -> None:\n        pass\n\n    @property\n    def produced_message_types(self) -> Sequence[type[BaseChatMessage]]:\n        return (TextMessage,)\n\nclass NestedCountingAgent(BaseChatAgent):\n    \"\"\"An agent that increments the last number in the input messages\n    multiple times using a nested counting team.\"\"\"\n    def __init__(self, name: str, counting_team: RoundRobinGroupChat) -> None:\n        super().__init__(name, description=\"An agent that counts numbers.\")\n        self._counting_team = counting_team\n\n    async def on_messages(self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) -> Response:\n        # Run the inner team with the given messages and returns the last message produced by the team.\n        result = await self._counting_team.run(task=messages, cancellation_token=cancellation_token)\n        # To stream the inner messages, implement `on_messages_stream` and use that to implement `on_messages`.\n        assert isinstance(result.messages[-1], TextMessage)\n        return Response(chat_message=result.messages[-1], inner_messages=result.messages[len(messages):-1])\n\n    async def on_reset(self, cancellation_token: CancellationToken) -> None:\n        # Reset the inner team.\n        await self._counting_team.reset()\n\n    @property\n    def produced_message_types(self) -> Sequence[type[BaseChatMessage]]:\n        return (TextMessage,)\n\nasync def main() -> None:\n    # Create a team of two counting agents as the inner team.\n    counting_agent_1 = CountingAgent(\"counting_agent_1\", description=\"An agent that counts numbers.\")\n    counting_agent_2 = CountingAgent(\"counting_agent_2\", description=\"An agent that counts numbers.\")\n    counting_team = RoundRobinGroupChat([counting_agent_1, counting_agent_2], max_turns=5)\n    # Create a nested counting agent that takes the inner team as a parameter.\n    nested_counting_agent = NestedCountingAgent(\"nested_counting_agent\", counting_team)\n    # Run the nested counting agent with a message starting from 1.\n    response = await nested_counting_agent.on_messages([TextMessage(content=\"1\", source=\"user\")], CancellationToken())\n    assert response.inner_messages is not None\n    for message in response.inner_messages:\n        print(message)\n    print(response.chat_message)\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "source='counting_agent_1' models_usage=None content='2' type='TextMessage'\nsource='counting_agent_2' models_usage=None content='3' type='TextMessage'\nsource='counting_agent_1' models_usage=None content='4' type='TextMessage'\nsource='counting_agent_2' models_usage=None content='5' type='TextMessage'\nsource='counting_agent_1' models_usage=None content='6' type='TextMessage'",
      "language": "rust"
    },
    {
      "code": "source='counting_agent_1' models_usage=None content='2' type='TextMessage'\nsource='counting_agent_2' models_usage=None content='3' type='TextMessage'\nsource='counting_agent_1' models_usage=None content='4' type='TextMessage'\nsource='counting_agent_2' models_usage=None content='5' type='TextMessage'\nsource='counting_agent_1' models_usage=None content='6' type='TextMessage'",
      "language": "rust"
    },
    {
      "code": "import asyncio\nfrom autogen_agentchat.messages import TextMessage\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_core import CancellationToken\nfrom autogen_core.model_context import BufferedChatCompletionContext\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", seed=42, temperature=0)\n\n    assistant = AssistantAgent(\n        name=\"assistant\",\n        system_message=\"You are a helpful assistant.\",\n        model_client=model_client,\n        model_context=BufferedChatCompletionContext(buffer_size=10), # Model can only view the last 10 messages.\n    )\n    while True:\n        user_input = input(\"User: \")\n        if user_input == \"exit\":\n            break\n        response = await assistant.on_messages([TextMessage(content=user_input, source=\"user\")], CancellationToken())\n        print(\"Assistant:\", response.chat_message.to_text())\n    \n    await model_client.close()\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom autogen_agentchat.messages import TextMessage\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_core import CancellationToken\nfrom autogen_core.model_context import BufferedChatCompletionContext\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", seed=42, temperature=0)\n\n    assistant = AssistantAgent(\n        name=\"assistant\",\n        system_message=\"You are a helpful assistant.\",\n        model_client=model_client,\n        model_context=BufferedChatCompletionContext(buffer_size=10), # Model can only view the last 10 messages.\n    )\n    while True:\n        user_input = input(\"User: \")\n        if user_input == \"exit\":\n            break\n        response = await assistant.on_messages([TextMessage(content=user_input, source=\"user\")], CancellationToken())\n        print(\"Assistant:\", response.chat_message.to_text())\n    \n    await model_client.close()\n\nasyncio.run(main())",
      "language": "python"
    }
  ],
  "patterns": [
    {
      "description": "AgentChat Migration Guide for v0.2 to v0.4 Migration Guide for v0.2 to v0.4# This is a migration guide for users of the v0.2.* versions of autogen-agentchat to the v0.4 version, which introduces a new set of APIs and features. The v0.4 version contains breaking changes. Please read this guide carefully. We still maintain the v0.2 version in the 0.2 branch; however, we highly recommend you upgrade to the v0.4 version. Note We no longer have admin access to the pyautogen PyPI package, and the releases from that package are no longer from Microsoft since version 0.2.34. To continue use the v0.2 version of AutoGen, install it using autogen-agentchat~=0.2. Please read our clarification statement regarding forks. What is v0.4?# Since the release of AutoGen in 2023, we have intensively listened to our community and users from small startups and large enterprises, gathering much feedback. Based on that feedback, we built AutoGen v0.4, a from-the-ground-up rewrite adopting an asynchronous, event-driven architecture to address issues such as observability, flexibility, interactive control, and scale. The v0.4 API is layered: the Core API is the foundation layer offering a scalable, event-driven actor framework for creating agentic workflows; the AgentChat API is built on Core, offering a task-driven, high-level framework for building interactive agentic applications. It is a replacement for AutoGen v0.2. Most of this guide focuses on v0.4’s AgentChat API; however, you can also build your own high-level framework using just the Core API. New to AutoGen?# Jump straight to the AgentChat Tutorial to get started with v0.4. What’s in this guide?# We provide a detailed guide on how to migrate your existing codebase from v0.2 to v0.4. See each feature below for detailed information on how to migrate. Migration Guide for v0.2 to v0.4 What is v0.4? New to AutoGen? What’s in this guide? Model Client Use component config Use model client class directly Model Client for OpenAI-Compatible APIs Model Client Cache Assistant Agent Multi-Modal Agent User Proxy RAG Agent Conversable Agent and Register Reply Save and Load Agent State Two-Agent Chat Tool Use Chat Result Conversion between v0.2 and v0.4 Messages Group Chat Group Chat with Resume Save and Load Group Chat State Group Chat with Tool Use Group Chat with Custom Selector (Stateflow) Nested Chat Sequential Chat GPTAssistantAgent Long Context Handling Observability and Control Code Executors The following features currently in v0.2 will be provided in the future releases of v0.4.* versions: Model Client Cost #4835 Teachable Agent RAG Agent We will update this guide when the missing features become available. Model Client# In v0.2 you configure the model client as follows, and create the OpenAIWrapper object. from autogen.oai import OpenAIWrapper config_list = [ {\"model\": \"gpt-4o\", \"api_key\": \"sk-xxx\"}, {\"model\": \"gpt-4o-mini\", \"api_key\": \"sk-xxx\"}, ] model_client = OpenAIWrapper(config_list=config_list) Note: In AutoGen 0.2, the OpenAI client would try configs in the list until one worked. 0.4 instead expects a specfic model configuration to be chosen. In v0.4, we offer two ways to create a model client. Use component config# AutoGen 0.4 has a generic component configuration system. Model clients are a great use case for this. See below for how to create an OpenAI chat completion client. from autogen_core.models import ChatCompletionClient config = { \"provider\": \"OpenAIChatCompletionClient\", \"config\": { \"model\": \"gpt-4o\", \"api_key\": \"sk-xxx\" # os.environ[\"...'] } } model_client = ChatCompletionClient.load_component(config) Use model client class directly# Open AI: from autogen_ext.models.openai import OpenAIChatCompletionClient model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", api_key=\"sk-xxx\") Azure OpenAI: from autogen_ext.models.openai import AzureOpenAIChatCompletionClient model_client = AzureOpenAIChatCompletionClient( azure_deployment=\"gpt-4o\", azure_endpoint=\"https://<your-endpoint>.openai.azure.com/\", model=\"gpt-4o\", api_version=\"2024-09-01-preview\", api_key=\"sk-xxx\", ) Read more on OpenAIChatCompletionClient. Model Client for OpenAI-Compatible APIs# You can use a the OpenAIChatCompletionClient to connect to an OpenAI-Compatible API, but you need to specify the base_url and model_info. from autogen_ext.models.openai import OpenAIChatCompletionClient custom_model_client = OpenAIChatCompletionClient( model=\"custom-model-name\", base_url=\"https://custom-model.com/reset/of/the/path\", api_key=\"placeholder\", model_info={ \"vision\": True, \"function_calling\": True, \"json_output\": True, \"family\": \"unknown\", \"structured_output\": True, }, ) Note: We don’t test all the OpenAI-Compatible APIs, and many of them works differently from the OpenAI API even though they may claim to suppor it. Please test them before using them. Read about Model Clients in AgentChat Tutorial and more detailed information on Core API Docs Support for other hosted models will be added in the future. Model Client Cache# In v0.2, you can set the cache seed through the cache_seed parameter in the LLM config. The cache is enabled by default. llm_config = { \"config_list\": [{\"model\": \"gpt-4o\", \"api_key\": \"sk-xxx\"}], \"seed\": 42, \"temperature\": 0, \"cache_seed\": 42, } In v0.4, the cache is not enabled by default, to use it you need to use a ChatCompletionCache wrapper around the model client. You can use a DiskCacheStore or RedisStore to store the cache. pip install -U \"autogen-ext[openai, diskcache, redis]\" Here’s an example of using diskcache for local caching: import asyncio import tempfile from autogen_core.models import UserMessage from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_ext.models.cache import ChatCompletionCache, CHAT_CACHE_VALUE_TYPE from autogen_ext.cache_store.diskcache import DiskCacheStore from diskcache import Cache async def main(): with tempfile.TemporaryDirectory() as tmpdirname: # Initialize the original client openai_model_client = OpenAIChatCompletionClient(model=\"gpt-4o\") # Then initialize the CacheStore, in this case with diskcache.Cache. # You can also use redis like: # from autogen_ext.cache_store.redis import RedisStore # import redis # redis_instance = redis.Redis() # cache_store = RedisCacheStore[CHAT_CACHE_VALUE_TYPE](redis_instance) cache_store = DiskCacheStore[CHAT_CACHE_VALUE_TYPE](Cache(tmpdirname)) cache_client = ChatCompletionCache(openai_model_client, cache_store) response = await cache_client.create([UserMessage(content=\"Hello, how are you?\", source=\"user\")]) print(response) # Should print response from OpenAI response = await cache_client.create([UserMessage(content=\"Hello, how are you?\", source=\"user\")]) print(response) # Should print cached response await openai_model_client.close() asyncio.run(main()) Assistant Agent# In v0.2, you create an assistant agent as follows: from autogen.agentchat import AssistantAgent llm_config = { \"config_list\": [{\"model\": \"gpt-4o\", \"api_key\": \"sk-xxx\"}], \"seed\": 42, \"temperature\": 0, } assistant = AssistantAgent( name=\"assistant\", system_message=\"You are a helpful assistant.\", llm_config=llm_config, ) In v0.4, it is similar, but you need to specify model_client instead of llm_config. from autogen_agentchat.agents import AssistantAgent from autogen_ext.models.openai import OpenAIChatCompletionClient model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", api_key=\"sk-xxx\", seed=42, temperature=0) assistant = AssistantAgent( name=\"assistant\", system_message=\"You are a helpful assistant.\", model_client=model_client, ) However, the usage is somewhat different. In v0.4, instead of calling assistant.send, you call assistant.on_messages or assistant.on_messages_stream to handle incoming messages. Furthermore, the on_messages and on_messages_stream methods are asynchronous, and the latter returns an async generator to stream the inner thoughts of the agent. Here is how you can call the assistant agent in v0.4 directly, continuing from the above example: import asyncio from autogen_agentchat.messages import TextMessage from autogen_agentchat.agents import AssistantAgent from autogen_core import CancellationToken from autogen_ext.models.openai import OpenAIChatCompletionClient async def main() -> None: model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", seed=42, temperature=0) assistant = AssistantAgent( name=\"assistant\", system_message=\"You are a helpful assistant.\", model_client=model_client, ) cancellation_token = CancellationToken() response = await assistant.on_messages([TextMessage(content=\"Hello!\", source=\"user\")], cancellation_token) print(response) await model_client.close() asyncio.run(main()) The CancellationToken can be used to cancel the request asynchronously when you call cancellation_token.cancel(), which will cause the await on the on_messages call to raise a CancelledError. Read more on Agent Tutorial and AssistantAgent. Multi-Modal Agent# The AssistantAgent in v0.4 supports multi-modal inputs if the model client supports it. The vision capability of the model client is used to determine if the agent supports multi-modal inputs. import asyncio from pathlib import Path from autogen_agentchat.messages import MultiModalMessage from autogen_agentchat.agents import AssistantAgent from autogen_core import CancellationToken, Image from autogen_ext.models.openai import OpenAIChatCompletionClient async def main() -> None: model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", seed=42, temperature=0) assistant = AssistantAgent( name=\"assistant\", system_message=\"You are a helpful assistant.\", model_client=model_client, ) cancellation_token = CancellationToken() message = MultiModalMessage( content=[\"Here is an image:\", Image.from_file(Path(\"test.png\"))], source=\"user\", ) response = await assistant.on_messages([message], cancellation_token) print(response) await model_client.close() asyncio.run(main()) User Proxy# In v0.2, you create a user proxy as follows: from autogen.agentchat import UserProxyAgent user_proxy = UserProxyAgent( name=\"user_proxy\", human_input_mode=\"NEVER\", max_consecutive_auto_reply=10, code_execution_config=False, llm_config=False, ) This user proxy would take input from the user through console, and would terminate if the incoming message ends with “TERMINATE”. In v0.4, a user proxy is simply an agent that takes user input only, there is no other special configuration needed. You can create a user proxy as follows: from autogen_agentchat.agents import UserProxyAgent user_proxy = UserProxyAgent(\"user_proxy\") See UserProxyAgent for more details and how to customize the input function with timeout. RAG Agent# In v0.2, there was the concept of teachable agents as well as a RAG agents that could take a database config. teachable_agent = ConversableAgent( name=\"teachable_agent\", llm_config=llm_config ) # Instantiate a Teachability object. Its parameters are all optional. teachability = Teachability( reset_db=False, path_to_db_dir=\"./tmp/interactive/teachability_db\" ) teachability.add_to_agent(teachable_agent) In v0.4, you can implement a RAG agent using the Memory class. Specifically, you can define a memory store class, and pass that as a parameter to the assistant agent. See the Memory tutorial for more details. This clear separation of concerns allows you to implement a memory store that uses any database or storage system you want (you have to inherit from the Memory class) and use it with an assistant agent. The example below shows how to use a ChromaDB vector memory store with the assistant agent. In addition, your application logic should determine how and when to add content to the memory store. For example, you may choose to call memory.add for every response from the assistant agent or use a separate LLM call to determine if the content should be added to the memory store. # ... # example of a ChromaDBVectorMemory class chroma_user_memory = ChromaDBVectorMemory( config=PersistentChromaDBVectorMemoryConfig( collection_name=\"preferences\", persistence_path=os.path.join(str(Path.home()), \".chromadb_autogen\"), k=2, # Return top k results score_threshold=0.4, # Minimum similarity score ) ) # you can add logic such as a document indexer that adds content to the memory store assistant_agent = AssistantAgent( name=\"assistant_agent\", model_client=OpenAIChatCompletionClient( model=\"gpt-4o\", ), tools=[get_weather], memory=[chroma_user_memory], ) Conversable Agent and Register Reply# In v0.2, you can create a conversable agent and register a reply function as follows: from typing import Any, Dict, List, Optional, Tuple, Union from autogen.agentchat import ConversableAgent llm_config = { \"config_list\": [{\"model\": \"gpt-4o\", \"api_key\": \"sk-xxx\"}], \"seed\": 42, \"temperature\": 0, } conversable_agent = ConversableAgent( name=\"conversable_agent\", system_message=\"You are a helpful assistant.\", llm_config=llm_config, code_execution_config={\"work_dir\": \"coding\"}, human_input_mode=\"NEVER\", max_consecutive_auto_reply=10, ) def reply_func( recipient: ConversableAgent, messages: Optional[List[Dict]] = None, sender: Optional[Agent] = None, config: Optional[Any] = None, ) -> Tuple[bool, Union[str, Dict, None]]: # Custom reply logic here return True, \"Custom reply\" # Register the reply function conversable_agent.register_reply([ConversableAgent], reply_func, position=0) # NOTE: An async reply function will only be invoked with async send. Rather than guessing what the reply_func does, all its parameters, and what the position should be, in v0.4, we can simply create a custom agent and implement the on_messages, on_reset, and produced_message_types methods. from typing import Sequence from autogen_core import CancellationToken from autogen_agentchat.agents import BaseChatAgent from autogen_agentchat.messages import TextMessage, BaseChatMessage from autogen_agentchat.base import Response class CustomAgent(BaseChatAgent): async def on_messages(self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) -> Response: return Response(chat_message=TextMessage(content=\"Custom reply\", source=self.name)) async def on_reset(self, cancellation_token: CancellationToken) -> None: pass @property def produced_message_types(self) -> Sequence[type[BaseChatMessage]]: return (TextMessage,) You can then use the custom agent in the same way as the AssistantAgent. See Custom Agent Tutorial for more details. Save and Load Agent State# In v0.2 there is no built-in way to save and load an agent’s state: you need to implement it yourself by exporting the chat_messages attribute of ConversableAgent and importing it back through the chat_messages parameter. In v0.4, you can call save_state and load_state methods on agents to save and load their state. import asyncio import json from autogen_agentchat.messages import TextMessage from autogen_agentchat.agents import AssistantAgent from autogen_core import CancellationToken from autogen_ext.models.openai import OpenAIChatCompletionClient async def main() -> None: model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", seed=42, temperature=0) assistant = AssistantAgent( name=\"assistant\", system_message=\"You are a helpful assistant.\", model_client=model_client, ) cancellation_token = CancellationToken() response = await assistant.on_messages([TextMessage(content=\"Hello!\", source=\"user\")], cancellation_token) print(response) # Save the state. state = await assistant.save_state() # (Optional) Write state to disk. with open(\"assistant_state.json\", \"w\") as f: json.dump(state, f) # (Optional) Load it back from disk. with open(\"assistant_state.json\", \"r\") as f: state = json.load(f) print(state) # Inspect the state, which contains the chat history. # Carry on the chat. response = await assistant.on_messages([TextMessage(content=\"Tell me a joke.\", source=\"user\")], cancellation_token) print(response) # Load the state, resulting the agent to revert to the previous state before the last message. await assistant.load_state(state) # Carry on the same chat again. response = await assistant.on_messages([TextMessage(content=\"Tell me a joke.\", source=\"user\")], cancellation_token) # Close the connection to the model client. await model_client.close() asyncio.run(main()) You can also call save_state and load_state on any teams, such as RoundRobinGroupChat to save and load the state of the entire team. Two-Agent Chat# In v0.2, you can create a two-agent chat for code execution as follows: from autogen.coding import LocalCommandLineCodeExecutor from autogen.agentchat import AssistantAgent, UserProxyAgent llm_config = { \"config_list\": [{\"model\": \"gpt-4o\", \"api_key\": \"sk-xxx\"}], \"seed\": 42, \"temperature\": 0, } assistant = AssistantAgent( name=\"assistant\", system_message=\"You are a helpful assistant. Write all code in python. Reply only 'TERMINATE' if the task is done.\", llm_config=llm_config, is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"), ) user_proxy = UserProxyAgent( name=\"user_proxy\", human_input_mode=\"NEVER\", max_consecutive_auto_reply=10, code_execution_config={\"code_executor\": LocalCommandLineCodeExecutor(work_dir=\"coding\")}, llm_config=False, is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"), ) chat_result = user_proxy.initiate_chat(assistant, message=\"Write a python script to print 'Hello, world!'\") # Intermediate messages are printed to the console directly. print(chat_result) To get the same behavior in v0.4, you can use the AssistantAgent and CodeExecutorAgent together in a RoundRobinGroupChat. import asyncio from autogen_agentchat.agents import AssistantAgent, CodeExecutorAgent from autogen_agentchat.teams import RoundRobinGroupChat from autogen_agentchat.conditions import TextMentionTermination, MaxMessageTermination from autogen_agentchat.ui import Console from autogen_ext.code_executors.local import LocalCommandLineCodeExecutor from autogen_ext.models.openai import OpenAIChatCompletionClient async def main() -> None: model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", seed=42, temperature=0) assistant = AssistantAgent( name=\"assistant\", system_message=\"You are a helpful assistant. Write all code in python. Reply only 'TERMINATE' if the task is done.\", model_client=model_client, ) code_executor = CodeExecutorAgent( name=\"code_executor\", code_executor=LocalCommandLineCodeExecutor(work_dir=\"coding\"), ) # The termination condition is a combination of text termination and max message termination, either of which will cause the chat to terminate. termination = TextMentionTermination(\"TERMINATE\") | MaxMessageTermination(10) # The group chat will alternate between the assistant and the code executor. group_chat = RoundRobinGroupChat([assistant, code_executor], termination_condition=termination) # `run_stream` returns an async generator to stream the intermediate messages. stream = group_chat.run_stream(task=\"Write a python script to print 'Hello, world!'\") # `Console` is a simple UI to display the stream. await Console(stream) # Close the connection to the model client. await model_client.close() asyncio.run(main()) Tool Use# In v0.2, to create a tool use chatbot, you must have two agents, one for calling the tool and one for executing the tool. You need to initiate a two-agent chat for every user request. from autogen.agentchat import AssistantAgent, UserProxyAgent, register_function llm_config = { \"config_list\": [{\"model\": \"gpt-4o\", \"api_key\": \"sk-xxx\"}], \"seed\": 42, \"temperature\": 0, } tool_caller = AssistantAgent( name=\"tool_caller\", system_message=\"You are a helpful assistant. You can call tools to help user.\", llm_config=llm_config, max_consecutive_auto_reply=1, # Set to 1 so that we return to the application after each assistant reply as we are building a chatbot. ) tool_executor = UserProxyAgent( name=\"tool_executor\", human_input_mode=\"NEVER\", code_execution_config=False, llm_config=False, ) def get_weather(city: str) -> str: return f\"The weather in {city} is 72 degree and sunny.\" # Register the tool function to the tool caller and executor. register_function(get_weather, caller=tool_caller, executor=tool_executor) while True: user_input = input(\"User: \") if user_input == \"exit\": break chat_result = tool_executor.initiate_chat( tool_caller, message=user_input, summary_method=\"reflection_with_llm\", # To let the model reflect on the tool use, set to \"last_msg\" to return the tool call result directly. ) print(\"Assistant:\", chat_result.summary) In v0.4, you really just need one agent – the AssistantAgent – to handle both the tool calling and tool execution. import asyncio from autogen_core import CancellationToken from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.messages import TextMessage def get_weather(city: str) -> str: # Async tool is possible too. return f\"The weather in {city} is 72 degree and sunny.\" async def main() -> None: model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", seed=42, temperature=0) assistant = AssistantAgent( name=\"assistant\", system_message=\"You are a helpful assistant. You can call tools to help user.\", model_client=model_client, tools=[get_weather], reflect_on_tool_use=True, # Set to True to have the model reflect on the tool use, set to False to return the tool call result directly. ) while True: user_input = input(\"User: \") if user_input == \"exit\": break response = await assistant.on_messages([TextMessage(content=user_input, source=\"user\")], CancellationToken()) print(\"Assistant:\", response.chat_message.to_text()) await model_client.close() asyncio.run(main()) When using tool-equipped agents inside a group chat such as RoundRobinGroupChat, you simply do the same as above to add tools to the agents, and create a group chat with the agents. Chat Result# In v0.2, you get a ChatResult object from the initiate_chat method. For example: chat_result = tool_executor.initiate_chat( tool_caller, message=user_input, summary_method=\"reflection_with_llm\", ) print(chat_result.summary) # Get LLM-reflected summary of the chat. print(chat_result.chat_history) # Get the chat history. print(chat_result.cost) # Get the cost of the chat. print(chat_result.human_input) # Get the human input solicited by the chat. See ChatResult Docs for more details. In v0.4, you get a TaskResult object from a run or run_stream method. The TaskResult object contains the messages which is the message history of the chat, including both agents’ private (tool calls, etc.) and public messages. There are some notable differences between TaskResult and ChatResult: The messages list in TaskResult uses different message format than the ChatResult.chat_history list. There is no summary field. It is up to the application to decide how to summarize the chat using the messages list. human_input is not provided in the TaskResult object, as the user input can be extracted from the messages list by filtering with the source field. cost is not provided in the TaskResult object, however, you can calculate the cost based on token usage. It would be a great community extension to add cost calculation. See community extensions. Conversion between v0.2 and v0.4 Messages# You can use the following conversion functions to convert between a v0.4 message in autogen_agentchat.base.TaskResult.messages and a v0.2 message in ChatResult.chat_history. from typing import Any, Dict, List, Literal from autogen_agentchat.messages import ( BaseAgentEvent, BaseChatMessage, HandoffMessage, MultiModalMessage, StopMessage, TextMessage, ToolCallExecutionEvent, ToolCallRequestEvent, ToolCallSummaryMessage, ) from autogen_core import FunctionCall, Image from autogen_core.models import FunctionExecutionResult def convert_to_v02_message( message: BaseAgentEvent | BaseChatMessage, role: Literal[\"assistant\", \"user\", \"tool\"], image_detail: Literal[\"auto\", \"high\", \"low\"] = \"auto\", ) -> Dict[str, Any]: \"\"\"Convert a v0.4 AgentChat message to a v0.2 message. Args: message (BaseAgentEvent | BaseChatMessage): The message to convert. role (Literal[\"assistant\", \"user\", \"tool\"]): The role of the message. image_detail (Literal[\"auto\", \"high\", \"low\"], optional): The detail level of image content in multi-modal message. Defaults to \"auto\". Returns: Dict[str, Any]: The converted AutoGen v0.2 message. \"\"\" v02_message: Dict[str, Any] = {} if isinstance(message, TextMessage | StopMessage | HandoffMessage | ToolCallSummaryMessage): v02_message = {\"content\": message.content, \"role\": role, \"name\": message.source} elif isinstance(message, MultiModalMessage): v02_message = {\"content\": [], \"role\": role, \"name\": message.source} for modal in message.content: if isinstance(modal, str): v02_message[\"content\"].append({\"type\": \"text\", \"text\": modal}) elif isinstance(modal, Image): v02_message[\"content\"].append(modal.to_openai_format(detail=image_detail)) else: raise ValueError(f\"Invalid multimodal message content: {modal}\") elif isinstance(message, ToolCallRequestEvent): v02_message = {\"tool_calls\": [], \"role\": \"assistant\", \"content\": None, \"name\": message.source} for tool_call in message.content: v02_message[\"tool_calls\"].append( { \"id\": tool_call.id, \"type\": \"function\", \"function\": {\"name\": tool_call.name, \"args\": tool_call.arguments}, } ) elif isinstance(message, ToolCallExecutionEvent): tool_responses: List[Dict[str, str]] = [] for tool_result in message.content: tool_responses.append( { \"tool_call_id\": tool_result.call_id, \"role\": \"tool\", \"content\": tool_result.content, } ) content = \"\\n\\n\".join([response[\"content\"] for response in tool_responses]) v02_message = {\"tool_responses\": tool_responses, \"role\": \"tool\", \"content\": content} else: raise ValueError(f\"Invalid message type: {type(message)}\") return v02_message def convert_to_v04_message(message: Dict[str, Any]) -> BaseAgentEvent | BaseChatMessage: \"\"\"Convert a v0.2 message to a v0.4 AgentChat message.\"\"\" if \"tool_calls\" in message: tool_calls: List[FunctionCall] = [] for tool_call in message[\"tool_calls\"]: tool_calls.append( FunctionCall( id=tool_call[\"id\"], name=tool_call[\"function\"][\"name\"], arguments=tool_call[\"function\"][\"args\"], ) ) return ToolCallRequestEvent(source=message[\"name\"], content=tool_calls) elif \"tool_responses\" in message: tool_results: List[FunctionExecutionResult] = [] for tool_response in message[\"tool_responses\"]: tool_results.append( FunctionExecutionResult( call_id=tool_response[\"tool_call_id\"], content=tool_response[\"content\"], is_error=False, name=tool_response[\"name\"], ) ) return ToolCallExecutionEvent(source=\"tools\", content=tool_results) elif isinstance(message[\"content\"], list): content: List[str | Image] = [] for modal in message[\"content\"]: # type: ignore if modal[\"type\"] == \"text\": # type: ignore content.append(modal[\"text\"]) # type: ignore else: content.append(Image.from_uri(modal[\"image_url\"][\"url\"])) # type: ignore return MultiModalMessage(content=content, source=message[\"name\"]) elif isinstance(message[\"content\"], str): return TextMessage(content=message[\"content\"], source=message[\"name\"]) else: raise ValueError(f\"Unable to convert message: {message}\") Group Chat# In v0.2, you need to create a GroupChat class and pass it into a GroupChatManager, and have a participant that is a user proxy to initiate the chat. For a simple scenario of a writer and a critic, you can do the following: from autogen.agentchat import AssistantAgent, GroupChat, GroupChatManager llm_config = { \"config_list\": [{\"model\": \"gpt-4o\", \"api_key\": \"sk-xxx\"}], \"seed\": 42, \"temperature\": 0, } writer = AssistantAgent( name=\"writer\", description=\"A writer.\", system_message=\"You are a writer.\", llm_config=llm_config, is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"APPROVE\"), ) critic = AssistantAgent( name=\"critic\", description=\"A critic.\", system_message=\"You are a critic, provide feedback on the writing. Reply only 'APPROVE' if the task is done.\", llm_config=llm_config, ) # Create a group chat with the writer and critic. groupchat = GroupChat(agents=[writer, critic], messages=[], max_round=12) # Create a group chat manager to manage the group chat, use round-robin selection method. manager = GroupChatManager(groupchat=groupchat, llm_config=llm_config, speaker_selection_method=\"round_robin\") # Initiate the chat with the editor, intermediate messages are printed to the console directly. result = editor.initiate_chat( manager, message=\"Write a short story about a robot that discovers it has feelings.\", ) print(result.summary) In v0.4, you can use the RoundRobinGroupChat to achieve the same behavior. import asyncio from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.teams import RoundRobinGroupChat from autogen_agentchat.conditions import TextMentionTermination from autogen_agentchat.ui import Console from autogen_ext.models.openai import OpenAIChatCompletionClient async def main() -> None: model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", seed=42, temperature=0) writer = AssistantAgent( name=\"writer\", description=\"A writer.\", system_message=\"You are a writer.\", model_client=model_client, ) critic = AssistantAgent( name=\"critic\", description=\"A critic.\", system_message=\"You are a critic, provide feedback on the writing. Reply only 'APPROVE' if the task is done.\", model_client=model_client, ) # The termination condition is a text termination, which will cause the chat to terminate when the text \"APPROVE\" is received. termination = TextMentionTermination(\"APPROVE\") # The group chat will alternate between the writer and the critic. group_chat = RoundRobinGroupChat([writer, critic], termination_condition=termination, max_turns=12) # `run_stream` returns an async generator to stream the intermediate messages. stream = group_chat.run_stream(task=\"Write a short story about a robot that discovers it has feelings.\") # `Console` is a simple UI to display the stream. await Console(stream) # Close the connection to the model client. await model_client.close() asyncio.run(main()) For LLM-based speaker selection, you can use the SelectorGroupChat instead. See Selector Group Chat Tutorial and SelectorGroupChat for more details. Note: In v0.4, you do not need to register functions on a user proxy to use tools in a group chat. You can simply pass the tool functions to the AssistantAgent as shown in the Tool Use section. The agent will automatically call the tools when needed. If your tool doesn’t output well formed response, you can use the reflect_on_tool_use parameter to have the model reflect on the tool use. Group Chat with Resume# In v0.2, group chat with resume is a bit complicated. You need to explicitly save the group chat messages and load them back when you want to resume the chat. See Resuming Group Chat in v0.2 for more details. In v0.4, you can simply call run or run_stream again with the same group chat object to resume the chat. To export and load the state, you can use save_state and load_state methods. import asyncio import json from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.teams import RoundRobinGroupChat from autogen_agentchat.conditions import TextMentionTermination from autogen_agentchat.ui import Console from autogen_ext.models.openai import OpenAIChatCompletionClient def create_team(model_client : OpenAIChatCompletionClient) -> RoundRobinGroupChat: writer = AssistantAgent( name=\"writer\", description=\"A writer.\", system_message=\"You are a writer.\", model_client=model_client, ) critic = AssistantAgent( name=\"critic\", description=\"A critic.\", system_message=\"You are a critic, provide feedback on the writing. Reply only 'APPROVE' if the task is done.\", model_client=model_client, ) # The termination condition is a text termination, which will cause the chat to terminate when the text \"APPROVE\" is received. termination = TextMentionTermination(\"APPROVE\") # The group chat will alternate between the writer and the critic. group_chat = RoundRobinGroupChat([writer, critic], termination_condition=termination) return group_chat async def main() -> None: model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", seed=42, temperature=0) # Create team. group_chat = create_team(model_client) # `run_stream` returns an async generator to stream the intermediate messages. stream = group_chat.run_stream(task=\"Write a short story about a robot that discovers it has feelings.\") # `Console` is a simple UI to display the stream. await Console(stream) # Save the state of the group chat and all participants. state = await group_chat.save_state() with open(\"group_chat_state.json\", \"w\") as f: json.dump(state, f) # Create a new team with the same participants configuration. group_chat = create_team(model_client) # Load the state of the group chat and all participants. with open(\"group_chat_state.json\", \"r\") as f: state = json.load(f) await group_chat.load_state(state) # Resume the chat. stream = group_chat.run_stream(task=\"Translate the story into Chinese.\") await Console(stream) # Close the connection to the model client. await model_client.close() asyncio.run(main()) Save and Load Group Chat State# In v0.2, you need to explicitly save the group chat messages and load them back when you want to resume the chat. In v0.4, you can simply call save_state and load_state methods on the group chat object. See Group Chat with Resume for an example. Group Chat with Tool Use# In v0.2 group chat, when tools are involved, you need to register the tool functions on a user proxy, and include the user proxy in the group chat. The tool calls made by other agents will be routed to the user proxy to execute. We have observed numerous issues with this approach, such as the the tool call routing not working as expected, and the tool call request and result cannot be accepted by models without support for function calling. In v0.4, there is no need to register the tool functions on a user proxy, as the tools are directly executed within the AssistantAgent, which publishes the response from the tool to the group chat. So the group chat manager does not need to be involved in routing tool calls. See Selector Group Chat Tutorial for an example of using tools in a group chat. Group Chat with Custom Selector (Stateflow)# In v0.2 group chat, when the speaker_selection_method is set to a custom function, it can override the default selection method. This is useful for implementing a state-based selection method. For more details, see Custom Sepaker Selection in v0.2. In v0.4, you can use the SelectorGroupChat with selector_func to achieve the same behavior. The selector_func is a function that takes the current message thread of the group chat and returns the next speaker’s name. If None is returned, the LLM-based selection method will be used. Here is an example of using the state-based selection method to implement a web search/analysis scenario. import asyncio from typing import Sequence from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination from autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage from autogen_agentchat.teams import SelectorGroupChat from autogen_agentchat.ui import Console from autogen_ext.models.openai import OpenAIChatCompletionClient # Note: This example uses mock tools instead of real APIs for demonstration purposes def search_web_tool(query: str) -> str: if \"2006-2007\" in query: return \"\"\"Here are the total points scored by Miami Heat players in the 2006-2007 season: Udonis Haslem: 844 points Dwayne Wade: 1397 points James Posey: 550 points ... \"\"\" elif \"2007-2008\" in query: return \"The number of total rebounds for Dwayne Wade in the Miami Heat season 2007-2008 is 214.\" elif \"2008-2009\" in query: return \"The number of total rebounds for Dwayne Wade in the Miami Heat season 2008-2009 is 398.\" return \"No data found.\" def percentage_change_tool(start: float, end: float) -> float: return ((end - start) / start) * 100 def create_team(model_client : OpenAIChatCompletionClient) -> SelectorGroupChat: planning_agent = AssistantAgent( \"PlanningAgent\", description=\"An agent for planning tasks, this agent should be the first to engage when given a new task.\", model_client=model_client, system_message=\"\"\" You are a planning agent. Your job is to break down complex tasks into smaller, manageable subtasks. Your team members are: Web search agent: Searches for information Data analyst: Performs calculations You only plan and delegate tasks - you do not execute them yourself. When assigning tasks, use this format: 1. <agent> : <task> After all tasks are complete, summarize the findings and end with \"TERMINATE\". \"\"\", ) web_search_agent = AssistantAgent( \"WebSearchAgent\", description=\"A web search agent.\", tools=[search_web_tool], model_client=model_client, system_message=\"\"\" You are a web search agent. Your only tool is search_tool - use it to find information. You make only one search call at a time. Once you have the results, you never do calculations based on them. \"\"\", ) data_analyst_agent = AssistantAgent( \"DataAnalystAgent\", description=\"A data analyst agent. Useful for performing calculations.\", model_client=model_client, tools=[percentage_change_tool], system_message=\"\"\" You are a data analyst. Given the tasks you have been assigned, you should analyze the data and provide results using the tools provided. \"\"\", ) # The termination condition is a combination of text mention termination and max message termination. text_mention_termination = TextMentionTermination(\"TERMINATE\") max_messages_termination = MaxMessageTermination(max_messages=25) termination = text_mention_termination | max_messages_termination # The selector function is a function that takes the current message thread of the group chat # and returns the next speaker's name. If None is returned, the LLM-based selection method will be used. def selector_func(messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> str | None: if messages[-1].source != planning_agent.name: return planning_agent.name # Always return to the planning agent after the other agents have spoken. return None team = SelectorGroupChat( [planning_agent, web_search_agent, data_analyst_agent], model_client=OpenAIChatCompletionClient(model=\"gpt-4o-mini\"), # Use a smaller model for the selector. termination_condition=termination, selector_func=selector_func, ) return team async def main() -> None: model_client = OpenAIChatCompletionClient(model=\"gpt-4o\") team = create_team(model_client) task = \"Who was the Miami Heat player with the highest points in the 2006-2007 season, and what was the percentage change in his total rebounds between the 2007-2008 and 2008-2009 seasons?\" await Console(team.run_stream(task=task)) asyncio.run(main()) Nested Chat# Nested chat allows you to nest a whole team or another agent inside an agent. This is useful for creating a hierarchical structure of agents or “information silos”, as the nested agents cannot communicate directly with other agents outside of the same group. In v0.2, nested chat is supported by using the register_nested_chats method on the ConversableAgent class. You need to specify the nested sequence of agents using dictionaries, See Nested Chat in v0.2 for more details. In v0.4, nested chat is an implementation detail of a custom agent. You can create a custom agent that takes a team or another agent as a parameter and implements the on_messages method to trigger the nested team or agent. It is up to the application to decide how to pass or transform the messages from and to the nested team or agent. The following example shows a simple nested chat that counts numbers. import asyncio from typing import Sequence from autogen_core import CancellationToken from autogen_agentchat.agents import BaseChatAgent from autogen_agentchat.teams import RoundRobinGroupChat from autogen_agentchat.messages import TextMessage, BaseChatMessage from autogen_agentchat.base import Response class CountingAgent(BaseChatAgent): \"\"\"An agent that returns a new number by adding 1 to the last number in the input messages.\"\"\" async def on_messages(self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) -> Response: if len(messages) == 0: last_number = 0 # Start from 0 if no messages are given. else: assert isinstance(messages[-1], TextMessage) last_number = int(messages[-1].content) # Otherwise, start from the last number. return Response(chat_message=TextMessage(content=str(last_number + 1), source=self.name)) async def on_reset(self, cancellation_token: CancellationToken) -> None: pass @property def produced_message_types(self) -> Sequence[type[BaseChatMessage]]: return (TextMessage,) class NestedCountingAgent(BaseChatAgent): \"\"\"An agent that increments the last number in the input messages multiple times using a nested counting team.\"\"\" def __init__(self, name: str, counting_team: RoundRobinGroupChat) -> None: super().__init__(name, description=\"An agent that counts numbers.\") self._counting_team = counting_team async def on_messages(self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) -> Response: # Run the inner team with the given messages and returns the last message produced by the team. result = await self._counting_team.run(task=messages, cancellation_token=cancellation_token) # To stream the inner messages, implement `on_messages_stream` and use that to implement `on_messages`. assert isinstance(result.messages[-1], TextMessage) return Response(chat_message=result.messages[-1], inner_messages=result.messages[len(messages):-1]) async def on_reset(self, cancellation_token: CancellationToken) -> None: # Reset the inner team. await self._counting_team.reset() @property def produced_message_types(self) -> Sequence[type[BaseChatMessage]]: return (TextMessage,) async def main() -> None: # Create a team of two counting agents as the inner team. counting_agent_1 = CountingAgent(\"counting_agent_1\", description=\"An agent that counts numbers.\") counting_agent_2 = CountingAgent(\"counting_agent_2\", description=\"An agent that counts numbers.\") counting_team = RoundRobinGroupChat([counting_agent_1, counting_agent_2], max_turns=5) # Create a nested counting agent that takes the inner team as a parameter. nested_counting_agent = NestedCountingAgent(\"nested_counting_agent\", counting_team) # Run the nested counting agent with a message starting from 1. response = await nested_counting_agent.on_messages([TextMessage(content=\"1\", source=\"user\")], CancellationToken()) assert response.inner_messages is not None for message in response.inner_messages: print(message) print(response.chat_message) asyncio.run(main()) You should see the following output: source='counting_agent_1' models_usage=None content='2' type='TextMessage' source='counting_agent_2' models_usage=None content='3' type='TextMessage' source='counting_agent_1' models_usage=None content='4' type='TextMessage' source='counting_agent_2' models_usage=None content='5' type='TextMessage' source='counting_agent_1' models_usage=None content='6' type='TextMessage' You can take a look at SocietyOfMindAgent for a more complex implementation. Sequential Chat# In v0.2, sequential chat is supported by using the initiate_chats function. It takes input a list of dictionary configurations for each step of the sequence. See Sequential Chat in v0.2 for more details. Base on the feedback from the community, the initiate_chats function is too opinionated and not flexible enough to support the diverse set of scenarios that users want to implement. We often find users struggling to get the initiate_chats function to work when they can easily glue the steps together usign basic Python code. Therefore, in v0.4, we do not provide a built-in function for sequential chat in the AgentChat API. Instead, you can create an event-driven sequential workflow using the Core API, and use the other components provided the AgentChat API to implement each step of the workflow. See an example of sequential workflow in the Core API Tutorial. We recognize that the concept of workflow is at the heart of many applications, and we will provide more built-in support for workflows in the future. GPTAssistantAgent# In v0.2, GPTAssistantAgent is a special agent class that is backed by the OpenAI Assistant API. In v0.4, the equivalent is the OpenAIAssistantAgent class. It supports the same set of features as the GPTAssistantAgent in v0.2 with more such as customizable threads and file uploads. See OpenAIAssistantAgent for more details. Long Context Handling# In v0.2, long context that overflows the model’s context window can be handled by using the transforms capability that is added to an ConversableAgent after which is contructed. The feedbacks from our community has led us to believe this feature is essential and should be a built-in component of AssistantAgent, and can be used for every custom agent. In v0.4, we introduce the ChatCompletionContext base class that manages message history and provides a virtual view of the history. Applications can use built-in implementations such as BufferedChatCompletionContext to limit the message history sent to the model, or provide their own implementations that creates different virtual views. To use BufferedChatCompletionContext in an AssistantAgent in a chatbot scenario. import asyncio from autogen_agentchat.messages import TextMessage from autogen_agentchat.agents import AssistantAgent from autogen_core import CancellationToken from autogen_core.model_context import BufferedChatCompletionContext from autogen_ext.models.openai import OpenAIChatCompletionClient async def main() -> None: model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", seed=42, temperature=0) assistant = AssistantAgent( name=\"assistant\", system_message=\"You are a helpful assistant.\", model_client=model_client, model_context=BufferedChatCompletionContext(buffer_size=10), # Model can only view the last 10 messages. ) while True: user_input = input(\"User: \") if user_input == \"exit\": break response = await assistant.on_messages([TextMessage(content=user_input, source=\"user\")], CancellationToken()) print(\"Assistant:\", response.chat_message.to_text()) await model_client.close() asyncio.run(main()) In this example, the chatbot can only read the last 10 messages in the history. Observability and Control# In v0.4 AgentChat, you can observe the agents by using the on_messages_stream method which returns an async generator to stream the inner thoughts and actions of the agent. For teams, you can use the run_stream method to stream the inner conversation among the agents in the team. Your application can use these streams to observe the agents and teams in real-time. Both the on_messages_stream and run_stream methods takes a CancellationToken as a parameter which can be used to cancel the output stream asynchronously and stop the agent or team. For teams, you can also use termination conditions to stop the team when a certain condition is met. See Termination Condition Tutorial for more details. Unlike the v0.2 which comes with a special logging module, the v0.4 API simply uses Python’s logging module to log events such as model client calls. See Logging in the Core API documentation for more details. Code Executors# The code executors in v0.2 and v0.4 are nearly identical except the v0.4 executors support async API. You can also use CancellationToken to cancel a code execution if it takes too long. See Command Line Code Executors Tutorial in the Core API documentation. We also added ACADynamicSessionsCodeExecutor that can use Azure Container Apps (ACA) dynamic sessions for code execution. See ACA Dynamic Sessions Code Executor Docs. previous Quickstart next Introduction On this page What is v0.4? New to AutoGen? What’s in this guide? Model Client Use component config Use model client class directly Model Client for OpenAI-Compatible APIs Model Client Cache Assistant Agent Multi-Modal Agent User Proxy RAG Agent Conversable Agent and Register Reply Save and Load Agent State Two-Agent Chat Tool Use Chat Result Conversion between v0.2 and v0.4 Messages Group Chat Group Chat with Resume Save and Load Group Chat State Group Chat with Tool Use Group Chat with Custom Selector (Stateflow) Nested Chat Sequential Chat GPTAssistantAgent Long Context Handling Observability and Control Code Executors Edit on GitHub Show Source",
      "code": "v0.2.*"
    },
    {
      "description": "AgentChat Migration Guide for v0.2 to v0.4 Migration Guide for v0.2 to v0.4# This is a migration guide for users of the v0.2.* versions of autogen-agentchat to the v0.4 version, which introduces a new set of APIs and features. The v0.4 version contains breaking changes. Please read this guide carefully. We still maintain the v0.2 version in the 0.2 branch; however, we highly recommend you upgrade to the v0.4 version. Note We no longer have admin access to the pyautogen PyPI package, and the releases from that package are no longer from Microsoft since version 0.2.34. To continue use the v0.2 version of AutoGen, install it using autogen-agentchat~=0.2. Please read our clarification statement regarding forks. What is v0.4?# Since the release of AutoGen in 2023, we have intensively listened to our community and users from small startups and large enterprises, gathering much feedback. Based on that feedback, we built AutoGen v0.4, a from-the-ground-up rewrite adopting an asynchronous, event-driven architecture to address issues such as observability, flexibility, interactive control, and scale. The v0.4 API is layered: the Core API is the foundation layer offering a scalable, event-driven actor framework for creating agentic workflows; the AgentChat API is built on Core, offering a task-driven, high-level framework for building interactive agentic applications. It is a replacement for AutoGen v0.2. Most of this guide focuses on v0.4’s AgentChat API; however, you can also build your own high-level framework using just the Core API. New to AutoGen?# Jump straight to the AgentChat Tutorial to get started with v0.4. What’s in this guide?# We provide a detailed guide on how to migrate your existing codebase from v0.2 to v0.4. See each feature below for detailed information on how to migrate. Migration Guide for v0.2 to v0.4 What is v0.4? New to AutoGen? What’s in this guide? Model Client Use component config Use model client class directly Model Client for OpenAI-Compatible APIs Model Client Cache Assistant Agent Multi-Modal Agent User Proxy RAG Agent Conversable Agent and Register Reply Save and Load Agent State Two-Agent Chat Tool Use Chat Result Conversion between v0.2 and v0.4 Messages Group Chat Group Chat with Resume Save and Load Group Chat State Group Chat with Tool Use Group Chat with Custom Selector (Stateflow) Nested Chat Sequential Chat GPTAssistantAgent Long Context Handling Observability and Control Code Executors The following features currently in v0.2 will be provided in the future releases of v0.4.* versions: Model Client Cost #4835 Teachable Agent RAG Agent We will update this guide when the missing features become available. Model Client# In v0.2 you configure the model client as follows, and create the OpenAIWrapper object. from autogen.oai import OpenAIWrapper config_list = [ {\"model\": \"gpt-4o\", \"api_key\": \"sk-xxx\"}, {\"model\": \"gpt-4o-mini\", \"api_key\": \"sk-xxx\"}, ] model_client = OpenAIWrapper(config_list=config_list) Note: In AutoGen 0.2, the OpenAI client would try configs in the list until one worked. 0.4 instead expects a specfic model configuration to be chosen. In v0.4, we offer two ways to create a model client. Use component config# AutoGen 0.4 has a generic component configuration system. Model clients are a great use case for this. See below for how to create an OpenAI chat completion client. from autogen_core.models import ChatCompletionClient config = { \"provider\": \"OpenAIChatCompletionClient\", \"config\": { \"model\": \"gpt-4o\", \"api_key\": \"sk-xxx\" # os.environ[\"...'] } } model_client = ChatCompletionClient.load_component(config) Use model client class directly# Open AI: from autogen_ext.models.openai import OpenAIChatCompletionClient model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", api_key=\"sk-xxx\") Azure OpenAI: from autogen_ext.models.openai import AzureOpenAIChatCompletionClient model_client = AzureOpenAIChatCompletionClient( azure_deployment=\"gpt-4o\", azure_endpoint=\"https://<your-endpoint>.openai.azure.com/\", model=\"gpt-4o\", api_version=\"2024-09-01-preview\", api_key=\"sk-xxx\", ) Read more on OpenAIChatCompletionClient. Model Client for OpenAI-Compatible APIs# You can use a the OpenAIChatCompletionClient to connect to an OpenAI-Compatible API, but you need to specify the base_url and model_info. from autogen_ext.models.openai import OpenAIChatCompletionClient custom_model_client = OpenAIChatCompletionClient( model=\"custom-model-name\", base_url=\"https://custom-model.com/reset/of/the/path\", api_key=\"placeholder\", model_info={ \"vision\": True, \"function_calling\": True, \"json_output\": True, \"family\": \"unknown\", \"structured_output\": True, }, ) Note: We don’t test all the OpenAI-Compatible APIs, and many of them works differently from the OpenAI API even though they may claim to suppor it. Please test them before using them. Read about Model Clients in AgentChat Tutorial and more detailed information on Core API Docs Support for other hosted models will be added in the future. Model Client Cache# In v0.2, you can set the cache seed through the cache_seed parameter in the LLM config. The cache is enabled by default. llm_config = { \"config_list\": [{\"model\": \"gpt-4o\", \"api_key\": \"sk-xxx\"}], \"seed\": 42, \"temperature\": 0, \"cache_seed\": 42, } In v0.4, the cache is not enabled by default, to use it you need to use a ChatCompletionCache wrapper around the model client. You can use a DiskCacheStore or RedisStore to store the cache. pip install -U \"autogen-ext[openai, diskcache, redis]\" Here’s an example of using diskcache for local caching: import asyncio import tempfile from autogen_core.models import UserMessage from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_ext.models.cache import ChatCompletionCache, CHAT_CACHE_VALUE_TYPE from autogen_ext.cache_store.diskcache import DiskCacheStore from diskcache import Cache async def main(): with tempfile.TemporaryDirectory() as tmpdirname: # Initialize the original client openai_model_client = OpenAIChatCompletionClient(model=\"gpt-4o\") # Then initialize the CacheStore, in this case with diskcache.Cache. # You can also use redis like: # from autogen_ext.cache_store.redis import RedisStore # import redis # redis_instance = redis.Redis() # cache_store = RedisCacheStore[CHAT_CACHE_VALUE_TYPE](redis_instance) cache_store = DiskCacheStore[CHAT_CACHE_VALUE_TYPE](Cache(tmpdirname)) cache_client = ChatCompletionCache(openai_model_client, cache_store) response = await cache_client.create([UserMessage(content=\"Hello, how are you?\", source=\"user\")]) print(response) # Should print response from OpenAI response = await cache_client.create([UserMessage(content=\"Hello, how are you?\", source=\"user\")]) print(response) # Should print cached response await openai_model_client.close() asyncio.run(main()) Assistant Agent# In v0.2, you create an assistant agent as follows: from autogen.agentchat import AssistantAgent llm_config = { \"config_list\": [{\"model\": \"gpt-4o\", \"api_key\": \"sk-xxx\"}], \"seed\": 42, \"temperature\": 0, } assistant = AssistantAgent( name=\"assistant\", system_message=\"You are a helpful assistant.\", llm_config=llm_config, ) In v0.4, it is similar, but you need to specify model_client instead of llm_config. from autogen_agentchat.agents import AssistantAgent from autogen_ext.models.openai import OpenAIChatCompletionClient model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", api_key=\"sk-xxx\", seed=42, temperature=0) assistant = AssistantAgent( name=\"assistant\", system_message=\"You are a helpful assistant.\", model_client=model_client, ) However, the usage is somewhat different. In v0.4, instead of calling assistant.send, you call assistant.on_messages or assistant.on_messages_stream to handle incoming messages. Furthermore, the on_messages and on_messages_stream methods are asynchronous, and the latter returns an async generator to stream the inner thoughts of the agent. Here is how you can call the assistant agent in v0.4 directly, continuing from the above example: import asyncio from autogen_agentchat.messages import TextMessage from autogen_agentchat.agents import AssistantAgent from autogen_core import CancellationToken from autogen_ext.models.openai import OpenAIChatCompletionClient async def main() -> None: model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", seed=42, temperature=0) assistant = AssistantAgent( name=\"assistant\", system_message=\"You are a helpful assistant.\", model_client=model_client, ) cancellation_token = CancellationToken() response = await assistant.on_messages([TextMessage(content=\"Hello!\", source=\"user\")], cancellation_token) print(response) await model_client.close() asyncio.run(main()) The CancellationToken can be used to cancel the request asynchronously when you call cancellation_token.cancel(), which will cause the await on the on_messages call to raise a CancelledError. Read more on Agent Tutorial and AssistantAgent. Multi-Modal Agent# The AssistantAgent in v0.4 supports multi-modal inputs if the model client supports it. The vision capability of the model client is used to determine if the agent supports multi-modal inputs. import asyncio from pathlib import Path from autogen_agentchat.messages import MultiModalMessage from autogen_agentchat.agents import AssistantAgent from autogen_core import CancellationToken, Image from autogen_ext.models.openai import OpenAIChatCompletionClient async def main() -> None: model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", seed=42, temperature=0) assistant = AssistantAgent( name=\"assistant\", system_message=\"You are a helpful assistant.\", model_client=model_client, ) cancellation_token = CancellationToken() message = MultiModalMessage( content=[\"Here is an image:\", Image.from_file(Path(\"test.png\"))], source=\"user\", ) response = await assistant.on_messages([message], cancellation_token) print(response) await model_client.close() asyncio.run(main()) User Proxy# In v0.2, you create a user proxy as follows: from autogen.agentchat import UserProxyAgent user_proxy = UserProxyAgent( name=\"user_proxy\", human_input_mode=\"NEVER\", max_consecutive_auto_reply=10, code_execution_config=False, llm_config=False, ) This user proxy would take input from the user through console, and would terminate if the incoming message ends with “TERMINATE”. In v0.4, a user proxy is simply an agent that takes user input only, there is no other special configuration needed. You can create a user proxy as follows: from autogen_agentchat.agents import UserProxyAgent user_proxy = UserProxyAgent(\"user_proxy\") See UserProxyAgent for more details and how to customize the input function with timeout. RAG Agent# In v0.2, there was the concept of teachable agents as well as a RAG agents that could take a database config. teachable_agent = ConversableAgent( name=\"teachable_agent\", llm_config=llm_config ) # Instantiate a Teachability object. Its parameters are all optional. teachability = Teachability( reset_db=False, path_to_db_dir=\"./tmp/interactive/teachability_db\" ) teachability.add_to_agent(teachable_agent) In v0.4, you can implement a RAG agent using the Memory class. Specifically, you can define a memory store class, and pass that as a parameter to the assistant agent. See the Memory tutorial for more details. This clear separation of concerns allows you to implement a memory store that uses any database or storage system you want (you have to inherit from the Memory class) and use it with an assistant agent. The example below shows how to use a ChromaDB vector memory store with the assistant agent. In addition, your application logic should determine how and when to add content to the memory store. For example, you may choose to call memory.add for every response from the assistant agent or use a separate LLM call to determine if the content should be added to the memory store. # ... # example of a ChromaDBVectorMemory class chroma_user_memory = ChromaDBVectorMemory( config=PersistentChromaDBVectorMemoryConfig( collection_name=\"preferences\", persistence_path=os.path.join(str(Path.home()), \".chromadb_autogen\"), k=2, # Return top k results score_threshold=0.4, # Minimum similarity score ) ) # you can add logic such as a document indexer that adds content to the memory store assistant_agent = AssistantAgent( name=\"assistant_agent\", model_client=OpenAIChatCompletionClient( model=\"gpt-4o\", ), tools=[get_weather], memory=[chroma_user_memory], ) Conversable Agent and Register Reply# In v0.2, you can create a conversable agent and register a reply function as follows: from typing import Any, Dict, List, Optional, Tuple, Union from autogen.agentchat import ConversableAgent llm_config = { \"config_list\": [{\"model\": \"gpt-4o\", \"api_key\": \"sk-xxx\"}], \"seed\": 42, \"temperature\": 0, } conversable_agent = ConversableAgent( name=\"conversable_agent\", system_message=\"You are a helpful assistant.\", llm_config=llm_config, code_execution_config={\"work_dir\": \"coding\"}, human_input_mode=\"NEVER\", max_consecutive_auto_reply=10, ) def reply_func( recipient: ConversableAgent, messages: Optional[List[Dict]] = None, sender: Optional[Agent] = None, config: Optional[Any] = None, ) -> Tuple[bool, Union[str, Dict, None]]: # Custom reply logic here return True, \"Custom reply\" # Register the reply function conversable_agent.register_reply([ConversableAgent], reply_func, position=0) # NOTE: An async reply function will only be invoked with async send. Rather than guessing what the reply_func does, all its parameters, and what the position should be, in v0.4, we can simply create a custom agent and implement the on_messages, on_reset, and produced_message_types methods. from typing import Sequence from autogen_core import CancellationToken from autogen_agentchat.agents import BaseChatAgent from autogen_agentchat.messages import TextMessage, BaseChatMessage from autogen_agentchat.base import Response class CustomAgent(BaseChatAgent): async def on_messages(self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) -> Response: return Response(chat_message=TextMessage(content=\"Custom reply\", source=self.name)) async def on_reset(self, cancellation_token: CancellationToken) -> None: pass @property def produced_message_types(self) -> Sequence[type[BaseChatMessage]]: return (TextMessage,) You can then use the custom agent in the same way as the AssistantAgent. See Custom Agent Tutorial for more details. Save and Load Agent State# In v0.2 there is no built-in way to save and load an agent’s state: you need to implement it yourself by exporting the chat_messages attribute of ConversableAgent and importing it back through the chat_messages parameter. In v0.4, you can call save_state and load_state methods on agents to save and load their state. import asyncio import json from autogen_agentchat.messages import TextMessage from autogen_agentchat.agents import AssistantAgent from autogen_core import CancellationToken from autogen_ext.models.openai import OpenAIChatCompletionClient async def main() -> None: model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", seed=42, temperature=0) assistant = AssistantAgent( name=\"assistant\", system_message=\"You are a helpful assistant.\", model_client=model_client, ) cancellation_token = CancellationToken() response = await assistant.on_messages([TextMessage(content=\"Hello!\", source=\"user\")], cancellation_token) print(response) # Save the state. state = await assistant.save_state() # (Optional) Write state to disk. with open(\"assistant_state.json\", \"w\") as f: json.dump(state, f) # (Optional) Load it back from disk. with open(\"assistant_state.json\", \"r\") as f: state = json.load(f) print(state) # Inspect the state, which contains the chat history. # Carry on the chat. response = await assistant.on_messages([TextMessage(content=\"Tell me a joke.\", source=\"user\")], cancellation_token) print(response) # Load the state, resulting the agent to revert to the previous state before the last message. await assistant.load_state(state) # Carry on the same chat again. response = await assistant.on_messages([TextMessage(content=\"Tell me a joke.\", source=\"user\")], cancellation_token) # Close the connection to the model client. await model_client.close() asyncio.run(main()) You can also call save_state and load_state on any teams, such as RoundRobinGroupChat to save and load the state of the entire team. Two-Agent Chat# In v0.2, you can create a two-agent chat for code execution as follows: from autogen.coding import LocalCommandLineCodeExecutor from autogen.agentchat import AssistantAgent, UserProxyAgent llm_config = { \"config_list\": [{\"model\": \"gpt-4o\", \"api_key\": \"sk-xxx\"}], \"seed\": 42, \"temperature\": 0, } assistant = AssistantAgent( name=\"assistant\", system_message=\"You are a helpful assistant. Write all code in python. Reply only 'TERMINATE' if the task is done.\", llm_config=llm_config, is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"), ) user_proxy = UserProxyAgent( name=\"user_proxy\", human_input_mode=\"NEVER\", max_consecutive_auto_reply=10, code_execution_config={\"code_executor\": LocalCommandLineCodeExecutor(work_dir=\"coding\")}, llm_config=False, is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"), ) chat_result = user_proxy.initiate_chat(assistant, message=\"Write a python script to print 'Hello, world!'\") # Intermediate messages are printed to the console directly. print(chat_result) To get the same behavior in v0.4, you can use the AssistantAgent and CodeExecutorAgent together in a RoundRobinGroupChat. import asyncio from autogen_agentchat.agents import AssistantAgent, CodeExecutorAgent from autogen_agentchat.teams import RoundRobinGroupChat from autogen_agentchat.conditions import TextMentionTermination, MaxMessageTermination from autogen_agentchat.ui import Console from autogen_ext.code_executors.local import LocalCommandLineCodeExecutor from autogen_ext.models.openai import OpenAIChatCompletionClient async def main() -> None: model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", seed=42, temperature=0) assistant = AssistantAgent( name=\"assistant\", system_message=\"You are a helpful assistant. Write all code in python. Reply only 'TERMINATE' if the task is done.\", model_client=model_client, ) code_executor = CodeExecutorAgent( name=\"code_executor\", code_executor=LocalCommandLineCodeExecutor(work_dir=\"coding\"), ) # The termination condition is a combination of text termination and max message termination, either of which will cause the chat to terminate. termination = TextMentionTermination(\"TERMINATE\") | MaxMessageTermination(10) # The group chat will alternate between the assistant and the code executor. group_chat = RoundRobinGroupChat([assistant, code_executor], termination_condition=termination) # `run_stream` returns an async generator to stream the intermediate messages. stream = group_chat.run_stream(task=\"Write a python script to print 'Hello, world!'\") # `Console` is a simple UI to display the stream. await Console(stream) # Close the connection to the model client. await model_client.close() asyncio.run(main()) Tool Use# In v0.2, to create a tool use chatbot, you must have two agents, one for calling the tool and one for executing the tool. You need to initiate a two-agent chat for every user request. from autogen.agentchat import AssistantAgent, UserProxyAgent, register_function llm_config = { \"config_list\": [{\"model\": \"gpt-4o\", \"api_key\": \"sk-xxx\"}], \"seed\": 42, \"temperature\": 0, } tool_caller = AssistantAgent( name=\"tool_caller\", system_message=\"You are a helpful assistant. You can call tools to help user.\", llm_config=llm_config, max_consecutive_auto_reply=1, # Set to 1 so that we return to the application after each assistant reply as we are building a chatbot. ) tool_executor = UserProxyAgent( name=\"tool_executor\", human_input_mode=\"NEVER\", code_execution_config=False, llm_config=False, ) def get_weather(city: str) -> str: return f\"The weather in {city} is 72 degree and sunny.\" # Register the tool function to the tool caller and executor. register_function(get_weather, caller=tool_caller, executor=tool_executor) while True: user_input = input(\"User: \") if user_input == \"exit\": break chat_result = tool_executor.initiate_chat( tool_caller, message=user_input, summary_method=\"reflection_with_llm\", # To let the model reflect on the tool use, set to \"last_msg\" to return the tool call result directly. ) print(\"Assistant:\", chat_result.summary) In v0.4, you really just need one agent – the AssistantAgent – to handle both the tool calling and tool execution. import asyncio from autogen_core import CancellationToken from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.messages import TextMessage def get_weather(city: str) -> str: # Async tool is possible too. return f\"The weather in {city} is 72 degree and sunny.\" async def main() -> None: model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", seed=42, temperature=0) assistant = AssistantAgent( name=\"assistant\", system_message=\"You are a helpful assistant. You can call tools to help user.\", model_client=model_client, tools=[get_weather], reflect_on_tool_use=True, # Set to True to have the model reflect on the tool use, set to False to return the tool call result directly. ) while True: user_input = input(\"User: \") if user_input == \"exit\": break response = await assistant.on_messages([TextMessage(content=user_input, source=\"user\")], CancellationToken()) print(\"Assistant:\", response.chat_message.to_text()) await model_client.close() asyncio.run(main()) When using tool-equipped agents inside a group chat such as RoundRobinGroupChat, you simply do the same as above to add tools to the agents, and create a group chat with the agents. Chat Result# In v0.2, you get a ChatResult object from the initiate_chat method. For example: chat_result = tool_executor.initiate_chat( tool_caller, message=user_input, summary_method=\"reflection_with_llm\", ) print(chat_result.summary) # Get LLM-reflected summary of the chat. print(chat_result.chat_history) # Get the chat history. print(chat_result.cost) # Get the cost of the chat. print(chat_result.human_input) # Get the human input solicited by the chat. See ChatResult Docs for more details. In v0.4, you get a TaskResult object from a run or run_stream method. The TaskResult object contains the messages which is the message history of the chat, including both agents’ private (tool calls, etc.) and public messages. There are some notable differences between TaskResult and ChatResult: The messages list in TaskResult uses different message format than the ChatResult.chat_history list. There is no summary field. It is up to the application to decide how to summarize the chat using the messages list. human_input is not provided in the TaskResult object, as the user input can be extracted from the messages list by filtering with the source field. cost is not provided in the TaskResult object, however, you can calculate the cost based on token usage. It would be a great community extension to add cost calculation. See community extensions. Conversion between v0.2 and v0.4 Messages# You can use the following conversion functions to convert between a v0.4 message in autogen_agentchat.base.TaskResult.messages and a v0.2 message in ChatResult.chat_history. from typing import Any, Dict, List, Literal from autogen_agentchat.messages import ( BaseAgentEvent, BaseChatMessage, HandoffMessage, MultiModalMessage, StopMessage, TextMessage, ToolCallExecutionEvent, ToolCallRequestEvent, ToolCallSummaryMessage, ) from autogen_core import FunctionCall, Image from autogen_core.models import FunctionExecutionResult def convert_to_v02_message( message: BaseAgentEvent | BaseChatMessage, role: Literal[\"assistant\", \"user\", \"tool\"], image_detail: Literal[\"auto\", \"high\", \"low\"] = \"auto\", ) -> Dict[str, Any]: \"\"\"Convert a v0.4 AgentChat message to a v0.2 message. Args: message (BaseAgentEvent | BaseChatMessage): The message to convert. role (Literal[\"assistant\", \"user\", \"tool\"]): The role of the message. image_detail (Literal[\"auto\", \"high\", \"low\"], optional): The detail level of image content in multi-modal message. Defaults to \"auto\". Returns: Dict[str, Any]: The converted AutoGen v0.2 message. \"\"\" v02_message: Dict[str, Any] = {} if isinstance(message, TextMessage | StopMessage | HandoffMessage | ToolCallSummaryMessage): v02_message = {\"content\": message.content, \"role\": role, \"name\": message.source} elif isinstance(message, MultiModalMessage): v02_message = {\"content\": [], \"role\": role, \"name\": message.source} for modal in message.content: if isinstance(modal, str): v02_message[\"content\"].append({\"type\": \"text\", \"text\": modal}) elif isinstance(modal, Image): v02_message[\"content\"].append(modal.to_openai_format(detail=image_detail)) else: raise ValueError(f\"Invalid multimodal message content: {modal}\") elif isinstance(message, ToolCallRequestEvent): v02_message = {\"tool_calls\": [], \"role\": \"assistant\", \"content\": None, \"name\": message.source} for tool_call in message.content: v02_message[\"tool_calls\"].append( { \"id\": tool_call.id, \"type\": \"function\", \"function\": {\"name\": tool_call.name, \"args\": tool_call.arguments}, } ) elif isinstance(message, ToolCallExecutionEvent): tool_responses: List[Dict[str, str]] = [] for tool_result in message.content: tool_responses.append( { \"tool_call_id\": tool_result.call_id, \"role\": \"tool\", \"content\": tool_result.content, } ) content = \"\\n\\n\".join([response[\"content\"] for response in tool_responses]) v02_message = {\"tool_responses\": tool_responses, \"role\": \"tool\", \"content\": content} else: raise ValueError(f\"Invalid message type: {type(message)}\") return v02_message def convert_to_v04_message(message: Dict[str, Any]) -> BaseAgentEvent | BaseChatMessage: \"\"\"Convert a v0.2 message to a v0.4 AgentChat message.\"\"\" if \"tool_calls\" in message: tool_calls: List[FunctionCall] = [] for tool_call in message[\"tool_calls\"]: tool_calls.append( FunctionCall( id=tool_call[\"id\"], name=tool_call[\"function\"][\"name\"], arguments=tool_call[\"function\"][\"args\"], ) ) return ToolCallRequestEvent(source=message[\"name\"], content=tool_calls) elif \"tool_responses\" in message: tool_results: List[FunctionExecutionResult] = [] for tool_response in message[\"tool_responses\"]: tool_results.append( FunctionExecutionResult( call_id=tool_response[\"tool_call_id\"], content=tool_response[\"content\"], is_error=False, name=tool_response[\"name\"], ) ) return ToolCallExecutionEvent(source=\"tools\", content=tool_results) elif isinstance(message[\"content\"], list): content: List[str | Image] = [] for modal in message[\"content\"]: # type: ignore if modal[\"type\"] == \"text\": # type: ignore content.append(modal[\"text\"]) # type: ignore else: content.append(Image.from_uri(modal[\"image_url\"][\"url\"])) # type: ignore return MultiModalMessage(content=content, source=message[\"name\"]) elif isinstance(message[\"content\"], str): return TextMessage(content=message[\"content\"], source=message[\"name\"]) else: raise ValueError(f\"Unable to convert message: {message}\") Group Chat# In v0.2, you need to create a GroupChat class and pass it into a GroupChatManager, and have a participant that is a user proxy to initiate the chat. For a simple scenario of a writer and a critic, you can do the following: from autogen.agentchat import AssistantAgent, GroupChat, GroupChatManager llm_config = { \"config_list\": [{\"model\": \"gpt-4o\", \"api_key\": \"sk-xxx\"}], \"seed\": 42, \"temperature\": 0, } writer = AssistantAgent( name=\"writer\", description=\"A writer.\", system_message=\"You are a writer.\", llm_config=llm_config, is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"APPROVE\"), ) critic = AssistantAgent( name=\"critic\", description=\"A critic.\", system_message=\"You are a critic, provide feedback on the writing. Reply only 'APPROVE' if the task is done.\", llm_config=llm_config, ) # Create a group chat with the writer and critic. groupchat = GroupChat(agents=[writer, critic], messages=[], max_round=12) # Create a group chat manager to manage the group chat, use round-robin selection method. manager = GroupChatManager(groupchat=groupchat, llm_config=llm_config, speaker_selection_method=\"round_robin\") # Initiate the chat with the editor, intermediate messages are printed to the console directly. result = editor.initiate_chat( manager, message=\"Write a short story about a robot that discovers it has feelings.\", ) print(result.summary) In v0.4, you can use the RoundRobinGroupChat to achieve the same behavior. import asyncio from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.teams import RoundRobinGroupChat from autogen_agentchat.conditions import TextMentionTermination from autogen_agentchat.ui import Console from autogen_ext.models.openai import OpenAIChatCompletionClient async def main() -> None: model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", seed=42, temperature=0) writer = AssistantAgent( name=\"writer\", description=\"A writer.\", system_message=\"You are a writer.\", model_client=model_client, ) critic = AssistantAgent( name=\"critic\", description=\"A critic.\", system_message=\"You are a critic, provide feedback on the writing. Reply only 'APPROVE' if the task is done.\", model_client=model_client, ) # The termination condition is a text termination, which will cause the chat to terminate when the text \"APPROVE\" is received. termination = TextMentionTermination(\"APPROVE\") # The group chat will alternate between the writer and the critic. group_chat = RoundRobinGroupChat([writer, critic], termination_condition=termination, max_turns=12) # `run_stream` returns an async generator to stream the intermediate messages. stream = group_chat.run_stream(task=\"Write a short story about a robot that discovers it has feelings.\") # `Console` is a simple UI to display the stream. await Console(stream) # Close the connection to the model client. await model_client.close() asyncio.run(main()) For LLM-based speaker selection, you can use the SelectorGroupChat instead. See Selector Group Chat Tutorial and SelectorGroupChat for more details. Note: In v0.4, you do not need to register functions on a user proxy to use tools in a group chat. You can simply pass the tool functions to the AssistantAgent as shown in the Tool Use section. The agent will automatically call the tools when needed. If your tool doesn’t output well formed response, you can use the reflect_on_tool_use parameter to have the model reflect on the tool use. Group Chat with Resume# In v0.2, group chat with resume is a bit complicated. You need to explicitly save the group chat messages and load them back when you want to resume the chat. See Resuming Group Chat in v0.2 for more details. In v0.4, you can simply call run or run_stream again with the same group chat object to resume the chat. To export and load the state, you can use save_state and load_state methods. import asyncio import json from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.teams import RoundRobinGroupChat from autogen_agentchat.conditions import TextMentionTermination from autogen_agentchat.ui import Console from autogen_ext.models.openai import OpenAIChatCompletionClient def create_team(model_client : OpenAIChatCompletionClient) -> RoundRobinGroupChat: writer = AssistantAgent( name=\"writer\", description=\"A writer.\", system_message=\"You are a writer.\", model_client=model_client, ) critic = AssistantAgent( name=\"critic\", description=\"A critic.\", system_message=\"You are a critic, provide feedback on the writing. Reply only 'APPROVE' if the task is done.\", model_client=model_client, ) # The termination condition is a text termination, which will cause the chat to terminate when the text \"APPROVE\" is received. termination = TextMentionTermination(\"APPROVE\") # The group chat will alternate between the writer and the critic. group_chat = RoundRobinGroupChat([writer, critic], termination_condition=termination) return group_chat async def main() -> None: model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", seed=42, temperature=0) # Create team. group_chat = create_team(model_client) # `run_stream` returns an async generator to stream the intermediate messages. stream = group_chat.run_stream(task=\"Write a short story about a robot that discovers it has feelings.\") # `Console` is a simple UI to display the stream. await Console(stream) # Save the state of the group chat and all participants. state = await group_chat.save_state() with open(\"group_chat_state.json\", \"w\") as f: json.dump(state, f) # Create a new team with the same participants configuration. group_chat = create_team(model_client) # Load the state of the group chat and all participants. with open(\"group_chat_state.json\", \"r\") as f: state = json.load(f) await group_chat.load_state(state) # Resume the chat. stream = group_chat.run_stream(task=\"Translate the story into Chinese.\") await Console(stream) # Close the connection to the model client. await model_client.close() asyncio.run(main()) Save and Load Group Chat State# In v0.2, you need to explicitly save the group chat messages and load them back when you want to resume the chat. In v0.4, you can simply call save_state and load_state methods on the group chat object. See Group Chat with Resume for an example. Group Chat with Tool Use# In v0.2 group chat, when tools are involved, you need to register the tool functions on a user proxy, and include the user proxy in the group chat. The tool calls made by other agents will be routed to the user proxy to execute. We have observed numerous issues with this approach, such as the the tool call routing not working as expected, and the tool call request and result cannot be accepted by models without support for function calling. In v0.4, there is no need to register the tool functions on a user proxy, as the tools are directly executed within the AssistantAgent, which publishes the response from the tool to the group chat. So the group chat manager does not need to be involved in routing tool calls. See Selector Group Chat Tutorial for an example of using tools in a group chat. Group Chat with Custom Selector (Stateflow)# In v0.2 group chat, when the speaker_selection_method is set to a custom function, it can override the default selection method. This is useful for implementing a state-based selection method. For more details, see Custom Sepaker Selection in v0.2. In v0.4, you can use the SelectorGroupChat with selector_func to achieve the same behavior. The selector_func is a function that takes the current message thread of the group chat and returns the next speaker’s name. If None is returned, the LLM-based selection method will be used. Here is an example of using the state-based selection method to implement a web search/analysis scenario. import asyncio from typing import Sequence from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination from autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage from autogen_agentchat.teams import SelectorGroupChat from autogen_agentchat.ui import Console from autogen_ext.models.openai import OpenAIChatCompletionClient # Note: This example uses mock tools instead of real APIs for demonstration purposes def search_web_tool(query: str) -> str: if \"2006-2007\" in query: return \"\"\"Here are the total points scored by Miami Heat players in the 2006-2007 season: Udonis Haslem: 844 points Dwayne Wade: 1397 points James Posey: 550 points ... \"\"\" elif \"2007-2008\" in query: return \"The number of total rebounds for Dwayne Wade in the Miami Heat season 2007-2008 is 214.\" elif \"2008-2009\" in query: return \"The number of total rebounds for Dwayne Wade in the Miami Heat season 2008-2009 is 398.\" return \"No data found.\" def percentage_change_tool(start: float, end: float) -> float: return ((end - start) / start) * 100 def create_team(model_client : OpenAIChatCompletionClient) -> SelectorGroupChat: planning_agent = AssistantAgent( \"PlanningAgent\", description=\"An agent for planning tasks, this agent should be the first to engage when given a new task.\", model_client=model_client, system_message=\"\"\" You are a planning agent. Your job is to break down complex tasks into smaller, manageable subtasks. Your team members are: Web search agent: Searches for information Data analyst: Performs calculations You only plan and delegate tasks - you do not execute them yourself. When assigning tasks, use this format: 1. <agent> : <task> After all tasks are complete, summarize the findings and end with \"TERMINATE\". \"\"\", ) web_search_agent = AssistantAgent( \"WebSearchAgent\", description=\"A web search agent.\", tools=[search_web_tool], model_client=model_client, system_message=\"\"\" You are a web search agent. Your only tool is search_tool - use it to find information. You make only one search call at a time. Once you have the results, you never do calculations based on them. \"\"\", ) data_analyst_agent = AssistantAgent( \"DataAnalystAgent\", description=\"A data analyst agent. Useful for performing calculations.\", model_client=model_client, tools=[percentage_change_tool], system_message=\"\"\" You are a data analyst. Given the tasks you have been assigned, you should analyze the data and provide results using the tools provided. \"\"\", ) # The termination condition is a combination of text mention termination and max message termination. text_mention_termination = TextMentionTermination(\"TERMINATE\") max_messages_termination = MaxMessageTermination(max_messages=25) termination = text_mention_termination | max_messages_termination # The selector function is a function that takes the current message thread of the group chat # and returns the next speaker's name. If None is returned, the LLM-based selection method will be used. def selector_func(messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> str | None: if messages[-1].source != planning_agent.name: return planning_agent.name # Always return to the planning agent after the other agents have spoken. return None team = SelectorGroupChat( [planning_agent, web_search_agent, data_analyst_agent], model_client=OpenAIChatCompletionClient(model=\"gpt-4o-mini\"), # Use a smaller model for the selector. termination_condition=termination, selector_func=selector_func, ) return team async def main() -> None: model_client = OpenAIChatCompletionClient(model=\"gpt-4o\") team = create_team(model_client) task = \"Who was the Miami Heat player with the highest points in the 2006-2007 season, and what was the percentage change in his total rebounds between the 2007-2008 and 2008-2009 seasons?\" await Console(team.run_stream(task=task)) asyncio.run(main()) Nested Chat# Nested chat allows you to nest a whole team or another agent inside an agent. This is useful for creating a hierarchical structure of agents or “information silos”, as the nested agents cannot communicate directly with other agents outside of the same group. In v0.2, nested chat is supported by using the register_nested_chats method on the ConversableAgent class. You need to specify the nested sequence of agents using dictionaries, See Nested Chat in v0.2 for more details. In v0.4, nested chat is an implementation detail of a custom agent. You can create a custom agent that takes a team or another agent as a parameter and implements the on_messages method to trigger the nested team or agent. It is up to the application to decide how to pass or transform the messages from and to the nested team or agent. The following example shows a simple nested chat that counts numbers. import asyncio from typing import Sequence from autogen_core import CancellationToken from autogen_agentchat.agents import BaseChatAgent from autogen_agentchat.teams import RoundRobinGroupChat from autogen_agentchat.messages import TextMessage, BaseChatMessage from autogen_agentchat.base import Response class CountingAgent(BaseChatAgent): \"\"\"An agent that returns a new number by adding 1 to the last number in the input messages.\"\"\" async def on_messages(self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) -> Response: if len(messages) == 0: last_number = 0 # Start from 0 if no messages are given. else: assert isinstance(messages[-1], TextMessage) last_number = int(messages[-1].content) # Otherwise, start from the last number. return Response(chat_message=TextMessage(content=str(last_number + 1), source=self.name)) async def on_reset(self, cancellation_token: CancellationToken) -> None: pass @property def produced_message_types(self) -> Sequence[type[BaseChatMessage]]: return (TextMessage,) class NestedCountingAgent(BaseChatAgent): \"\"\"An agent that increments the last number in the input messages multiple times using a nested counting team.\"\"\" def __init__(self, name: str, counting_team: RoundRobinGroupChat) -> None: super().__init__(name, description=\"An agent that counts numbers.\") self._counting_team = counting_team async def on_messages(self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) -> Response: # Run the inner team with the given messages and returns the last message produced by the team. result = await self._counting_team.run(task=messages, cancellation_token=cancellation_token) # To stream the inner messages, implement `on_messages_stream` and use that to implement `on_messages`. assert isinstance(result.messages[-1], TextMessage) return Response(chat_message=result.messages[-1], inner_messages=result.messages[len(messages):-1]) async def on_reset(self, cancellation_token: CancellationToken) -> None: # Reset the inner team. await self._counting_team.reset() @property def produced_message_types(self) -> Sequence[type[BaseChatMessage]]: return (TextMessage,) async def main() -> None: # Create a team of two counting agents as the inner team. counting_agent_1 = CountingAgent(\"counting_agent_1\", description=\"An agent that counts numbers.\") counting_agent_2 = CountingAgent(\"counting_agent_2\", description=\"An agent that counts numbers.\") counting_team = RoundRobinGroupChat([counting_agent_1, counting_agent_2], max_turns=5) # Create a nested counting agent that takes the inner team as a parameter. nested_counting_agent = NestedCountingAgent(\"nested_counting_agent\", counting_team) # Run the nested counting agent with a message starting from 1. response = await nested_counting_agent.on_messages([TextMessage(content=\"1\", source=\"user\")], CancellationToken()) assert response.inner_messages is not None for message in response.inner_messages: print(message) print(response.chat_message) asyncio.run(main()) You should see the following output: source='counting_agent_1' models_usage=None content='2' type='TextMessage' source='counting_agent_2' models_usage=None content='3' type='TextMessage' source='counting_agent_1' models_usage=None content='4' type='TextMessage' source='counting_agent_2' models_usage=None content='5' type='TextMessage' source='counting_agent_1' models_usage=None content='6' type='TextMessage' You can take a look at SocietyOfMindAgent for a more complex implementation. Sequential Chat# In v0.2, sequential chat is supported by using the initiate_chats function. It takes input a list of dictionary configurations for each step of the sequence. See Sequential Chat in v0.2 for more details. Base on the feedback from the community, the initiate_chats function is too opinionated and not flexible enough to support the diverse set of scenarios that users want to implement. We often find users struggling to get the initiate_chats function to work when they can easily glue the steps together usign basic Python code. Therefore, in v0.4, we do not provide a built-in function for sequential chat in the AgentChat API. Instead, you can create an event-driven sequential workflow using the Core API, and use the other components provided the AgentChat API to implement each step of the workflow. See an example of sequential workflow in the Core API Tutorial. We recognize that the concept of workflow is at the heart of many applications, and we will provide more built-in support for workflows in the future. GPTAssistantAgent# In v0.2, GPTAssistantAgent is a special agent class that is backed by the OpenAI Assistant API. In v0.4, the equivalent is the OpenAIAssistantAgent class. It supports the same set of features as the GPTAssistantAgent in v0.2 with more such as customizable threads and file uploads. See OpenAIAssistantAgent for more details. Long Context Handling# In v0.2, long context that overflows the model’s context window can be handled by using the transforms capability that is added to an ConversableAgent after which is contructed. The feedbacks from our community has led us to believe this feature is essential and should be a built-in component of AssistantAgent, and can be used for every custom agent. In v0.4, we introduce the ChatCompletionContext base class that manages message history and provides a virtual view of the history. Applications can use built-in implementations such as BufferedChatCompletionContext to limit the message history sent to the model, or provide their own implementations that creates different virtual views. To use BufferedChatCompletionContext in an AssistantAgent in a chatbot scenario. import asyncio from autogen_agentchat.messages import TextMessage from autogen_agentchat.agents import AssistantAgent from autogen_core import CancellationToken from autogen_core.model_context import BufferedChatCompletionContext from autogen_ext.models.openai import OpenAIChatCompletionClient async def main() -> None: model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", seed=42, temperature=0) assistant = AssistantAgent( name=\"assistant\", system_message=\"You are a helpful assistant.\", model_client=model_client, model_context=BufferedChatCompletionContext(buffer_size=10), # Model can only view the last 10 messages. ) while True: user_input = input(\"User: \") if user_input == \"exit\": break response = await assistant.on_messages([TextMessage(content=user_input, source=\"user\")], CancellationToken()) print(\"Assistant:\", response.chat_message.to_text()) await model_client.close() asyncio.run(main()) In this example, the chatbot can only read the last 10 messages in the history. Observability and Control# In v0.4 AgentChat, you can observe the agents by using the on_messages_stream method which returns an async generator to stream the inner thoughts and actions of the agent. For teams, you can use the run_stream method to stream the inner conversation among the agents in the team. Your application can use these streams to observe the agents and teams in real-time. Both the on_messages_stream and run_stream methods takes a CancellationToken as a parameter which can be used to cancel the output stream asynchronously and stop the agent or team. For teams, you can also use termination conditions to stop the team when a certain condition is met. See Termination Condition Tutorial for more details. Unlike the v0.2 which comes with a special logging module, the v0.4 API simply uses Python’s logging module to log events such as model client calls. See Logging in the Core API documentation for more details. Code Executors# The code executors in v0.2 and v0.4 are nearly identical except the v0.4 executors support async API. You can also use CancellationToken to cancel a code execution if it takes too long. See Command Line Code Executors Tutorial in the Core API documentation. We also added ACADynamicSessionsCodeExecutor that can use Azure Container Apps (ACA) dynamic sessions for code execution. See ACA Dynamic Sessions Code Executor Docs. previous Quickstart next Introduction",
      "code": "v0.2.*"
    },
    {
      "description": "Here is how you can call the assistant agent in v0.4 directly, continuing from the above example:",
      "code": "v0.4"
    },
    {
      "description": "In v0.2, you get a ChatResult object from the initiate_chat method. For example:",
      "code": "v0.2"
    }
  ],
  "links": [
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/migration-guide.html",
    "https://microsoft.github.io/autogen/stable/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/extensions-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/reference/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/installation.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/quickstart.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/models.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/messages.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/agents.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/teams.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/human-in-the-loop.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/termination.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/state.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/custom-agents.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/selector-group-chat.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/swarm.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/magentic-one.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/graph-flow.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/memory.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/logging.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/serialize-components.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tracing.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/examples/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/examples/travel-planning.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/examples/company-research.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/examples/literature-review.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/framework/component-config.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.openai.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/components/model-clients.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.cache.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.cache_store.diskcache.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.cache_store.redis.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.agents.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.memory.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.teams.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.base.html",
    "https://microsoft.github.io/autogen/stable/user-guide/extensions-user-guide/discover.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/design-patterns/sequential-workflow.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.openai.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.model_context.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/framework/logging.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/components/command-line-code-executors.html",
    "https://microsoft.github.io/autogen/stable/user-guide/extensions-user-guide/azure-container-code-executor.html"
  ]
}