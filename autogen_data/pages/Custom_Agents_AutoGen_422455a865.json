{
  "url": "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/custom-agents.html",
  "title": "Custom Agents — AutoGen",
  "content": "You may have agents with behaviors that do not fall into a preset. In such cases, you can build custom agents.\n\nAll agents in AgentChat inherit from BaseChatAgent class and implement the following abstract methods and attributes:\n\non_messages(): The abstract method that defines the behavior of the agent in response to messages. This method is called when the agent is asked to provide a response in run(). It returns a Response object.\n\non_reset(): The abstract method that resets the agent to its initial state. This method is called when the agent is asked to reset itself.\n\nproduced_message_types: The list of possible BaseChatMessage message types the agent can produce in its response.\n\nOptionally, you can implement the the on_messages_stream() method to stream messages as they are generated by the agent. This method is called by run_stream() to stream messages. If this method is not implemented, the agent uses the default implementation of on_messages_stream() that calls the on_messages() method and yields all messages in the response.\n\nIn this example, we create a simple agent that counts down from a given number to zero, and produces a stream of messages with the current count.\n\nIn this example, we create an agent class that can perform simple arithmetic operations on a given integer. Then, we will use different instances of this agent class in a SelectorGroupChat to transform a given integer into another integer by applying a sequence of arithmetic operations.\n\nThe ArithmeticAgent class takes an operator_func that takes an integer and returns an integer, after applying an arithmetic operation to the integer. In its on_messages method, it applies the operator_func to the integer in the input message, and returns a response with the result.\n\nThe on_messages method may be called with an empty list of messages, in which case it means the agent was called previously and is now being called again, without any new messages from the caller. So it is important to keep a history of the previous messages received by the agent, and use that history to generate the response.\n\nNow we can create a SelectorGroupChat with 5 instances of ArithmeticAgent:\n\none that adds 1 to the input integer,\n\none that subtracts 1 from the input integer,\n\none that multiplies the input integer by 2,\n\none that divides the input integer by 2 and rounds down to the nearest integer, and\n\none that returns the input integer unchanged.\n\nWe then create a SelectorGroupChat with these agents, and set the appropriate selector settings:\n\nallow the same agent to be selected consecutively to allow for repeated operations, and\n\ncustomize the selector prompt to tailor the model’s response to the specific task.\n\nFrom the output, we can see that the agents have successfully transformed the input integer from 10 to 25 by choosing appropriate agents that apply the arithmetic operations in sequence.\n\nOne of the key features of the AssistantAgent preset in AgentChat is that it takes a model_client argument and can use it in responding to messages. However, in some cases, you may want your agent to use a custom model client that is not currently supported (see supported model clients) or custom model behaviours.\n\nYou can accomplish this with a custom agent that implements your custom model client.\n\nIn the example below, we will walk through an example of a custom agent that uses the Google Gemini SDK directly to respond to messages.\n\nNote: You will need to install the Google Gemini SDK to run this example. You can install it using the following command:\n\nIn the example above, we have chosen to provide model, api_key and system_message as arguments - you can choose to provide any other arguments that are required by the model client you are using or fits with your application design.\n\nNow, let us explore how to use this custom agent as part of a team in AgentChat.\n\nIn section above, we show several very important concepts:\n\nWe have developed a custom agent that uses the Google Gemini SDK to respond to messages.\n\nWe show that this custom agent can be used as part of the broader AgentChat ecosystem - in this case as a participant in a RoundRobinGroupChat as long as it inherits from BaseChatAgent.\n\nAutogen provides a Component interface for making the configuration of components serializable to a declarative format. This is useful for saving and loading configurations, and for sharing configurations with others.\n\nWe accomplish this by inheriting from the Component class and implementing the _from_config and _to_config methods. The declarative class can be serialized to a JSON format using the dump_component method, and deserialized from a JSON format using the load_component method.\n\nNow that we have the required methods implemented, we can now load and dump the custom agent to and from a JSON format, and then load the agent from the JSON format.\n\nNote: You should set the component_provider_override class variable to the full path of the module containing the custom agent class e.g., (mypackage.agents.GeminiAssistantAgent). This is used by load_component method to determine how to instantiate the class.\n\nSo far, we have seen how to create custom agents, add custom model clients to agents, and make custom agents declarative. There are a few ways in which this basic sample can be extended:\n\nExtend the Gemini model client to handle function calling similar to the AssistantAgent class. https://ai.google.dev/gemini-api/docs/function-calling\n\nImplement a package with a custom agent and experiment with using its declarative format in a tool like AutoGen Studio.",
  "headings": [
    {
      "level": "h1",
      "text": "Custom Agents#",
      "id": ""
    },
    {
      "level": "h2",
      "text": "CountDownAgent#",
      "id": ""
    },
    {
      "level": "h2",
      "text": "ArithmeticAgent#",
      "id": ""
    },
    {
      "level": "h2",
      "text": "Using Custom Model Clients in Custom Agents#",
      "id": ""
    },
    {
      "level": "h2",
      "text": "Making the Custom Agent Declarative#",
      "id": ""
    },
    {
      "level": "h2",
      "text": "Next Steps#",
      "id": ""
    }
  ],
  "code_samples": [
    {
      "code": "from typing import AsyncGenerator, List, Sequence\n\nfrom autogen_agentchat.agents import BaseChatAgent\nfrom autogen_agentchat.base import Response\nfrom autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage, TextMessage\nfrom autogen_core import CancellationToken\n\n\nclass CountDownAgent(BaseChatAgent):\n    def __init__(self, name: str, count: int = 3):\n        super().__init__(name, \"A simple agent that counts down.\")\n        self._count = count\n\n    @property\n    def produced_message_types(self) -> Sequence[type[BaseChatMessage]]:\n        return (TextMessage,)\n\n    async def on_messages(self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) -> Response:\n        # Calls the on_messages_stream.\n        response: Response | None = None\n        async for message in self.on_messages_stream(messages, cancellation_token):\n            if isinstance(message, Response):\n                response = message\n        assert response is not None\n        return response\n\n    async def on_messages_stream(\n        self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken\n    ) -> AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None]:\n        inner_messages: List[BaseAgentEvent | BaseChatMessage] = []\n        for i in range(self._count, 0, -1):\n            msg = TextMessage(content=f\"{i}...\", source=self.name)\n            inner_messages.append(msg)\n            yield msg\n        # The response is returned at the end of the stream.\n        # It contains the final message and all the inner messages.\n        yield Response(chat_message=TextMessage(content=\"Done!\", source=self.name), inner_messages=inner_messages)\n\n    async def on_reset(self, cancellation_token: CancellationToken) -> None:\n        pass\n\n\nasync def run_countdown_agent() -> None:\n    # Create a countdown agent.\n    countdown_agent = CountDownAgent(\"countdown\")\n\n    # Run the agent with a given task and stream the response.\n    async for message in countdown_agent.on_messages_stream([], CancellationToken()):\n        if isinstance(message, Response):\n            print(message.chat_message)\n        else:\n            print(message)\n\n\n# Use asyncio.run(run_countdown_agent()) when running in a script.\nawait run_countdown_agent()",
      "language": "python"
    },
    {
      "code": "from typing import AsyncGenerator, List, Sequence\n\nfrom autogen_agentchat.agents import BaseChatAgent\nfrom autogen_agentchat.base import Response\nfrom autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage, TextMessage\nfrom autogen_core import CancellationToken\n\n\nclass CountDownAgent(BaseChatAgent):\n    def __init__(self, name: str, count: int = 3):\n        super().__init__(name, \"A simple agent that counts down.\")\n        self._count = count\n\n    @property\n    def produced_message_types(self) -> Sequence[type[BaseChatMessage]]:\n        return (TextMessage,)\n\n    async def on_messages(self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) -> Response:\n        # Calls the on_messages_stream.\n        response: Response | None = None\n        async for message in self.on_messages_stream(messages, cancellation_token):\n            if isinstance(message, Response):\n                response = message\n        assert response is not None\n        return response\n\n    async def on_messages_stream(\n        self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken\n    ) -> AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None]:\n        inner_messages: List[BaseAgentEvent | BaseChatMessage] = []\n        for i in range(self._count, 0, -1):\n            msg = TextMessage(content=f\"{i}...\", source=self.name)\n            inner_messages.append(msg)\n            yield msg\n        # The response is returned at the end of the stream.\n        # It contains the final message and all the inner messages.\n        yield Response(chat_message=TextMessage(content=\"Done!\", source=self.name), inner_messages=inner_messages)\n\n    async def on_reset(self, cancellation_token: CancellationToken) -> None:\n        pass\n\n\nasync def run_countdown_agent() -> None:\n    # Create a countdown agent.\n    countdown_agent = CountDownAgent(\"countdown\")\n\n    # Run the agent with a given task and stream the response.\n    async for message in countdown_agent.on_messages_stream([], CancellationToken()):\n        if isinstance(message, Response):\n            print(message.chat_message)\n        else:\n            print(message)\n\n\n# Use asyncio.run(run_countdown_agent()) when running in a script.\nawait run_countdown_agent()",
      "language": "python"
    },
    {
      "code": "3...\n2...\n1...\nDone!",
      "language": "unknown"
    },
    {
      "code": "3...\n2...\n1...\nDone!",
      "language": "unknown"
    },
    {
      "code": "from typing import Callable, Sequence\n\nfrom autogen_agentchat.agents import BaseChatAgent\nfrom autogen_agentchat.base import Response\nfrom autogen_agentchat.conditions import MaxMessageTermination\nfrom autogen_agentchat.messages import BaseChatMessage\nfrom autogen_agentchat.teams import SelectorGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_core import CancellationToken\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n\nclass ArithmeticAgent(BaseChatAgent):\n    def __init__(self, name: str, description: str, operator_func: Callable[[int], int]) -> None:\n        super().__init__(name, description=description)\n        self._operator_func = operator_func\n        self._message_history: List[BaseChatMessage] = []\n\n    @property\n    def produced_message_types(self) -> Sequence[type[BaseChatMessage]]:\n        return (TextMessage,)\n\n    async def on_messages(self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) -> Response:\n        # Update the message history.\n        # NOTE: it is possible the messages is an empty list, which means the agent was selected previously.\n        self._message_history.extend(messages)\n        # Parse the number in the last message.\n        assert isinstance(self._message_history[-1], TextMessage)\n        number = int(self._message_history[-1].content)\n        # Apply the operator function to the number.\n        result = self._operator_func(number)\n        # Create a new message with the result.\n        response_message = TextMessage(content=str(result), source=self.name)\n        # Update the message history.\n        self._message_history.append(response_message)\n        # Return the response.\n        return Response(chat_message=response_message)\n\n    async def on_reset(self, cancellation_token: CancellationToken) -> None:\n        pass",
      "language": "python"
    },
    {
      "code": "from typing import Callable, Sequence\n\nfrom autogen_agentchat.agents import BaseChatAgent\nfrom autogen_agentchat.base import Response\nfrom autogen_agentchat.conditions import MaxMessageTermination\nfrom autogen_agentchat.messages import BaseChatMessage\nfrom autogen_agentchat.teams import SelectorGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_core import CancellationToken\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n\nclass ArithmeticAgent(BaseChatAgent):\n    def __init__(self, name: str, description: str, operator_func: Callable[[int], int]) -> None:\n        super().__init__(name, description=description)\n        self._operator_func = operator_func\n        self._message_history: List[BaseChatMessage] = []\n\n    @property\n    def produced_message_types(self) -> Sequence[type[BaseChatMessage]]:\n        return (TextMessage,)\n\n    async def on_messages(self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) -> Response:\n        # Update the message history.\n        # NOTE: it is possible the messages is an empty list, which means the agent was selected previously.\n        self._message_history.extend(messages)\n        # Parse the number in the last message.\n        assert isinstance(self._message_history[-1], TextMessage)\n        number = int(self._message_history[-1].content)\n        # Apply the operator function to the number.\n        result = self._operator_func(number)\n        # Create a new message with the result.\n        response_message = TextMessage(content=str(result), source=self.name)\n        # Update the message history.\n        self._message_history.append(response_message)\n        # Return the response.\n        return Response(chat_message=response_message)\n\n    async def on_reset(self, cancellation_token: CancellationToken) -> None:\n        pass",
      "language": "python"
    },
    {
      "code": "async def run_number_agents() -> None:\n    # Create agents for number operations.\n    add_agent = ArithmeticAgent(\"add_agent\", \"Adds 1 to the number.\", lambda x: x + 1)\n    multiply_agent = ArithmeticAgent(\"multiply_agent\", \"Multiplies the number by 2.\", lambda x: x * 2)\n    subtract_agent = ArithmeticAgent(\"subtract_agent\", \"Subtracts 1 from the number.\", lambda x: x - 1)\n    divide_agent = ArithmeticAgent(\"divide_agent\", \"Divides the number by 2 and rounds down.\", lambda x: x // 2)\n    identity_agent = ArithmeticAgent(\"identity_agent\", \"Returns the number as is.\", lambda x: x)\n\n    # The termination condition is to stop after 10 messages.\n    termination_condition = MaxMessageTermination(10)\n\n    # Create a selector group chat.\n    selector_group_chat = SelectorGroupChat(\n        [add_agent, multiply_agent, subtract_agent, divide_agent, identity_agent],\n        model_client=OpenAIChatCompletionClient(model=\"gpt-4o\"),\n        termination_condition=termination_condition,\n        allow_repeated_speaker=True,  # Allow the same agent to speak multiple times, necessary for this task.\n        selector_prompt=(\n            \"Available roles:\\n{roles}\\nTheir job descriptions:\\n{participants}\\n\"\n            \"Current conversation history:\\n{history}\\n\"\n            \"Please select the most appropriate role for the next message, and only return the role name.\"\n        ),\n    )\n\n    # Run the selector group chat with a given task and stream the response.\n    task: List[BaseChatMessage] = [\n        TextMessage(content=\"Apply the operations to turn the given number into 25.\", source=\"user\"),\n        TextMessage(content=\"10\", source=\"user\"),\n    ]\n    stream = selector_group_chat.run_stream(task=task)\n    await Console(stream)\n\n\n# Use asyncio.run(run_number_agents()) when running in a script.\nawait run_number_agents()",
      "language": "python"
    },
    {
      "code": "async def run_number_agents() -> None:\n    # Create agents for number operations.\n    add_agent = ArithmeticAgent(\"add_agent\", \"Adds 1 to the number.\", lambda x: x + 1)\n    multiply_agent = ArithmeticAgent(\"multiply_agent\", \"Multiplies the number by 2.\", lambda x: x * 2)\n    subtract_agent = ArithmeticAgent(\"subtract_agent\", \"Subtracts 1 from the number.\", lambda x: x - 1)\n    divide_agent = ArithmeticAgent(\"divide_agent\", \"Divides the number by 2 and rounds down.\", lambda x: x // 2)\n    identity_agent = ArithmeticAgent(\"identity_agent\", \"Returns the number as is.\", lambda x: x)\n\n    # The termination condition is to stop after 10 messages.\n    termination_condition = MaxMessageTermination(10)\n\n    # Create a selector group chat.\n    selector_group_chat = SelectorGroupChat(\n        [add_agent, multiply_agent, subtract_agent, divide_agent, identity_agent],\n        model_client=OpenAIChatCompletionClient(model=\"gpt-4o\"),\n        termination_condition=termination_condition,\n        allow_repeated_speaker=True,  # Allow the same agent to speak multiple times, necessary for this task.\n        selector_prompt=(\n            \"Available roles:\\n{roles}\\nTheir job descriptions:\\n{participants}\\n\"\n            \"Current conversation history:\\n{history}\\n\"\n            \"Please select the most appropriate role for the next message, and only return the role name.\"\n        ),\n    )\n\n    # Run the selector group chat with a given task and stream the response.\n    task: List[BaseChatMessage] = [\n        TextMessage(content=\"Apply the operations to turn the given number into 25.\", source=\"user\"),\n        TextMessage(content=\"10\", source=\"user\"),\n    ]\n    stream = selector_group_chat.run_stream(task=task)\n    await Console(stream)\n\n\n# Use asyncio.run(run_number_agents()) when running in a script.\nawait run_number_agents()",
      "language": "python"
    },
    {
      "code": "---------- user ----------\nApply the operations to turn the given number into 25.\n---------- user ----------\n10\n---------- multiply_agent ----------\n20\n---------- add_agent ----------\n21\n---------- multiply_agent ----------\n42\n---------- divide_agent ----------\n21\n---------- add_agent ----------\n22\n---------- add_agent ----------\n23\n---------- add_agent ----------\n24\n---------- add_agent ----------\n25\n---------- Summary ----------\nNumber of messages: 10\nFinish reason: Maximum number of messages 10 reached, current message count: 10\nTotal prompt tokens: 0\nTotal completion tokens: 0\nDuration: 2.40 seconds",
      "language": "yaml"
    },
    {
      "code": "---------- user ----------\nApply the operations to turn the given number into 25.\n---------- user ----------\n10\n---------- multiply_agent ----------\n20\n---------- add_agent ----------\n21\n---------- multiply_agent ----------\n42\n---------- divide_agent ----------\n21\n---------- add_agent ----------\n22\n---------- add_agent ----------\n23\n---------- add_agent ----------\n24\n---------- add_agent ----------\n25\n---------- Summary ----------\nNumber of messages: 10\nFinish reason: Maximum number of messages 10 reached, current message count: 10\nTotal prompt tokens: 0\nTotal completion tokens: 0\nDuration: 2.40 seconds",
      "language": "yaml"
    },
    {
      "code": "pip install google-genai",
      "language": "unknown"
    },
    {
      "code": "pip install google-genai",
      "language": "unknown"
    },
    {
      "code": "# !pip install google-genai\nimport os\nfrom typing import AsyncGenerator, Sequence\n\nfrom autogen_agentchat.agents import BaseChatAgent\nfrom autogen_agentchat.base import Response\nfrom autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage\nfrom autogen_core import CancellationToken\nfrom autogen_core.model_context import UnboundedChatCompletionContext\nfrom autogen_core.models import AssistantMessage, RequestUsage, UserMessage\nfrom google import genai\nfrom google.genai import types\n\n\nclass GeminiAssistantAgent(BaseChatAgent):\n    def __init__(\n        self,\n        name: str,\n        description: str = \"An agent that provides assistance with ability to use tools.\",\n        model: str = \"gemini-1.5-flash-002\",\n        api_key: str = os.environ[\"GEMINI_API_KEY\"],\n        system_message: str\n        | None = \"You are a helpful assistant that can respond to messages. Reply with TERMINATE when the task has been completed.\",\n    ):\n        super().__init__(name=name, description=description)\n        self._model_context = UnboundedChatCompletionContext()\n        self._model_client = genai.Client(api_key=api_key)\n        self._system_message = system_message\n        self._model = model\n\n    @property\n    def produced_message_types(self) -> Sequence[type[BaseChatMessage]]:\n        return (TextMessage,)\n\n    async def on_messages(self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) -> Response:\n        final_response = None\n        async for message in self.on_messages_stream(messages, cancellation_token):\n            if isinstance(message, Response):\n                final_response = message\n\n        if final_response is None:\n            raise AssertionError(\"The stream should have returned the final result.\")\n\n        return final_response\n\n    async def on_messages_stream(\n        self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken\n    ) -> AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None]:\n        # Add messages to the model context\n        for msg in messages:\n            await self._model_context.add_message(msg.to_model_message())\n\n        # Get conversation history\n        history = [\n            (msg.source if hasattr(msg, \"source\") else \"system\")\n            + \": \"\n            + (msg.content if isinstance(msg.content, str) else \"\")\n            + \"\\n\"\n            for msg in await self._model_context.get_messages()\n        ]\n        # Generate response using Gemini\n        response = self._model_client.models.generate_content(\n            model=self._model,\n            contents=f\"History: {history}\\nGiven the history, please provide a response\",\n            config=types.GenerateContentConfig(\n                system_instruction=self._system_message,\n                temperature=0.3,\n            ),\n        )\n\n        # Create usage metadata\n        usage = RequestUsage(\n            prompt_tokens=response.usage_metadata.prompt_token_count,\n            completion_tokens=response.usage_metadata.candidates_token_count,\n        )\n\n        # Add response to model context\n        await self._model_context.add_message(AssistantMessage(content=response.text, source=self.name))\n\n        # Yield the final response\n        yield Response(\n            chat_message=TextMessage(content=response.text, source=self.name, models_usage=usage),\n            inner_messages=[],\n        )\n\n    async def on_reset(self, cancellation_token: CancellationToken) -> None:\n        \"\"\"Reset the assistant by clearing the model context.\"\"\"\n        await self._model_context.clear()",
      "language": "python"
    },
    {
      "code": "# !pip install google-genai\nimport os\nfrom typing import AsyncGenerator, Sequence\n\nfrom autogen_agentchat.agents import BaseChatAgent\nfrom autogen_agentchat.base import Response\nfrom autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage\nfrom autogen_core import CancellationToken\nfrom autogen_core.model_context import UnboundedChatCompletionContext\nfrom autogen_core.models import AssistantMessage, RequestUsage, UserMessage\nfrom google import genai\nfrom google.genai import types\n\n\nclass GeminiAssistantAgent(BaseChatAgent):\n    def __init__(\n        self,\n        name: str,\n        description: str = \"An agent that provides assistance with ability to use tools.\",\n        model: str = \"gemini-1.5-flash-002\",\n        api_key: str = os.environ[\"GEMINI_API_KEY\"],\n        system_message: str\n        | None = \"You are a helpful assistant that can respond to messages. Reply with TERMINATE when the task has been completed.\",\n    ):\n        super().__init__(name=name, description=description)\n        self._model_context = UnboundedChatCompletionContext()\n        self._model_client = genai.Client(api_key=api_key)\n        self._system_message = system_message\n        self._model = model\n\n    @property\n    def produced_message_types(self) -> Sequence[type[BaseChatMessage]]:\n        return (TextMessage,)\n\n    async def on_messages(self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) -> Response:\n        final_response = None\n        async for message in self.on_messages_stream(messages, cancellation_token):\n            if isinstance(message, Response):\n                final_response = message\n\n        if final_response is None:\n            raise AssertionError(\"The stream should have returned the final result.\")\n\n        return final_response\n\n    async def on_messages_stream(\n        self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken\n    ) -> AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None]:\n        # Add messages to the model context\n        for msg in messages:\n            await self._model_context.add_message(msg.to_model_message())\n\n        # Get conversation history\n        history = [\n            (msg.source if hasattr(msg, \"source\") else \"system\")\n            + \": \"\n            + (msg.content if isinstance(msg.content, str) else \"\")\n            + \"\\n\"\n            for msg in await self._model_context.get_messages()\n        ]\n        # Generate response using Gemini\n        response = self._model_client.models.generate_content(\n            model=self._model,\n            contents=f\"History: {history}\\nGiven the history, please provide a response\",\n            config=types.GenerateContentConfig(\n                system_instruction=self._system_message,\n                temperature=0.3,\n            ),\n        )\n\n        # Create usage metadata\n        usage = RequestUsage(\n            prompt_tokens=response.usage_metadata.prompt_token_count,\n            completion_tokens=response.usage_metadata.candidates_token_count,\n        )\n\n        # Add response to model context\n        await self._model_context.add_message(AssistantMessage(content=response.text, source=self.name))\n\n        # Yield the final response\n        yield Response(\n            chat_message=TextMessage(content=response.text, source=self.name, models_usage=usage),\n            inner_messages=[],\n        )\n\n    async def on_reset(self, cancellation_token: CancellationToken) -> None:\n        \"\"\"Reset the assistant by clearing the model context.\"\"\"\n        await self._model_context.clear()",
      "language": "python"
    },
    {
      "code": "gemini_assistant = GeminiAssistantAgent(\"gemini_assistant\")\nawait Console(gemini_assistant.run_stream(task=\"What is the capital of New York?\"))",
      "language": "csharp"
    },
    {
      "code": "gemini_assistant = GeminiAssistantAgent(\"gemini_assistant\")\nawait Console(gemini_assistant.run_stream(task=\"What is the capital of New York?\"))",
      "language": "csharp"
    },
    {
      "code": "---------- user ----------\nWhat is the capital of New York?\n---------- gemini_assistant ----------\nAlbany\nTERMINATE",
      "language": "yaml"
    },
    {
      "code": "---------- user ----------\nWhat is the capital of New York?\n---------- gemini_assistant ----------\nAlbany\nTERMINATE",
      "language": "yaml"
    },
    {
      "code": "TaskResult(messages=[TextMessage(source='user', models_usage=None, content='What is the capital of New York?', type='TextMessage'), TextMessage(source='gemini_assistant', models_usage=RequestUsage(prompt_tokens=46, completion_tokens=5), content='Albany\\nTERMINATE\\n', type='TextMessage')], stop_reason=None)",
      "language": "rust"
    },
    {
      "code": "TaskResult(messages=[TextMessage(source='user', models_usage=None, content='What is the capital of New York?', type='TextMessage'), TextMessage(source='gemini_assistant', models_usage=RequestUsage(prompt_tokens=46, completion_tokens=5), content='Albany\\nTERMINATE\\n', type='TextMessage')], stop_reason=None)",
      "language": "rust"
    },
    {
      "code": "from autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.conditions import TextMentionTermination\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.ui import Console\n\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n\n# Create the primary agent.\nprimary_agent = AssistantAgent(\n    \"primary\",\n    model_client=model_client,\n    system_message=\"You are a helpful AI assistant.\",\n)\n\n# Create a critic agent based on our new GeminiAssistantAgent.\ngemini_critic_agent = GeminiAssistantAgent(\n    \"gemini_critic\",\n    system_message=\"Provide constructive feedback. Respond with 'APPROVE' to when your feedbacks are addressed.\",\n)\n\n\n# Define a termination condition that stops the task if the critic approves or after 10 messages.\ntermination = TextMentionTermination(\"APPROVE\") | MaxMessageTermination(10)\n\n# Create a team with the primary and critic agents.\nteam = RoundRobinGroupChat([primary_agent, gemini_critic_agent], termination_condition=termination)\n\nawait Console(team.run_stream(task=\"Write a Haiku poem with 4 lines about the fall season.\"))\nawait model_client.close()",
      "language": "python"
    },
    {
      "code": "from autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.conditions import TextMentionTermination\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.ui import Console\n\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n\n# Create the primary agent.\nprimary_agent = AssistantAgent(\n    \"primary\",\n    model_client=model_client,\n    system_message=\"You are a helpful AI assistant.\",\n)\n\n# Create a critic agent based on our new GeminiAssistantAgent.\ngemini_critic_agent = GeminiAssistantAgent(\n    \"gemini_critic\",\n    system_message=\"Provide constructive feedback. Respond with 'APPROVE' to when your feedbacks are addressed.\",\n)\n\n\n# Define a termination condition that stops the task if the critic approves or after 10 messages.\ntermination = TextMentionTermination(\"APPROVE\") | MaxMessageTermination(10)\n\n# Create a team with the primary and critic agents.\nteam = RoundRobinGroupChat([primary_agent, gemini_critic_agent], termination_condition=termination)\n\nawait Console(team.run_stream(task=\"Write a Haiku poem with 4 lines about the fall season.\"))\nawait model_client.close()",
      "language": "python"
    },
    {
      "code": "---------- user ----------\nWrite a Haiku poem with 4 lines about the fall season.\n---------- primary ----------\nCrimson leaves cascade,  \nWhispering winds sing of change,  \nChill wraps the fading,  \nNature's quilt, rich and warm.\n---------- gemini_critic ----------\nThe poem is good, but it has four lines instead of three.  A haiku must have three lines with a 5-7-5 syllable structure.  The content is evocative of autumn, but the form is incorrect.  Please revise to adhere to the haiku's syllable structure.\n\n---------- primary ----------\nThank you for your feedback! Here’s a revised haiku that follows the 5-7-5 syllable structure:\n\nCrimson leaves drift down,  \nChill winds whisper through the gold,  \nAutumn’s breath is near.\n---------- gemini_critic ----------\nThe revised haiku is much improved.  It correctly follows the 5-7-5 syllable structure and maintains the evocative imagery of autumn.  APPROVE",
      "language": "yaml"
    },
    {
      "code": "---------- user ----------\nWrite a Haiku poem with 4 lines about the fall season.\n---------- primary ----------\nCrimson leaves cascade,  \nWhispering winds sing of change,  \nChill wraps the fading,  \nNature's quilt, rich and warm.\n---------- gemini_critic ----------\nThe poem is good, but it has four lines instead of three.  A haiku must have three lines with a 5-7-5 syllable structure.  The content is evocative of autumn, but the form is incorrect.  Please revise to adhere to the haiku's syllable structure.\n\n---------- primary ----------\nThank you for your feedback! Here’s a revised haiku that follows the 5-7-5 syllable structure:\n\nCrimson leaves drift down,  \nChill winds whisper through the gold,  \nAutumn’s breath is near.\n---------- gemini_critic ----------\nThe revised haiku is much improved.  It correctly follows the 5-7-5 syllable structure and maintains the evocative imagery of autumn.  APPROVE",
      "language": "yaml"
    },
    {
      "code": "TaskResult(messages=[TextMessage(source='user', models_usage=None, content='Write a Haiku poem with 4 lines about the fall season.', type='TextMessage'), TextMessage(source='primary', models_usage=RequestUsage(prompt_tokens=33, completion_tokens=31), content=\"Crimson leaves cascade,  \\nWhispering winds sing of change,  \\nChill wraps the fading,  \\nNature's quilt, rich and warm.\", type='TextMessage'), TextMessage(source='gemini_critic', models_usage=RequestUsage(prompt_tokens=86, completion_tokens=60), content=\"The poem is good, but it has four lines instead of three.  A haiku must have three lines with a 5-7-5 syllable structure.  The content is evocative of autumn, but the form is incorrect.  Please revise to adhere to the haiku's syllable structure.\\n\", type='TextMessage'), TextMessage(source='primary', models_usage=RequestUsage(prompt_tokens=141, completion_tokens=49), content='Thank you for your feedback! Here’s a revised haiku that follows the 5-7-5 syllable structure:\\n\\nCrimson leaves drift down,  \\nChill winds whisper through the gold,  \\nAutumn’s breath is near.', type='TextMessage'), TextMessage(source='gemini_critic', models_usage=RequestUsage(prompt_tokens=211, completion_tokens=32), content='The revised haiku is much improved.  It correctly follows the 5-7-5 syllable structure and maintains the evocative imagery of autumn.  APPROVE\\n', type='TextMessage')], stop_reason=\"Text 'APPROVE' mentioned\")",
      "language": "rust"
    },
    {
      "code": "TaskResult(messages=[TextMessage(source='user', models_usage=None, content='Write a Haiku poem with 4 lines about the fall season.', type='TextMessage'), TextMessage(source='primary', models_usage=RequestUsage(prompt_tokens=33, completion_tokens=31), content=\"Crimson leaves cascade,  \\nWhispering winds sing of change,  \\nChill wraps the fading,  \\nNature's quilt, rich and warm.\", type='TextMessage'), TextMessage(source='gemini_critic', models_usage=RequestUsage(prompt_tokens=86, completion_tokens=60), content=\"The poem is good, but it has four lines instead of three.  A haiku must have three lines with a 5-7-5 syllable structure.  The content is evocative of autumn, but the form is incorrect.  Please revise to adhere to the haiku's syllable structure.\\n\", type='TextMessage'), TextMessage(source='primary', models_usage=RequestUsage(prompt_tokens=141, completion_tokens=49), content='Thank you for your feedback! Here’s a revised haiku that follows the 5-7-5 syllable structure:\\n\\nCrimson leaves drift down,  \\nChill winds whisper through the gold,  \\nAutumn’s breath is near.', type='TextMessage'), TextMessage(source='gemini_critic', models_usage=RequestUsage(prompt_tokens=211, completion_tokens=32), content='The revised haiku is much improved.  It correctly follows the 5-7-5 syllable structure and maintains the evocative imagery of autumn.  APPROVE\\n', type='TextMessage')], stop_reason=\"Text 'APPROVE' mentioned\")",
      "language": "rust"
    },
    {
      "code": "import os\nfrom typing import AsyncGenerator, Sequence\n\nfrom autogen_agentchat.agents import BaseChatAgent\nfrom autogen_agentchat.base import Response\nfrom autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage\nfrom autogen_core import CancellationToken, Component\nfrom pydantic import BaseModel\nfrom typing_extensions import Self\n\n\nclass GeminiAssistantAgentConfig(BaseModel):\n    name: str\n    description: str = \"An agent that provides assistance with ability to use tools.\"\n    model: str = \"gemini-1.5-flash-002\"\n    system_message: str | None = None\n\n\nclass GeminiAssistantAgent(BaseChatAgent, Component[GeminiAssistantAgentConfig]):  # type: ignore[no-redef]\n    component_config_schema = GeminiAssistantAgentConfig\n    # component_provider_override = \"mypackage.agents.GeminiAssistantAgent\"\n\n    def __init__(\n        self,\n        name: str,\n        description: str = \"An agent that provides assistance with ability to use tools.\",\n        model: str = \"gemini-1.5-flash-002\",\n        api_key: str = os.environ[\"GEMINI_API_KEY\"],\n        system_message: str\n        | None = \"You are a helpful assistant that can respond to messages. Reply with TERMINATE when the task has been completed.\",\n    ):\n        super().__init__(name=name, description=description)\n        self._model_context = UnboundedChatCompletionContext()\n        self._model_client = genai.Client(api_key=api_key)\n        self._system_message = system_message\n        self._model = model\n\n    @property\n    def produced_message_types(self) -> Sequence[type[BaseChatMessage]]:\n        return (TextMessage,)\n\n    async def on_messages(self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) -> Response:\n        final_response = None\n        async for message in self.on_messages_stream(messages, cancellation_token):\n            if isinstance(message, Response):\n                final_response = message\n\n        if final_response is None:\n            raise AssertionError(\"The stream should have returned the final result.\")\n\n        return final_response\n\n    async def on_messages_stream(\n        self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken\n    ) -> AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None]:\n        # Add messages to the model context\n        for msg in messages:\n            await self._model_context.add_message(msg.to_model_message())\n\n        # Get conversation history\n        history = [\n            (msg.source if hasattr(msg, \"source\") else \"system\")\n            + \": \"\n            + (msg.content if isinstance(msg.content, str) else \"\")\n            + \"\\n\"\n            for msg in await self._model_context.get_messages()\n        ]\n\n        # Generate response using Gemini\n        response = self._model_client.models.generate_content(\n            model=self._model,\n            contents=f\"History: {history}\\nGiven the history, please provide a response\",\n            config=types.GenerateContentConfig(\n                system_instruction=self._system_message,\n                temperature=0.3,\n            ),\n        )\n\n        # Create usage metadata\n        usage = RequestUsage(\n            prompt_tokens=response.usage_metadata.prompt_token_count,\n            completion_tokens=response.usage_metadata.candidates_token_count,\n        )\n\n        # Add response to model context\n        await self._model_context.add_message(AssistantMessage(content=response.text, source=self.name))\n\n        # Yield the final response\n        yield Response(\n            chat_message=TextMessage(content=response.text, source=self.name, models_usage=usage),\n            inner_messages=[],\n        )\n\n    async def on_reset(self, cancellation_token: CancellationToken) -> None:\n        \"\"\"Reset the assistant by clearing the model context.\"\"\"\n        await self._model_context.clear()\n\n    @classmethod\n    def _from_config(cls, config: GeminiAssistantAgentConfig) -> Self:\n        return cls(\n            name=config.name, description=config.description, model=config.model, system_message=config.system_message\n        )\n\n    def _to_config(self) -> GeminiAssistantAgentConfig:\n        return GeminiAssistantAgentConfig(\n            name=self.name,\n            description=self.description,\n            model=self._model,\n            system_message=self._system_message,\n        )",
      "language": "python"
    },
    {
      "code": "import os\nfrom typing import AsyncGenerator, Sequence\n\nfrom autogen_agentchat.agents import BaseChatAgent\nfrom autogen_agentchat.base import Response\nfrom autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage\nfrom autogen_core import CancellationToken, Component\nfrom pydantic import BaseModel\nfrom typing_extensions import Self\n\n\nclass GeminiAssistantAgentConfig(BaseModel):\n    name: str\n    description: str = \"An agent that provides assistance with ability to use tools.\"\n    model: str = \"gemini-1.5-flash-002\"\n    system_message: str | None = None\n\n\nclass GeminiAssistantAgent(BaseChatAgent, Component[GeminiAssistantAgentConfig]):  # type: ignore[no-redef]\n    component_config_schema = GeminiAssistantAgentConfig\n    # component_provider_override = \"mypackage.agents.GeminiAssistantAgent\"\n\n    def __init__(\n        self,\n        name: str,\n        description: str = \"An agent that provides assistance with ability to use tools.\",\n        model: str = \"gemini-1.5-flash-002\",\n        api_key: str = os.environ[\"GEMINI_API_KEY\"],\n        system_message: str\n        | None = \"You are a helpful assistant that can respond to messages. Reply with TERMINATE when the task has been completed.\",\n    ):\n        super().__init__(name=name, description=description)\n        self._model_context = UnboundedChatCompletionContext()\n        self._model_client = genai.Client(api_key=api_key)\n        self._system_message = system_message\n        self._model = model\n\n    @property\n    def produced_message_types(self) -> Sequence[type[BaseChatMessage]]:\n        return (TextMessage,)\n\n    async def on_messages(self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) -> Response:\n        final_response = None\n        async for message in self.on_messages_stream(messages, cancellation_token):\n            if isinstance(message, Response):\n                final_response = message\n\n        if final_response is None:\n            raise AssertionError(\"The stream should have returned the final result.\")\n\n        return final_response\n\n    async def on_messages_stream(\n        self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken\n    ) -> AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None]:\n        # Add messages to the model context\n        for msg in messages:\n            await self._model_context.add_message(msg.to_model_message())\n\n        # Get conversation history\n        history = [\n            (msg.source if hasattr(msg, \"source\") else \"system\")\n            + \": \"\n            + (msg.content if isinstance(msg.content, str) else \"\")\n            + \"\\n\"\n            for msg in await self._model_context.get_messages()\n        ]\n\n        # Generate response using Gemini\n        response = self._model_client.models.generate_content(\n            model=self._model,\n            contents=f\"History: {history}\\nGiven the history, please provide a response\",\n            config=types.GenerateContentConfig(\n                system_instruction=self._system_message,\n                temperature=0.3,\n            ),\n        )\n\n        # Create usage metadata\n        usage = RequestUsage(\n            prompt_tokens=response.usage_metadata.prompt_token_count,\n            completion_tokens=response.usage_metadata.candidates_token_count,\n        )\n\n        # Add response to model context\n        await self._model_context.add_message(AssistantMessage(content=response.text, source=self.name))\n\n        # Yield the final response\n        yield Response(\n            chat_message=TextMessage(content=response.text, source=self.name, models_usage=usage),\n            inner_messages=[],\n        )\n\n    async def on_reset(self, cancellation_token: CancellationToken) -> None:\n        \"\"\"Reset the assistant by clearing the model context.\"\"\"\n        await self._model_context.clear()\n\n    @classmethod\n    def _from_config(cls, config: GeminiAssistantAgentConfig) -> Self:\n        return cls(\n            name=config.name, description=config.description, model=config.model, system_message=config.system_message\n        )\n\n    def _to_config(self) -> GeminiAssistantAgentConfig:\n        return GeminiAssistantAgentConfig(\n            name=self.name,\n            description=self.description,\n            model=self._model,\n            system_message=self._system_message,\n        )",
      "language": "python"
    },
    {
      "code": "gemini_assistant = GeminiAssistantAgent(\"gemini_assistant\")\nconfig = gemini_assistant.dump_component()\nprint(config.model_dump_json(indent=2))\nloaded_agent = GeminiAssistantAgent.load_component(config)\nprint(loaded_agent)",
      "language": "unknown"
    },
    {
      "code": "gemini_assistant = GeminiAssistantAgent(\"gemini_assistant\")\nconfig = gemini_assistant.dump_component()\nprint(config.model_dump_json(indent=2))\nloaded_agent = GeminiAssistantAgent.load_component(config)\nprint(loaded_agent)",
      "language": "unknown"
    },
    {
      "code": "{\n  \"provider\": \"__main__.GeminiAssistantAgent\",\n  \"component_type\": \"agent\",\n  \"version\": 1,\n  \"component_version\": 1,\n  \"description\": null,\n  \"label\": \"GeminiAssistantAgent\",\n  \"config\": {\n    \"name\": \"gemini_assistant\",\n    \"description\": \"An agent that provides assistance with ability to use tools.\",\n    \"model\": \"gemini-1.5-flash-002\",\n    \"system_message\": \"You are a helpful assistant that can respond to messages. Reply with TERMINATE when the task has been completed.\"\n  }\n}\n<__main__.GeminiAssistantAgent object at 0x11a5c5a90>",
      "language": "json"
    },
    {
      "code": "{\n  \"provider\": \"__main__.GeminiAssistantAgent\",\n  \"component_type\": \"agent\",\n  \"version\": 1,\n  \"component_version\": 1,\n  \"description\": null,\n  \"label\": \"GeminiAssistantAgent\",\n  \"config\": {\n    \"name\": \"gemini_assistant\",\n    \"description\": \"An agent that provides assistance with ability to use tools.\",\n    \"model\": \"gemini-1.5-flash-002\",\n    \"system_message\": \"You are a helpful assistant that can respond to messages. Reply with TERMINATE when the task has been completed.\"\n  }\n}\n<__main__.GeminiAssistantAgent object at 0x11a5c5a90>",
      "language": "json"
    }
  ],
  "patterns": [],
  "links": [
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/custom-agents.html",
    "https://microsoft.github.io/autogen/stable/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/extensions-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/reference/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/installation.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/quickstart.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/migration-guide.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/models.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/messages.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/agents.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/teams.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/human-in-the-loop.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/termination.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/state.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/selector-group-chat.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/swarm.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/magentic-one.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/graph-flow.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/memory.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/logging.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/serialize-components.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tracing.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/examples/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/examples/travel-planning.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/examples/company-research.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/examples/literature-review.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.agents.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.base.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.messages.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.teams.html"
  ]
}