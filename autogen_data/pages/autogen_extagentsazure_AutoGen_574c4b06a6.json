{
  "url": "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.azure.html",
  "title": "autogen_ext.agents.azure — AutoGen",
  "content": "Azure AI Assistant agent for AutoGen.\n\nThis agent leverages the Azure AI Assistant API to create AI assistants with capabilities like:\n\nCode interpretation and execution\n\nGrounding with Bing search\n\nFile handling and search\n\nCustom function calling\n\nMulti-turn conversations\n\nThe agent integrates with AutoGen’s messaging system, providing a seamless way to use Azure AI capabilities within the AutoGen framework. It supports tools like code interpreter, file search, and various grounding mechanisms.\n\nIt must start with a letter (A-Z, a-z) or an underscore (_).\n\nIt can only contain letters, digits (0-9), or underscores.\n\nIt cannot be a Python keyword.\n\nIt cannot contain spaces or special characters.\n\nIt cannot start with a digit.\n\nCheck here on how to create a new secured agent with user-managed identity: https://learn.microsoft.com/en-us/azure/ai-services/agents/how-to/virtual-networks\n\nUse the AzureAIAgent to create an agent grounded with Bing:\n\nUse the AzureAIAgent to create an agent with file search capability:\n\nUse the AzureAIAgent to create an agent with code interpreter capability:\n\nThe types of messages that the assistant agent produces.\n\nThe description of the agent. This is used by team to make decisions about which agents to use. The description should describe the agent’s capabilities and how to interact with it.\n\nGet the list of tools available to the agent.\n\nList[ToolDefinition] – The list of tool definitions.\n\nProcess incoming messages and return a response from the Azure AI agent.\n\nThis method is the primary entry point for interaction with the agent. It delegates to on_messages_stream and returns the final response.\n\nmessages (Sequence[BaseChatMessage]) – The messages to process\n\ncancellation_token (CancellationToken) – Token for cancellation handling\n\nmessage_limit (int, optional) – Maximum number of messages to retrieve from the thread\n\nResponse – The agent’s response, including the chat message and any inner events\n\nAssertionError – If the stream doesn’t return a final result\n\nProcess incoming messages and yield streaming responses from the Azure AI agent.\n\nThis method handles the complete interaction flow with the Azure AI agent: 1. Processing input messages 2. Creating and monitoring a run 3. Handling tool calls and their results 4. Retrieving and returning the agent’s final response\n\nThe method yields events during processing (like tool calls) and finally yields the complete Response with the agent’s message.\n\nmessages (Sequence[BaseChatMessage]) – The messages to process\n\ncancellation_token (CancellationToken) – Token for cancellation handling\n\nmessage_limit (int, optional) – Maximum number of messages to retrieve from the thread\n\npolling_interval (float, optional) – Time to sleep between polling for run status\n\nAgentEvent | ChatMessage | Response – Events during processing and the final response\n\nValueError – If the run fails or no message is received from the assistant\n\nHandle a text message by adding it to the conversation thread.\n\ncontent (str) – The text content of the message\n\ncancellation_token (CancellationToken) – Token for cancellation handling\n\nReset the agent’s conversation by creating a new thread.\n\nThis method allows for resetting a conversation without losing the agent definition or capabilities. It creates a new thread for fresh conversations.\n\nNote: Currently the Azure AI Agent API has no support for deleting messages, so a new thread is created instead.\n\ncancellation_token (CancellationToken) – Token for cancellation handling\n\nSave the current state of the agent for future restoration.\n\nThis method serializes the agent’s state including IDs for the agent, thread, messages, and associated resources like vector stores and uploaded files.\n\nMapping[str, Any] – A dictionary containing the serialized state data\n\nLoad a previously saved state into this agent.\n\nThis method deserializes and restores a previously saved agent state, setting up the agent to continue a previous conversation or session.\n\nstate (Mapping[str, Any]) – The previously saved state dictionary\n\nUpload files to be used with the code interpreter tool.\n\nThis method uploads files for the agent’s code interpreter tool and updates the thread’s tool resources to include these files.\n\nfile_paths (str | Iterable[str]) – Path(s) to file(s) to upload\n\ncancellation_token (Optional[CancellationToken]) – Token for cancellation handling\n\npolling_interval (float) – Time to sleep between polling for file status\n\nValueError – If file upload fails or the agent doesn’t have code interpreter capability\n\nUpload files to be used with the file search tool.\n\nThis method handles uploading files for the file search capability, creating a vector store if necessary, and updating the agent’s configuration to use the vector store.\n\nfile_paths (str | Iterable[str]) – Path(s) to file(s) to upload\n\ncancellation_token (CancellationToken) – Token for cancellation handling\n\nvector_store_name (Optional[str]) – Name to assign to the vector store if creating a new one\n\ndata_sources (Optional[List[VectorStoreDataSource]]) – Additional data sources for the vector store\n\nexpires_after (Optional[VectorStoreExpirationPolicy]) – Expiration policy for vector store content\n\nchunking_strategy (Optional[VectorStoreChunkingStrategyRequest]) – Strategy for chunking file content\n\nvector_store_metadata (Optional[Dict[str, str]]) – Additional metadata for the vector store\n\nvector_store_polling_interval (float) – Time to sleep between polling for vector store status\n\nValueError – If file search is not enabled for this agent or file upload fails\n\nClose the Azure AI agent and release any resources.\n\nautogen_ext.agents.file_surfer",
  "headings": [
    {
      "level": "h1",
      "text": "autogen_ext.agents.azure#",
      "id": ""
    }
  ],
  "code_samples": [
    {
      "code": "pip install \"autogen-ext[azure]\"  # For Azure AI Foundry Agent Service",
      "language": "unknown"
    },
    {
      "code": "pip install \"autogen-ext[azure]\"  # For Azure AI Foundry Agent Service",
      "language": "unknown"
    },
    {
      "code": "import asyncio\nimport os\n\nfrom autogen_agentchat.messages import TextMessage\nfrom autogen_core import CancellationToken\nfrom autogen_ext.agents.azure._azure_ai_agent import AzureAIAgent\nfrom azure.ai.projects.aio import AIProjectClient\nfrom azure.identity.aio import DefaultAzureCredential\nfrom azure.ai.agents.models import BingGroundingTool\nimport dotenv\n\n\nasync def bing_example():\n    async with DefaultAzureCredential() as credential:\n        async with AIProjectClient(  # type: ignore\n            credential=credential, endpoint=os.getenv(\"AZURE_PROJECT_ENDPOINT\", \"\")\n        ) as project_client:\n            conn = await project_client.connections.get(name=os.getenv(\"BING_CONNECTION_NAME\", \"\"))\n\n            bing_tool = BingGroundingTool(conn.id)\n            agent_with_bing_grounding = AzureAIAgent(\n                name=\"bing_agent\",\n                description=\"An AI assistant with Bing grounding\",\n                project_client=project_client,\n                deployment_name=\"gpt-4o\",\n                instructions=\"You are a helpful assistant.\",\n                tools=bing_tool.definitions,\n                metadata={\"source\": \"AzureAIAgent\"},\n            )\n\n            # For the bing grounding tool to return the citations, the message must contain an instruction for the model to do return them.\n            # For example: \"Please provide citations for the answers\"\n\n            result = await agent_with_bing_grounding.on_messages(\n                messages=[\n                    TextMessage(\n                        content=\"What is Microsoft\\'s annual leave policy? Provide citations for your answers.\",\n                        source=\"user\",\n                    )\n                ],\n                cancellation_token=CancellationToken(),\n                message_limit=5,\n            )\n            print(result)\n\n\nif __name__ == \"__main__\":\n    dotenv.load_dotenv()\n    asyncio.run(bing_example())",
      "language": "python"
    },
    {
      "code": "import asyncio\nimport os\n\nfrom autogen_agentchat.messages import TextMessage\nfrom autogen_core import CancellationToken\nfrom autogen_ext.agents.azure._azure_ai_agent import AzureAIAgent\nfrom azure.ai.projects.aio import AIProjectClient\nfrom azure.identity.aio import DefaultAzureCredential\nfrom azure.ai.agents.models import BingGroundingTool\nimport dotenv\n\n\nasync def bing_example():\n    async with DefaultAzureCredential() as credential:\n        async with AIProjectClient(  # type: ignore\n            credential=credential, endpoint=os.getenv(\"AZURE_PROJECT_ENDPOINT\", \"\")\n        ) as project_client:\n            conn = await project_client.connections.get(name=os.getenv(\"BING_CONNECTION_NAME\", \"\"))\n\n            bing_tool = BingGroundingTool(conn.id)\n            agent_with_bing_grounding = AzureAIAgent(\n                name=\"bing_agent\",\n                description=\"An AI assistant with Bing grounding\",\n                project_client=project_client,\n                deployment_name=\"gpt-4o\",\n                instructions=\"You are a helpful assistant.\",\n                tools=bing_tool.definitions,\n                metadata={\"source\": \"AzureAIAgent\"},\n            )\n\n            # For the bing grounding tool to return the citations, the message must contain an instruction for the model to do return them.\n            # For example: \"Please provide citations for the answers\"\n\n            result = await agent_with_bing_grounding.on_messages(\n                messages=[\n                    TextMessage(\n                        content=\"What is Microsoft\\'s annual leave policy? Provide citations for your answers.\",\n                        source=\"user\",\n                    )\n                ],\n                cancellation_token=CancellationToken(),\n                message_limit=5,\n            )\n            print(result)\n\n\nif __name__ == \"__main__\":\n    dotenv.load_dotenv()\n    asyncio.run(bing_example())",
      "language": "python"
    },
    {
      "code": "import asyncio\nimport os\nimport tempfile\nimport urllib.request\n\nimport dotenv\nfrom autogen_agentchat.messages import TextMessage\nfrom autogen_core import CancellationToken\nfrom autogen_ext.agents.azure._azure_ai_agent import AzureAIAgent\nfrom azure.ai.projects.aio import AIProjectClient\nfrom azure.identity.aio import DefaultAzureCredential\n\n\nasync def file_search_example():\n    # Download README.md from GitHub\n    readme_url = \"https://raw.githubusercontent.com/microsoft/autogen/refs/heads/main/README.md\"\n    temp_file = None\n\n    try:\n        # Create a temporary file to store the downloaded README\n        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".md\")\n        urllib.request.urlretrieve(readme_url, temp_file.name)\n        print(f\"Downloaded README.md to {temp_file.name}\")\n\n        async with DefaultAzureCredential() as credential:\n            async with AIProjectClient(  # type: ignore\n                credential=credential, endpoint=os.getenv(\"AZURE_PROJECT_ENDPOINT\", \"\")\n            ) as project_client:\n                agent_with_file_search = AzureAIAgent(\n                    name=\"file_search_agent\",\n                    description=\"An AI assistant with file search capabilities\",\n                    project_client=project_client,\n                    deployment_name=\"gpt-4.1-mini\",\n                    instructions=\"You are a helpful assistant.\",\n                    tools=[\"file_search\"],\n                    metadata={\"source\": \"AzureAIAgent\"},\n                )\n\n                ct: CancellationToken = CancellationToken()\n                # Use the downloaded README file for file search\n                await agent_with_file_search.on_upload_for_file_search(\n                    file_paths=[temp_file.name],\n                    vector_store_name=\"file_upload_index\",\n                    vector_store_metadata={\"source\": \"AzureAIAgent\"},\n                    cancellation_token=ct,\n                    vector_store_polling_interval=60,\n                )\n                result = await agent_with_file_search.on_messages(\n                    messages=[\n                        TextMessage(\n                            content=\"Hello, what is AutoGen and what capabilities does it have?\", source=\"user\"\n                        )\n                    ],\n                    cancellation_token=ct,\n                    message_limit=5,\n                )\n                print(result)\n    finally:\n        # Clean up the temporary file\n        if temp_file and os.path.exists(temp_file.name):\n            os.unlink(temp_file.name)\n            print(f\"Removed temporary file {temp_file.name}\")\n\n\nif __name__ == \"__main__\":\n    dotenv.load_dotenv()\n    asyncio.run(file_search_example())",
      "language": "python"
    },
    {
      "code": "import asyncio\nimport os\nimport tempfile\nimport urllib.request\n\nimport dotenv\nfrom autogen_agentchat.messages import TextMessage\nfrom autogen_core import CancellationToken\nfrom autogen_ext.agents.azure._azure_ai_agent import AzureAIAgent\nfrom azure.ai.projects.aio import AIProjectClient\nfrom azure.identity.aio import DefaultAzureCredential\n\n\nasync def file_search_example():\n    # Download README.md from GitHub\n    readme_url = \"https://raw.githubusercontent.com/microsoft/autogen/refs/heads/main/README.md\"\n    temp_file = None\n\n    try:\n        # Create a temporary file to store the downloaded README\n        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".md\")\n        urllib.request.urlretrieve(readme_url, temp_file.name)\n        print(f\"Downloaded README.md to {temp_file.name}\")\n\n        async with DefaultAzureCredential() as credential:\n            async with AIProjectClient(  # type: ignore\n                credential=credential, endpoint=os.getenv(\"AZURE_PROJECT_ENDPOINT\", \"\")\n            ) as project_client:\n                agent_with_file_search = AzureAIAgent(\n                    name=\"file_search_agent\",\n                    description=\"An AI assistant with file search capabilities\",\n                    project_client=project_client,\n                    deployment_name=\"gpt-4.1-mini\",\n                    instructions=\"You are a helpful assistant.\",\n                    tools=[\"file_search\"],\n                    metadata={\"source\": \"AzureAIAgent\"},\n                )\n\n                ct: CancellationToken = CancellationToken()\n                # Use the downloaded README file for file search\n                await agent_with_file_search.on_upload_for_file_search(\n                    file_paths=[temp_file.name],\n                    vector_store_name=\"file_upload_index\",\n                    vector_store_metadata={\"source\": \"AzureAIAgent\"},\n                    cancellation_token=ct,\n                    vector_store_polling_interval=60,\n                )\n                result = await agent_with_file_search.on_messages(\n                    messages=[\n                        TextMessage(\n                            content=\"Hello, what is AutoGen and what capabilities does it have?\", source=\"user\"\n                        )\n                    ],\n                    cancellation_token=ct,\n                    message_limit=5,\n                )\n                print(result)\n    finally:\n        # Clean up the temporary file\n        if temp_file and os.path.exists(temp_file.name):\n            os.unlink(temp_file.name)\n            print(f\"Removed temporary file {temp_file.name}\")\n\n\nif __name__ == \"__main__\":\n    dotenv.load_dotenv()\n    asyncio.run(file_search_example())",
      "language": "python"
    },
    {
      "code": "import asyncio\nimport os\n\nimport dotenv\nfrom autogen_agentchat.messages import TextMessage\nfrom autogen_core import CancellationToken\nfrom autogen_ext.agents.azure._azure_ai_agent import AzureAIAgent\nfrom azure.ai.projects.aio import AIProjectClient\nfrom azure.identity.aio import DefaultAzureCredential\n\n\nasync def code_interpreter_example():\n    async with DefaultAzureCredential() as credential:\n        async with AIProjectClient(  # type: ignore\n            credential=credential, endpoint=os.getenv(\"AZURE_PROJECT_ENDPOINT\", \"\")\n        ) as project_client:\n            agent_with_code_interpreter = AzureAIAgent(\n                name=\"code_interpreter_agent\",\n                description=\"An AI assistant with code interpreter capabilities\",\n                project_client=project_client,\n                deployment_name=\"gpt-4.1-mini\",\n                instructions=\"You are a helpful assistant.\",\n                tools=[\"code_interpreter\"],\n                metadata={\"source\": \"AzureAIAgent\"},\n            )\n\n            await agent_with_code_interpreter.on_upload_for_code_interpreter(\n                file_paths=\"/workspaces/autogen/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/data/nifty_500_quarterly_results.csv\",\n                cancellation_token=CancellationToken(),\n                polling_interval=5,\n            )\n\n            result = await agent_with_code_interpreter.on_messages(\n                messages=[\n                    TextMessage(\n                        content=\"Aggregate the number of stocks per industry and give me a markdown table as a result?\",\n                        source=\"user\",\n                    )\n                ],\n                cancellation_token=CancellationToken(),\n            )\n\n            print(result)\n\n\nif __name__ == \"__main__\":\n    dotenv.load_dotenv()\n    asyncio.run(code_interpreter_example())",
      "language": "python"
    },
    {
      "code": "import asyncio\nimport os\n\nimport dotenv\nfrom autogen_agentchat.messages import TextMessage\nfrom autogen_core import CancellationToken\nfrom autogen_ext.agents.azure._azure_ai_agent import AzureAIAgent\nfrom azure.ai.projects.aio import AIProjectClient\nfrom azure.identity.aio import DefaultAzureCredential\n\n\nasync def code_interpreter_example():\n    async with DefaultAzureCredential() as credential:\n        async with AIProjectClient(  # type: ignore\n            credential=credential, endpoint=os.getenv(\"AZURE_PROJECT_ENDPOINT\", \"\")\n        ) as project_client:\n            agent_with_code_interpreter = AzureAIAgent(\n                name=\"code_interpreter_agent\",\n                description=\"An AI assistant with code interpreter capabilities\",\n                project_client=project_client,\n                deployment_name=\"gpt-4.1-mini\",\n                instructions=\"You are a helpful assistant.\",\n                tools=[\"code_interpreter\"],\n                metadata={\"source\": \"AzureAIAgent\"},\n            )\n\n            await agent_with_code_interpreter.on_upload_for_code_interpreter(\n                file_paths=\"/workspaces/autogen/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/data/nifty_500_quarterly_results.csv\",\n                cancellation_token=CancellationToken(),\n                polling_interval=5,\n            )\n\n            result = await agent_with_code_interpreter.on_messages(\n                messages=[\n                    TextMessage(\n                        content=\"Aggregate the number of stocks per industry and give me a markdown table as a result?\",\n                        source=\"user\",\n                    )\n                ],\n                cancellation_token=CancellationToken(),\n            )\n\n            print(result)\n\n\nif __name__ == \"__main__\":\n    dotenv.load_dotenv()\n    asyncio.run(code_interpreter_example())",
      "language": "python"
    }
  ],
  "patterns": [
    {
      "description": "API Reference autogen_ext.agents.azure autogen_ext.agents.azure# class AzureAIAgent(name: str, description: str, project_client: AIProjectClient, deployment_name: str, instructions: str, tools: Iterable[Literal['file_search', 'code_interpreter', 'bing_grounding', 'azure_ai_search', 'azure_function'] | BingGroundingToolDefinition | CodeInterpreterToolDefinition | AzureAISearchToolDefinition | FileSearchToolDefinition | AzureFunctionToolDefinition | Tool | Callable[[...], Any] | Callable[[...], Awaitable[Any]]] | None = None, agent_id: str | None = None, thread_id: str | None = None, metadata: Dict[str, str] | None = None, response_format: AgentsResponseFormat | None = None, temperature: float | None = None, tool_resources: ToolResources | None = None, top_p: float | None = None)[source]# Bases: BaseChatAgent Azure AI Assistant agent for AutoGen. Installation: pip install \"autogen-ext[azure]\" # For Azure AI Foundry Agent Service This agent leverages the Azure AI Assistant API to create AI assistants with capabilities like: Code interpretation and execution Grounding with Bing search File handling and search Custom function calling Multi-turn conversations The agent integrates with AutoGen’s messaging system, providing a seamless way to use Azure AI capabilities within the AutoGen framework. It supports tools like code interpreter, file search, and various grounding mechanisms. Agent name must be a valid Python identifier: It must start with a letter (A-Z, a-z) or an underscore (_). It can only contain letters, digits (0-9), or underscores. It cannot be a Python keyword. It cannot contain spaces or special characters. It cannot start with a digit. Check here on how to create a new secured agent with user-managed identity: https://learn.microsoft.com/en-us/azure/ai-services/agents/how-to/virtual-networks Examples Use the AzureAIAgent to create an agent grounded with Bing: import asyncio import os from autogen_agentchat.messages import TextMessage from autogen_core import CancellationToken from autogen_ext.agents.azure._azure_ai_agent import AzureAIAgent from azure.ai.projects.aio import AIProjectClient from azure.identity.aio import DefaultAzureCredential from azure.ai.agents.models import BingGroundingTool import dotenv async def bing_example(): async with DefaultAzureCredential() as credential: async with AIProjectClient( # type: ignore credential=credential, endpoint=os.getenv(\"AZURE_PROJECT_ENDPOINT\", \"\") ) as project_client: conn = await project_client.connections.get(name=os.getenv(\"BING_CONNECTION_NAME\", \"\")) bing_tool = BingGroundingTool(conn.id) agent_with_bing_grounding = AzureAIAgent( name=\"bing_agent\", description=\"An AI assistant with Bing grounding\", project_client=project_client, deployment_name=\"gpt-4o\", instructions=\"You are a helpful assistant.\", tools=bing_tool.definitions, metadata={\"source\": \"AzureAIAgent\"}, ) # For the bing grounding tool to return the citations, the message must contain an instruction for the model to do return them. # For example: \"Please provide citations for the answers\" result = await agent_with_bing_grounding.on_messages( messages=[ TextMessage( content=\"What is Microsoft\\'s annual leave policy? Provide citations for your answers.\", source=\"user\", ) ], cancellation_token=CancellationToken(), message_limit=5, ) print(result) if __name__ == \"__main__\": dotenv.load_dotenv() asyncio.run(bing_example()) Use the AzureAIAgent to create an agent with file search capability: import asyncio import os import tempfile import urllib.request import dotenv from autogen_agentchat.messages import TextMessage from autogen_core import CancellationToken from autogen_ext.agents.azure._azure_ai_agent import AzureAIAgent from azure.ai.projects.aio import AIProjectClient from azure.identity.aio import DefaultAzureCredential async def file_search_example(): # Download README.md from GitHub readme_url = \"https://raw.githubusercontent.com/microsoft/autogen/refs/heads/main/README.md\" temp_file = None try: # Create a temporary file to store the downloaded README temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".md\") urllib.request.urlretrieve(readme_url, temp_file.name) print(f\"Downloaded README.md to {temp_file.name}\") async with DefaultAzureCredential() as credential: async with AIProjectClient( # type: ignore credential=credential, endpoint=os.getenv(\"AZURE_PROJECT_ENDPOINT\", \"\") ) as project_client: agent_with_file_search = AzureAIAgent( name=\"file_search_agent\", description=\"An AI assistant with file search capabilities\", project_client=project_client, deployment_name=\"gpt-4.1-mini\", instructions=\"You are a helpful assistant.\", tools=[\"file_search\"], metadata={\"source\": \"AzureAIAgent\"}, ) ct: CancellationToken = CancellationToken() # Use the downloaded README file for file search await agent_with_file_search.on_upload_for_file_search( file_paths=[temp_file.name], vector_store_name=\"file_upload_index\", vector_store_metadata={\"source\": \"AzureAIAgent\"}, cancellation_token=ct, vector_store_polling_interval=60, ) result = await agent_with_file_search.on_messages( messages=[ TextMessage( content=\"Hello, what is AutoGen and what capabilities does it have?\", source=\"user\" ) ], cancellation_token=ct, message_limit=5, ) print(result) finally: # Clean up the temporary file if temp_file and os.path.exists(temp_file.name): os.unlink(temp_file.name) print(f\"Removed temporary file {temp_file.name}\") if __name__ == \"__main__\": dotenv.load_dotenv() asyncio.run(file_search_example()) Use the AzureAIAgent to create an agent with code interpreter capability: import asyncio import os import dotenv from autogen_agentchat.messages import TextMessage from autogen_core import CancellationToken from autogen_ext.agents.azure._azure_ai_agent import AzureAIAgent from azure.ai.projects.aio import AIProjectClient from azure.identity.aio import DefaultAzureCredential async def code_interpreter_example(): async with DefaultAzureCredential() as credential: async with AIProjectClient( # type: ignore credential=credential, endpoint=os.getenv(\"AZURE_PROJECT_ENDPOINT\", \"\") ) as project_client: agent_with_code_interpreter = AzureAIAgent( name=\"code_interpreter_agent\", description=\"An AI assistant with code interpreter capabilities\", project_client=project_client, deployment_name=\"gpt-4.1-mini\", instructions=\"You are a helpful assistant.\", tools=[\"code_interpreter\"], metadata={\"source\": \"AzureAIAgent\"}, ) await agent_with_code_interpreter.on_upload_for_code_interpreter( file_paths=\"/workspaces/autogen/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/data/nifty_500_quarterly_results.csv\", cancellation_token=CancellationToken(), polling_interval=5, ) result = await agent_with_code_interpreter.on_messages( messages=[ TextMessage( content=\"Aggregate the number of stocks per industry and give me a markdown table as a result?\", source=\"user\", ) ], cancellation_token=CancellationToken(), ) print(result) if __name__ == \"__main__\": dotenv.load_dotenv() asyncio.run(code_interpreter_example()) property produced_message_types: Sequence[type[Annotated[TextMessage | MultiModalMessage | StopMessage | ToolCallSummaryMessage | HandoffMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]]]# The types of messages that the assistant agent produces. property thread_id: str# property description: str# The description of the agent. This is used by team to make decisions about which agents to use. The description should describe the agent’s capabilities and how to interact with it. property agent_id: str# property deployment_name: str# property instructions: str# property tools: List[ToolDefinition]# Get the list of tools available to the agent. Returns: List[ToolDefinition] – The list of tool definitions. async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken | None = None, message_limit: int = 1) → Response[source]# Process incoming messages and return a response from the Azure AI agent. This method is the primary entry point for interaction with the agent. It delegates to on_messages_stream and returns the final response. Parameters: messages (Sequence[BaseChatMessage]) – The messages to process cancellation_token (CancellationToken) – Token for cancellation handling message_limit (int, optional) – Maximum number of messages to retrieve from the thread Returns: Response – The agent’s response, including the chat message and any inner events Raises: AssertionError – If the stream doesn’t return a final result async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken | None = None, message_limit: int = 1, polling_interval: float = 0.5) → AsyncGenerator[Annotated[ToolCallRequestEvent | ToolCallExecutionEvent | MemoryQueryEvent | UserInputRequestedEvent | ModelClientStreamingChunkEvent | ThoughtEvent | SelectSpeakerEvent | CodeGenerationEvent | CodeExecutionEvent, FieldInfo(annotation=NoneType, required=True, discriminator='type')] | Annotated[TextMessage | MultiModalMessage | StopMessage | ToolCallSummaryMessage | HandoffMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')] | Response, None][source]# Process incoming messages and yield streaming responses from the Azure AI agent. This method handles the complete interaction flow with the Azure AI agent: 1. Processing input messages 2. Creating and monitoring a run 3. Handling tool calls and their results 4. Retrieving and returning the agent’s final response The method yields events during processing (like tool calls) and finally yields the complete Response with the agent’s message. Parameters: messages (Sequence[BaseChatMessage]) – The messages to process cancellation_token (CancellationToken) – Token for cancellation handling message_limit (int, optional) – Maximum number of messages to retrieve from the thread polling_interval (float, optional) – Time to sleep between polling for run status Yields: AgentEvent | ChatMessage | Response – Events during processing and the final response Raises: ValueError – If the run fails or no message is received from the assistant async handle_text_message(content: str, cancellation_token: CancellationToken | None = None) → None[source]# Handle a text message by adding it to the conversation thread. Parameters: content (str) – The text content of the message cancellation_token (CancellationToken) – Token for cancellation handling Returns: None async on_reset(cancellation_token: CancellationToken) → None[source]# Reset the agent’s conversation by creating a new thread. This method allows for resetting a conversation without losing the agent definition or capabilities. It creates a new thread for fresh conversations. Note: Currently the Azure AI Agent API has no support for deleting messages, so a new thread is created instead. Parameters: cancellation_token (CancellationToken) – Token for cancellation handling async save_state() → Mapping[str, Any][source]# Save the current state of the agent for future restoration. This method serializes the agent’s state including IDs for the agent, thread, messages, and associated resources like vector stores and uploaded files. Returns: Mapping[str, Any] – A dictionary containing the serialized state data async load_state(state: Mapping[str, Any]) → None[source]# Load a previously saved state into this agent. This method deserializes and restores a previously saved agent state, setting up the agent to continue a previous conversation or session. Parameters: state (Mapping[str, Any]) – The previously saved state dictionary async on_upload_for_code_interpreter(file_paths: str | Iterable[str], cancellation_token: CancellationToken | None = None, polling_interval: float = 0.5) → None[source]# Upload files to be used with the code interpreter tool. This method uploads files for the agent’s code interpreter tool and updates the thread’s tool resources to include these files. Parameters: file_paths (str | Iterable[str]) – Path(s) to file(s) to upload cancellation_token (Optional[CancellationToken]) – Token for cancellation handling polling_interval (float) – Time to sleep between polling for file status Raises: ValueError – If file upload fails or the agent doesn’t have code interpreter capability async on_upload_for_file_search(file_paths: str | Iterable[str], cancellation_token: CancellationToken, vector_store_name: str | None = None, data_sources: List[VectorStoreDataSource] | None = None, expires_after: VectorStoreExpirationPolicy | None = None, chunking_strategy: VectorStoreChunkingStrategyRequest | None = None, vector_store_metadata: Dict[str, str] | None = None, vector_store_polling_interval: float = 1) → None[source]# Upload files to be used with the file search tool. This method handles uploading files for the file search capability, creating a vector store if necessary, and updating the agent’s configuration to use the vector store. Parameters: file_paths (str | Iterable[str]) – Path(s) to file(s) to upload cancellation_token (CancellationToken) – Token for cancellation handling vector_store_name (Optional[str]) – Name to assign to the vector store if creating a new one data_sources (Optional[List[VectorStoreDataSource]]) – Additional data sources for the vector store expires_after (Optional[VectorStoreExpirationPolicy]) – Expiration policy for vector store content chunking_strategy (Optional[VectorStoreChunkingStrategyRequest]) – Strategy for chunking file content vector_store_metadata (Optional[Dict[str, str]]) – Additional metadata for the vector store vector_store_polling_interval (float) – Time to sleep between polling for vector store status Raises: ValueError – If file search is not enabled for this agent or file upload fails async close() → None[source]# Close the Azure AI agent and release any resources. previous autogen_ext.ui next autogen_ext.agents.file_surfer On this page AzureAIAgent AzureAIAgent.produced_message_types AzureAIAgent.thread_id AzureAIAgent.description AzureAIAgent.agent_id AzureAIAgent.deployment_name AzureAIAgent.instructions AzureAIAgent.tools AzureAIAgent.on_messages() AzureAIAgent.on_messages_stream() AzureAIAgent.handle_text_message() AzureAIAgent.on_reset() AzureAIAgent.save_state() AzureAIAgent.load_state() AzureAIAgent.on_upload_for_code_interpreter() AzureAIAgent.on_upload_for_file_search() AzureAIAgent.close() Edit on GitHub Show Source",
      "code": "BaseChatAgent"
    },
    {
      "description": "API Reference autogen_ext.agents.azure autogen_ext.agents.azure# class AzureAIAgent(name: str, description: str, project_client: AIProjectClient, deployment_name: str, instructions: str, tools: Iterable[Literal['file_search', 'code_interpreter', 'bing_grounding', 'azure_ai_search', 'azure_function'] | BingGroundingToolDefinition | CodeInterpreterToolDefinition | AzureAISearchToolDefinition | FileSearchToolDefinition | AzureFunctionToolDefinition | Tool | Callable[[...], Any] | Callable[[...], Awaitable[Any]]] | None = None, agent_id: str | None = None, thread_id: str | None = None, metadata: Dict[str, str] | None = None, response_format: AgentsResponseFormat | None = None, temperature: float | None = None, tool_resources: ToolResources | None = None, top_p: float | None = None)[source]# Bases: BaseChatAgent Azure AI Assistant agent for AutoGen. Installation: pip install \"autogen-ext[azure]\" # For Azure AI Foundry Agent Service This agent leverages the Azure AI Assistant API to create AI assistants with capabilities like: Code interpretation and execution Grounding with Bing search File handling and search Custom function calling Multi-turn conversations The agent integrates with AutoGen’s messaging system, providing a seamless way to use Azure AI capabilities within the AutoGen framework. It supports tools like code interpreter, file search, and various grounding mechanisms. Agent name must be a valid Python identifier: It must start with a letter (A-Z, a-z) or an underscore (_). It can only contain letters, digits (0-9), or underscores. It cannot be a Python keyword. It cannot contain spaces or special characters. It cannot start with a digit. Check here on how to create a new secured agent with user-managed identity: https://learn.microsoft.com/en-us/azure/ai-services/agents/how-to/virtual-networks Examples Use the AzureAIAgent to create an agent grounded with Bing: import asyncio import os from autogen_agentchat.messages import TextMessage from autogen_core import CancellationToken from autogen_ext.agents.azure._azure_ai_agent import AzureAIAgent from azure.ai.projects.aio import AIProjectClient from azure.identity.aio import DefaultAzureCredential from azure.ai.agents.models import BingGroundingTool import dotenv async def bing_example(): async with DefaultAzureCredential() as credential: async with AIProjectClient( # type: ignore credential=credential, endpoint=os.getenv(\"AZURE_PROJECT_ENDPOINT\", \"\") ) as project_client: conn = await project_client.connections.get(name=os.getenv(\"BING_CONNECTION_NAME\", \"\")) bing_tool = BingGroundingTool(conn.id) agent_with_bing_grounding = AzureAIAgent( name=\"bing_agent\", description=\"An AI assistant with Bing grounding\", project_client=project_client, deployment_name=\"gpt-4o\", instructions=\"You are a helpful assistant.\", tools=bing_tool.definitions, metadata={\"source\": \"AzureAIAgent\"}, ) # For the bing grounding tool to return the citations, the message must contain an instruction for the model to do return them. # For example: \"Please provide citations for the answers\" result = await agent_with_bing_grounding.on_messages( messages=[ TextMessage( content=\"What is Microsoft\\'s annual leave policy? Provide citations for your answers.\", source=\"user\", ) ], cancellation_token=CancellationToken(), message_limit=5, ) print(result) if __name__ == \"__main__\": dotenv.load_dotenv() asyncio.run(bing_example()) Use the AzureAIAgent to create an agent with file search capability: import asyncio import os import tempfile import urllib.request import dotenv from autogen_agentchat.messages import TextMessage from autogen_core import CancellationToken from autogen_ext.agents.azure._azure_ai_agent import AzureAIAgent from azure.ai.projects.aio import AIProjectClient from azure.identity.aio import DefaultAzureCredential async def file_search_example(): # Download README.md from GitHub readme_url = \"https://raw.githubusercontent.com/microsoft/autogen/refs/heads/main/README.md\" temp_file = None try: # Create a temporary file to store the downloaded README temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".md\") urllib.request.urlretrieve(readme_url, temp_file.name) print(f\"Downloaded README.md to {temp_file.name}\") async with DefaultAzureCredential() as credential: async with AIProjectClient( # type: ignore credential=credential, endpoint=os.getenv(\"AZURE_PROJECT_ENDPOINT\", \"\") ) as project_client: agent_with_file_search = AzureAIAgent( name=\"file_search_agent\", description=\"An AI assistant with file search capabilities\", project_client=project_client, deployment_name=\"gpt-4.1-mini\", instructions=\"You are a helpful assistant.\", tools=[\"file_search\"], metadata={\"source\": \"AzureAIAgent\"}, ) ct: CancellationToken = CancellationToken() # Use the downloaded README file for file search await agent_with_file_search.on_upload_for_file_search( file_paths=[temp_file.name], vector_store_name=\"file_upload_index\", vector_store_metadata={\"source\": \"AzureAIAgent\"}, cancellation_token=ct, vector_store_polling_interval=60, ) result = await agent_with_file_search.on_messages( messages=[ TextMessage( content=\"Hello, what is AutoGen and what capabilities does it have?\", source=\"user\" ) ], cancellation_token=ct, message_limit=5, ) print(result) finally: # Clean up the temporary file if temp_file and os.path.exists(temp_file.name): os.unlink(temp_file.name) print(f\"Removed temporary file {temp_file.name}\") if __name__ == \"__main__\": dotenv.load_dotenv() asyncio.run(file_search_example()) Use the AzureAIAgent to create an agent with code interpreter capability: import asyncio import os import dotenv from autogen_agentchat.messages import TextMessage from autogen_core import CancellationToken from autogen_ext.agents.azure._azure_ai_agent import AzureAIAgent from azure.ai.projects.aio import AIProjectClient from azure.identity.aio import DefaultAzureCredential async def code_interpreter_example(): async with DefaultAzureCredential() as credential: async with AIProjectClient( # type: ignore credential=credential, endpoint=os.getenv(\"AZURE_PROJECT_ENDPOINT\", \"\") ) as project_client: agent_with_code_interpreter = AzureAIAgent( name=\"code_interpreter_agent\", description=\"An AI assistant with code interpreter capabilities\", project_client=project_client, deployment_name=\"gpt-4.1-mini\", instructions=\"You are a helpful assistant.\", tools=[\"code_interpreter\"], metadata={\"source\": \"AzureAIAgent\"}, ) await agent_with_code_interpreter.on_upload_for_code_interpreter( file_paths=\"/workspaces/autogen/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/data/nifty_500_quarterly_results.csv\", cancellation_token=CancellationToken(), polling_interval=5, ) result = await agent_with_code_interpreter.on_messages( messages=[ TextMessage( content=\"Aggregate the number of stocks per industry and give me a markdown table as a result?\", source=\"user\", ) ], cancellation_token=CancellationToken(), ) print(result) if __name__ == \"__main__\": dotenv.load_dotenv() asyncio.run(code_interpreter_example()) property produced_message_types: Sequence[type[Annotated[TextMessage | MultiModalMessage | StopMessage | ToolCallSummaryMessage | HandoffMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]]]# The types of messages that the assistant agent produces. property thread_id: str# property description: str# The description of the agent. This is used by team to make decisions about which agents to use. The description should describe the agent’s capabilities and how to interact with it. property agent_id: str# property deployment_name: str# property instructions: str# property tools: List[ToolDefinition]# Get the list of tools available to the agent. Returns: List[ToolDefinition] – The list of tool definitions. async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken | None = None, message_limit: int = 1) → Response[source]# Process incoming messages and return a response from the Azure AI agent. This method is the primary entry point for interaction with the agent. It delegates to on_messages_stream and returns the final response. Parameters: messages (Sequence[BaseChatMessage]) – The messages to process cancellation_token (CancellationToken) – Token for cancellation handling message_limit (int, optional) – Maximum number of messages to retrieve from the thread Returns: Response – The agent’s response, including the chat message and any inner events Raises: AssertionError – If the stream doesn’t return a final result async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken | None = None, message_limit: int = 1, polling_interval: float = 0.5) → AsyncGenerator[Annotated[ToolCallRequestEvent | ToolCallExecutionEvent | MemoryQueryEvent | UserInputRequestedEvent | ModelClientStreamingChunkEvent | ThoughtEvent | SelectSpeakerEvent | CodeGenerationEvent | CodeExecutionEvent, FieldInfo(annotation=NoneType, required=True, discriminator='type')] | Annotated[TextMessage | MultiModalMessage | StopMessage | ToolCallSummaryMessage | HandoffMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')] | Response, None][source]# Process incoming messages and yield streaming responses from the Azure AI agent. This method handles the complete interaction flow with the Azure AI agent: 1. Processing input messages 2. Creating and monitoring a run 3. Handling tool calls and their results 4. Retrieving and returning the agent’s final response The method yields events during processing (like tool calls) and finally yields the complete Response with the agent’s message. Parameters: messages (Sequence[BaseChatMessage]) – The messages to process cancellation_token (CancellationToken) – Token for cancellation handling message_limit (int, optional) – Maximum number of messages to retrieve from the thread polling_interval (float, optional) – Time to sleep between polling for run status Yields: AgentEvent | ChatMessage | Response – Events during processing and the final response Raises: ValueError – If the run fails or no message is received from the assistant async handle_text_message(content: str, cancellation_token: CancellationToken | None = None) → None[source]# Handle a text message by adding it to the conversation thread. Parameters: content (str) – The text content of the message cancellation_token (CancellationToken) – Token for cancellation handling Returns: None async on_reset(cancellation_token: CancellationToken) → None[source]# Reset the agent’s conversation by creating a new thread. This method allows for resetting a conversation without losing the agent definition or capabilities. It creates a new thread for fresh conversations. Note: Currently the Azure AI Agent API has no support for deleting messages, so a new thread is created instead. Parameters: cancellation_token (CancellationToken) – Token for cancellation handling async save_state() → Mapping[str, Any][source]# Save the current state of the agent for future restoration. This method serializes the agent’s state including IDs for the agent, thread, messages, and associated resources like vector stores and uploaded files. Returns: Mapping[str, Any] – A dictionary containing the serialized state data async load_state(state: Mapping[str, Any]) → None[source]# Load a previously saved state into this agent. This method deserializes and restores a previously saved agent state, setting up the agent to continue a previous conversation or session. Parameters: state (Mapping[str, Any]) – The previously saved state dictionary async on_upload_for_code_interpreter(file_paths: str | Iterable[str], cancellation_token: CancellationToken | None = None, polling_interval: float = 0.5) → None[source]# Upload files to be used with the code interpreter tool. This method uploads files for the agent’s code interpreter tool and updates the thread’s tool resources to include these files. Parameters: file_paths (str | Iterable[str]) – Path(s) to file(s) to upload cancellation_token (Optional[CancellationToken]) – Token for cancellation handling polling_interval (float) – Time to sleep between polling for file status Raises: ValueError – If file upload fails or the agent doesn’t have code interpreter capability async on_upload_for_file_search(file_paths: str | Iterable[str], cancellation_token: CancellationToken, vector_store_name: str | None = None, data_sources: List[VectorStoreDataSource] | None = None, expires_after: VectorStoreExpirationPolicy | None = None, chunking_strategy: VectorStoreChunkingStrategyRequest | None = None, vector_store_metadata: Dict[str, str] | None = None, vector_store_polling_interval: float = 1) → None[source]# Upload files to be used with the file search tool. This method handles uploading files for the file search capability, creating a vector store if necessary, and updating the agent’s configuration to use the vector store. Parameters: file_paths (str | Iterable[str]) – Path(s) to file(s) to upload cancellation_token (CancellationToken) – Token for cancellation handling vector_store_name (Optional[str]) – Name to assign to the vector store if creating a new one data_sources (Optional[List[VectorStoreDataSource]]) – Additional data sources for the vector store expires_after (Optional[VectorStoreExpirationPolicy]) – Expiration policy for vector store content chunking_strategy (Optional[VectorStoreChunkingStrategyRequest]) – Strategy for chunking file content vector_store_metadata (Optional[Dict[str, str]]) – Additional metadata for the vector store vector_store_polling_interval (float) – Time to sleep between polling for vector store status Raises: ValueError – If file search is not enabled for this agent or file upload fails async close() → None[source]# Close the Azure AI agent and release any resources. previous autogen_ext.ui next autogen_ext.agents.file_surfer",
      "code": "BaseChatAgent"
    },
    {
      "description": "import asyncio import os from autogen_agentchat.messages import TextMessage from autogen_core import CancellationToken from autogen_ext.agents.azure._azure_ai_agent import AzureAIAgent from azure.ai.projects.aio import AIProjectClient from azure.identity.aio import DefaultAzureCredential from azure.ai.agents.models import BingGroundingTool import dotenv async def bing_example(): async with DefaultAzureCredential() as credential: async with AIProjectClient( # type: ignore credential=credential, endpoint=os.getenv(\"AZURE_PROJECT_ENDPOINT\", \"\") ) as project_client: conn = await project_client.connections.get(name=os.getenv(\"BING_CONNECTION_NAME\", \"\")) bing_tool = BingGroundingTool(conn.id) agent_with_bing_grounding = AzureAIAgent( name=\"bing_agent\", description=\"An AI assistant with Bing grounding\", project_client=project_client, deployment_name=\"gpt-4o\", instructions=\"You are a helpful assistant.\", tools=bing_tool.definitions, metadata={\"source\": \"AzureAIAgent\"}, ) # For the bing grounding tool to return the citations, the message must contain an instruction for the model to do return them. # For example: \"Please provide citations for the answers\" result = await agent_with_bing_grounding.on_messages( messages=[ TextMessage( content=\"What is Microsoft\\'s annual leave policy? Provide citations for your answers.\", source=\"user\", ) ], cancellation_token=CancellationToken(), message_limit=5, ) print(result) if __name__ == \"__main__\": dotenv.load_dotenv() asyncio.run(bing_example())",
      "code": "import asyncio\nimport os\n\nfrom autogen_agentchat.messages import TextMessage\nfrom autogen_core import CancellationToken\nfrom autogen_ext.agents.azure._azure_ai_agent import AzureAIAgent\nfrom azure.ai.projects.aio import AIProjectClient\nfrom azure.identity.aio import DefaultAzureCredential\nfrom azure.ai.agents.models import BingGroundingTool\nimport dotenv\n\n\nasync def bing_example():\n    async with DefaultAzureCredential() as credential:\n        async with AIProjectClient(  # type: ignore\n            credential=credential, endpoint=os.getenv(\"AZURE_PROJECT_ENDPOINT\", \"\")\n        ) as project_client:\n            conn = await project_client.connections.get(name=os.getenv(\"BING_CONNECTION_NAME\", \"\"))\n\n            bing_tool = BingGroundingTool(conn.id)\n            agent_with_bing_grounding = AzureAIAgent(\n                name=\"bing_agent\",\n                description=\"An AI assistant with Bing grounding\",\n                project_client=project_client,\n                deployment_name=\"gpt-4o\",\n                instructions=\"You are a helpful assistant.\",\n                tools=bing_tool.definitions,\n                metadata={\"source\": \"AzureAIAgent\"},\n            )\n\n            # For the bing grounding tool to return the citations, the message must contain an instruction for the model to do return them.\n            # For example: \"Please provide citations for the answers\"\n\n            result = await agent_with_bing_grounding.on_messages(\n                messages=[\n                    TextMessage(\n                        content=\"What is Microsoft\\'s annual leave policy? Provide citations for your answers.\",\n                        source=\"user\",\n                    )\n                ],\n                cancellation_token=CancellationToken(),\n                message_limit=5,\n            )\n            print(result)\n\n\nif __name__ == \"__main__\":\n    dotenv.load_dotenv()\n    asyncio.run(bing_example())"
    },
    {
      "description": "import asyncio import os from autogen_agentchat.messages import TextMessage from autogen_core import CancellationToken from autogen_ext.agents.azure._azure_ai_agent import AzureAIAgent from azure.ai.projects.aio import AIProjectClient from azure.identity.aio import DefaultAzureCredential from azure.ai.agents.models import BingGroundingTool import dotenv async def bing_example(): async with DefaultAzureCredential() as credential: async with AIProjectClient( # type: ignore credential=credential, endpoint=os.getenv(\"AZURE_PROJECT_ENDPOINT\", \"\") ) as project_client: conn = await project_client.connections.get(name=os.getenv(\"BING_CONNECTION_NAME\", \"\")) bing_tool = BingGroundingTool(conn.id) agent_with_bing_grounding = AzureAIAgent( name=\"bing_agent\", description=\"An AI assistant with Bing grounding\", project_client=project_client, deployment_name=\"gpt-4o\", instructions=\"You are a helpful assistant.\", tools=bing_tool.definitions, metadata={\"source\": \"AzureAIAgent\"}, ) # For the bing grounding tool to return the citations, the message must contain an instruction for the model to do return them. # For example: \"Please provide citations for the answers\" result = await agent_with_bing_grounding.on_messages( messages=[ TextMessage( content=\"What is Microsoft\\'s annual leave policy? Provide citations for your answers.\", source=\"user\", ) ], cancellation_token=CancellationToken(), message_limit=5, ) print(result) if __name__ == \"__main__\": dotenv.load_dotenv() asyncio.run(bing_example())",
      "code": "import asyncio\nimport os\n\nfrom autogen_agentchat.messages import TextMessage\nfrom autogen_core import CancellationToken\nfrom autogen_ext.agents.azure._azure_ai_agent import AzureAIAgent\nfrom azure.ai.projects.aio import AIProjectClient\nfrom azure.identity.aio import DefaultAzureCredential\nfrom azure.ai.agents.models import BingGroundingTool\nimport dotenv\n\n\nasync def bing_example():\n    async with DefaultAzureCredential() as credential:\n        async with AIProjectClient(  # type: ignore\n            credential=credential, endpoint=os.getenv(\"AZURE_PROJECT_ENDPOINT\", \"\")\n        ) as project_client:\n            conn = await project_client.connections.get(name=os.getenv(\"BING_CONNECTION_NAME\", \"\"))\n\n            bing_tool = BingGroundingTool(conn.id)\n            agent_with_bing_grounding = AzureAIAgent(\n                name=\"bing_agent\",\n                description=\"An AI assistant with Bing grounding\",\n                project_client=project_client,\n                deployment_name=\"gpt-4o\",\n                instructions=\"You are a helpful assistant.\",\n                tools=bing_tool.definitions,\n                metadata={\"source\": \"AzureAIAgent\"},\n            )\n\n            # For the bing grounding tool to return the citations, the message must contain an instruction for the model to do return them.\n            # For example: \"Please provide citations for the answers\"\n\n            result = await agent_with_bing_grounding.on_messages(\n                messages=[\n                    TextMessage(\n                        content=\"What is Microsoft\\'s annual leave policy? Provide citations for your answers.\",\n                        source=\"user\",\n                    )\n                ],\n                cancellation_token=CancellationToken(),\n                message_limit=5,\n            )\n            print(result)\n\n\nif __name__ == \"__main__\":\n    dotenv.load_dotenv()\n    asyncio.run(bing_example())"
    }
  ],
  "links": [
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.azure.html",
    "https://microsoft.github.io/autogen/stable/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/extensions-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/reference/index.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.agents.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.base.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.conditions.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.messages.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.state.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.teams.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.tools.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.ui.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.utils.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.code_executor.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.exceptions.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.logging.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.memory.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.model_context.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.models.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.tool_agent.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.tools.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.utils.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.cache_store.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.experimental.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.memory.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.teams.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.ui.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.file_surfer.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.magentic_one.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.openai.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.video_surfer.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.web_surfer.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.auth.azure.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.cache_store.diskcache.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.cache_store.redis.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.azure.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.docker.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.docker_jupyter.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.jupyter.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.local.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.experimental.task_centric_memory.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.memory.canvas.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.memory.chromadb.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.memory.mem0.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.memory.redis.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.anthropic.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.azure.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.cache.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.llama_cpp.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.ollama.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.openai.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.replay.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.semantic_kernel.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.teams.magentic_one.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.azure.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.code_execution.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.graphrag.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.http.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.langchain.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.mcp.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.semantic_kernel.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.video_surfer.tools.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.web_surfer.playwright_controller.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.experimental.task_centric_memory.utils.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.anthropic.config.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.azure.config.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.ollama.config.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.openai.config.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.protos.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.protos.agent_worker_pb2.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.protos.agent_worker_pb2_grpc.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.protos.cloudevent_pb2.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.protos.cloudevent_pb2_grpc.html"
  ]
}