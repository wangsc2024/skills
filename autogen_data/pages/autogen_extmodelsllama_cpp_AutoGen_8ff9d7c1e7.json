{
  "url": "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.llama_cpp.html",
  "title": "autogen_ext.models.llama_cpp — AutoGen",
  "content": "Bases: ChatCompletionClient\n\nChat completion client for LlamaCpp models. To use this client, you must install the llama-cpp extra:\n\nThis client allows you to interact with LlamaCpp models, either by specifying a local model path or by downloading a model from Hugging Face Hub.\n\nmodel_info (optional, ModelInfo) – The information about the model. Defaults to DEFAULT_MODEL_INFO.\n\nmodel_path (optional, str) – The path to the LlamaCpp model file. Required if repo_id and filename are not provided.\n\nrepo_id (optional, str) – The Hugging Face Hub repository ID. Required if model_path is not provided.\n\nfilename (optional, str) – The filename of the model within the Hugging Face Hub repository. Required if model_path is not provided.\n\nn_gpu_layers (optional, int) – The number of layers to put on the GPU.\n\nn_ctx (optional, int) – The context size.\n\nn_batch (optional, int) – The batch size.\n\nverbose (optional, bool) – Whether to print verbose output.\n\n**kwargs – Additional parameters to pass to the Llama class.\n\nThe following code snippet shows how to use the client with a local model file:\n\nThe following code snippet shows how to use the client with a model from Hugging Face Hub:\n\nCreates a single response from the model.\n\nmessages (Sequence[LLMMessage]) – The messages to send to the model.\n\ntools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to [].\n\ntool_choice (Tool | Literal[\"auto\", \"required\", \"none\"], optional) – A single Tool object to force the model to use, “auto” to let the model choose any available tool, “required” to force tool usage, or “none” to disable tool usage. Defaults to “auto”.\n\njson_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither. Defaults to None. If set to a Pydantic BaseModel type, it will be used as the output type for structured output. If set to a boolean, it will be used to determine whether to use JSON mode or not. If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.\n\nextra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}.\n\ncancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None.\n\nCreateResult – The result of the model call.\n\nCreates a stream of string chunks from the model ending with a CreateResult.\n\nmessages (Sequence[LLMMessage]) – The messages to send to the model.\n\ntools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to [].\n\ntool_choice (Tool | Literal[\"auto\", \"required\", \"none\"], optional) – A single Tool object to force the model to use, “auto” to let the model choose any available tool, “required” to force tool usage, or “none” to disable tool usage. Defaults to “auto”.\n\njson_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither. Defaults to None. If set to a Pydantic BaseModel type, it will be used as the output type for structured output. If set to a boolean, it will be used to determine whether to use JSON mode or not. If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.\n\nWhether to use JSON mode, structured output, or neither. Defaults to None. If set to a Pydantic BaseModel type, it will be used as the output type for structured output. If set to a boolean, it will be used to determine whether to use JSON mode or not. If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.\n\nextra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}.\n\ncancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None.\n\nAsyncGenerator[Union[str, CreateResult], None] – A generator that yields string chunks and ends with a CreateResult.\n\nClose the LlamaCpp client.\n\nautogen_ext.models.cache\n\nautogen_ext.models.ollama",
  "headings": [
    {
      "level": "h1",
      "text": "autogen_ext.models.llama_cpp#",
      "id": ""
    }
  ],
  "code_samples": [
    {
      "code": "pip install \"autogen-ext[llama-cpp]\"",
      "language": "unknown"
    },
    {
      "code": "pip install \"autogen-ext[llama-cpp]\"",
      "language": "unknown"
    },
    {
      "code": "import asyncio\n\nfrom autogen_core.models import UserMessage\nfrom autogen_ext.models.llama_cpp import LlamaCppChatCompletionClient\n\n\nasync def main():\n    llama_client = LlamaCppChatCompletionClient(model_path=\"/path/to/your/model.gguf\")\n    result = await llama_client.create([UserMessage(content=\"What is the capital of France?\", source=\"user\")])\n    print(result)\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\n\nfrom autogen_core.models import UserMessage\nfrom autogen_ext.models.llama_cpp import LlamaCppChatCompletionClient\n\n\nasync def main():\n    llama_client = LlamaCppChatCompletionClient(model_path=\"/path/to/your/model.gguf\")\n    result = await llama_client.create([UserMessage(content=\"What is the capital of France?\", source=\"user\")])\n    print(result)\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\n\nfrom autogen_core.models import UserMessage\nfrom autogen_ext.models.llama_cpp import LlamaCppChatCompletionClient\n\n\nasync def main():\n    llama_client = LlamaCppChatCompletionClient(\n        repo_id=\"unsloth/phi-4-GGUF\", filename=\"phi-4-Q2_K_L.gguf\", n_gpu_layers=-1, seed=1337, n_ctx=5000\n    )\n    result = await llama_client.create([UserMessage(content=\"What is the capital of France?\", source=\"user\")])\n    print(result)\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\n\nfrom autogen_core.models import UserMessage\nfrom autogen_ext.models.llama_cpp import LlamaCppChatCompletionClient\n\n\nasync def main():\n    llama_client = LlamaCppChatCompletionClient(\n        repo_id=\"unsloth/phi-4-GGUF\", filename=\"phi-4-Q2_K_L.gguf\", n_gpu_layers=-1, seed=1337, n_ctx=5000\n    )\n    result = await llama_client.create([UserMessage(content=\"What is the capital of France?\", source=\"user\")])\n    print(result)\n\n\nasyncio.run(main())",
      "language": "python"
    }
  ],
  "patterns": [],
  "links": [
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.llama_cpp.html",
    "https://microsoft.github.io/autogen/stable/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/extensions-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/reference/index.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.agents.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.base.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.conditions.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.messages.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.state.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.teams.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.tools.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.ui.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.utils.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.code_executor.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.exceptions.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.logging.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.memory.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.model_context.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.models.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.tool_agent.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.tools.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.utils.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.cache_store.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.experimental.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.memory.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.teams.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.ui.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.azure.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.file_surfer.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.magentic_one.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.openai.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.video_surfer.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.web_surfer.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.auth.azure.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.cache_store.diskcache.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.cache_store.redis.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.azure.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.docker.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.docker_jupyter.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.jupyter.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.local.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.experimental.task_centric_memory.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.memory.canvas.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.memory.chromadb.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.memory.mem0.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.memory.redis.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.anthropic.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.azure.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.cache.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.ollama.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.openai.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.replay.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.semantic_kernel.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.teams.magentic_one.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.azure.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.code_execution.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.graphrag.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.http.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.langchain.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.mcp.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.semantic_kernel.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.video_surfer.tools.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.web_surfer.playwright_controller.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.experimental.task_centric_memory.utils.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.anthropic.config.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.azure.config.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.ollama.config.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.openai.config.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.protos.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.protos.agent_worker_pb2.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.protos.agent_worker_pb2_grpc.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.protos.cloudevent_pb2.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.protos.cloudevent_pb2_grpc.html"
  ]
}