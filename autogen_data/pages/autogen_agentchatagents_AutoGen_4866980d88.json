{
  "url": "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.agents.html",
  "title": "autogen_agentchat.agents — AutoGen",
  "content": "This module initializes various pre-defined agents provided by the package. BaseChatAgent is the base class for all agents in AgentChat.\n\nBases: ChatAgent, ABC, ComponentBase[BaseModel]\n\nBase class for a chat agent.\n\nThis abstract class provides a base implementation for a ChatAgent. To create a new chat agent, subclass this class and implement the on_messages(), on_reset(), and produced_message_types. If streaming is required, also implement the on_messages_stream() method.\n\nAn agent is considered stateful and maintains its state between calls to the on_messages() or on_messages_stream() methods. The agent should store its state in the agent instance. The agent should also implement the on_reset() method to reset the agent to its initialization state.\n\nThe caller should only pass the new messages to the agent on each call to the on_messages() or on_messages_stream() method. Do not pass the entire conversation history to the agent on each call. This design principle must be followed when creating a new agent.\n\nThe logical type of the component.\n\nThe name of the agent. This is used by team to uniquely identify the agent. It should be unique within the team.\n\nThe description of the agent. This is used by team to make decisions about which agents to use. The description should describe the agent’s capabilities and how to interact with it.\n\nThe types of messages that the agent produces in the Response.chat_message field. They must be BaseChatMessage types.\n\nHandles incoming messages and returns a response.\n\nAgents are stateful and the messages passed to this method should be the new messages since the last call to this method. The agent should maintain its state between calls to this method. For example, if the agent needs to remember the previous messages to respond to the current message, it should store the previous messages in the agent state.\n\nHandles incoming messages and returns a stream of messages and and the final item is the response. The base implementation in BaseChatAgent simply calls on_messages() and yields the messages in the response.\n\nAgents are stateful and the messages passed to this method should be the new messages since the last call to this method. The agent should maintain its state between calls to this method. For example, if the agent needs to remember the previous messages to respond to the current message, it should store the previous messages in the agent state.\n\nRun the agent with the given task and return the result.\n\nRun the agent with the given task and return a stream of messages and the final task result as the last item in the stream.\n\ntask – The task to run. Can be a string, a single message, or a sequence of messages.\n\ncancellation_token – The cancellation token to kill the task immediately.\n\noutput_task_messages – Whether to include task messages in the output stream. Defaults to True for backward compatibility.\n\nResets the agent to its initialization state.\n\nCalled when the agent is paused while running in its on_messages() or on_messages_stream() method. This is a no-op by default in the BaseChatAgent class. Subclasses can override this method to implement custom pause behavior.\n\nCalled when the agent is resumed from a pause while running in its on_messages() or on_messages_stream() method. This is a no-op by default in the BaseChatAgent class. Subclasses can override this method to implement custom resume behavior.\n\nExport state. Default implementation for stateless agents.\n\nRestore agent from saved state. Default implementation for stateless agents.\n\nRelease any resources held by the agent. This is a no-op by default in the BaseChatAgent class. Subclasses can override this method to implement custom close behavior.\n\nBases: BaseChatAgent, Component[AssistantAgentConfig]\n\nAn agent that provides assistance with tool use. The on_messages() returns a Response in which chat_message is the final response message.\n\nThe on_messages_stream() creates an async generator that produces the inner messages as they are created, and the Response object as the last item before closing the generator.\n\nThe BaseChatAgent.run() method returns a TaskResult containing the messages produced by the agent. In the list of messages, messages, the last message is the final response message.\n\nThe BaseChatAgent.run_stream() method creates an async generator that produces the inner messages as they are created, and the TaskResult object as the last item before closing the generator.\n\nThe caller must only pass the new messages to the agent on each call to the on_messages(), on_messages_stream(), BaseChatAgent.run(), or BaseChatAgent.run_stream() methods. The agent maintains its state between calls to these methods. Do not pass the entire conversation history to the agent on each call.\n\nThe assistant agent is not thread-safe or coroutine-safe. It should not be shared between multiple tasks or coroutines, and it should not call its methods concurrently.\n\nThe following diagram shows how the assistant agent works:\n\nIf the output_content_type is set, the agent will respond with a StructuredMessage instead of a TextMessage in the final response by default.\n\nCurrently, setting output_content_type prevents the agent from being able to call load_component and dum_component methods for serializable configuration. This will be fixed soon in the future.\n\nIf the model returns no tool call, then the response is immediately returned as a TextMessage or a StructuredMessage (when using structured output) in chat_message. This ends the tool call iteration loop regardless of the max_tool_iterations setting.\n\nWhen reflect_on_tool_use is False, the tool call results are returned as a ToolCallSummaryMessage in chat_message. You can customise the summary with either a static format string (tool_call_summary_format) or a callable (tool_call_summary_formatter); the callable is evaluated once per tool call.\n\nWhen reflect_on_tool_use is True, the another model inference is made using the tool calls and results, and final response is returned as a TextMessage or a StructuredMessage (when using structured output) in chat_message.\n\nreflect_on_tool_use is set to True by default when output_content_type is set.\n\nreflect_on_tool_use is set to False by default when output_content_type is not set.\n\nIf the model returns multiple tool calls, they will be executed concurrently. To disable parallel tool calls you need to configure the model client. For example, set parallel_tool_calls=False for OpenAIChatCompletionClient and AzureOpenAIChatCompletionClient.\n\nThe max_tool_iterations parameter controls how many sequential tool call iterations the agent can perform in a single run. When set to 1 (default), the agent executes tool calls once and returns the result. When set higher, the agent can make additional model calls to execute more tool calls if the model continues to request them, enabling multi-step tool-based workflows. The agent stops when either the model returns a text response (instead of tool calls) or the maximum number of iterations is reached.\n\nBy default, the tool call results are returned as the response when tool calls are made, so pay close attention to how the tools’ return values are formatted—especially if another agent expects a specific schema.\n\nUse `tool_call_summary_format` for a simple static template.\n\nUse `tool_call_summary_formatter` for full programmatic control (e.g., “hide large success payloads, show full details on error”).\n\nNote: tool_call_summary_formatter is not serializable and will be ignored when an agent is loaded from, or exported to, YAML/JSON configuration files.\n\nIf a handoff is triggered, a HandoffMessage will be returned in chat_message.\n\nIf there are tool calls, they will also be executed right away before returning the handoff.\n\nThe tool calls and results are passed to the target agent through context.\n\nIf multiple handoffs are detected, only the first handoff is executed. To avoid this, disable parallel tool calls in the model client configuration.\n\nLimit context size sent to the model:\n\nYou can limit the number of messages sent to the model by setting the model_context parameter to a BufferedChatCompletionContext. This will limit the number of recent messages sent to the model and can be useful when the model has a limit on the number of tokens it can process. Another option is to use a TokenLimitedChatCompletionContext which will limit the number of tokens sent to the model. You can also create your own model context by subclassing ChatCompletionContext.\n\nThe assistant agent can be used in streaming mode by setting model_client_stream=True. In this mode, the on_messages_stream() and BaseChatAgent.run_stream() methods will also yield ModelClientStreamingChunkEvent messages as the model client produces chunks of response. The chunk messages will not be included in the final response’s inner messages.\n\nname (str) – The name of the agent.\n\nmodel_client (ChatCompletionClient) – The model client to use for inference.\n\ntools (List[BaseTool[Any, Any] | Callable[..., Any] | Callable[..., Awaitable[Any]]] | None, optional) – The tools to register with the agent.\n\nworkbench (Workbench | Sequence[Workbench] | None, optional) – The workbench or list of workbenches to use for the agent. Tools cannot be used when workbench is set and vice versa.\n\nhandoffs (List[HandoffBase | str] | None, optional) – The handoff configurations for the agent, allowing it to transfer to other agents by responding with a HandoffMessage. The transfer is only executed when the team is in Swarm. If a handoff is a string, it should represent the target agent’s name.\n\nmodel_context (ChatCompletionContext | None, optional) – The model context for storing and retrieving LLMMessage. It can be preloaded with initial messages. The initial messages will be cleared when the agent is reset.\n\ndescription (str, optional) – The description of the agent.\n\nsystem_message (str, optional) – The system message for the model. If provided, it will be prepended to the messages in the model context when making an inference. Set to None to disable.\n\nmodel_client_stream (bool, optional) – If True, the model client will be used in streaming mode. on_messages_stream() and BaseChatAgent.run_stream() methods will also yield ModelClientStreamingChunkEvent messages as the model client produces chunks of response. Defaults to False.\n\nreflect_on_tool_use (bool, optional) – If True, the agent will make another model inference using the tool call and result to generate a response. If False, the tool call result will be returned as the response. By default, if output_content_type is set, this will be True; if output_content_type is not set, this will be False.\n\noutput_content_type (type[BaseModel] | None, optional) – The output content type for StructuredMessage response as a Pydantic model. This will be used with the model client to generate structured output. If this is set, the agent will respond with a StructuredMessage instead of a TextMessage in the final response, unless reflect_on_tool_use is False and a tool call is made.\n\noutput_content_type_format (str | None, optional) – (Experimental) The format string used for the content of a StructuredMessage response.\n\nmax_tool_iterations (int, optional) – The maximum number of tool iterations to perform until the model stops making tool calls. Defaults to 1, which means the agent will only execute the tool calls made by the model once, and return the result as a ToolCallSummaryMessage, or a TextMessage or a StructuredMessage (when using structured output) in chat_message as the final response. As soon as the model stops making tool calls, the agent will stop executing tool calls and return the result as the final response. The value must be greater than or equal to 1.\n\ntool_call_summary_format (str, optional) – Static format string applied to each tool call result when composing the ToolCallSummaryMessage. Defaults to \"{result}\". Ignored if tool_call_summary_formatter is provided. When reflect_on_tool_use is False, the summaries for all tool calls are concatenated with a newline (’n’) and returned as the response. Placeholders available in the template: {tool_name}, {arguments}, {result}, {is_error}.\n\ntool_call_summary_formatter (Callable[[FunctionCall, FunctionExecutionResult], str] | None, optional) – Callable that receives the FunctionCall and its FunctionExecutionResult and returns the summary string. Overrides tool_call_summary_format when supplied and allows conditional logic — for example, emitting static string like \"Tool FooBar executed successfully.\" on success and a full payload (including all passed arguments etc.) only on failure. Limitation: The callable is not serializable; values provided via YAML/JSON configs are ignored.\n\nCallable that receives the FunctionCall and its FunctionExecutionResult and returns the summary string. Overrides tool_call_summary_format when supplied and allows conditional logic — for example, emitting static string like \"Tool FooBar executed successfully.\" on success and a full payload (including all passed arguments etc.) only on failure.\n\nLimitation: The callable is not serializable; values provided via YAML/JSON configs are ignored.\n\ntool_call_summary_formatter is intended for in-code use only. It cannot currently be saved or restored via configuration files.\n\nmemory (Sequence[Memory] | None, optional): The memory store to use for the agent. Defaults to None. metadata (Dict[str, str] | None, optional): Optional metadata for tracking.\n\nValueError – If tool names are not unique.\n\nValueError – If handoff names are not unique.\n\nValueError – If handoff names are not unique from tool names.\n\nValueError – If maximum number of tool iterations is less than 1.\n\nExample 1: basic agent\n\nThe following example demonstrates how to create an assistant agent with a model client and generate a response to a simple task.\n\nExample 2: model client token streaming\n\nThis example demonstrates how to create an assistant agent with a model client and generate a token stream by setting model_client_stream=True.\n\nExample 3: agent with tools\n\nThe following example demonstrates how to create an assistant agent with a model client and a tool, generate a stream of messages for a task, and print the messages to the console using Console.\n\nThe tool is a simple function that returns the current time. Under the hood, the function is wrapped in a FunctionTool and used with the agent’s model client. The doc string of the function is used as the tool description, the function name is used as the tool name, and the function signature including the type hints is used as the tool arguments.\n\nExample 4: agent with max_tool_iterations\n\nThe following example demonstrates how to use the max_tool_iterations parameter to control how many times the agent can execute tool calls in a single run. This is useful when you want the agent to perform multiple sequential tool operations to reach a goal.\n\nExample 5: agent with Model-Context Protocol (MCP) workbench\n\nThe following example demonstrates how to create an assistant agent with a model client and an McpWorkbench for interacting with a Model-Context Protocol (MCP) server.\n\nExample 6: agent with structured output and tool\n\nThe following example demonstrates how to create an assistant agent with a model client configured to use structured output and a tool. Note that you need to use FunctionTool to create the tool and the strict=True is required for structured output mode. Because the model is configured to use structured output, the output reflection response will be a JSON formatted string.\n\nExample 7: agent with bounded model context\n\nThe following example shows how to use a BufferedChatCompletionContext that only keeps the last 2 messages (1 user + 1 assistant). Bounded model context is useful when the model has a limit on the number of tokens it can process.\n\nExample 8: agent with memory\n\nThe following example shows how to use a list-based memory with the assistant agent. The memory is preloaded with some initial content. Under the hood, the memory is used to update the model context before making an inference, using the update_context() method.\n\nExample 9: agent with `o1-mini`\n\nThe following example shows how to use o1-mini model with the assistant agent.\n\nThe o1-preview and o1-mini models do not support system message and function calling. So the system_message should be set to None and the tools and handoffs should not be set. See o1 beta limitations for more details.\n\nExample 10: agent using reasoning model with custom model context.\n\nThe following example shows how to use a reasoning model (DeepSeek R1) with the assistant agent. The model context is used to filter out the thought field from the assistant message.\n\nFor detailed examples and usage, see the Examples section below.\n\nThe version of the component, if schema incompatibilities are introduced this should be updated.\n\nalias of AssistantAgentConfig\n\nOverride the provider string for the component. This should be used to prevent internal module names being a part of the module name.\n\nGet the types of messages this agent can produce.\n\nSequence of message types this agent can generate\n\nGet the model context used by this agent.\n\nThe chat completion context for this agent\n\nProcess incoming messages and generate a response.\n\nmessages – Sequence of messages to process\n\ncancellation_token – Token for cancelling operation\n\nResponse containing the agent’s reply\n\nProcess messages and stream the response.\n\nmessages – Sequence of messages to process\n\ncancellation_token – Token for cancelling operation\n\nEvents, messages and final response during processing\n\nReset the assistant agent to its initialization state.\n\nSave the current state of the assistant agent.\n\nLoad the state of the assistant agent\n\nBases: BaseChatAgent, Component[CodeExecutorAgentConfig]\n\n(Experimental) An agent that generates and executes code snippets based on user instructions.\n\nThis agent is experimental and may change in future releases.\n\nIt is typically used within a team with another agent that generates code snippets to be executed or alone with model_client provided so that it can generate code based on user query, execute it and reflect on the code result.\n\nWhen used with model_client, it will generate code snippets using the model and execute them using the provided code_executor. The model will also reflect on the code execution results. The agent will yield the final reflection result from the model as the final response.\n\nWhen used without model_client, it will only execute code blocks found in TextMessage messages and returns the output of the code execution.\n\nUsing AssistantAgent with PythonCodeExecutionTool is an alternative to this agent. However, the model for that agent will have to generate properly escaped code string as a parameter to the tool.\n\nname (str) – The name of the agent.\n\ncode_executor (CodeExecutor) – The code executor responsible for executing code received in messages (DockerCommandLineCodeExecutor recommended. See example below)\n\nmodel_client (ChatCompletionClient, optional) – The model client to use for inference and generating code. If not provided, the agent will only execute code blocks found in input messages. Currently, the model must support structured output mode, which is required for the automatic retry mechanism to work.\n\nmodel_client_stream (bool, optional) – If True, the model client will be used in streaming mode. on_messages_stream() and BaseChatAgent.run_stream() methods will also yield ModelClientStreamingChunkEvent messages as the model client produces chunks of response. Defaults to False.\n\ndescription (str, optional) – The description of the agent. If not provided, DEFAULT_AGENT_DESCRIPTION will be used.\n\nsystem_message (str, optional) – The system message for the model. If provided, it will be prepended to the messages in the model context when making an inference. Set to None to disable. Defaults to DEFAULT_SYSTEM_MESSAGE. This is only used if model_client is provided.\n\nsources (Sequence[str], optional) – Check only messages from the specified agents for the code to execute. This is useful when the agent is part of a group chat and you want to limit the code execution to messages from specific agents. If not provided, all messages will be checked for code blocks. This is only used if model_client is not provided.\n\nmax_retries_on_error (int, optional) – The maximum number of retries on error. If the code execution fails, the agent will retry up to this number of times. If the code execution fails after this number of retries, the agent will yield a reflection result.\n\nsupported_languages (List[str], optional) – List of programming languages that will be parsed and executed from agent response; others will be ignored. Defaults to DEFAULT_SUPPORTED_LANGUAGES.\n\napproval_func (Optional[Union[Callable[[ApprovalRequest], ApprovalResponse], Callable[[ApprovalRequest], Awaitable[ApprovalResponse]]]], optional) – A function that is called before each code execution to get approval. The function takes an ApprovalRequest containing the code to be executed and the current context, and returns an ApprovalResponse. The function can be either synchronous or asynchronous. If None (default), all code executions are automatically approved. If set, the agent cannot be serialized using dump_component().\n\nIt is recommended that the CodeExecutorAgent agent uses a Docker container to execute code. This ensures that model-generated code is executed in an isolated environment. To use Docker, your environment must have Docker installed and running. Follow the installation instructions for Docker.\n\nThe code executor only processes code that is properly formatted in markdown code blocks using triple backticks. For example:\n\nIn this example, we show how to set up a CodeExecutorAgent agent that uses the DockerCommandLineCodeExecutor to execute code snippets in a Docker container. The work_dir parameter indicates where all executed files are first saved locally before being executed in the Docker container.\n\nIn this example, we show how to set up a CodeExecutorAgent agent that uses the DeviceRequest to expose a GPU to the container for cuda-accelerated code execution.\n\nIn the following example, we show how to setup CodeExecutorAgent without model_client parameter for executing code blocks generated by other agents in a group chat using DockerCommandLineCodeExecutor\n\nIn the following example, we show how to setup CodeExecutorAgent with model_client that can generate its own code without the help of any other agent and executing it in DockerCommandLineCodeExecutor. It also demonstrates using a model-based approval function that reviews the code for safety before execution.\n\nalias of CodeExecutorAgentConfig\n\nOverride the provider string for the component. This should be used to prevent internal module names being a part of the module name.\n\nThe types of messages that the code executor agent produces.\n\nThe model context in use by the agent.\n\nHandles incoming messages and returns a response.\n\nAgents are stateful and the messages passed to this method should be the new messages since the last call to this method. The agent should maintain its state between calls to this method. For example, if the agent needs to remember the previous messages to respond to the current message, it should store the previous messages in the agent state.\n\nProcess the incoming messages with the assistant agent and yield events/responses as they happen.\n\nIts a no-op as the code executor agent has no mutable state.\n\nDump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.\n\nT – The configuration of the component.\n\nCreate a new instance of the component from a configuration object.\n\nconfig (T) – The configuration object.\n\nSelf – The new instance of the component.\n\nBases: BaseChatAgent, Component[SocietyOfMindAgentConfig]\n\nAn agent that uses an inner team of agents to generate responses.\n\nEach time the agent’s on_messages() or on_messages_stream() method is called, it runs the inner team of agents and then uses the model client to generate a response based on the inner team’s messages. Once the response is generated, the agent resets the inner team by calling Team.reset().\n\nLimit context size sent to the model:\n\nYou can limit the number of messages sent to the model by setting the model_context parameter to a BufferedChatCompletionContext. This will limit the number of recent messages sent to the model and can be useful when the model has a limit on the number of tokens it can process. You can also create your own model context by subclassing ChatCompletionContext.\n\nname (str) – The name of the agent.\n\nteam (Team) – The team of agents to use.\n\nmodel_client (ChatCompletionClient) – The model client to use for preparing responses.\n\ndescription (str, optional) – The description of the agent.\n\ninstruction (str, optional) – The instruction to use when generating a response using the inner team’s messages. Defaults to DEFAULT_INSTRUCTION. It assumes the role of ‘system’.\n\nresponse_prompt (str, optional) – The response prompt to use when generating a response using the inner team’s messages. Defaults to DEFAULT_RESPONSE_PROMPT. It assumes the role of ‘system’.\n\nmodel_context (ChatCompletionContext | None, optional) – The model context for storing and retrieving LLMMessage. It can be preloaded with initial messages. The initial messages will be cleared when the agent is reset.\n\nalias of SocietyOfMindAgentConfig\n\nOverride the provider string for the component. This should be used to prevent internal module names being a part of the module name.\n\nThe default instruction to use when generating a response using the inner team’s messages. The instruction will be prepended to the inner team’s messages when generating a response using the model. It assumes the role of ‘system’.\n\nThe default response prompt to use when generating a response using the inner team’s messages. It assumes the role of ‘system’.\n\nThe default description for a SocietyOfMindAgent.\n\nThe types of messages that the agent produces in the Response.chat_message field. They must be BaseChatMessage types.\n\nThe model context in use by the agent.\n\nHandles incoming messages and returns a response.\n\nAgents are stateful and the messages passed to this method should be the new messages since the last call to this method. The agent should maintain its state between calls to this method. For example, if the agent needs to remember the previous messages to respond to the current message, it should store the previous messages in the agent state.\n\nHandles incoming messages and returns a stream of messages and and the final item is the response. The base implementation in BaseChatAgent simply calls on_messages() and yields the messages in the response.\n\nAgents are stateful and the messages passed to this method should be the new messages since the last call to this method. The agent should maintain its state between calls to this method. For example, if the agent needs to remember the previous messages to respond to the current message, it should store the previous messages in the agent state.\n\nResets the agent to its initialization state.\n\nExport state. Default implementation for stateless agents.\n\nRestore agent from saved state. Default implementation for stateless agents.\n\nDump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.\n\nT – The configuration of the component.\n\nCreate a new instance of the component from a configuration object.\n\nconfig (T) – The configuration object.\n\nSelf – The new instance of the component.\n\nBases: BaseChatAgent, Component[UserProxyAgentConfig]\n\nAn agent that can represent a human user through an input function.\n\nThis agent can be used to represent a human user in a chat system by providing a custom input function.\n\nUsing UserProxyAgent puts a running team in a temporary blocked state until the user responds. So it is important to time out the user input function and cancel using the CancellationToken if the user does not respond. The input function should also handle exceptions and return a default response if needed.\n\nFor typical use cases that involve slow human responses, it is recommended to use termination conditions such as HandoffTermination or SourceMatchTermination to stop the running team and return the control to the application. You can run the team again with the user input. This way, the state of the team can be saved and restored when the user responds.\n\nSee Human-in-the-loop for more information.\n\nname (str) – The name of the agent.\n\ndescription (str, optional) – A description of the agent.\n\ninput_func (Optional[Callable[[str], str]], Callable[[str, Optional[CancellationToken]], Awaitable[str]]) – A function that takes a prompt and returns a user input string.\n\nFor examples of integrating with web and UI frameworks, see the following:\n\nCancellable usage case:\n\nThe logical type of the component.\n\nOverride the provider string for the component. This should be used to prevent internal module names being a part of the module name.\n\nalias of UserProxyAgentConfig\n\nMessage types this agent can produce.\n\nHandles incoming messages and returns a response.\n\nAgents are stateful and the messages passed to this method should be the new messages since the last call to this method. The agent should maintain its state between calls to this method. For example, if the agent needs to remember the previous messages to respond to the current message, it should store the previous messages in the agent state.\n\nHandle incoming messages by requesting user input.\n\nDump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.\n\nT – The configuration of the component.\n\nCreate a new instance of the component from a configuration object.\n\nconfig (T) – The configuration object.\n\nSelf – The new instance of the component.\n\nBases: BaseChatAgent, Component[MessageFilterAgentConfig]\n\nA wrapper agent that filters incoming messages before passing them to the inner agent.\n\nThis is an experimental feature, and the API will change in the future releases.\n\nThis is useful in scenarios like multi-agent workflows where an agent should only process a subset of the full message history—for example, only the last message from each upstream agent, or only the first message from a specific source.\n\nFiltering is configured using MessageFilterConfig, which supports: - Filtering by message source (e.g., only messages from “user” or another agent) - Selecting the first N or last N messages from each source - If position is None, all messages from that source are included\n\nThis agent is compatible with both direct message passing and team-based execution such as GraphFlow.\n\nSuppose you have a looping multi-agent graph: A → B → A → B → C.\n\nYou want: - A to only see the user message and the last message from B - B to see the user message, last message from A, and its own prior responses (for reflection) - C to see the user message and the last message from B\n\nWrap the agents like so:\n\nThen define the graph:\n\nThis will ensure each agent sees only what is needed for its decision or action logic.\n\nalias of MessageFilterAgentConfig\n\nOverride the provider string for the component. This should be used to prevent internal module names being a part of the module name.\n\nThe types of messages that the agent produces in the Response.chat_message field. They must be BaseChatMessage types.\n\nHandles incoming messages and returns a response.\n\nAgents are stateful and the messages passed to this method should be the new messages since the last call to this method. The agent should maintain its state between calls to this method. For example, if the agent needs to remember the previous messages to respond to the current message, it should store the previous messages in the agent state.\n\nHandles incoming messages and returns a stream of messages and and the final item is the response. The base implementation in BaseChatAgent simply calls on_messages() and yields the messages in the response.\n\nAgents are stateful and the messages passed to this method should be the new messages since the last call to this method. The agent should maintain its state between calls to this method. For example, if the agent needs to remember the previous messages to respond to the current message, it should store the previous messages in the agent state.\n\nResets the agent to its initialization state.\n\nDump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.\n\nT – The configuration of the component.\n\nCreate a new instance of the component from a configuration object.\n\nconfig (T) – The configuration object.\n\nSelf – The new instance of the component.\n\nShow JSON schema{ \"title\": \"MessageFilterConfig\", \"type\": \"object\", \"properties\": { \"per_source\": { \"items\": { \"$ref\": \"#/$defs/PerSourceFilter\" }, \"title\": \"Per Source\", \"type\": \"array\" } }, \"$defs\": { \"PerSourceFilter\": { \"properties\": { \"source\": { \"title\": \"Source\", \"type\": \"string\" }, \"position\": { \"anyOf\": [ { \"enum\": [ \"first\", \"last\" ], \"type\": \"string\" }, { \"type\": \"null\" } ], \"default\": null, \"title\": \"Position\" }, \"count\": { \"anyOf\": [ { \"type\": \"integer\" }, { \"type\": \"null\" } ], \"default\": null, \"title\": \"Count\" } }, \"required\": [ \"source\" ], \"title\": \"PerSourceFilter\", \"type\": \"object\" } }, \"required\": [ \"per_source\" ] }\n\nper_source (List[autogen_agentchat.agents._message_filter_agent.PerSourceFilter])\n\nShow JSON schema{ \"title\": \"PerSourceFilter\", \"type\": \"object\", \"properties\": { \"source\": { \"title\": \"Source\", \"type\": \"string\" }, \"position\": { \"anyOf\": [ { \"enum\": [ \"first\", \"last\" ], \"type\": \"string\" }, { \"type\": \"null\" } ], \"default\": null, \"title\": \"Position\" }, \"count\": { \"anyOf\": [ { \"type\": \"integer\" }, { \"type\": \"null\" } ], \"default\": null, \"title\": \"Count\" } }, \"required\": [ \"source\" ] }\n\nposition (Literal['first', 'last'] | None)\n\nRequest for approval of code execution.\n\nShow JSON schema{ \"title\": \"ApprovalRequest\", \"description\": \"Request for approval of code execution.\", \"type\": \"object\", \"properties\": { \"code\": { \"title\": \"Code\", \"type\": \"string\" }, \"context\": { \"items\": { \"discriminator\": { \"mapping\": { \"AssistantMessage\": \"#/$defs/AssistantMessage\", \"FunctionExecutionResultMessage\": \"#/$defs/FunctionExecutionResultMessage\", \"SystemMessage\": \"#/$defs/SystemMessage\", \"UserMessage\": \"#/$defs/UserMessage\" }, \"propertyName\": \"type\" }, \"oneOf\": [ { \"$ref\": \"#/$defs/SystemMessage\" }, { \"$ref\": \"#/$defs/UserMessage\" }, { \"$ref\": \"#/$defs/AssistantMessage\" }, { \"$ref\": \"#/$defs/FunctionExecutionResultMessage\" } ] }, \"title\": \"Context\", \"type\": \"array\" } }, \"$defs\": { \"AssistantMessage\": { \"description\": \"Assistant message are sampled from the language model.\", \"properties\": { \"content\": { \"anyOf\": [ { \"type\": \"string\" }, { \"items\": { \"$ref\": \"#/$defs/FunctionCall\" }, \"type\": \"array\" } ], \"title\": \"Content\" }, \"thought\": { \"anyOf\": [ { \"type\": \"string\" }, { \"type\": \"null\" } ], \"default\": null, \"title\": \"Thought\" }, \"source\": { \"title\": \"Source\", \"type\": \"string\" }, \"type\": { \"const\": \"AssistantMessage\", \"default\": \"AssistantMessage\", \"title\": \"Type\", \"type\": \"string\" } }, \"required\": [ \"content\", \"source\" ], \"title\": \"AssistantMessage\", \"type\": \"object\" }, \"FunctionCall\": { \"properties\": { \"id\": { \"title\": \"Id\", \"type\": \"string\" }, \"arguments\": { \"title\": \"Arguments\", \"type\": \"string\" }, \"name\": { \"title\": \"Name\", \"type\": \"string\" } }, \"required\": [ \"id\", \"arguments\", \"name\" ], \"title\": \"FunctionCall\", \"type\": \"object\" }, \"FunctionExecutionResult\": { \"description\": \"Function execution result contains the output of a function call.\", \"properties\": { \"content\": { \"title\": \"Content\", \"type\": \"string\" }, \"name\": { \"title\": \"Name\", \"type\": \"string\" }, \"call_id\": { \"title\": \"Call Id\", \"type\": \"string\" }, \"is_error\": { \"anyOf\": [ { \"type\": \"boolean\" }, { \"type\": \"null\" } ], \"default\": null, \"title\": \"Is Error\" } }, \"required\": [ \"content\", \"name\", \"call_id\" ], \"title\": \"FunctionExecutionResult\", \"type\": \"object\" }, \"FunctionExecutionResultMessage\": { \"description\": \"Function execution result message contains the output of multiple function calls.\", \"properties\": { \"content\": { \"items\": { \"$ref\": \"#/$defs/FunctionExecutionResult\" }, \"title\": \"Content\", \"type\": \"array\" }, \"type\": { \"const\": \"FunctionExecutionResultMessage\", \"default\": \"FunctionExecutionResultMessage\", \"title\": \"Type\", \"type\": \"string\" } }, \"required\": [ \"content\" ], \"title\": \"FunctionExecutionResultMessage\", \"type\": \"object\" }, \"SystemMessage\": { \"description\": \"System message contains instructions for the model coming from the developer.\\n\\n.. note::\\n\\n Open AI is moving away from using 'system' role in favor of 'developer' role.\\n See `Model Spec <https://cdn.openai.com/spec/model-spec-2024-05-08.html#definitions>`_ for more details.\\n However, the 'system' role is still allowed in their API and will be automatically converted to 'developer' role\\n on the server side.\\n So, you can use `SystemMessage` for developer messages.\", \"properties\": { \"content\": { \"title\": \"Content\", \"type\": \"string\" }, \"type\": { \"const\": \"SystemMessage\", \"default\": \"SystemMessage\", \"title\": \"Type\", \"type\": \"string\" } }, \"required\": [ \"content\" ], \"title\": \"SystemMessage\", \"type\": \"object\" }, \"UserMessage\": { \"description\": \"User message contains input from end users, or a catch-all for data provided to the model.\", \"properties\": { \"content\": { \"anyOf\": [ { \"type\": \"string\" }, { \"items\": { \"anyOf\": [ { \"type\": \"string\" }, {} ] }, \"type\": \"array\" } ], \"title\": \"Content\" }, \"source\": { \"title\": \"Source\", \"type\": \"string\" }, \"type\": { \"const\": \"UserMessage\", \"default\": \"UserMessage\", \"title\": \"Type\", \"type\": \"string\" } }, \"required\": [ \"content\", \"source\" ], \"title\": \"UserMessage\", \"type\": \"object\" } }, \"required\": [ \"code\", \"context\" ] }\n\ncontext (List[autogen_core.models._types.SystemMessage | autogen_core.models._types.UserMessage | autogen_core.models._types.AssistantMessage | autogen_core.models._types.FunctionExecutionResultMessage])\n\nResponse to approval request.\n\nShow JSON schema{ \"title\": \"ApprovalResponse\", \"description\": \"Response to approval request.\", \"type\": \"object\", \"properties\": { \"approved\": { \"title\": \"Approved\", \"type\": \"boolean\" }, \"reason\": { \"title\": \"Reason\", \"type\": \"string\" } }, \"required\": [ \"approved\", \"reason\" ] }\n\nautogen_agentchat.base",
  "headings": [
    {
      "level": "h1",
      "text": "autogen_agentchat.agents#",
      "id": ""
    }
  ],
  "code_samples": [
    {
      "code": "import asyncio\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_agentchat.agents import AssistantAgent\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(\n        model=\"gpt-4o\",\n        # api_key = \"your_openai_api_key\"\n    )\n    agent = AssistantAgent(name=\"assistant\", model_client=model_client)\n\n    result = await agent.run(task=\"Name two cities in North America.\")\n    print(result)\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_agentchat.agents import AssistantAgent\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(\n        model=\"gpt-4o\",\n        # api_key = \"your_openai_api_key\"\n    )\n    agent = AssistantAgent(name=\"assistant\", model_client=model_client)\n\n    result = await agent.run(task=\"Name two cities in North America.\")\n    print(result)\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_agentchat.agents import AssistantAgent\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(\n        model=\"gpt-4o\",\n        # api_key = \"your_openai_api_key\"\n    )\n    agent = AssistantAgent(\n        name=\"assistant\",\n        model_client=model_client,\n        model_client_stream=True,\n    )\n\n    stream = agent.run_stream(task=\"Name two cities in North America.\")\n    async for message in stream:\n        print(message)\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_agentchat.agents import AssistantAgent\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(\n        model=\"gpt-4o\",\n        # api_key = \"your_openai_api_key\"\n    )\n    agent = AssistantAgent(\n        name=\"assistant\",\n        model_client=model_client,\n        model_client_stream=True,\n    )\n\n    stream = agent.run_stream(task=\"Name two cities in North America.\")\n    async for message in stream:\n        print(message)\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "source='user' models_usage=None metadata={} content='Name two cities in North America.' type='TextMessage'\nsource='assistant' models_usage=None metadata={} content='Two' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content=' cities' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content=' in' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content=' North' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content=' America' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content=' are' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content=' New' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content=' York' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content=' City' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content=' and' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content=' Toronto' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content='.' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content=' TERMIN' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content='ATE' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=RequestUsage(prompt_tokens=0, completion_tokens=0) metadata={} content='Two cities in North America are New York City and Toronto. TERMINATE' type='TextMessage'\nmessages=[TextMessage(source='user', models_usage=None, metadata={}, content='Name two cities in North America.', type='TextMessage'), TextMessage(source='assistant', models_usage=RequestUsage(prompt_tokens=0, completion_tokens=0), metadata={}, content='Two cities in North America are New York City and Toronto. TERMINATE', type='TextMessage')] stop_reason=None",
      "language": "rust"
    },
    {
      "code": "source='user' models_usage=None metadata={} content='Name two cities in North America.' type='TextMessage'\nsource='assistant' models_usage=None metadata={} content='Two' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content=' cities' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content=' in' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content=' North' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content=' America' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content=' are' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content=' New' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content=' York' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content=' City' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content=' and' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content=' Toronto' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content='.' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content=' TERMIN' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content='ATE' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=RequestUsage(prompt_tokens=0, completion_tokens=0) metadata={} content='Two cities in North America are New York City and Toronto. TERMINATE' type='TextMessage'\nmessages=[TextMessage(source='user', models_usage=None, metadata={}, content='Name two cities in North America.', type='TextMessage'), TextMessage(source='assistant', models_usage=RequestUsage(prompt_tokens=0, completion_tokens=0), metadata={}, content='Two cities in North America are New York City and Toronto. TERMINATE', type='TextMessage')] stop_reason=None",
      "language": "rust"
    },
    {
      "code": "import asyncio\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.ui import Console\n\n\nasync def get_current_time() -> str:\n    return \"The current time is 12:00 PM.\"\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(\n        model=\"gpt-4o\",\n        # api_key = \"your_openai_api_key\"\n    )\n    agent = AssistantAgent(name=\"assistant\", model_client=model_client, tools=[get_current_time])\n    await Console(agent.run_stream(task=\"What is the current time?\"))\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.ui import Console\n\n\nasync def get_current_time() -> str:\n    return \"The current time is 12:00 PM.\"\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(\n        model=\"gpt-4o\",\n        # api_key = \"your_openai_api_key\"\n    )\n    agent = AssistantAgent(name=\"assistant\", model_client=model_client, tools=[get_current_time])\n    await Console(agent.run_stream(task=\"What is the current time?\"))\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.ui import Console\n\n\n# Global counter state\ncounter = 0\n\n\ndef increment_counter() -> str:\n    \"\"\"Increment the counter by 1 and return the current value.\"\"\"\n    global counter\n    counter += 1\n    return f\"Counter incremented to: {counter}\"\n\n\ndef get_counter() -> str:\n    \"\"\"Get the current counter value.\"\"\"\n    global counter\n    return f\"Current counter value: {counter}\"\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(\n        model=\"gpt-4o\",\n        # api_key = \"your_openai_api_key\"\n    )\n\n    # Create agent with max_tool_iterations=5 to allow multiple tool calls\n    agent = AssistantAgent(\n        name=\"assistant\",\n        model_client=model_client,\n        tools=[increment_counter, get_counter],\n        max_tool_iterations=5,  # Allow up to 5 tool call iterations\n        reflect_on_tool_use=True,  # Get a final summary after tool calls\n    )\n\n    await Console(agent.run_stream(task=\"Increment the counter 3 times and then tell me the final value.\"))\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.ui import Console\n\n\n# Global counter state\ncounter = 0\n\n\ndef increment_counter() -> str:\n    \"\"\"Increment the counter by 1 and return the current value.\"\"\"\n    global counter\n    counter += 1\n    return f\"Counter incremented to: {counter}\"\n\n\ndef get_counter() -> str:\n    \"\"\"Get the current counter value.\"\"\"\n    global counter\n    return f\"Current counter value: {counter}\"\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(\n        model=\"gpt-4o\",\n        # api_key = \"your_openai_api_key\"\n    )\n\n    # Create agent with max_tool_iterations=5 to allow multiple tool calls\n    agent = AssistantAgent(\n        name=\"assistant\",\n        model_client=model_client,\n        tools=[increment_counter, get_counter],\n        max_tool_iterations=5,  # Allow up to 5 tool call iterations\n        reflect_on_tool_use=True,  # Get a final summary after tool calls\n    )\n\n    await Console(agent.run_stream(task=\"Increment the counter 3 times and then tell me the final value.\"))\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_ext.tools.mcp import StdioServerParams, McpWorkbench\n\n\nasync def main() -> None:\n    params = StdioServerParams(\n        command=\"uvx\",\n        args=[\"mcp-server-fetch\"],\n        read_timeout_seconds=60,\n    )\n\n    # You can also use `start()` and `stop()` to manage the session.\n    async with McpWorkbench(server_params=params) as workbench:\n        model_client = OpenAIChatCompletionClient(model=\"gpt-4.1-nano\")\n        assistant = AssistantAgent(\n            name=\"Assistant\",\n            model_client=model_client,\n            workbench=workbench,\n            reflect_on_tool_use=True,\n        )\n        await Console(\n            assistant.run_stream(task=\"Go to https://github.com/microsoft/autogen and tell me what you see.\")\n        )\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_ext.tools.mcp import StdioServerParams, McpWorkbench\n\n\nasync def main() -> None:\n    params = StdioServerParams(\n        command=\"uvx\",\n        args=[\"mcp-server-fetch\"],\n        read_timeout_seconds=60,\n    )\n\n    # You can also use `start()` and `stop()` to manage the session.\n    async with McpWorkbench(server_params=params) as workbench:\n        model_client = OpenAIChatCompletionClient(model=\"gpt-4.1-nano\")\n        assistant = AssistantAgent(\n            name=\"Assistant\",\n            model_client=model_client,\n            workbench=workbench,\n            reflect_on_tool_use=True,\n        )\n        await Console(\n            assistant.run_stream(task=\"Go to https://github.com/microsoft/autogen and tell me what you see.\")\n        )\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom typing import Literal\n\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.ui import Console\nfrom autogen_core.tools import FunctionTool\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom pydantic import BaseModel\n\n\n# Define the structured output format.\nclass AgentResponse(BaseModel):\n    thoughts: str\n    response: Literal[\"happy\", \"sad\", \"neutral\"]\n\n\n# Define the function to be called as a tool.\ndef sentiment_analysis(text: str) -> str:\n    \"\"\"Given a text, return the sentiment.\"\"\"\n    return \"happy\" if \"happy\" in text else \"sad\" if \"sad\" in text else \"neutral\"\n\n\n# Create a FunctionTool instance with `strict=True`,\n# which is required for structured output mode.\ntool = FunctionTool(sentiment_analysis, description=\"Sentiment Analysis\", strict=True)\n\n# Create an OpenAIChatCompletionClient instance that supports structured output.\nmodel_client = OpenAIChatCompletionClient(\n    model=\"gpt-4o-mini\",\n)\n\n# Create an AssistantAgent instance that uses the tool and model client.\nagent = AssistantAgent(\n    name=\"assistant\",\n    model_client=model_client,\n    tools=[tool],\n    system_message=\"Use the tool to analyze sentiment.\",\n    output_content_type=AgentResponse,\n)\n\n\nasync def main() -> None:\n    stream = agent.run_stream(task=\"I am happy today!\")\n    await Console(stream)\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom typing import Literal\n\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.ui import Console\nfrom autogen_core.tools import FunctionTool\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom pydantic import BaseModel\n\n\n# Define the structured output format.\nclass AgentResponse(BaseModel):\n    thoughts: str\n    response: Literal[\"happy\", \"sad\", \"neutral\"]\n\n\n# Define the function to be called as a tool.\ndef sentiment_analysis(text: str) -> str:\n    \"\"\"Given a text, return the sentiment.\"\"\"\n    return \"happy\" if \"happy\" in text else \"sad\" if \"sad\" in text else \"neutral\"\n\n\n# Create a FunctionTool instance with `strict=True`,\n# which is required for structured output mode.\ntool = FunctionTool(sentiment_analysis, description=\"Sentiment Analysis\", strict=True)\n\n# Create an OpenAIChatCompletionClient instance that supports structured output.\nmodel_client = OpenAIChatCompletionClient(\n    model=\"gpt-4o-mini\",\n)\n\n# Create an AssistantAgent instance that uses the tool and model client.\nagent = AssistantAgent(\n    name=\"assistant\",\n    model_client=model_client,\n    tools=[tool],\n    system_message=\"Use the tool to analyze sentiment.\",\n    output_content_type=AgentResponse,\n)\n\n\nasync def main() -> None:\n    stream = agent.run_stream(task=\"I am happy today!\")\n    await Console(stream)\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "---------- assistant ----------\n[FunctionCall(id='call_tIZjAVyKEDuijbBwLY6RHV2p', arguments='{\"text\":\"I am happy today!\"}', name='sentiment_analysis')]\n---------- assistant ----------\n[FunctionExecutionResult(content='happy', call_id='call_tIZjAVyKEDuijbBwLY6RHV2p', is_error=False)]\n---------- assistant ----------\n{\"thoughts\":\"The user expresses a clear positive emotion by stating they are happy today, suggesting an upbeat mood.\",\"response\":\"happy\"}",
      "language": "json"
    },
    {
      "code": "---------- assistant ----------\n[FunctionCall(id='call_tIZjAVyKEDuijbBwLY6RHV2p', arguments='{\"text\":\"I am happy today!\"}', name='sentiment_analysis')]\n---------- assistant ----------\n[FunctionExecutionResult(content='happy', call_id='call_tIZjAVyKEDuijbBwLY6RHV2p', is_error=False)]\n---------- assistant ----------\n{\"thoughts\":\"The user expresses a clear positive emotion by stating they are happy today, suggesting an upbeat mood.\",\"response\":\"happy\"}",
      "language": "json"
    },
    {
      "code": "import asyncio\n\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_core.model_context import BufferedChatCompletionContext\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n\nasync def main() -> None:\n    # Create a model client.\n    model_client = OpenAIChatCompletionClient(\n        model=\"gpt-4o-mini\",\n        # api_key = \"your_openai_api_key\"\n    )\n\n    # Create a model context that only keeps the last 2 messages (1 user + 1 assistant).\n    model_context = BufferedChatCompletionContext(buffer_size=2)\n\n    # Create an AssistantAgent instance with the model client and context.\n    agent = AssistantAgent(\n        name=\"assistant\",\n        model_client=model_client,\n        model_context=model_context,\n        system_message=\"You are a helpful assistant.\",\n    )\n\n    result = await agent.run(task=\"Name two cities in North America.\")\n    print(result.messages[-1].content)  # type: ignore\n\n    result = await agent.run(task=\"My favorite color is blue.\")\n    print(result.messages[-1].content)  # type: ignore\n\n    result = await agent.run(task=\"Did I ask you any question?\")\n    print(result.messages[-1].content)  # type: ignore\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\n\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_core.model_context import BufferedChatCompletionContext\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n\nasync def main() -> None:\n    # Create a model client.\n    model_client = OpenAIChatCompletionClient(\n        model=\"gpt-4o-mini\",\n        # api_key = \"your_openai_api_key\"\n    )\n\n    # Create a model context that only keeps the last 2 messages (1 user + 1 assistant).\n    model_context = BufferedChatCompletionContext(buffer_size=2)\n\n    # Create an AssistantAgent instance with the model client and context.\n    agent = AssistantAgent(\n        name=\"assistant\",\n        model_client=model_client,\n        model_context=model_context,\n        system_message=\"You are a helpful assistant.\",\n    )\n\n    result = await agent.run(task=\"Name two cities in North America.\")\n    print(result.messages[-1].content)  # type: ignore\n\n    result = await agent.run(task=\"My favorite color is blue.\")\n    print(result.messages[-1].content)  # type: ignore\n\n    result = await agent.run(task=\"Did I ask you any question?\")\n    print(result.messages[-1].content)  # type: ignore\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "Two cities in North America are New York City and Toronto.\nThat's great! Blue is often associated with calmness and serenity. Do you have a specific shade of blue that you like, or any particular reason why it's your favorite?\nNo, you didn't ask a question. I apologize for any misunderstanding. If you have something specific you'd like to discuss or ask, feel free to let me know!",
      "language": "unknown"
    },
    {
      "code": "Two cities in North America are New York City and Toronto.\nThat's great! Blue is often associated with calmness and serenity. Do you have a specific shade of blue that you like, or any particular reason why it's your favorite?\nNo, you didn't ask a question. I apologize for any misunderstanding. If you have something specific you'd like to discuss or ask, feel free to let me know!",
      "language": "unknown"
    },
    {
      "code": "import asyncio\n\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_core.memory import ListMemory, MemoryContent\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n\nasync def main() -> None:\n    # Create a model client.\n    model_client = OpenAIChatCompletionClient(\n        model=\"gpt-4o-mini\",\n        # api_key = \"your_openai_api_key\"\n    )\n\n    # Create a list-based memory with some initial content.\n    memory = ListMemory()\n    await memory.add(MemoryContent(content=\"User likes pizza.\", mime_type=\"text/plain\"))\n    await memory.add(MemoryContent(content=\"User dislikes cheese.\", mime_type=\"text/plain\"))\n\n    # Create an AssistantAgent instance with the model client and memory.\n    agent = AssistantAgent(\n        name=\"assistant\",\n        model_client=model_client,\n        memory=[memory],\n        system_message=\"You are a helpful assistant.\",\n    )\n\n    result = await agent.run(task=\"What is a good dinner idea?\")\n    print(result.messages[-1].content)  # type: ignore\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\n\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_core.memory import ListMemory, MemoryContent\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n\nasync def main() -> None:\n    # Create a model client.\n    model_client = OpenAIChatCompletionClient(\n        model=\"gpt-4o-mini\",\n        # api_key = \"your_openai_api_key\"\n    )\n\n    # Create a list-based memory with some initial content.\n    memory = ListMemory()\n    await memory.add(MemoryContent(content=\"User likes pizza.\", mime_type=\"text/plain\"))\n    await memory.add(MemoryContent(content=\"User dislikes cheese.\", mime_type=\"text/plain\"))\n\n    # Create an AssistantAgent instance with the model client and memory.\n    agent = AssistantAgent(\n        name=\"assistant\",\n        model_client=model_client,\n        memory=[memory],\n        system_message=\"You are a helpful assistant.\",\n    )\n\n    result = await agent.run(task=\"What is a good dinner idea?\")\n    print(result.messages[-1].content)  # type: ignore\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "How about making a delicious pizza without cheese? You can create a flavorful veggie pizza with a variety of toppings. Here's a quick idea:\n\n**Veggie Tomato Sauce Pizza**\n- Start with a pizza crust (store-bought or homemade).\n- Spread a layer of marinara or tomato sauce evenly over the crust.\n- Top with your favorite vegetables like bell peppers, mushrooms, onions, olives, and spinach.\n- Add some protein if you'd like, such as grilled chicken or pepperoni (ensure it's cheese-free).\n- Sprinkle with herbs like oregano and basil, and maybe a drizzle of olive oil.\n- Bake according to the crust instructions until the edges are golden and the veggies are cooked.\n\nServe it with a side salad or some garlic bread to complete the meal! Enjoy your dinner!",
      "language": "typescript"
    },
    {
      "code": "How about making a delicious pizza without cheese? You can create a flavorful veggie pizza with a variety of toppings. Here's a quick idea:\n\n**Veggie Tomato Sauce Pizza**\n- Start with a pizza crust (store-bought or homemade).\n- Spread a layer of marinara or tomato sauce evenly over the crust.\n- Top with your favorite vegetables like bell peppers, mushrooms, onions, olives, and spinach.\n- Add some protein if you'd like, such as grilled chicken or pepperoni (ensure it's cheese-free).\n- Sprinkle with herbs like oregano and basil, and maybe a drizzle of olive oil.\n- Bake according to the crust instructions until the edges are golden and the veggies are cooked.\n\nServe it with a side salad or some garlic bread to complete the meal! Enjoy your dinner!",
      "language": "typescript"
    },
    {
      "code": "import asyncio\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_agentchat.agents import AssistantAgent\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(\n        model=\"o1-mini\",\n        # api_key = \"your_openai_api_key\"\n    )\n    # The system message is not supported by the o1 series model.\n    agent = AssistantAgent(name=\"assistant\", model_client=model_client, system_message=None)\n\n    result = await agent.run(task=\"What is the capital of France?\")\n    print(result.messages[-1].content)  # type: ignore\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_agentchat.agents import AssistantAgent\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(\n        model=\"o1-mini\",\n        # api_key = \"your_openai_api_key\"\n    )\n    # The system message is not supported by the o1 series model.\n    agent = AssistantAgent(name=\"assistant\", model_client=model_client, system_message=None)\n\n    result = await agent.run(task=\"What is the capital of France?\")\n    print(result.messages[-1].content)  # type: ignore\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom typing import List\n\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_core.model_context import UnboundedChatCompletionContext\nfrom autogen_core.models import AssistantMessage, LLMMessage, ModelFamily\nfrom autogen_ext.models.ollama import OllamaChatCompletionClient\n\n\nclass ReasoningModelContext(UnboundedChatCompletionContext):\n    \"\"\"A model context for reasoning models.\"\"\"\n\n    async def get_messages(self) -> List[LLMMessage]:\n        messages = await super().get_messages()\n        # Filter out thought field from AssistantMessage.\n        messages_out: List[LLMMessage] = []\n        for message in messages:\n            if isinstance(message, AssistantMessage):\n                message.thought = None\n            messages_out.append(message)\n        return messages_out\n\n\n# Create an instance of the model client for DeepSeek R1 hosted locally on Ollama.\nmodel_client = OllamaChatCompletionClient(\n    model=\"deepseek-r1:8b\",\n    model_info={\n        \"vision\": False,\n        \"function_calling\": False,\n        \"json_output\": False,\n        \"family\": ModelFamily.R1,\n        \"structured_output\": True,\n    },\n)\n\nagent = AssistantAgent(\n    \"reasoning_agent\",\n    model_client=model_client,\n    model_context=ReasoningModelContext(),  # Use the custom model context.\n)\n\n\nasync def run_reasoning_agent() -> None:\n    result = await agent.run(task=\"What is the capital of France?\")\n    print(result)\n\n\nasyncio.run(run_reasoning_agent())",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom typing import List\n\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_core.model_context import UnboundedChatCompletionContext\nfrom autogen_core.models import AssistantMessage, LLMMessage, ModelFamily\nfrom autogen_ext.models.ollama import OllamaChatCompletionClient\n\n\nclass ReasoningModelContext(UnboundedChatCompletionContext):\n    \"\"\"A model context for reasoning models.\"\"\"\n\n    async def get_messages(self) -> List[LLMMessage]:\n        messages = await super().get_messages()\n        # Filter out thought field from AssistantMessage.\n        messages_out: List[LLMMessage] = []\n        for message in messages:\n            if isinstance(message, AssistantMessage):\n                message.thought = None\n            messages_out.append(message)\n        return messages_out\n\n\n# Create an instance of the model client for DeepSeek R1 hosted locally on Ollama.\nmodel_client = OllamaChatCompletionClient(\n    model=\"deepseek-r1:8b\",\n    model_info={\n        \"vision\": False,\n        \"function_calling\": False,\n        \"json_output\": False,\n        \"family\": ModelFamily.R1,\n        \"structured_output\": True,\n    },\n)\n\nagent = AssistantAgent(\n    \"reasoning_agent\",\n    model_client=model_client,\n    model_context=ReasoningModelContext(),  # Use the custom model context.\n)\n\n\nasync def run_reasoning_agent() -> None:\n    result = await agent.run(task=\"What is the capital of France?\")\n    print(result)\n\n\nasyncio.run(run_reasoning_agent())",
      "language": "python"
    },
    {
      "code": "```python\nprint(\"Hello World\")\n```\n\n# or\n\n```sh\necho \"Hello World\"\n```",
      "language": "markdown"
    },
    {
      "code": "```python\nprint(\"Hello World\")\n```\n\n# or\n\n```sh\necho \"Hello World\"\n```",
      "language": "markdown"
    },
    {
      "code": "import asyncio\nfrom autogen_agentchat.agents import CodeExecutorAgent, ApprovalRequest, ApprovalResponse\nfrom autogen_agentchat.messages import TextMessage\nfrom autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor\nfrom autogen_core import CancellationToken\n\n\ndef simple_approval_func(request: ApprovalRequest) -> ApprovalResponse:\n    \"\"\"Simple approval function that requests user input for code execution approval.\"\"\"\n    print(\"Code execution approval requested:\")\n    print(\"=\" * 50)\n    print(request.code)\n    print(\"=\" * 50)\n\n    while True:\n        user_input = input(\"Do you want to execute this code? (y/n): \").strip().lower()\n        if user_input in ['y', 'yes']:\n            return ApprovalResponse(approved=True, reason='Approved by user')\n        elif user_input in ['n', 'no']:\n            return ApprovalResponse(approved=False, reason='Denied by user')\n        else:\n            print(\"Please enter 'y' for yes or 'n' for no.\")\n\n\nasync def run_code_executor_agent() -> None:\n    # Create a code executor agent that uses a Docker container to execute code.\n    code_executor = DockerCommandLineCodeExecutor(work_dir=\"coding\")\n    await code_executor.start()\n    code_executor_agent = CodeExecutorAgent(\n        \"code_executor\",\n        code_executor=code_executor,\n        approval_func=simple_approval_func\n    )\n\n    # Run the agent with a given code snippet.\n    task = TextMessage(\n        content='''Here is some code\n```python\nprint('Hello world')\n```\n''',\n        source=\"user\",\n    )\n    response = await code_executor_agent.on_messages([task], CancellationToken())\n    print(response.chat_message)\n\n    # Stop the code executor.\n    await code_executor.stop()\n\n\nasyncio.run(run_code_executor_agent())",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom autogen_agentchat.agents import CodeExecutorAgent, ApprovalRequest, ApprovalResponse\nfrom autogen_agentchat.messages import TextMessage\nfrom autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor\nfrom autogen_core import CancellationToken\n\n\ndef simple_approval_func(request: ApprovalRequest) -> ApprovalResponse:\n    \"\"\"Simple approval function that requests user input for code execution approval.\"\"\"\n    print(\"Code execution approval requested:\")\n    print(\"=\" * 50)\n    print(request.code)\n    print(\"=\" * 50)\n\n    while True:\n        user_input = input(\"Do you want to execute this code? (y/n): \").strip().lower()\n        if user_input in ['y', 'yes']:\n            return ApprovalResponse(approved=True, reason='Approved by user')\n        elif user_input in ['n', 'no']:\n            return ApprovalResponse(approved=False, reason='Denied by user')\n        else:\n            print(\"Please enter 'y' for yes or 'n' for no.\")\n\n\nasync def run_code_executor_agent() -> None:\n    # Create a code executor agent that uses a Docker container to execute code.\n    code_executor = DockerCommandLineCodeExecutor(work_dir=\"coding\")\n    await code_executor.start()\n    code_executor_agent = CodeExecutorAgent(\n        \"code_executor\",\n        code_executor=code_executor,\n        approval_func=simple_approval_func\n    )\n\n    # Run the agent with a given code snippet.\n    task = TextMessage(\n        content='''Here is some code\n```python\nprint('Hello world')\n```\n''',\n        source=\"user\",\n    )\n    response = await code_executor_agent.on_messages([task], CancellationToken())\n    print(response.chat_message)\n\n    # Stop the code executor.\n    await code_executor.stop()\n\n\nasyncio.run(run_code_executor_agent())",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom autogen_agentchat.agents import CodeExecutorAgent\nfrom autogen_agentchat.messages import TextMessage\nfrom autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor\nfrom autogen_core import CancellationToken\nfrom docker.types import DeviceRequest\n\n\nasync def run_code_executor_agent() -> None:\n    # Create a code executor agent that uses a Docker container to execute code.\n    code_executor = DockerCommandLineCodeExecutor(\n        work_dir=\"coding\", device_requests=[DeviceRequest(count=-1, capabilities=[[\"gpu\"]])]\n    )\n    await code_executor.start()\n    code_executor_agent = CodeExecutorAgent(\"code_executor\", code_executor=code_executor)\n\n    # Display the GPU information\n    task = TextMessage(\n        content='''Here is some code\n```sh\nnvidia-smi\n```\n''',\n        source=\"user\",\n    )\n    response = await code_executor_agent.on_messages([task], CancellationToken())\n    print(response.chat_message)\n\n    # Stop the code executor.\n    await code_executor.stop()\n\n\nasyncio.run(run_code_executor_agent())",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom autogen_agentchat.agents import CodeExecutorAgent\nfrom autogen_agentchat.messages import TextMessage\nfrom autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor\nfrom autogen_core import CancellationToken\nfrom docker.types import DeviceRequest\n\n\nasync def run_code_executor_agent() -> None:\n    # Create a code executor agent that uses a Docker container to execute code.\n    code_executor = DockerCommandLineCodeExecutor(\n        work_dir=\"coding\", device_requests=[DeviceRequest(count=-1, capabilities=[[\"gpu\"]])]\n    )\n    await code_executor.start()\n    code_executor_agent = CodeExecutorAgent(\"code_executor\", code_executor=code_executor)\n\n    # Display the GPU information\n    task = TextMessage(\n        content='''Here is some code\n```sh\nnvidia-smi\n```\n''',\n        source=\"user\",\n    )\n    response = await code_executor_agent.on_messages([task], CancellationToken())\n    print(response.chat_message)\n\n    # Stop the code executor.\n    await code_executor.stop()\n\n\nasyncio.run(run_code_executor_agent())",
      "language": "python"
    },
    {
      "code": "import asyncio\n\nfrom autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nfrom autogen_agentchat.agents import AssistantAgent, CodeExecutorAgent, ApprovalRequest, ApprovalResponse\nfrom autogen_agentchat.conditions import MaxMessageTermination\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.ui import Console\n\ntermination_condition = MaxMessageTermination(3)\n\n\ndef group_chat_approval_func(request: ApprovalRequest) -> ApprovalResponse:\n    \"\"\"Approval function for group chat that allows basic Python operations.\"\"\"\n    # Allow common safe operations\n    safe_operations = [\"print(\", \"import \", \"def \", \"class \", \"if \", \"for \", \"while \"]\n    if any(op in request.code for op in safe_operations):\n        return ApprovalResponse(approved=True, reason='Safe Python operation')\n\n    # Deny file system operations in group chat\n    dangerous_operations = [\"open(\", \"file(\", \"os.\", \"subprocess\", \"eval(\", \"exec(\"]\n    if any(op in request.code for op in dangerous_operations):\n        return ApprovalResponse(approved=False, reason='File system or dangerous operation not allowed')\n\n    return ApprovalResponse(approved=True, reason='Operation approved')\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n    # define the Docker CLI Code Executor\n    code_executor = DockerCommandLineCodeExecutor(work_dir=\"coding\")\n\n    # start the execution container\n    await code_executor.start()\n\n    code_executor_agent = CodeExecutorAgent(\n        \"code_executor_agent\",\n        code_executor=code_executor,\n        approval_func=group_chat_approval_func\n    )\n    coder_agent = AssistantAgent(\"coder_agent\", model_client=model_client)\n\n    groupchat = RoundRobinGroupChat(\n        participants=[coder_agent, code_executor_agent], termination_condition=termination_condition\n    )\n\n    task = \"Write python code to print Hello World!\"\n    await Console(groupchat.run_stream(task=task))\n\n    # stop the execution container\n    await code_executor.stop()\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\n\nfrom autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nfrom autogen_agentchat.agents import AssistantAgent, CodeExecutorAgent, ApprovalRequest, ApprovalResponse\nfrom autogen_agentchat.conditions import MaxMessageTermination\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.ui import Console\n\ntermination_condition = MaxMessageTermination(3)\n\n\ndef group_chat_approval_func(request: ApprovalRequest) -> ApprovalResponse:\n    \"\"\"Approval function for group chat that allows basic Python operations.\"\"\"\n    # Allow common safe operations\n    safe_operations = [\"print(\", \"import \", \"def \", \"class \", \"if \", \"for \", \"while \"]\n    if any(op in request.code for op in safe_operations):\n        return ApprovalResponse(approved=True, reason='Safe Python operation')\n\n    # Deny file system operations in group chat\n    dangerous_operations = [\"open(\", \"file(\", \"os.\", \"subprocess\", \"eval(\", \"exec(\"]\n    if any(op in request.code for op in dangerous_operations):\n        return ApprovalResponse(approved=False, reason='File system or dangerous operation not allowed')\n\n    return ApprovalResponse(approved=True, reason='Operation approved')\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n    # define the Docker CLI Code Executor\n    code_executor = DockerCommandLineCodeExecutor(work_dir=\"coding\")\n\n    # start the execution container\n    await code_executor.start()\n\n    code_executor_agent = CodeExecutorAgent(\n        \"code_executor_agent\",\n        code_executor=code_executor,\n        approval_func=group_chat_approval_func\n    )\n    coder_agent = AssistantAgent(\"coder_agent\", model_client=model_client)\n\n    groupchat = RoundRobinGroupChat(\n        participants=[coder_agent, code_executor_agent], termination_condition=termination_condition\n    )\n\n    task = \"Write python code to print Hello World!\"\n    await Console(groupchat.run_stream(task=task))\n\n    # stop the execution container\n    await code_executor.stop()\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "---------- user ----------\nWrite python code to print Hello World!\n---------- coder_agent ----------\nCertainly! Here's a simple Python code to print \"Hello World!\":\n\n```python\nprint(\"Hello World!\")\n```\n\nYou can run this code in any Python environment to display the message.\n---------- code_executor_agent ----------\nHello World!",
      "language": "python"
    },
    {
      "code": "---------- user ----------\nWrite python code to print Hello World!\n---------- coder_agent ----------\nCertainly! Here's a simple Python code to print \"Hello World!\":\n\n```python\nprint(\"Hello World!\")\n```\n\nYou can run this code in any Python environment to display the message.\n---------- code_executor_agent ----------\nHello World!",
      "language": "python"
    },
    {
      "code": "import asyncio\n\nfrom autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_core.models import SystemMessage, UserMessage\n\nfrom autogen_agentchat.agents import CodeExecutorAgent, ApprovalRequest, ApprovalResponse\nfrom autogen_agentchat.conditions import TextMessageTermination\nfrom autogen_agentchat.ui import Console\n\ntermination_condition = TextMessageTermination(\"code_executor_agent\")\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n    async def model_client_approval_func(request: ApprovalRequest) -> ApprovalResponse:\n        instruction = \"Approve or reject the code in the last message based on whether it is dangerous or not. Use the following JSON format for your response: {approved: true/false, reason: 'your reason here'}\"\n        response = await model_client.create(\n            messages=[SystemMessage(content=instruction)]\n            + request.context\n            + [UserMessage(content=request.code, source=\"user\")],\n            json_output=ApprovalResponse,\n        )\n        assert isinstance(response.content, str)\n        return ApprovalResponse.model_validate_json(response.content)\n\n    # define the Docker CLI Code Executor\n    code_executor = DockerCommandLineCodeExecutor(work_dir=\"coding\")\n\n    # start the execution container\n    await code_executor.start()\n\n    code_executor_agent = CodeExecutorAgent(\n        \"code_executor_agent\",\n        code_executor=code_executor,\n        model_client=model_client,\n        approval_func=model_client_approval_func,\n    )\n\n    task = \"Write python code to print Hello World!\"\n    await Console(code_executor_agent.run_stream(task=task))\n\n    # stop the execution container\n    await code_executor.stop()\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\n\nfrom autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_core.models import SystemMessage, UserMessage\n\nfrom autogen_agentchat.agents import CodeExecutorAgent, ApprovalRequest, ApprovalResponse\nfrom autogen_agentchat.conditions import TextMessageTermination\nfrom autogen_agentchat.ui import Console\n\ntermination_condition = TextMessageTermination(\"code_executor_agent\")\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n    async def model_client_approval_func(request: ApprovalRequest) -> ApprovalResponse:\n        instruction = \"Approve or reject the code in the last message based on whether it is dangerous or not. Use the following JSON format for your response: {approved: true/false, reason: 'your reason here'}\"\n        response = await model_client.create(\n            messages=[SystemMessage(content=instruction)]\n            + request.context\n            + [UserMessage(content=request.code, source=\"user\")],\n            json_output=ApprovalResponse,\n        )\n        assert isinstance(response.content, str)\n        return ApprovalResponse.model_validate_json(response.content)\n\n    # define the Docker CLI Code Executor\n    code_executor = DockerCommandLineCodeExecutor(work_dir=\"coding\")\n\n    # start the execution container\n    await code_executor.start()\n\n    code_executor_agent = CodeExecutorAgent(\n        \"code_executor_agent\",\n        code_executor=code_executor,\n        model_client=model_client,\n        approval_func=model_client_approval_func,\n    )\n\n    task = \"Write python code to print Hello World!\"\n    await Console(code_executor_agent.run_stream(task=task))\n\n    # stop the execution container\n    await code_executor.stop()\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "---------- user ----------\nWrite python code to print Hello World!\n---------- code_executor_agent ----------\nCertainly! Here is a simple Python code to print \"Hello World!\" to the console:\n\n```python\nprint(\"Hello World!\")\n```\n\nLet's execute it to confirm the output.\n---------- code_executor_agent ----------\nHello World!\n\n---------- code_executor_agent ----------\nThe code has been executed successfully, and it printed \"Hello World!\" as expected. If you have any more requests or questions, feel free to ask!",
      "language": "typescript"
    },
    {
      "code": "---------- user ----------\nWrite python code to print Hello World!\n---------- code_executor_agent ----------\nCertainly! Here is a simple Python code to print \"Hello World!\" to the console:\n\n```python\nprint(\"Hello World!\")\n```\n\nLet's execute it to confirm the output.\n---------- code_executor_agent ----------\nHello World!\n\n---------- code_executor_agent ----------\nThe code has been executed successfully, and it printed \"Hello World!\" as expected. If you have any more requests or questions, feel free to ask!",
      "language": "typescript"
    },
    {
      "code": "import asyncio\nfrom autogen_agentchat.ui import Console\nfrom autogen_agentchat.agents import AssistantAgent, SocietyOfMindAgent\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.conditions import TextMentionTermination\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n    agent1 = AssistantAgent(\"assistant1\", model_client=model_client, system_message=\"You are a writer, write well.\")\n    agent2 = AssistantAgent(\n        \"assistant2\",\n        model_client=model_client,\n        system_message=\"You are an editor, provide critical feedback. Respond with 'APPROVE' if the text addresses all feedbacks.\",\n    )\n    inner_termination = TextMentionTermination(\"APPROVE\")\n    inner_team = RoundRobinGroupChat([agent1, agent2], termination_condition=inner_termination)\n\n    society_of_mind_agent = SocietyOfMindAgent(\"society_of_mind\", team=inner_team, model_client=model_client)\n\n    agent3 = AssistantAgent(\n        \"assistant3\", model_client=model_client, system_message=\"Translate the text to Spanish.\"\n    )\n    team = RoundRobinGroupChat([society_of_mind_agent, agent3], max_turns=2)\n\n    stream = team.run_stream(task=\"Write a short story with a surprising ending.\")\n    await Console(stream)\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom autogen_agentchat.ui import Console\nfrom autogen_agentchat.agents import AssistantAgent, SocietyOfMindAgent\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.conditions import TextMentionTermination\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n    agent1 = AssistantAgent(\"assistant1\", model_client=model_client, system_message=\"You are a writer, write well.\")\n    agent2 = AssistantAgent(\n        \"assistant2\",\n        model_client=model_client,\n        system_message=\"You are an editor, provide critical feedback. Respond with 'APPROVE' if the text addresses all feedbacks.\",\n    )\n    inner_termination = TextMentionTermination(\"APPROVE\")\n    inner_team = RoundRobinGroupChat([agent1, agent2], termination_condition=inner_termination)\n\n    society_of_mind_agent = SocietyOfMindAgent(\"society_of_mind\", team=inner_team, model_client=model_client)\n\n    agent3 = AssistantAgent(\n        \"assistant3\", model_client=model_client, system_message=\"Translate the text to Spanish.\"\n    )\n    team = RoundRobinGroupChat([society_of_mind_agent, agent3], max_turns=2)\n\n    stream = team.run_stream(task=\"Write a short story with a surprising ending.\")\n    await Console(stream)\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom autogen_core import CancellationToken\nfrom autogen_agentchat.agents import UserProxyAgent\nfrom autogen_agentchat.messages import TextMessage\n\n\nasync def simple_user_agent():\n    agent = UserProxyAgent(\"user_proxy\")\n    response = await asyncio.create_task(\n        agent.on_messages(\n            [TextMessage(content=\"What is your name? \", source=\"user\")],\n            cancellation_token=CancellationToken(),\n        )\n    )\n    assert isinstance(response.chat_message, TextMessage)\n    print(f\"Your name is {response.chat_message.content}\")",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom autogen_core import CancellationToken\nfrom autogen_agentchat.agents import UserProxyAgent\nfrom autogen_agentchat.messages import TextMessage\n\n\nasync def simple_user_agent():\n    agent = UserProxyAgent(\"user_proxy\")\n    response = await asyncio.create_task(\n        agent.on_messages(\n            [TextMessage(content=\"What is your name? \", source=\"user\")],\n            cancellation_token=CancellationToken(),\n        )\n    )\n    assert isinstance(response.chat_message, TextMessage)\n    print(f\"Your name is {response.chat_message.content}\")",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom typing import Any\nfrom autogen_core import CancellationToken\nfrom autogen_agentchat.agents import UserProxyAgent\nfrom autogen_agentchat.messages import TextMessage\n\n\ntoken = CancellationToken()\nagent = UserProxyAgent(\"user_proxy\")\n\n\nasync def timeout(delay: float):\n    await asyncio.sleep(delay)\n\n\ndef cancellation_callback(task: asyncio.Task[Any]):\n    token.cancel()\n\n\nasync def cancellable_user_agent():\n    try:\n        timeout_task = asyncio.create_task(timeout(3))\n        timeout_task.add_done_callback(cancellation_callback)\n        agent_task = asyncio.create_task(\n            agent.on_messages(\n                [TextMessage(content=\"What is your name? \", source=\"user\")],\n                cancellation_token=token,\n            )\n        )\n        response = await agent_task\n        assert isinstance(response.chat_message, TextMessage)\n        print(f\"Your name is {response.chat_message.content}\")\n    except Exception as e:\n        print(f\"Exception: {e}\")\n    except BaseException as e:\n        print(f\"BaseException: {e}\")",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom typing import Any\nfrom autogen_core import CancellationToken\nfrom autogen_agentchat.agents import UserProxyAgent\nfrom autogen_agentchat.messages import TextMessage\n\n\ntoken = CancellationToken()\nagent = UserProxyAgent(\"user_proxy\")\n\n\nasync def timeout(delay: float):\n    await asyncio.sleep(delay)\n\n\ndef cancellation_callback(task: asyncio.Task[Any]):\n    token.cancel()\n\n\nasync def cancellable_user_agent():\n    try:\n        timeout_task = asyncio.create_task(timeout(3))\n        timeout_task.add_done_callback(cancellation_callback)\n        agent_task = asyncio.create_task(\n            agent.on_messages(\n                [TextMessage(content=\"What is your name? \", source=\"user\")],\n                cancellation_token=token,\n            )\n        )\n        response = await agent_task\n        assert isinstance(response.chat_message, TextMessage)\n        print(f\"Your name is {response.chat_message.content}\")\n    except Exception as e:\n        print(f\"Exception: {e}\")\n    except BaseException as e:\n        print(f\"BaseException: {e}\")",
      "language": "python"
    },
    {
      "code": ">>> agent_a = MessageFilterAgent(\n...     name=\"A\",\n...     wrapped_agent=some_other_agent,\n...     filter=MessageFilterConfig(\n...         per_source=[\n...             PerSourceFilter(source=\"user\", position=\"first\", count=1),\n...             PerSourceFilter(source=\"B\", position=\"last\", count=2),\n...         ]\n...     ),\n... )",
      "language": "unknown"
    },
    {
      "code": ">>> agent_a = MessageFilterAgent(\n...     name=\"A\",\n...     wrapped_agent=some_other_agent,\n...     filter=MessageFilterConfig(\n...         per_source=[\n...             PerSourceFilter(source=\"user\", position=\"first\", count=1),\n...             PerSourceFilter(source=\"B\", position=\"last\", count=2),\n...         ]\n...     ),\n... )",
      "language": "unknown"
    },
    {
      "code": ">>> agent_a = MessageFilterAgent(\n...     name=\"A\",\n...     wrapped_agent=agent_a_inner,\n...     filter=MessageFilterConfig(\n...         per_source=[\n...             PerSourceFilter(source=\"user\", position=\"first\", count=1),\n...             PerSourceFilter(source=\"B\", position=\"last\", count=1),\n...         ]\n...     ),\n... )",
      "language": "unknown"
    },
    {
      "code": ">>> agent_a = MessageFilterAgent(\n...     name=\"A\",\n...     wrapped_agent=agent_a_inner,\n...     filter=MessageFilterConfig(\n...         per_source=[\n...             PerSourceFilter(source=\"user\", position=\"first\", count=1),\n...             PerSourceFilter(source=\"B\", position=\"last\", count=1),\n...         ]\n...     ),\n... )",
      "language": "unknown"
    },
    {
      "code": ">>> agent_b = MessageFilterAgent(\n...     name=\"B\",\n...     wrapped_agent=agent_b_inner,\n...     filter=MessageFilterConfig(\n...         per_source=[\n...             PerSourceFilter(source=\"user\", position=\"first\", count=1),\n...             PerSourceFilter(source=\"A\", position=\"last\", count=1),\n...             PerSourceFilter(source=\"B\", position=\"last\", count=10),\n...         ]\n...     ),\n... )",
      "language": "unknown"
    },
    {
      "code": ">>> agent_b = MessageFilterAgent(\n...     name=\"B\",\n...     wrapped_agent=agent_b_inner,\n...     filter=MessageFilterConfig(\n...         per_source=[\n...             PerSourceFilter(source=\"user\", position=\"first\", count=1),\n...             PerSourceFilter(source=\"A\", position=\"last\", count=1),\n...             PerSourceFilter(source=\"B\", position=\"last\", count=10),\n...         ]\n...     ),\n... )",
      "language": "unknown"
    },
    {
      "code": ">>> agent_c = MessageFilterAgent(\n...     name=\"C\",\n...     wrapped_agent=agent_c_inner,\n...     filter=MessageFilterConfig(\n...         per_source=[\n...             PerSourceFilter(source=\"user\", position=\"first\", count=1),\n...             PerSourceFilter(source=\"B\", position=\"last\", count=1),\n...         ]\n...     ),\n... )",
      "language": "unknown"
    },
    {
      "code": ">>> agent_c = MessageFilterAgent(\n...     name=\"C\",\n...     wrapped_agent=agent_c_inner,\n...     filter=MessageFilterConfig(\n...         per_source=[\n...             PerSourceFilter(source=\"user\", position=\"first\", count=1),\n...             PerSourceFilter(source=\"B\", position=\"last\", count=1),\n...         ]\n...     ),\n... )",
      "language": "unknown"
    },
    {
      "code": ">>> graph = DiGraph(\n...     nodes={\n...         \"A\": DiGraphNode(name=\"A\", edges=[DiGraphEdge(target=\"B\")]),\n...         \"B\": DiGraphNode(\n...             name=\"B\",\n...             edges=[\n...                 DiGraphEdge(target=\"C\", condition=\"exit\"),\n...                 DiGraphEdge(target=\"A\", condition=\"loop\"),\n...             ],\n...         ),\n...         \"C\": DiGraphNode(name=\"C\", edges=[]),\n...     },\n...     default_start_node=\"A\",\n... )",
      "language": "json"
    },
    {
      "code": ">>> graph = DiGraph(\n...     nodes={\n...         \"A\": DiGraphNode(name=\"A\", edges=[DiGraphEdge(target=\"B\")]),\n...         \"B\": DiGraphNode(\n...             name=\"B\",\n...             edges=[\n...                 DiGraphEdge(target=\"C\", condition=\"exit\"),\n...                 DiGraphEdge(target=\"A\", condition=\"loop\"),\n...             ],\n...         ),\n...         \"C\": DiGraphNode(name=\"C\", edges=[]),\n...     },\n...     default_start_node=\"A\",\n... )",
      "language": "json"
    },
    {
      "code": "{\n   \"title\": \"MessageFilterConfig\",\n   \"type\": \"object\",\n   \"properties\": {\n      \"per_source\": {\n         \"items\": {\n            \"$ref\": \"#/$defs/PerSourceFilter\"\n         },\n         \"title\": \"Per Source\",\n         \"type\": \"array\"\n      }\n   },\n   \"$defs\": {\n      \"PerSourceFilter\": {\n         \"properties\": {\n            \"source\": {\n               \"title\": \"Source\",\n               \"type\": \"string\"\n            },\n            \"position\": {\n               \"anyOf\": [\n                  {\n                     \"enum\": [\n                        \"first\",\n                        \"last\"\n                     ],\n                     \"type\": \"string\"\n                  },\n                  {\n                     \"type\": \"null\"\n                  }\n               ],\n               \"default\": null,\n               \"title\": \"Position\"\n            },\n            \"count\": {\n               \"anyOf\": [\n                  {\n                     \"type\": \"integer\"\n                  },\n                  {\n                     \"type\": \"null\"\n                  }\n               ],\n               \"default\": null,\n               \"title\": \"Count\"\n            }\n         },\n         \"required\": [\n            \"source\"\n         ],\n         \"title\": \"PerSourceFilter\",\n         \"type\": \"object\"\n      }\n   },\n   \"required\": [\n      \"per_source\"\n   ]\n}",
      "language": "json"
    },
    {
      "code": "{\n   \"title\": \"MessageFilterConfig\",\n   \"type\": \"object\",\n   \"properties\": {\n      \"per_source\": {\n         \"items\": {\n            \"$ref\": \"#/$defs/PerSourceFilter\"\n         },\n         \"title\": \"Per Source\",\n         \"type\": \"array\"\n      }\n   },\n   \"$defs\": {\n      \"PerSourceFilter\": {\n         \"properties\": {\n            \"source\": {\n               \"title\": \"Source\",\n               \"type\": \"string\"\n            },\n            \"position\": {\n               \"anyOf\": [\n                  {\n                     \"enum\": [\n                        \"first\",\n                        \"last\"\n                     ],\n                     \"type\": \"string\"\n                  },\n                  {\n                     \"type\": \"null\"\n                  }\n               ],\n               \"default\": null,\n               \"title\": \"Position\"\n            },\n            \"count\": {\n               \"anyOf\": [\n                  {\n                     \"type\": \"integer\"\n                  },\n                  {\n                     \"type\": \"null\"\n                  }\n               ],\n               \"default\": null,\n               \"title\": \"Count\"\n            }\n         },\n         \"required\": [\n            \"source\"\n         ],\n         \"title\": \"PerSourceFilter\",\n         \"type\": \"object\"\n      }\n   },\n   \"required\": [\n      \"per_source\"\n   ]\n}",
      "language": "json"
    },
    {
      "code": "{\n   \"title\": \"PerSourceFilter\",\n   \"type\": \"object\",\n   \"properties\": {\n      \"source\": {\n         \"title\": \"Source\",\n         \"type\": \"string\"\n      },\n      \"position\": {\n         \"anyOf\": [\n            {\n               \"enum\": [\n                  \"first\",\n                  \"last\"\n               ],\n               \"type\": \"string\"\n            },\n            {\n               \"type\": \"null\"\n            }\n         ],\n         \"default\": null,\n         \"title\": \"Position\"\n      },\n      \"count\": {\n         \"anyOf\": [\n            {\n               \"type\": \"integer\"\n            },\n            {\n               \"type\": \"null\"\n            }\n         ],\n         \"default\": null,\n         \"title\": \"Count\"\n      }\n   },\n   \"required\": [\n      \"source\"\n   ]\n}",
      "language": "json"
    },
    {
      "code": "{\n   \"title\": \"PerSourceFilter\",\n   \"type\": \"object\",\n   \"properties\": {\n      \"source\": {\n         \"title\": \"Source\",\n         \"type\": \"string\"\n      },\n      \"position\": {\n         \"anyOf\": [\n            {\n               \"enum\": [\n                  \"first\",\n                  \"last\"\n               ],\n               \"type\": \"string\"\n            },\n            {\n               \"type\": \"null\"\n            }\n         ],\n         \"default\": null,\n         \"title\": \"Position\"\n      },\n      \"count\": {\n         \"anyOf\": [\n            {\n               \"type\": \"integer\"\n            },\n            {\n               \"type\": \"null\"\n            }\n         ],\n         \"default\": null,\n         \"title\": \"Count\"\n      }\n   },\n   \"required\": [\n      \"source\"\n   ]\n}",
      "language": "json"
    },
    {
      "code": "{\n   \"title\": \"ApprovalRequest\",\n   \"description\": \"Request for approval of code execution.\",\n   \"type\": \"object\",\n   \"properties\": {\n      \"code\": {\n         \"title\": \"Code\",\n         \"type\": \"string\"\n      },\n      \"context\": {\n         \"items\": {\n            \"discriminator\": {\n               \"mapping\": {\n                  \"AssistantMessage\": \"#/$defs/AssistantMessage\",\n                  \"FunctionExecutionResultMessage\": \"#/$defs/FunctionExecutionResultMessage\",\n                  \"SystemMessage\": \"#/$defs/SystemMessage\",\n                  \"UserMessage\": \"#/$defs/UserMessage\"\n               },\n               \"propertyName\": \"type\"\n            },\n            \"oneOf\": [\n               {\n                  \"$ref\": \"#/$defs/SystemMessage\"\n               },\n               {\n                  \"$ref\": \"#/$defs/UserMessage\"\n               },\n               {\n                  \"$ref\": \"#/$defs/AssistantMessage\"\n               },\n               {\n                  \"$ref\": \"#/$defs/FunctionExecutionResultMessage\"\n               }\n            ]\n         },\n         \"title\": \"Context\",\n         \"type\": \"array\"\n      }\n   },\n   \"$defs\": {\n      \"AssistantMessage\": {\n         \"description\": \"Assistant message are sampled from the language model.\",\n         \"properties\": {\n            \"content\": {\n               \"anyOf\": [\n                  {\n                     \"type\": \"string\"\n                  },\n                  {\n                     \"items\": {\n                        \"$ref\": \"#/$defs/FunctionCall\"\n                     },\n                     \"type\": \"array\"\n                  }\n               ],\n               \"title\": \"Content\"\n            },\n            \"thought\": {\n               \"anyOf\": [\n                  {\n                     \"type\": \"string\"\n                  },\n                  {\n                     \"type\": \"null\"\n                  }\n               ],\n               \"default\": null,\n               \"title\": \"Thought\"\n            },\n            \"source\": {\n               \"title\": \"Source\",\n               \"type\": \"string\"\n            },\n            \"type\": {\n               \"const\": \"AssistantMessage\",\n               \"default\": \"AssistantMessage\",\n               \"title\": \"Type\",\n               \"type\": \"string\"\n            }\n         },\n         \"required\": [\n            \"content\",\n            \"source\"\n         ],\n         \"title\": \"AssistantMessage\",\n         \"type\": \"object\"\n      },\n      \"FunctionCall\": {\n         \"properties\": {\n            \"id\": {\n               \"title\": \"Id\",\n               \"type\": \"string\"\n            },\n            \"arguments\": {\n               \"title\": \"Arguments\",\n               \"type\": \"string\"\n            },\n            \"name\": {\n               \"title\": \"Name\",\n               \"type\": \"string\"\n            }\n         },\n         \"required\": [\n            \"id\",\n            \"arguments\",\n            \"name\"\n         ],\n         \"title\": \"FunctionCall\",\n         \"type\": \"object\"\n      },\n      \"FunctionExecutionResult\": {\n         \"description\": \"Function execution result contains the output of a function call.\",\n         \"properties\": {\n            \"content\": {\n               \"title\": \"Content\",\n               \"type\": \"string\"\n            },\n            \"name\": {\n               \"title\": \"Name\",\n               \"type\": \"string\"\n            },\n            \"call_id\": {\n               \"title\": \"Call Id\",\n               \"type\": \"string\"\n            },\n            \"is_error\": {\n               \"anyOf\": [\n                  {\n                     \"type\": \"boolean\"\n                  },\n                  {\n                     \"type\": \"null\"\n                  }\n               ],\n               \"default\": null,\n               \"title\": \"Is Error\"\n            }\n         },\n         \"required\": [\n            \"content\",\n            \"name\",\n            \"call_id\"\n         ],\n         \"title\": \"FunctionExecutionResult\",\n         \"type\": \"object\"\n      },\n      \"FunctionExecutionResultMessage\": {\n         \"description\": \"Function execution result message contains the output of multiple function calls.\",\n         \"properties\": {\n            \"content\": {\n               \"items\": {\n                  \"$ref\": \"#/$defs/FunctionExecutionResult\"\n               },\n               \"title\": \"Content\",\n               \"type\": \"array\"\n            },\n            \"type\": {\n               \"const\": \"FunctionExecutionResultMessage\",\n               \"default\": \"FunctionExecutionResultMessage\",\n               \"title\": \"Type\",\n               \"type\": \"string\"\n            }\n         },\n         \"required\": [\n            \"content\"\n         ],\n         \"title\": \"FunctionExecutionResultMessage\",\n         \"type\": \"object\"\n      },\n      \"SystemMessage\": {\n         \"description\": \"System message contains instructions for the model coming from the developer.\\n\\n.. note::\\n\\n    Open AI is moving away from using 'system' role in favor of 'developer' role.\\n    See `Model Spec <https://cdn.openai.com/spec/model-spec-2024-05-08.html#definitions>`_ for more details.\\n    However, the 'system' role is still allowed in their API and will be automatically converted to 'developer' role\\n    on the server side.\\n    So, you can use `SystemMessage` for developer messages.\",\n         \"properties\": {\n            \"content\": {\n               \"title\": \"Content\",\n               \"type\": \"string\"\n            },\n            \"type\": {\n               \"const\": \"SystemMessage\",\n               \"default\": \"SystemMessage\",\n               \"title\": \"Type\",\n               \"type\": \"string\"\n            }\n         },\n         \"required\": [\n            \"content\"\n         ],\n         \"title\": \"SystemMessage\",\n         \"type\": \"object\"\n      },\n      \"UserMessage\": {\n         \"description\": \"User message contains input from end users, or a catch-all for data provided to the model.\",\n         \"properties\": {\n            \"content\": {\n               \"anyOf\": [\n                  {\n                     \"type\": \"string\"\n                  },\n                  {\n                     \"items\": {\n                        \"anyOf\": [\n                           {\n                              \"type\": \"string\"\n                           },\n                           {}\n                        ]\n                     },\n                     \"type\": \"array\"\n                  }\n               ],\n               \"title\": \"Content\"\n            },\n            \"source\": {\n               \"title\": \"Source\",\n               \"type\": \"string\"\n            },\n            \"type\": {\n               \"const\": \"UserMessage\",\n               \"default\": \"UserMessage\",\n               \"title\": \"Type\",\n               \"type\": \"string\"\n            }\n         },\n         \"required\": [\n            \"content\",\n            \"source\"\n         ],\n         \"title\": \"UserMessage\",\n         \"type\": \"object\"\n      }\n   },\n   \"required\": [\n      \"code\",\n      \"context\"\n   ]\n}",
      "language": "json"
    },
    {
      "code": "{\n   \"title\": \"ApprovalRequest\",\n   \"description\": \"Request for approval of code execution.\",\n   \"type\": \"object\",\n   \"properties\": {\n      \"code\": {\n         \"title\": \"Code\",\n         \"type\": \"string\"\n      },\n      \"context\": {\n         \"items\": {\n            \"discriminator\": {\n               \"mapping\": {\n                  \"AssistantMessage\": \"#/$defs/AssistantMessage\",\n                  \"FunctionExecutionResultMessage\": \"#/$defs/FunctionExecutionResultMessage\",\n                  \"SystemMessage\": \"#/$defs/SystemMessage\",\n                  \"UserMessage\": \"#/$defs/UserMessage\"\n               },\n               \"propertyName\": \"type\"\n            },\n            \"oneOf\": [\n               {\n                  \"$ref\": \"#/$defs/SystemMessage\"\n               },\n               {\n                  \"$ref\": \"#/$defs/UserMessage\"\n               },\n               {\n                  \"$ref\": \"#/$defs/AssistantMessage\"\n               },\n               {\n                  \"$ref\": \"#/$defs/FunctionExecutionResultMessage\"\n               }\n            ]\n         },\n         \"title\": \"Context\",\n         \"type\": \"array\"\n      }\n   },\n   \"$defs\": {\n      \"AssistantMessage\": {\n         \"description\": \"Assistant message are sampled from the language model.\",\n         \"properties\": {\n            \"content\": {\n               \"anyOf\": [\n                  {\n                     \"type\": \"string\"\n                  },\n                  {\n                     \"items\": {\n                        \"$ref\": \"#/$defs/FunctionCall\"\n                     },\n                     \"type\": \"array\"\n                  }\n               ],\n               \"title\": \"Content\"\n            },\n            \"thought\": {\n               \"anyOf\": [\n                  {\n                     \"type\": \"string\"\n                  },\n                  {\n                     \"type\": \"null\"\n                  }\n               ],\n               \"default\": null,\n               \"title\": \"Thought\"\n            },\n            \"source\": {\n               \"title\": \"Source\",\n               \"type\": \"string\"\n            },\n            \"type\": {\n               \"const\": \"AssistantMessage\",\n               \"default\": \"AssistantMessage\",\n               \"title\": \"Type\",\n               \"type\": \"string\"\n            }\n         },\n         \"required\": [\n            \"content\",\n            \"source\"\n         ],\n         \"title\": \"AssistantMessage\",\n         \"type\": \"object\"\n      },\n      \"FunctionCall\": {\n         \"properties\": {\n            \"id\": {\n               \"title\": \"Id\",\n               \"type\": \"string\"\n            },\n            \"arguments\": {\n               \"title\": \"Arguments\",\n               \"type\": \"string\"\n            },\n            \"name\": {\n               \"title\": \"Name\",\n               \"type\": \"string\"\n            }\n         },\n         \"required\": [\n            \"id\",\n            \"arguments\",\n            \"name\"\n         ],\n         \"title\": \"FunctionCall\",\n         \"type\": \"object\"\n      },\n      \"FunctionExecutionResult\": {\n         \"description\": \"Function execution result contains the output of a function call.\",\n         \"properties\": {\n            \"content\": {\n               \"title\": \"Content\",\n               \"type\": \"string\"\n            },\n            \"name\": {\n               \"title\": \"Name\",\n               \"type\": \"string\"\n            },\n            \"call_id\": {\n               \"title\": \"Call Id\",\n               \"type\": \"string\"\n            },\n            \"is_error\": {\n               \"anyOf\": [\n                  {\n                     \"type\": \"boolean\"\n                  },\n                  {\n                     \"type\": \"null\"\n                  }\n               ],\n               \"default\": null,\n               \"title\": \"Is Error\"\n            }\n         },\n         \"required\": [\n            \"content\",\n            \"name\",\n            \"call_id\"\n         ],\n         \"title\": \"FunctionExecutionResult\",\n         \"type\": \"object\"\n      },\n      \"FunctionExecutionResultMessage\": {\n         \"description\": \"Function execution result message contains the output of multiple function calls.\",\n         \"properties\": {\n            \"content\": {\n               \"items\": {\n                  \"$ref\": \"#/$defs/FunctionExecutionResult\"\n               },\n               \"title\": \"Content\",\n               \"type\": \"array\"\n            },\n            \"type\": {\n               \"const\": \"FunctionExecutionResultMessage\",\n               \"default\": \"FunctionExecutionResultMessage\",\n               \"title\": \"Type\",\n               \"type\": \"string\"\n            }\n         },\n         \"required\": [\n            \"content\"\n         ],\n         \"title\": \"FunctionExecutionResultMessage\",\n         \"type\": \"object\"\n      },\n      \"SystemMessage\": {\n         \"description\": \"System message contains instructions for the model coming from the developer.\\n\\n.. note::\\n\\n    Open AI is moving away from using 'system' role in favor of 'developer' role.\\n    See `Model Spec <https://cdn.openai.com/spec/model-spec-2024-05-08.html#definitions>`_ for more details.\\n    However, the 'system' role is still allowed in their API and will be automatically converted to 'developer' role\\n    on the server side.\\n    So, you can use `SystemMessage` for developer messages.\",\n         \"properties\": {\n            \"content\": {\n               \"title\": \"Content\",\n               \"type\": \"string\"\n            },\n            \"type\": {\n               \"const\": \"SystemMessage\",\n               \"default\": \"SystemMessage\",\n               \"title\": \"Type\",\n               \"type\": \"string\"\n            }\n         },\n         \"required\": [\n            \"content\"\n         ],\n         \"title\": \"SystemMessage\",\n         \"type\": \"object\"\n      },\n      \"UserMessage\": {\n         \"description\": \"User message contains input from end users, or a catch-all for data provided to the model.\",\n         \"properties\": {\n            \"content\": {\n               \"anyOf\": [\n                  {\n                     \"type\": \"string\"\n                  },\n                  {\n                     \"items\": {\n                        \"anyOf\": [\n                           {\n                              \"type\": \"string\"\n                           },\n                           {}\n                        ]\n                     },\n                     \"type\": \"array\"\n                  }\n               ],\n               \"title\": \"Content\"\n            },\n            \"source\": {\n               \"title\": \"Source\",\n               \"type\": \"string\"\n            },\n            \"type\": {\n               \"const\": \"UserMessage\",\n               \"default\": \"UserMessage\",\n               \"title\": \"Type\",\n               \"type\": \"string\"\n            }\n         },\n         \"required\": [\n            \"content\",\n            \"source\"\n         ],\n         \"title\": \"UserMessage\",\n         \"type\": \"object\"\n      }\n   },\n   \"required\": [\n      \"code\",\n      \"context\"\n   ]\n}",
      "language": "json"
    },
    {
      "code": "{\n   \"title\": \"ApprovalResponse\",\n   \"description\": \"Response to approval request.\",\n   \"type\": \"object\",\n   \"properties\": {\n      \"approved\": {\n         \"title\": \"Approved\",\n         \"type\": \"boolean\"\n      },\n      \"reason\": {\n         \"title\": \"Reason\",\n         \"type\": \"string\"\n      }\n   },\n   \"required\": [\n      \"approved\",\n      \"reason\"\n   ]\n}",
      "language": "json"
    },
    {
      "code": "{\n   \"title\": \"ApprovalResponse\",\n   \"description\": \"Response to approval request.\",\n   \"type\": \"object\",\n   \"properties\": {\n      \"approved\": {\n         \"title\": \"Approved\",\n         \"type\": \"boolean\"\n      },\n      \"reason\": {\n         \"title\": \"Reason\",\n         \"type\": \"string\"\n      }\n   },\n   \"required\": [\n      \"approved\",\n      \"reason\"\n   ]\n}",
      "language": "json"
    }
  ],
  "patterns": [
    {
      "description": "API Reference autogen_agentchat.agents autogen_agentchat.agents# This module initializes various pre-defined agents provided by the package. BaseChatAgent is the base class for all agents in AgentChat. class BaseChatAgent(name: str, description: str)[source]# Bases: ChatAgent, ABC, ComponentBase[BaseModel] Base class for a chat agent. This abstract class provides a base implementation for a ChatAgent. To create a new chat agent, subclass this class and implement the on_messages(), on_reset(), and produced_message_types. If streaming is required, also implement the on_messages_stream() method. An agent is considered stateful and maintains its state between calls to the on_messages() or on_messages_stream() methods. The agent should store its state in the agent instance. The agent should also implement the on_reset() method to reset the agent to its initialization state. Note The caller should only pass the new messages to the agent on each call to the on_messages() or on_messages_stream() method. Do not pass the entire conversation history to the agent on each call. This design principle must be followed when creating a new agent. component_type: ClassVar[ComponentType] = 'agent'# The logical type of the component. property name: str# The name of the agent. This is used by team to uniquely identify the agent. It should be unique within the team. property description: str# The description of the agent. This is used by team to make decisions about which agents to use. The description should describe the agent’s capabilities and how to interact with it. abstract property produced_message_types: Sequence[type[BaseChatMessage]]# The types of messages that the agent produces in the Response.chat_message field. They must be BaseChatMessage types. abstract async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]# Handles incoming messages and returns a response. Note Agents are stateful and the messages passed to this method should be the new messages since the last call to this method. The agent should maintain its state between calls to this method. For example, if the agent needs to remember the previous messages to respond to the current message, it should store the previous messages in the agent state. async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None][source]# Handles incoming messages and returns a stream of messages and and the final item is the response. The base implementation in BaseChatAgent simply calls on_messages() and yields the messages in the response. Note Agents are stateful and the messages passed to this method should be the new messages since the last call to this method. The agent should maintain its state between calls to this method. For example, if the agent needs to remember the previous messages to respond to the current message, it should store the previous messages in the agent state. async run(*, task: str | BaseChatMessage | Sequence[BaseChatMessage] | None = None, cancellation_token: CancellationToken | None = None, output_task_messages: bool = True) → TaskResult[source]# Run the agent with the given task and return the result. async run_stream(*, task: str | BaseChatMessage | Sequence[BaseChatMessage] | None = None, cancellation_token: CancellationToken | None = None, output_task_messages: bool = True) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | TaskResult, None][source]# Run the agent with the given task and return a stream of messages and the final task result as the last item in the stream. Parameters: task – The task to run. Can be a string, a single message, or a sequence of messages. cancellation_token – The cancellation token to kill the task immediately. output_task_messages – Whether to include task messages in the output stream. Defaults to True for backward compatibility. abstract async on_reset(cancellation_token: CancellationToken) → None[source]# Resets the agent to its initialization state. async on_pause(cancellation_token: CancellationToken) → None[source]# Called when the agent is paused while running in its on_messages() or on_messages_stream() method. This is a no-op by default in the BaseChatAgent class. Subclasses can override this method to implement custom pause behavior. async on_resume(cancellation_token: CancellationToken) → None[source]# Called when the agent is resumed from a pause while running in its on_messages() or on_messages_stream() method. This is a no-op by default in the BaseChatAgent class. Subclasses can override this method to implement custom resume behavior. async save_state() → Mapping[str, Any][source]# Export state. Default implementation for stateless agents. async load_state(state: Mapping[str, Any]) → None[source]# Restore agent from saved state. Default implementation for stateless agents. async close() → None[source]# Release any resources held by the agent. This is a no-op by default in the BaseChatAgent class. Subclasses can override this method to implement custom close behavior. class AssistantAgent(name: str, model_client: ChatCompletionClient, *, tools: List[BaseTool[Any, Any] | Callable[[...], Any] | Callable[[...], Awaitable[Any]]] | None = None, workbench: Workbench | Sequence[Workbench] | None = None, handoffs: List[Handoff | str] | None = None, model_context: ChatCompletionContext | None = None, description: str = 'An agent that provides assistance with ability to use tools.', system_message: str | None = 'You are a helpful AI assistant. Solve tasks using your tools. Reply with TERMINATE when the task has been completed.', model_client_stream: bool = False, reflect_on_tool_use: bool | None = None, max_tool_iterations: int = 1, tool_call_summary_format: str = '{result}', tool_call_summary_formatter: Callable[[FunctionCall, FunctionExecutionResult], str] | None = None, output_content_type: type[BaseModel] | None = None, output_content_type_format: str | None = None, memory: Sequence[Memory] | None = None, metadata: Dict[str, str] | None = None)[source]# Bases: BaseChatAgent, Component[AssistantAgentConfig] An agent that provides assistance with tool use. The on_messages() returns a Response in which chat_message is the final response message. The on_messages_stream() creates an async generator that produces the inner messages as they are created, and the Response object as the last item before closing the generator. The BaseChatAgent.run() method returns a TaskResult containing the messages produced by the agent. In the list of messages, messages, the last message is the final response message. The BaseChatAgent.run_stream() method creates an async generator that produces the inner messages as they are created, and the TaskResult object as the last item before closing the generator. Attention The caller must only pass the new messages to the agent on each call to the on_messages(), on_messages_stream(), BaseChatAgent.run(), or BaseChatAgent.run_stream() methods. The agent maintains its state between calls to these methods. Do not pass the entire conversation history to the agent on each call. Warning The assistant agent is not thread-safe or coroutine-safe. It should not be shared between multiple tasks or coroutines, and it should not call its methods concurrently. The following diagram shows how the assistant agent works: Structured output: If the output_content_type is set, the agent will respond with a StructuredMessage instead of a TextMessage in the final response by default. Note Currently, setting output_content_type prevents the agent from being able to call load_component and dum_component methods for serializable configuration. This will be fixed soon in the future. Tool call behavior: If the model returns no tool call, then the response is immediately returned as a TextMessage or a StructuredMessage (when using structured output) in chat_message. This ends the tool call iteration loop regardless of the max_tool_iterations setting. When the model returns tool calls, they will be executed right away: When reflect_on_tool_use is False, the tool call results are returned as a ToolCallSummaryMessage in chat_message. You can customise the summary with either a static format string (tool_call_summary_format) or a callable (tool_call_summary_formatter); the callable is evaluated once per tool call. When reflect_on_tool_use is True, the another model inference is made using the tool calls and results, and final response is returned as a TextMessage or a StructuredMessage (when using structured output) in chat_message. reflect_on_tool_use is set to True by default when output_content_type is set. reflect_on_tool_use is set to False by default when output_content_type is not set. If the model returns multiple tool calls, they will be executed concurrently. To disable parallel tool calls you need to configure the model client. For example, set parallel_tool_calls=False for OpenAIChatCompletionClient and AzureOpenAIChatCompletionClient. The max_tool_iterations parameter controls how many sequential tool call iterations the agent can perform in a single run. When set to 1 (default), the agent executes tool calls once and returns the result. When set higher, the agent can make additional model calls to execute more tool calls if the model continues to request them, enabling multi-step tool-based workflows. The agent stops when either the model returns a text response (instead of tool calls) or the maximum number of iterations is reached. Tip By default, the tool call results are returned as the response when tool calls are made, so pay close attention to how the tools’ return values are formatted—especially if another agent expects a specific schema. Use `tool_call_summary_format` for a simple static template. Use `tool_call_summary_formatter` for full programmatic control (e.g., “hide large success payloads, show full details on error”). Note: tool_call_summary_formatter is not serializable and will be ignored when an agent is loaded from, or exported to, YAML/JSON configuration files. Hand off behavior: If a handoff is triggered, a HandoffMessage will be returned in chat_message. If there are tool calls, they will also be executed right away before returning the handoff. The tool calls and results are passed to the target agent through context. Note If multiple handoffs are detected, only the first handoff is executed. To avoid this, disable parallel tool calls in the model client configuration. Limit context size sent to the model: You can limit the number of messages sent to the model by setting the model_context parameter to a BufferedChatCompletionContext. This will limit the number of recent messages sent to the model and can be useful when the model has a limit on the number of tokens it can process. Another option is to use a TokenLimitedChatCompletionContext which will limit the number of tokens sent to the model. You can also create your own model context by subclassing ChatCompletionContext. Streaming mode: The assistant agent can be used in streaming mode by setting model_client_stream=True. In this mode, the on_messages_stream() and BaseChatAgent.run_stream() methods will also yield ModelClientStreamingChunkEvent messages as the model client produces chunks of response. The chunk messages will not be included in the final response’s inner messages. Parameters: name (str) – The name of the agent. model_client (ChatCompletionClient) – The model client to use for inference. tools (List[BaseTool[Any, Any] | Callable[..., Any] | Callable[..., Awaitable[Any]]] | None, optional) – The tools to register with the agent. workbench (Workbench | Sequence[Workbench] | None, optional) – The workbench or list of workbenches to use for the agent. Tools cannot be used when workbench is set and vice versa. handoffs (List[HandoffBase | str] | None, optional) – The handoff configurations for the agent, allowing it to transfer to other agents by responding with a HandoffMessage. The transfer is only executed when the team is in Swarm. If a handoff is a string, it should represent the target agent’s name. model_context (ChatCompletionContext | None, optional) – The model context for storing and retrieving LLMMessage. It can be preloaded with initial messages. The initial messages will be cleared when the agent is reset. description (str, optional) – The description of the agent. system_message (str, optional) – The system message for the model. If provided, it will be prepended to the messages in the model context when making an inference. Set to None to disable. model_client_stream (bool, optional) – If True, the model client will be used in streaming mode. on_messages_stream() and BaseChatAgent.run_stream() methods will also yield ModelClientStreamingChunkEvent messages as the model client produces chunks of response. Defaults to False. reflect_on_tool_use (bool, optional) – If True, the agent will make another model inference using the tool call and result to generate a response. If False, the tool call result will be returned as the response. By default, if output_content_type is set, this will be True; if output_content_type is not set, this will be False. output_content_type (type[BaseModel] | None, optional) – The output content type for StructuredMessage response as a Pydantic model. This will be used with the model client to generate structured output. If this is set, the agent will respond with a StructuredMessage instead of a TextMessage in the final response, unless reflect_on_tool_use is False and a tool call is made. output_content_type_format (str | None, optional) – (Experimental) The format string used for the content of a StructuredMessage response. max_tool_iterations (int, optional) – The maximum number of tool iterations to perform until the model stops making tool calls. Defaults to 1, which means the agent will only execute the tool calls made by the model once, and return the result as a ToolCallSummaryMessage, or a TextMessage or a StructuredMessage (when using structured output) in chat_message as the final response. As soon as the model stops making tool calls, the agent will stop executing tool calls and return the result as the final response. The value must be greater than or equal to 1. tool_call_summary_format (str, optional) – Static format string applied to each tool call result when composing the ToolCallSummaryMessage. Defaults to \"{result}\". Ignored if tool_call_summary_formatter is provided. When reflect_on_tool_use is False, the summaries for all tool calls are concatenated with a newline (’n’) and returned as the response. Placeholders available in the template: {tool_name}, {arguments}, {result}, {is_error}. tool_call_summary_formatter (Callable[[FunctionCall, FunctionExecutionResult], str] | None, optional) – Callable that receives the FunctionCall and its FunctionExecutionResult and returns the summary string. Overrides tool_call_summary_format when supplied and allows conditional logic — for example, emitting static string like \"Tool FooBar executed successfully.\" on success and a full payload (including all passed arguments etc.) only on failure. Limitation: The callable is not serializable; values provided via YAML/JSON configs are ignored. Note tool_call_summary_formatter is intended for in-code use only. It cannot currently be saved or restored via configuration files. memory (Sequence[Memory] | None, optional): The memory store to use for the agent. Defaults to None. metadata (Dict[str, str] | None, optional): Optional metadata for tracking. Raises: ValueError – If tool names are not unique. ValueError – If handoff names are not unique. ValueError – If handoff names are not unique from tool names. ValueError – If maximum number of tool iterations is less than 1. Examples Example 1: basic agent The following example demonstrates how to create an assistant agent with a model client and generate a response to a simple task. import asyncio from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_agentchat.agents import AssistantAgent async def main() -> None: model_client = OpenAIChatCompletionClient( model=\"gpt-4o\", # api_key = \"your_openai_api_key\" ) agent = AssistantAgent(name=\"assistant\", model_client=model_client) result = await agent.run(task=\"Name two cities in North America.\") print(result) asyncio.run(main()) Example 2: model client token streaming This example demonstrates how to create an assistant agent with a model client and generate a token stream by setting model_client_stream=True. import asyncio from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_agentchat.agents import AssistantAgent async def main() -> None: model_client = OpenAIChatCompletionClient( model=\"gpt-4o\", # api_key = \"your_openai_api_key\" ) agent = AssistantAgent( name=\"assistant\", model_client=model_client, model_client_stream=True, ) stream = agent.run_stream(task=\"Name two cities in North America.\") async for message in stream: print(message) asyncio.run(main()) source='user' models_usage=None metadata={} content='Name two cities in North America.' type='TextMessage' source='assistant' models_usage=None metadata={} content='Two' type='ModelClientStreamingChunkEvent' source='assistant' models_usage=None metadata={} content=' cities' type='ModelClientStreamingChunkEvent' source='assistant' models_usage=None metadata={} content=' in' type='ModelClientStreamingChunkEvent' source='assistant' models_usage=None metadata={} content=' North' type='ModelClientStreamingChunkEvent' source='assistant' models_usage=None metadata={} content=' America' type='ModelClientStreamingChunkEvent' source='assistant' models_usage=None metadata={} content=' are' type='ModelClientStreamingChunkEvent' source='assistant' models_usage=None metadata={} content=' New' type='ModelClientStreamingChunkEvent' source='assistant' models_usage=None metadata={} content=' York' type='ModelClientStreamingChunkEvent' source='assistant' models_usage=None metadata={} content=' City' type='ModelClientStreamingChunkEvent' source='assistant' models_usage=None metadata={} content=' and' type='ModelClientStreamingChunkEvent' source='assistant' models_usage=None metadata={} content=' Toronto' type='ModelClientStreamingChunkEvent' source='assistant' models_usage=None metadata={} content='.' type='ModelClientStreamingChunkEvent' source='assistant' models_usage=None metadata={} content=' TERMIN' type='ModelClientStreamingChunkEvent' source='assistant' models_usage=None metadata={} content='ATE' type='ModelClientStreamingChunkEvent' source='assistant' models_usage=RequestUsage(prompt_tokens=0, completion_tokens=0) metadata={} content='Two cities in North America are New York City and Toronto. TERMINATE' type='TextMessage' messages=[TextMessage(source='user', models_usage=None, metadata={}, content='Name two cities in North America.', type='TextMessage'), TextMessage(source='assistant', models_usage=RequestUsage(prompt_tokens=0, completion_tokens=0), metadata={}, content='Two cities in North America are New York City and Toronto. TERMINATE', type='TextMessage')] stop_reason=None Example 3: agent with tools The following example demonstrates how to create an assistant agent with a model client and a tool, generate a stream of messages for a task, and print the messages to the console using Console. The tool is a simple function that returns the current time. Under the hood, the function is wrapped in a FunctionTool and used with the agent’s model client. The doc string of the function is used as the tool description, the function name is used as the tool name, and the function signature including the type hints is used as the tool arguments. import asyncio from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.ui import Console async def get_current_time() -> str: return \"The current time is 12:00 PM.\" async def main() -> None: model_client = OpenAIChatCompletionClient( model=\"gpt-4o\", # api_key = \"your_openai_api_key\" ) agent = AssistantAgent(name=\"assistant\", model_client=model_client, tools=[get_current_time]) await Console(agent.run_stream(task=\"What is the current time?\")) asyncio.run(main()) Example 4: agent with max_tool_iterations The following example demonstrates how to use the max_tool_iterations parameter to control how many times the agent can execute tool calls in a single run. This is useful when you want the agent to perform multiple sequential tool operations to reach a goal. import asyncio from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.ui import Console # Global counter state counter = 0 def increment_counter() -> str: \"\"\"Increment the counter by 1 and return the current value.\"\"\" global counter counter += 1 return f\"Counter incremented to: {counter}\" def get_counter() -> str: \"\"\"Get the current counter value.\"\"\" global counter return f\"Current counter value: {counter}\" async def main() -> None: model_client = OpenAIChatCompletionClient( model=\"gpt-4o\", # api_key = \"your_openai_api_key\" ) # Create agent with max_tool_iterations=5 to allow multiple tool calls agent = AssistantAgent( name=\"assistant\", model_client=model_client, tools=[increment_counter, get_counter], max_tool_iterations=5, # Allow up to 5 tool call iterations reflect_on_tool_use=True, # Get a final summary after tool calls ) await Console(agent.run_stream(task=\"Increment the counter 3 times and then tell me the final value.\")) asyncio.run(main()) Example 5: agent with Model-Context Protocol (MCP) workbench The following example demonstrates how to create an assistant agent with a model client and an McpWorkbench for interacting with a Model-Context Protocol (MCP) server. import asyncio from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.ui import Console from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_ext.tools.mcp import StdioServerParams, McpWorkbench async def main() -> None: params = StdioServerParams( command=\"uvx\", args=[\"mcp-server-fetch\"], read_timeout_seconds=60, ) # You can also use `start()` and `stop()` to manage the session. async with McpWorkbench(server_params=params) as workbench: model_client = OpenAIChatCompletionClient(model=\"gpt-4.1-nano\") assistant = AssistantAgent( name=\"Assistant\", model_client=model_client, workbench=workbench, reflect_on_tool_use=True, ) await Console( assistant.run_stream(task=\"Go to https://github.com/microsoft/autogen and tell me what you see.\") ) asyncio.run(main()) Example 6: agent with structured output and tool The following example demonstrates how to create an assistant agent with a model client configured to use structured output and a tool. Note that you need to use FunctionTool to create the tool and the strict=True is required for structured output mode. Because the model is configured to use structured output, the output reflection response will be a JSON formatted string. import asyncio from typing import Literal from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.ui import Console from autogen_core.tools import FunctionTool from autogen_ext.models.openai import OpenAIChatCompletionClient from pydantic import BaseModel # Define the structured output format. class AgentResponse(BaseModel): thoughts: str response: Literal[\"happy\", \"sad\", \"neutral\"] # Define the function to be called as a tool. def sentiment_analysis(text: str) -> str: \"\"\"Given a text, return the sentiment.\"\"\" return \"happy\" if \"happy\" in text else \"sad\" if \"sad\" in text else \"neutral\" # Create a FunctionTool instance with `strict=True`, # which is required for structured output mode. tool = FunctionTool(sentiment_analysis, description=\"Sentiment Analysis\", strict=True) # Create an OpenAIChatCompletionClient instance that supports structured output. model_client = OpenAIChatCompletionClient( model=\"gpt-4o-mini\", ) # Create an AssistantAgent instance that uses the tool and model client. agent = AssistantAgent( name=\"assistant\", model_client=model_client, tools=[tool], system_message=\"Use the tool to analyze sentiment.\", output_content_type=AgentResponse, ) async def main() -> None: stream = agent.run_stream(task=\"I am happy today!\") await Console(stream) asyncio.run(main()) ---------- assistant ---------- [FunctionCall(id='call_tIZjAVyKEDuijbBwLY6RHV2p', arguments='{\"text\":\"I am happy today!\"}', name='sentiment_analysis')] ---------- assistant ---------- [FunctionExecutionResult(content='happy', call_id='call_tIZjAVyKEDuijbBwLY6RHV2p', is_error=False)] ---------- assistant ---------- {\"thoughts\":\"The user expresses a clear positive emotion by stating they are happy today, suggesting an upbeat mood.\",\"response\":\"happy\"} Example 7: agent with bounded model context The following example shows how to use a BufferedChatCompletionContext that only keeps the last 2 messages (1 user + 1 assistant). Bounded model context is useful when the model has a limit on the number of tokens it can process. import asyncio from autogen_agentchat.agents import AssistantAgent from autogen_core.model_context import BufferedChatCompletionContext from autogen_ext.models.openai import OpenAIChatCompletionClient async def main() -> None: # Create a model client. model_client = OpenAIChatCompletionClient( model=\"gpt-4o-mini\", # api_key = \"your_openai_api_key\" ) # Create a model context that only keeps the last 2 messages (1 user + 1 assistant). model_context = BufferedChatCompletionContext(buffer_size=2) # Create an AssistantAgent instance with the model client and context. agent = AssistantAgent( name=\"assistant\", model_client=model_client, model_context=model_context, system_message=\"You are a helpful assistant.\", ) result = await agent.run(task=\"Name two cities in North America.\") print(result.messages[-1].content) # type: ignore result = await agent.run(task=\"My favorite color is blue.\") print(result.messages[-1].content) # type: ignore result = await agent.run(task=\"Did I ask you any question?\") print(result.messages[-1].content) # type: ignore asyncio.run(main()) Two cities in North America are New York City and Toronto. That's great! Blue is often associated with calmness and serenity. Do you have a specific shade of blue that you like, or any particular reason why it's your favorite? No, you didn't ask a question. I apologize for any misunderstanding. If you have something specific you'd like to discuss or ask, feel free to let me know! Example 8: agent with memory The following example shows how to use a list-based memory with the assistant agent. The memory is preloaded with some initial content. Under the hood, the memory is used to update the model context before making an inference, using the update_context() method. import asyncio from autogen_agentchat.agents import AssistantAgent from autogen_core.memory import ListMemory, MemoryContent from autogen_ext.models.openai import OpenAIChatCompletionClient async def main() -> None: # Create a model client. model_client = OpenAIChatCompletionClient( model=\"gpt-4o-mini\", # api_key = \"your_openai_api_key\" ) # Create a list-based memory with some initial content. memory = ListMemory() await memory.add(MemoryContent(content=\"User likes pizza.\", mime_type=\"text/plain\")) await memory.add(MemoryContent(content=\"User dislikes cheese.\", mime_type=\"text/plain\")) # Create an AssistantAgent instance with the model client and memory. agent = AssistantAgent( name=\"assistant\", model_client=model_client, memory=[memory], system_message=\"You are a helpful assistant.\", ) result = await agent.run(task=\"What is a good dinner idea?\") print(result.messages[-1].content) # type: ignore asyncio.run(main()) How about making a delicious pizza without cheese? You can create a flavorful veggie pizza with a variety of toppings. Here's a quick idea: **Veggie Tomato Sauce Pizza** - Start with a pizza crust (store-bought or homemade). - Spread a layer of marinara or tomato sauce evenly over the crust. - Top with your favorite vegetables like bell peppers, mushrooms, onions, olives, and spinach. - Add some protein if you'd like, such as grilled chicken or pepperoni (ensure it's cheese-free). - Sprinkle with herbs like oregano and basil, and maybe a drizzle of olive oil. - Bake according to the crust instructions until the edges are golden and the veggies are cooked. Serve it with a side salad or some garlic bread to complete the meal! Enjoy your dinner! Example 9: agent with `o1-mini` The following example shows how to use o1-mini model with the assistant agent. import asyncio from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_agentchat.agents import AssistantAgent async def main() -> None: model_client = OpenAIChatCompletionClient( model=\"o1-mini\", # api_key = \"your_openai_api_key\" ) # The system message is not supported by the o1 series model. agent = AssistantAgent(name=\"assistant\", model_client=model_client, system_message=None) result = await agent.run(task=\"What is the capital of France?\") print(result.messages[-1].content) # type: ignore asyncio.run(main()) Note The o1-preview and o1-mini models do not support system message and function calling. So the system_message should be set to None and the tools and handoffs should not be set. See o1 beta limitations for more details. Example 10: agent using reasoning model with custom model context. The following example shows how to use a reasoning model (DeepSeek R1) with the assistant agent. The model context is used to filter out the thought field from the assistant message. import asyncio from typing import List from autogen_agentchat.agents import AssistantAgent from autogen_core.model_context import UnboundedChatCompletionContext from autogen_core.models import AssistantMessage, LLMMessage, ModelFamily from autogen_ext.models.ollama import OllamaChatCompletionClient class ReasoningModelContext(UnboundedChatCompletionContext): \"\"\"A model context for reasoning models.\"\"\" async def get_messages(self) -> List[LLMMessage]: messages = await super().get_messages() # Filter out thought field from AssistantMessage. messages_out: List[LLMMessage] = [] for message in messages: if isinstance(message, AssistantMessage): message.thought = None messages_out.append(message) return messages_out # Create an instance of the model client for DeepSeek R1 hosted locally on Ollama. model_client = OllamaChatCompletionClient( model=\"deepseek-r1:8b\", model_info={ \"vision\": False, \"function_calling\": False, \"json_output\": False, \"family\": ModelFamily.R1, \"structured_output\": True, }, ) agent = AssistantAgent( \"reasoning_agent\", model_client=model_client, model_context=ReasoningModelContext(), # Use the custom model context. ) async def run_reasoning_agent() -> None: result = await agent.run(task=\"What is the capital of France?\") print(result) asyncio.run(run_reasoning_agent()) For detailed examples and usage, see the Examples section below. component_version: ClassVar[int] = 2# The version of the component, if schema incompatibilities are introduced this should be updated. component_config_schema# alias of AssistantAgentConfig component_provider_override: ClassVar[str | None] = 'autogen_agentchat.agents.AssistantAgent'# Override the provider string for the component. This should be used to prevent internal module names being a part of the module name. property produced_message_types: Sequence[type[BaseChatMessage]]# Get the types of messages this agent can produce. Returns: Sequence of message types this agent can generate property model_context: ChatCompletionContext# Get the model context used by this agent. Returns: The chat completion context for this agent async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]# Process incoming messages and generate a response. Parameters: messages – Sequence of messages to process cancellation_token – Token for cancelling operation Returns: Response containing the agent’s reply async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None][source]# Process messages and stream the response. Parameters: messages – Sequence of messages to process cancellation_token – Token for cancelling operation Yields: Events, messages and final response during processing async on_reset(cancellation_token: CancellationToken) → None[source]# Reset the assistant agent to its initialization state. async save_state() → Mapping[str, Any][source]# Save the current state of the assistant agent. async load_state(state: Mapping[str, Any]) → None[source]# Load the state of the assistant agent class CodeExecutorAgent(name: str, code_executor: CodeExecutor, *, model_client: ChatCompletionClient | None = None, model_context: ChatCompletionContext | None = None, model_client_stream: bool = False, max_retries_on_error: int = 0, description: str | None = None, system_message: str | None = DEFAULT_SYSTEM_MESSAGE, sources: Sequence[str] | None = None, supported_languages: List[str] | None = None, approval_func: Callable[[ApprovalRequest], ApprovalResponse] | Callable[[ApprovalRequest], Awaitable[ApprovalResponse]] | None = None)[source]# Bases: BaseChatAgent, Component[CodeExecutorAgentConfig] (Experimental) An agent that generates and executes code snippets based on user instructions. Note This agent is experimental and may change in future releases. It is typically used within a team with another agent that generates code snippets to be executed or alone with model_client provided so that it can generate code based on user query, execute it and reflect on the code result. When used with model_client, it will generate code snippets using the model and execute them using the provided code_executor. The model will also reflect on the code execution results. The agent will yield the final reflection result from the model as the final response. When used without model_client, it will only execute code blocks found in TextMessage messages and returns the output of the code execution. Note Using AssistantAgent with PythonCodeExecutionTool is an alternative to this agent. However, the model for that agent will have to generate properly escaped code string as a parameter to the tool. Parameters: name (str) – The name of the agent. code_executor (CodeExecutor) – The code executor responsible for executing code received in messages (DockerCommandLineCodeExecutor recommended. See example below) model_client (ChatCompletionClient, optional) – The model client to use for inference and generating code. If not provided, the agent will only execute code blocks found in input messages. Currently, the model must support structured output mode, which is required for the automatic retry mechanism to work. model_client_stream (bool, optional) – If True, the model client will be used in streaming mode. on_messages_stream() and BaseChatAgent.run_stream() methods will also yield ModelClientStreamingChunkEvent messages as the model client produces chunks of response. Defaults to False. description (str, optional) – The description of the agent. If not provided, DEFAULT_AGENT_DESCRIPTION will be used. system_message (str, optional) – The system message for the model. If provided, it will be prepended to the messages in the model context when making an inference. Set to None to disable. Defaults to DEFAULT_SYSTEM_MESSAGE. This is only used if model_client is provided. sources (Sequence[str], optional) – Check only messages from the specified agents for the code to execute. This is useful when the agent is part of a group chat and you want to limit the code execution to messages from specific agents. If not provided, all messages will be checked for code blocks. This is only used if model_client is not provided. max_retries_on_error (int, optional) – The maximum number of retries on error. If the code execution fails, the agent will retry up to this number of times. If the code execution fails after this number of retries, the agent will yield a reflection result. supported_languages (List[str], optional) – List of programming languages that will be parsed and executed from agent response; others will be ignored. Defaults to DEFAULT_SUPPORTED_LANGUAGES. approval_func (Optional[Union[Callable[[ApprovalRequest], ApprovalResponse], Callable[[ApprovalRequest], Awaitable[ApprovalResponse]]]], optional) – A function that is called before each code execution to get approval. The function takes an ApprovalRequest containing the code to be executed and the current context, and returns an ApprovalResponse. The function can be either synchronous or asynchronous. If None (default), all code executions are automatically approved. If set, the agent cannot be serialized using dump_component(). Note It is recommended that the CodeExecutorAgent agent uses a Docker container to execute code. This ensures that model-generated code is executed in an isolated environment. To use Docker, your environment must have Docker installed and running. Follow the installation instructions for Docker. Note The code executor only processes code that is properly formatted in markdown code blocks using triple backticks. For example: ```python print(\"Hello World\") ``` # or ```sh echo \"Hello World\" ``` In this example, we show how to set up a CodeExecutorAgent agent that uses the DockerCommandLineCodeExecutor to execute code snippets in a Docker container. The work_dir parameter indicates where all executed files are first saved locally before being executed in the Docker container. import asyncio from autogen_agentchat.agents import CodeExecutorAgent, ApprovalRequest, ApprovalResponse from autogen_agentchat.messages import TextMessage from autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor from autogen_core import CancellationToken def simple_approval_func(request: ApprovalRequest) -> ApprovalResponse: \"\"\"Simple approval function that requests user input for code execution approval.\"\"\" print(\"Code execution approval requested:\") print(\"=\" * 50) print(request.code) print(\"=\" * 50) while True: user_input = input(\"Do you want to execute this code? (y/n): \").strip().lower() if user_input in ['y', 'yes']: return ApprovalResponse(approved=True, reason='Approved by user') elif user_input in ['n', 'no']: return ApprovalResponse(approved=False, reason='Denied by user') else: print(\"Please enter 'y' for yes or 'n' for no.\") async def run_code_executor_agent() -> None: # Create a code executor agent that uses a Docker container to execute code. code_executor = DockerCommandLineCodeExecutor(work_dir=\"coding\") await code_executor.start() code_executor_agent = CodeExecutorAgent( \"code_executor\", code_executor=code_executor, approval_func=simple_approval_func ) # Run the agent with a given code snippet. task = TextMessage( content='''Here is some code ```python print('Hello world') ``` ''', source=\"user\", ) response = await code_executor_agent.on_messages([task], CancellationToken()) print(response.chat_message) # Stop the code executor. await code_executor.stop() asyncio.run(run_code_executor_agent()) In this example, we show how to set up a CodeExecutorAgent agent that uses the DeviceRequest to expose a GPU to the container for cuda-accelerated code execution. import asyncio from autogen_agentchat.agents import CodeExecutorAgent from autogen_agentchat.messages import TextMessage from autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor from autogen_core import CancellationToken from docker.types import DeviceRequest async def run_code_executor_agent() -> None: # Create a code executor agent that uses a Docker container to execute code. code_executor = DockerCommandLineCodeExecutor( work_dir=\"coding\", device_requests=[DeviceRequest(count=-1, capabilities=[[\"gpu\"]])] ) await code_executor.start() code_executor_agent = CodeExecutorAgent(\"code_executor\", code_executor=code_executor) # Display the GPU information task = TextMessage( content='''Here is some code ```sh nvidia-smi ``` ''', source=\"user\", ) response = await code_executor_agent.on_messages([task], CancellationToken()) print(response.chat_message) # Stop the code executor. await code_executor.stop() asyncio.run(run_code_executor_agent()) In the following example, we show how to setup CodeExecutorAgent without model_client parameter for executing code blocks generated by other agents in a group chat using DockerCommandLineCodeExecutor import asyncio from autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_agentchat.agents import AssistantAgent, CodeExecutorAgent, ApprovalRequest, ApprovalResponse from autogen_agentchat.conditions import MaxMessageTermination from autogen_agentchat.teams import RoundRobinGroupChat from autogen_agentchat.ui import Console termination_condition = MaxMessageTermination(3) def group_chat_approval_func(request: ApprovalRequest) -> ApprovalResponse: \"\"\"Approval function for group chat that allows basic Python operations.\"\"\" # Allow common safe operations safe_operations = [\"print(\", \"import \", \"def \", \"class \", \"if \", \"for \", \"while \"] if any(op in request.code for op in safe_operations): return ApprovalResponse(approved=True, reason='Safe Python operation') # Deny file system operations in group chat dangerous_operations = [\"open(\", \"file(\", \"os.\", \"subprocess\", \"eval(\", \"exec(\"] if any(op in request.code for op in dangerous_operations): return ApprovalResponse(approved=False, reason='File system or dangerous operation not allowed') return ApprovalResponse(approved=True, reason='Operation approved') async def main() -> None: model_client = OpenAIChatCompletionClient(model=\"gpt-4o\") # define the Docker CLI Code Executor code_executor = DockerCommandLineCodeExecutor(work_dir=\"coding\") # start the execution container await code_executor.start() code_executor_agent = CodeExecutorAgent( \"code_executor_agent\", code_executor=code_executor, approval_func=group_chat_approval_func ) coder_agent = AssistantAgent(\"coder_agent\", model_client=model_client) groupchat = RoundRobinGroupChat( participants=[coder_agent, code_executor_agent], termination_condition=termination_condition ) task = \"Write python code to print Hello World!\" await Console(groupchat.run_stream(task=task)) # stop the execution container await code_executor.stop() asyncio.run(main()) ---------- user ---------- Write python code to print Hello World! ---------- coder_agent ---------- Certainly! Here's a simple Python code to print \"Hello World!\": ```python print(\"Hello World!\") ``` You can run this code in any Python environment to display the message. ---------- code_executor_agent ---------- Hello World! In the following example, we show how to setup CodeExecutorAgent with model_client that can generate its own code without the help of any other agent and executing it in DockerCommandLineCodeExecutor. It also demonstrates using a model-based approval function that reviews the code for safety before execution. import asyncio from autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_core.models import SystemMessage, UserMessage from autogen_agentchat.agents import CodeExecutorAgent, ApprovalRequest, ApprovalResponse from autogen_agentchat.conditions import TextMessageTermination from autogen_agentchat.ui import Console termination_condition = TextMessageTermination(\"code_executor_agent\") async def main() -> None: model_client = OpenAIChatCompletionClient(model=\"gpt-4o\") async def model_client_approval_func(request: ApprovalRequest) -> ApprovalResponse: instruction = \"Approve or reject the code in the last message based on whether it is dangerous or not. Use the following JSON format for your response: {approved: true/false, reason: 'your reason here'}\" response = await model_client.create( messages=[SystemMessage(content=instruction)] + request.context + [UserMessage(content=request.code, source=\"user\")], json_output=ApprovalResponse, ) assert isinstance(response.content, str) return ApprovalResponse.model_validate_json(response.content) # define the Docker CLI Code Executor code_executor = DockerCommandLineCodeExecutor(work_dir=\"coding\") # start the execution container await code_executor.start() code_executor_agent = CodeExecutorAgent( \"code_executor_agent\", code_executor=code_executor, model_client=model_client, approval_func=model_client_approval_func, ) task = \"Write python code to print Hello World!\" await Console(code_executor_agent.run_stream(task=task)) # stop the execution container await code_executor.stop() asyncio.run(main()) ---------- user ---------- Write python code to print Hello World! ---------- code_executor_agent ---------- Certainly! Here is a simple Python code to print \"Hello World!\" to the console: ```python print(\"Hello World!\") ``` Let's execute it to confirm the output. ---------- code_executor_agent ---------- Hello World! ---------- code_executor_agent ---------- The code has been executed successfully, and it printed \"Hello World!\" as expected. If you have any more requests or questions, feel free to ask! DEFAULT_TERMINAL_DESCRIPTION = 'A computer terminal that performs no other action than running Python scripts (provided to it quoted in ```python code blocks), or sh shell scripts (provided to it quoted in ```sh code blocks).'# DEFAULT_AGENT_DESCRIPTION = 'A Code Execution Agent that generates and executes Python and shell scripts based on user instructions. It ensures correctness, efficiency, and minimal errors while gracefully handling edge cases.'# DEFAULT_SYSTEM_MESSAGE = 'You are a Code Execution Agent. Your role is to generate and execute Python code and shell scripts based on user instructions, ensuring correctness, efficiency, and minimal errors. Handle edge cases gracefully. Python code should be provided in ```python code blocks, and sh shell scripts should be provided in ```sh code blocks for execution.'# NO_CODE_BLOCKS_FOUND_MESSAGE = 'No code blocks found in the thread. Please provide at least one markdown-encoded code block to execute (i.e., quoting code in ```python or ```sh code blocks).'# DEFAULT_SUPPORTED_LANGUAGES = ['python', 'sh']# component_config_schema# alias of CodeExecutorAgentConfig component_provider_override: ClassVar[str | None] = 'autogen_agentchat.agents.CodeExecutorAgent'# Override the provider string for the component. This should be used to prevent internal module names being a part of the module name. property produced_message_types: Sequence[type[BaseChatMessage]]# The types of messages that the code executor agent produces. property model_context: ChatCompletionContext# The model context in use by the agent. async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]# Handles incoming messages and returns a response. Note Agents are stateful and the messages passed to this method should be the new messages since the last call to this method. The agent should maintain its state between calls to this method. For example, if the agent needs to remember the previous messages to respond to the current message, it should store the previous messages in the agent state. async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None][source]# Process the incoming messages with the assistant agent and yield events/responses as they happen. async extract_code_blocks_from_messages(messages: Sequence[BaseChatMessage]) → List[CodeBlock][source]# async execute_code_block(code_blocks: List[CodeBlock], cancellation_token: CancellationToken) → CodeResult[source]# async on_reset(cancellation_token: CancellationToken) → None[source]# Its a no-op as the code executor agent has no mutable state. _to_config() → CodeExecutorAgentConfig[source]# Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance. Returns: T – The configuration of the component. classmethod _from_config(config: CodeExecutorAgentConfig) → Self[source]# Create a new instance of the component from a configuration object. Parameters: config (T) – The configuration object. Returns: Self – The new instance of the component. class SocietyOfMindAgent(name: str, team: Team, model_client: ChatCompletionClient, *, description: str = DEFAULT_DESCRIPTION, instruction: str = DEFAULT_INSTRUCTION, response_prompt: str = DEFAULT_RESPONSE_PROMPT, model_context: ChatCompletionContext | None = None)[source]# Bases: BaseChatAgent, Component[SocietyOfMindAgentConfig] An agent that uses an inner team of agents to generate responses. Each time the agent’s on_messages() or on_messages_stream() method is called, it runs the inner team of agents and then uses the model client to generate a response based on the inner team’s messages. Once the response is generated, the agent resets the inner team by calling Team.reset(). Limit context size sent to the model: You can limit the number of messages sent to the model by setting the model_context parameter to a BufferedChatCompletionContext. This will limit the number of recent messages sent to the model and can be useful when the model has a limit on the number of tokens it can process. You can also create your own model context by subclassing ChatCompletionContext. Parameters: name (str) – The name of the agent. team (Team) – The team of agents to use. model_client (ChatCompletionClient) – The model client to use for preparing responses. description (str, optional) – The description of the agent. instruction (str, optional) – The instruction to use when generating a response using the inner team’s messages. Defaults to DEFAULT_INSTRUCTION. It assumes the role of ‘system’. response_prompt (str, optional) – The response prompt to use when generating a response using the inner team’s messages. Defaults to DEFAULT_RESPONSE_PROMPT. It assumes the role of ‘system’. model_context (ChatCompletionContext | None, optional) – The model context for storing and retrieving LLMMessage. It can be preloaded with initial messages. The initial messages will be cleared when the agent is reset. Example: import asyncio from autogen_agentchat.ui import Console from autogen_agentchat.agents import AssistantAgent, SocietyOfMindAgent from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_agentchat.teams import RoundRobinGroupChat from autogen_agentchat.conditions import TextMentionTermination async def main() -> None: model_client = OpenAIChatCompletionClient(model=\"gpt-4o\") agent1 = AssistantAgent(\"assistant1\", model_client=model_client, system_message=\"You are a writer, write well.\") agent2 = AssistantAgent( \"assistant2\", model_client=model_client, system_message=\"You are an editor, provide critical feedback. Respond with 'APPROVE' if the text addresses all feedbacks.\", ) inner_termination = TextMentionTermination(\"APPROVE\") inner_team = RoundRobinGroupChat([agent1, agent2], termination_condition=inner_termination) society_of_mind_agent = SocietyOfMindAgent(\"society_of_mind\", team=inner_team, model_client=model_client) agent3 = AssistantAgent( \"assistant3\", model_client=model_client, system_message=\"Translate the text to Spanish.\" ) team = RoundRobinGroupChat([society_of_mind_agent, agent3], max_turns=2) stream = team.run_stream(task=\"Write a short story with a surprising ending.\") await Console(stream) asyncio.run(main()) component_config_schema# alias of SocietyOfMindAgentConfig component_provider_override: ClassVar[str | None] = 'autogen_agentchat.agents.SocietyOfMindAgent'# Override the provider string for the component. This should be used to prevent internal module names being a part of the module name. DEFAULT_INSTRUCTION = 'Earlier you were asked to fulfill a request. You and your team worked diligently to address that request. Here is a transcript of that conversation:'# The default instruction to use when generating a response using the inner team’s messages. The instruction will be prepended to the inner team’s messages when generating a response using the model. It assumes the role of ‘system’. Type: str DEFAULT_RESPONSE_PROMPT = 'Output a standalone response to the original request, without mentioning any of the intermediate discussion.'# The default response prompt to use when generating a response using the inner team’s messages. It assumes the role of ‘system’. Type: str DEFAULT_DESCRIPTION = 'An agent that uses an inner team of agents to generate responses.'# The default description for a SocietyOfMindAgent. Type: str property produced_message_types: Sequence[type[BaseChatMessage]]# The types of messages that the agent produces in the Response.chat_message field. They must be BaseChatMessage types. property model_context: ChatCompletionContext# The model context in use by the agent. async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]# Handles incoming messages and returns a response. Note Agents are stateful and the messages passed to this method should be the new messages since the last call to this method. The agent should maintain its state between calls to this method. For example, if the agent needs to remember the previous messages to respond to the current message, it should store the previous messages in the agent state. async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None][source]# Handles incoming messages and returns a stream of messages and and the final item is the response. The base implementation in BaseChatAgent simply calls on_messages() and yields the messages in the response. Note Agents are stateful and the messages passed to this method should be the new messages since the last call to this method. The agent should maintain its state between calls to this method. For example, if the agent needs to remember the previous messages to respond to the current message, it should store the previous messages in the agent state. async on_reset(cancellation_token: CancellationToken) → None[source]# Resets the agent to its initialization state. async save_state() → Mapping[str, Any][source]# Export state. Default implementation for stateless agents. async load_state(state: Mapping[str, Any]) → None[source]# Restore agent from saved state. Default implementation for stateless agents. _to_config() → SocietyOfMindAgentConfig[source]# Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance. Returns: T – The configuration of the component. classmethod _from_config(config: SocietyOfMindAgentConfig) → Self[source]# Create a new instance of the component from a configuration object. Parameters: config (T) – The configuration object. Returns: Self – The new instance of the component. class UserProxyAgent(name: str, *, description: str = 'A human user', input_func: Callable[[str], str] | Callable[[str, CancellationToken | None], Awaitable[str]] | None = None)[source]# Bases: BaseChatAgent, Component[UserProxyAgentConfig] An agent that can represent a human user through an input function. This agent can be used to represent a human user in a chat system by providing a custom input function. Note Using UserProxyAgent puts a running team in a temporary blocked state until the user responds. So it is important to time out the user input function and cancel using the CancellationToken if the user does not respond. The input function should also handle exceptions and return a default response if needed. For typical use cases that involve slow human responses, it is recommended to use termination conditions such as HandoffTermination or SourceMatchTermination to stop the running team and return the control to the application. You can run the team again with the user input. This way, the state of the team can be saved and restored when the user responds. See Human-in-the-loop for more information. Parameters: name (str) – The name of the agent. description (str, optional) – A description of the agent. input_func (Optional[Callable[[str], str]], Callable[[str, Optional[CancellationToken]], Awaitable[str]]) – A function that takes a prompt and returns a user input string. For examples of integrating with web and UI frameworks, see the following: FastAPI ChainLit Example Simple usage case: import asyncio from autogen_core import CancellationToken from autogen_agentchat.agents import UserProxyAgent from autogen_agentchat.messages import TextMessage async def simple_user_agent(): agent = UserProxyAgent(\"user_proxy\") response = await asyncio.create_task( agent.on_messages( [TextMessage(content=\"What is your name? \", source=\"user\")], cancellation_token=CancellationToken(), ) ) assert isinstance(response.chat_message, TextMessage) print(f\"Your name is {response.chat_message.content}\") Example Cancellable usage case: import asyncio from typing import Any from autogen_core import CancellationToken from autogen_agentchat.agents import UserProxyAgent from autogen_agentchat.messages import TextMessage token = CancellationToken() agent = UserProxyAgent(\"user_proxy\") async def timeout(delay: float): await asyncio.sleep(delay) def cancellation_callback(task: asyncio.Task[Any]): token.cancel() async def cancellable_user_agent(): try: timeout_task = asyncio.create_task(timeout(3)) timeout_task.add_done_callback(cancellation_callback) agent_task = asyncio.create_task( agent.on_messages( [TextMessage(content=\"What is your name? \", source=\"user\")], cancellation_token=token, ) ) response = await agent_task assert isinstance(response.chat_message, TextMessage) print(f\"Your name is {response.chat_message.content}\") except Exception as e: print(f\"Exception: {e}\") except BaseException as e: print(f\"BaseException: {e}\") component_type: ClassVar[ComponentType] = 'agent'# The logical type of the component. component_provider_override: ClassVar[str | None] = 'autogen_agentchat.agents.UserProxyAgent'# Override the provider string for the component. This should be used to prevent internal module names being a part of the module name. component_config_schema# alias of UserProxyAgentConfig class InputRequestContext[source]# Bases: object classmethod request_id() → str[source]# property produced_message_types: Sequence[type[BaseChatMessage]]# Message types this agent can produce. async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]# Handles incoming messages and returns a response. Note Agents are stateful and the messages passed to this method should be the new messages since the last call to this method. The agent should maintain its state between calls to this method. For example, if the agent needs to remember the previous messages to respond to the current message, it should store the previous messages in the agent state. async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None][source]# Handle incoming messages by requesting user input. async on_reset(cancellation_token: CancellationToken | None = None) → None[source]# Reset agent state. _to_config() → UserProxyAgentConfig[source]# Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance. Returns: T – The configuration of the component. classmethod _from_config(config: UserProxyAgentConfig) → Self[source]# Create a new instance of the component from a configuration object. Parameters: config (T) – The configuration object. Returns: Self – The new instance of the component. class MessageFilterAgent(name: str, wrapped_agent: BaseChatAgent, filter: MessageFilterConfig)[source]# Bases: BaseChatAgent, Component[MessageFilterAgentConfig] A wrapper agent that filters incoming messages before passing them to the inner agent. Warning This is an experimental feature, and the API will change in the future releases. This is useful in scenarios like multi-agent workflows where an agent should only process a subset of the full message history—for example, only the last message from each upstream agent, or only the first message from a specific source. Filtering is configured using MessageFilterConfig, which supports: - Filtering by message source (e.g., only messages from “user” or another agent) - Selecting the first N or last N messages from each source - If position is None, all messages from that source are included This agent is compatible with both direct message passing and team-based execution such as GraphFlow. Example >>> agent_a = MessageFilterAgent( ... name=\"A\", ... wrapped_agent=some_other_agent, ... filter=MessageFilterConfig( ... per_source=[ ... PerSourceFilter(source=\"user\", position=\"first\", count=1), ... PerSourceFilter(source=\"B\", position=\"last\", count=2), ... ] ... ), ... ) Example use case with Graph:Suppose you have a looping multi-agent graph: A → B → A → B → C. You want: - A to only see the user message and the last message from B - B to see the user message, last message from A, and its own prior responses (for reflection) - C to see the user message and the last message from B Wrap the agents like so: >>> agent_a = MessageFilterAgent( ... name=\"A\", ... wrapped_agent=agent_a_inner, ... filter=MessageFilterConfig( ... per_source=[ ... PerSourceFilter(source=\"user\", position=\"first\", count=1), ... PerSourceFilter(source=\"B\", position=\"last\", count=1), ... ] ... ), ... ) >>> agent_b = MessageFilterAgent( ... name=\"B\", ... wrapped_agent=agent_b_inner, ... filter=MessageFilterConfig( ... per_source=[ ... PerSourceFilter(source=\"user\", position=\"first\", count=1), ... PerSourceFilter(source=\"A\", position=\"last\", count=1), ... PerSourceFilter(source=\"B\", position=\"last\", count=10), ... ] ... ), ... ) >>> agent_c = MessageFilterAgent( ... name=\"C\", ... wrapped_agent=agent_c_inner, ... filter=MessageFilterConfig( ... per_source=[ ... PerSourceFilter(source=\"user\", position=\"first\", count=1), ... PerSourceFilter(source=\"B\", position=\"last\", count=1), ... ] ... ), ... ) Then define the graph: >>> graph = DiGraph( ... nodes={ ... \"A\": DiGraphNode(name=\"A\", edges=[DiGraphEdge(target=\"B\")]), ... \"B\": DiGraphNode( ... name=\"B\", ... edges=[ ... DiGraphEdge(target=\"C\", condition=\"exit\"), ... DiGraphEdge(target=\"A\", condition=\"loop\"), ... ], ... ), ... \"C\": DiGraphNode(name=\"C\", edges=[]), ... }, ... default_start_node=\"A\", ... ) This will ensure each agent sees only what is needed for its decision or action logic. component_config_schema# alias of MessageFilterAgentConfig component_provider_override: ClassVar[str | None] = 'autogen_agentchat.agents.MessageFilterAgent'# Override the provider string for the component. This should be used to prevent internal module names being a part of the module name. property produced_message_types: Sequence[type[BaseChatMessage]]# The types of messages that the agent produces in the Response.chat_message field. They must be BaseChatMessage types. async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]# Handles incoming messages and returns a response. Note Agents are stateful and the messages passed to this method should be the new messages since the last call to this method. The agent should maintain its state between calls to this method. For example, if the agent needs to remember the previous messages to respond to the current message, it should store the previous messages in the agent state. async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None][source]# Handles incoming messages and returns a stream of messages and and the final item is the response. The base implementation in BaseChatAgent simply calls on_messages() and yields the messages in the response. Note Agents are stateful and the messages passed to this method should be the new messages since the last call to this method. The agent should maintain its state between calls to this method. For example, if the agent needs to remember the previous messages to respond to the current message, it should store the previous messages in the agent state. async on_reset(cancellation_token: CancellationToken) → None[source]# Resets the agent to its initialization state. _to_config() → MessageFilterAgentConfig[source]# Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance. Returns: T – The configuration of the component. classmethod _from_config(config: MessageFilterAgentConfig) → MessageFilterAgent[source]# Create a new instance of the component from a configuration object. Parameters: config (T) – The configuration object. Returns: Self – The new instance of the component. pydantic model MessageFilterConfig[source]# Bases: BaseModel Show JSON schema{ \"title\": \"MessageFilterConfig\", \"type\": \"object\", \"properties\": { \"per_source\": { \"items\": { \"$ref\": \"#/$defs/PerSourceFilter\" }, \"title\": \"Per Source\", \"type\": \"array\" } }, \"$defs\": { \"PerSourceFilter\": { \"properties\": { \"source\": { \"title\": \"Source\", \"type\": \"string\" }, \"position\": { \"anyOf\": [ { \"enum\": [ \"first\", \"last\" ], \"type\": \"string\" }, { \"type\": \"null\" } ], \"default\": null, \"title\": \"Position\" }, \"count\": { \"anyOf\": [ { \"type\": \"integer\" }, { \"type\": \"null\" } ], \"default\": null, \"title\": \"Count\" } }, \"required\": [ \"source\" ], \"title\": \"PerSourceFilter\", \"type\": \"object\" } }, \"required\": [ \"per_source\" ] } Fields: per_source (List[autogen_agentchat.agents._message_filter_agent.PerSourceFilter]) field per_source: List[PerSourceFilter] [Required]# pydantic model PerSourceFilter[source]# Bases: BaseModel Show JSON schema{ \"title\": \"PerSourceFilter\", \"type\": \"object\", \"properties\": { \"source\": { \"title\": \"Source\", \"type\": \"string\" }, \"position\": { \"anyOf\": [ { \"enum\": [ \"first\", \"last\" ], \"type\": \"string\" }, { \"type\": \"null\" } ], \"default\": null, \"title\": \"Position\" }, \"count\": { \"anyOf\": [ { \"type\": \"integer\" }, { \"type\": \"null\" } ], \"default\": null, \"title\": \"Count\" } }, \"required\": [ \"source\" ] } Fields: count (int | None) position (Literal['first', 'last'] | None) source (str) field source: str [Required]# field position: Literal['first', 'last'] | None = None# field count: int | None = None# pydantic model ApprovalRequest[source]# Bases: BaseModel Request for approval of code execution. Show JSON schema{ \"title\": \"ApprovalRequest\", \"description\": \"Request for approval of code execution.\", \"type\": \"object\", \"properties\": { \"code\": { \"title\": \"Code\", \"type\": \"string\" }, \"context\": { \"items\": { \"discriminator\": { \"mapping\": { \"AssistantMessage\": \"#/$defs/AssistantMessage\", \"FunctionExecutionResultMessage\": \"#/$defs/FunctionExecutionResultMessage\", \"SystemMessage\": \"#/$defs/SystemMessage\", \"UserMessage\": \"#/$defs/UserMessage\" }, \"propertyName\": \"type\" }, \"oneOf\": [ { \"$ref\": \"#/$defs/SystemMessage\" }, { \"$ref\": \"#/$defs/UserMessage\" }, { \"$ref\": \"#/$defs/AssistantMessage\" }, { \"$ref\": \"#/$defs/FunctionExecutionResultMessage\" } ] }, \"title\": \"Context\", \"type\": \"array\" } }, \"$defs\": { \"AssistantMessage\": { \"description\": \"Assistant message are sampled from the language model.\", \"properties\": { \"content\": { \"anyOf\": [ { \"type\": \"string\" }, { \"items\": { \"$ref\": \"#/$defs/FunctionCall\" }, \"type\": \"array\" } ], \"title\": \"Content\" }, \"thought\": { \"anyOf\": [ { \"type\": \"string\" }, { \"type\": \"null\" } ], \"default\": null, \"title\": \"Thought\" }, \"source\": { \"title\": \"Source\", \"type\": \"string\" }, \"type\": { \"const\": \"AssistantMessage\", \"default\": \"AssistantMessage\", \"title\": \"Type\", \"type\": \"string\" } }, \"required\": [ \"content\", \"source\" ], \"title\": \"AssistantMessage\", \"type\": \"object\" }, \"FunctionCall\": { \"properties\": { \"id\": { \"title\": \"Id\", \"type\": \"string\" }, \"arguments\": { \"title\": \"Arguments\", \"type\": \"string\" }, \"name\": { \"title\": \"Name\", \"type\": \"string\" } }, \"required\": [ \"id\", \"arguments\", \"name\" ], \"title\": \"FunctionCall\", \"type\": \"object\" }, \"FunctionExecutionResult\": { \"description\": \"Function execution result contains the output of a function call.\", \"properties\": { \"content\": { \"title\": \"Content\", \"type\": \"string\" }, \"name\": { \"title\": \"Name\", \"type\": \"string\" }, \"call_id\": { \"title\": \"Call Id\", \"type\": \"string\" }, \"is_error\": { \"anyOf\": [ { \"type\": \"boolean\" }, { \"type\": \"null\" } ], \"default\": null, \"title\": \"Is Error\" } }, \"required\": [ \"content\", \"name\", \"call_id\" ], \"title\": \"FunctionExecutionResult\", \"type\": \"object\" }, \"FunctionExecutionResultMessage\": { \"description\": \"Function execution result message contains the output of multiple function calls.\", \"properties\": { \"content\": { \"items\": { \"$ref\": \"#/$defs/FunctionExecutionResult\" }, \"title\": \"Content\", \"type\": \"array\" }, \"type\": { \"const\": \"FunctionExecutionResultMessage\", \"default\": \"FunctionExecutionResultMessage\", \"title\": \"Type\", \"type\": \"string\" } }, \"required\": [ \"content\" ], \"title\": \"FunctionExecutionResultMessage\", \"type\": \"object\" }, \"SystemMessage\": { \"description\": \"System message contains instructions for the model coming from the developer.\\n\\n.. note::\\n\\n Open AI is moving away from using 'system' role in favor of 'developer' role.\\n See `Model Spec <https://cdn.openai.com/spec/model-spec-2024-05-08.html#definitions>`_ for more details.\\n However, the 'system' role is still allowed in their API and will be automatically converted to 'developer' role\\n on the server side.\\n So, you can use `SystemMessage` for developer messages.\", \"properties\": { \"content\": { \"title\": \"Content\", \"type\": \"string\" }, \"type\": { \"const\": \"SystemMessage\", \"default\": \"SystemMessage\", \"title\": \"Type\", \"type\": \"string\" } }, \"required\": [ \"content\" ], \"title\": \"SystemMessage\", \"type\": \"object\" }, \"UserMessage\": { \"description\": \"User message contains input from end users, or a catch-all for data provided to the model.\", \"properties\": { \"content\": { \"anyOf\": [ { \"type\": \"string\" }, { \"items\": { \"anyOf\": [ { \"type\": \"string\" }, {} ] }, \"type\": \"array\" } ], \"title\": \"Content\" }, \"source\": { \"title\": \"Source\", \"type\": \"string\" }, \"type\": { \"const\": \"UserMessage\", \"default\": \"UserMessage\", \"title\": \"Type\", \"type\": \"string\" } }, \"required\": [ \"content\", \"source\" ], \"title\": \"UserMessage\", \"type\": \"object\" } }, \"required\": [ \"code\", \"context\" ] } Fields: code (str) context (List[autogen_core.models._types.SystemMessage | autogen_core.models._types.UserMessage | autogen_core.models._types.AssistantMessage | autogen_core.models._types.FunctionExecutionResultMessage]) field code: str [Required]# field context: List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]] [Required]# pydantic model ApprovalResponse[source]# Bases: BaseModel Response to approval request. Show JSON schema{ \"title\": \"ApprovalResponse\", \"description\": \"Response to approval request.\", \"type\": \"object\", \"properties\": { \"approved\": { \"title\": \"Approved\", \"type\": \"boolean\" }, \"reason\": { \"title\": \"Reason\", \"type\": \"string\" } }, \"required\": [ \"approved\", \"reason\" ] } Fields: approved (bool) reason (str) field approved: bool [Required]# field reason: str [Required]# previous autogen_agentchat next autogen_agentchat.base On this page BaseChatAgent BaseChatAgent.component_type BaseChatAgent.name BaseChatAgent.description BaseChatAgent.produced_message_types BaseChatAgent.on_messages() BaseChatAgent.on_messages_stream() BaseChatAgent.run() BaseChatAgent.run_stream() BaseChatAgent.on_reset() BaseChatAgent.on_pause() BaseChatAgent.on_resume() BaseChatAgent.save_state() BaseChatAgent.load_state() BaseChatAgent.close() AssistantAgent AssistantAgent.component_version AssistantAgent.component_config_schema AssistantAgent.component_provider_override AssistantAgent.produced_message_types AssistantAgent.model_context AssistantAgent.on_messages() AssistantAgent.on_messages_stream() AssistantAgent.on_reset() AssistantAgent.save_state() AssistantAgent.load_state() CodeExecutorAgent CodeExecutorAgent.DEFAULT_TERMINAL_DESCRIPTION CodeExecutorAgent.DEFAULT_AGENT_DESCRIPTION CodeExecutorAgent.DEFAULT_SYSTEM_MESSAGE CodeExecutorAgent.NO_CODE_BLOCKS_FOUND_MESSAGE CodeExecutorAgent.DEFAULT_SUPPORTED_LANGUAGES CodeExecutorAgent.component_config_schema CodeExecutorAgent.component_provider_override CodeExecutorAgent.produced_message_types CodeExecutorAgent.model_context CodeExecutorAgent.on_messages() CodeExecutorAgent.on_messages_stream() CodeExecutorAgent.extract_code_blocks_from_messages() CodeExecutorAgent.execute_code_block() CodeExecutorAgent.on_reset() CodeExecutorAgent._to_config() CodeExecutorAgent._from_config() SocietyOfMindAgent SocietyOfMindAgent.component_config_schema SocietyOfMindAgent.component_provider_override SocietyOfMindAgent.DEFAULT_INSTRUCTION SocietyOfMindAgent.DEFAULT_RESPONSE_PROMPT SocietyOfMindAgent.DEFAULT_DESCRIPTION SocietyOfMindAgent.produced_message_types SocietyOfMindAgent.model_context SocietyOfMindAgent.on_messages() SocietyOfMindAgent.on_messages_stream() SocietyOfMindAgent.on_reset() SocietyOfMindAgent.save_state() SocietyOfMindAgent.load_state() SocietyOfMindAgent._to_config() SocietyOfMindAgent._from_config() UserProxyAgent UserProxyAgent.component_type UserProxyAgent.component_provider_override UserProxyAgent.component_config_schema UserProxyAgent.InputRequestContext UserProxyAgent.InputRequestContext.request_id() UserProxyAgent.produced_message_types UserProxyAgent.on_messages() UserProxyAgent.on_messages_stream() UserProxyAgent.on_reset() UserProxyAgent._to_config() UserProxyAgent._from_config() MessageFilterAgent MessageFilterAgent.component_config_schema MessageFilterAgent.component_provider_override MessageFilterAgent.produced_message_types MessageFilterAgent.on_messages() MessageFilterAgent.on_messages_stream() MessageFilterAgent.on_reset() MessageFilterAgent._to_config() MessageFilterAgent._from_config() MessageFilterConfig MessageFilterConfig.per_source PerSourceFilter PerSourceFilter.source PerSourceFilter.position PerSourceFilter.count ApprovalRequest ApprovalRequest.code ApprovalRequest.context ApprovalResponse ApprovalResponse.approved ApprovalResponse.reason Edit on GitHub Show Source",
      "code": "ChatAgent"
    },
    {
      "description": "API Reference autogen_agentchat.agents autogen_agentchat.agents# This module initializes various pre-defined agents provided by the package. BaseChatAgent is the base class for all agents in AgentChat. class BaseChatAgent(name: str, description: str)[source]# Bases: ChatAgent, ABC, ComponentBase[BaseModel] Base class for a chat agent. This abstract class provides a base implementation for a ChatAgent. To create a new chat agent, subclass this class and implement the on_messages(), on_reset(), and produced_message_types. If streaming is required, also implement the on_messages_stream() method. An agent is considered stateful and maintains its state between calls to the on_messages() or on_messages_stream() methods. The agent should store its state in the agent instance. The agent should also implement the on_reset() method to reset the agent to its initialization state. Note The caller should only pass the new messages to the agent on each call to the on_messages() or on_messages_stream() method. Do not pass the entire conversation history to the agent on each call. This design principle must be followed when creating a new agent. component_type: ClassVar[ComponentType] = 'agent'# The logical type of the component. property name: str# The name of the agent. This is used by team to uniquely identify the agent. It should be unique within the team. property description: str# The description of the agent. This is used by team to make decisions about which agents to use. The description should describe the agent’s capabilities and how to interact with it. abstract property produced_message_types: Sequence[type[BaseChatMessage]]# The types of messages that the agent produces in the Response.chat_message field. They must be BaseChatMessage types. abstract async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]# Handles incoming messages and returns a response. Note Agents are stateful and the messages passed to this method should be the new messages since the last call to this method. The agent should maintain its state between calls to this method. For example, if the agent needs to remember the previous messages to respond to the current message, it should store the previous messages in the agent state. async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None][source]# Handles incoming messages and returns a stream of messages and and the final item is the response. The base implementation in BaseChatAgent simply calls on_messages() and yields the messages in the response. Note Agents are stateful and the messages passed to this method should be the new messages since the last call to this method. The agent should maintain its state between calls to this method. For example, if the agent needs to remember the previous messages to respond to the current message, it should store the previous messages in the agent state. async run(*, task: str | BaseChatMessage | Sequence[BaseChatMessage] | None = None, cancellation_token: CancellationToken | None = None, output_task_messages: bool = True) → TaskResult[source]# Run the agent with the given task and return the result. async run_stream(*, task: str | BaseChatMessage | Sequence[BaseChatMessage] | None = None, cancellation_token: CancellationToken | None = None, output_task_messages: bool = True) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | TaskResult, None][source]# Run the agent with the given task and return a stream of messages and the final task result as the last item in the stream. Parameters: task – The task to run. Can be a string, a single message, or a sequence of messages. cancellation_token – The cancellation token to kill the task immediately. output_task_messages – Whether to include task messages in the output stream. Defaults to True for backward compatibility. abstract async on_reset(cancellation_token: CancellationToken) → None[source]# Resets the agent to its initialization state. async on_pause(cancellation_token: CancellationToken) → None[source]# Called when the agent is paused while running in its on_messages() or on_messages_stream() method. This is a no-op by default in the BaseChatAgent class. Subclasses can override this method to implement custom pause behavior. async on_resume(cancellation_token: CancellationToken) → None[source]# Called when the agent is resumed from a pause while running in its on_messages() or on_messages_stream() method. This is a no-op by default in the BaseChatAgent class. Subclasses can override this method to implement custom resume behavior. async save_state() → Mapping[str, Any][source]# Export state. Default implementation for stateless agents. async load_state(state: Mapping[str, Any]) → None[source]# Restore agent from saved state. Default implementation for stateless agents. async close() → None[source]# Release any resources held by the agent. This is a no-op by default in the BaseChatAgent class. Subclasses can override this method to implement custom close behavior. class AssistantAgent(name: str, model_client: ChatCompletionClient, *, tools: List[BaseTool[Any, Any] | Callable[[...], Any] | Callable[[...], Awaitable[Any]]] | None = None, workbench: Workbench | Sequence[Workbench] | None = None, handoffs: List[Handoff | str] | None = None, model_context: ChatCompletionContext | None = None, description: str = 'An agent that provides assistance with ability to use tools.', system_message: str | None = 'You are a helpful AI assistant. Solve tasks using your tools. Reply with TERMINATE when the task has been completed.', model_client_stream: bool = False, reflect_on_tool_use: bool | None = None, max_tool_iterations: int = 1, tool_call_summary_format: str = '{result}', tool_call_summary_formatter: Callable[[FunctionCall, FunctionExecutionResult], str] | None = None, output_content_type: type[BaseModel] | None = None, output_content_type_format: str | None = None, memory: Sequence[Memory] | None = None, metadata: Dict[str, str] | None = None)[source]# Bases: BaseChatAgent, Component[AssistantAgentConfig] An agent that provides assistance with tool use. The on_messages() returns a Response in which chat_message is the final response message. The on_messages_stream() creates an async generator that produces the inner messages as they are created, and the Response object as the last item before closing the generator. The BaseChatAgent.run() method returns a TaskResult containing the messages produced by the agent. In the list of messages, messages, the last message is the final response message. The BaseChatAgent.run_stream() method creates an async generator that produces the inner messages as they are created, and the TaskResult object as the last item before closing the generator. Attention The caller must only pass the new messages to the agent on each call to the on_messages(), on_messages_stream(), BaseChatAgent.run(), or BaseChatAgent.run_stream() methods. The agent maintains its state between calls to these methods. Do not pass the entire conversation history to the agent on each call. Warning The assistant agent is not thread-safe or coroutine-safe. It should not be shared between multiple tasks or coroutines, and it should not call its methods concurrently. The following diagram shows how the assistant agent works: Structured output: If the output_content_type is set, the agent will respond with a StructuredMessage instead of a TextMessage in the final response by default. Note Currently, setting output_content_type prevents the agent from being able to call load_component and dum_component methods for serializable configuration. This will be fixed soon in the future. Tool call behavior: If the model returns no tool call, then the response is immediately returned as a TextMessage or a StructuredMessage (when using structured output) in chat_message. This ends the tool call iteration loop regardless of the max_tool_iterations setting. When the model returns tool calls, they will be executed right away: When reflect_on_tool_use is False, the tool call results are returned as a ToolCallSummaryMessage in chat_message. You can customise the summary with either a static format string (tool_call_summary_format) or a callable (tool_call_summary_formatter); the callable is evaluated once per tool call. When reflect_on_tool_use is True, the another model inference is made using the tool calls and results, and final response is returned as a TextMessage or a StructuredMessage (when using structured output) in chat_message. reflect_on_tool_use is set to True by default when output_content_type is set. reflect_on_tool_use is set to False by default when output_content_type is not set. If the model returns multiple tool calls, they will be executed concurrently. To disable parallel tool calls you need to configure the model client. For example, set parallel_tool_calls=False for OpenAIChatCompletionClient and AzureOpenAIChatCompletionClient. The max_tool_iterations parameter controls how many sequential tool call iterations the agent can perform in a single run. When set to 1 (default), the agent executes tool calls once and returns the result. When set higher, the agent can make additional model calls to execute more tool calls if the model continues to request them, enabling multi-step tool-based workflows. The agent stops when either the model returns a text response (instead of tool calls) or the maximum number of iterations is reached. Tip By default, the tool call results are returned as the response when tool calls are made, so pay close attention to how the tools’ return values are formatted—especially if another agent expects a specific schema. Use `tool_call_summary_format` for a simple static template. Use `tool_call_summary_formatter` for full programmatic control (e.g., “hide large success payloads, show full details on error”). Note: tool_call_summary_formatter is not serializable and will be ignored when an agent is loaded from, or exported to, YAML/JSON configuration files. Hand off behavior: If a handoff is triggered, a HandoffMessage will be returned in chat_message. If there are tool calls, they will also be executed right away before returning the handoff. The tool calls and results are passed to the target agent through context. Note If multiple handoffs are detected, only the first handoff is executed. To avoid this, disable parallel tool calls in the model client configuration. Limit context size sent to the model: You can limit the number of messages sent to the model by setting the model_context parameter to a BufferedChatCompletionContext. This will limit the number of recent messages sent to the model and can be useful when the model has a limit on the number of tokens it can process. Another option is to use a TokenLimitedChatCompletionContext which will limit the number of tokens sent to the model. You can also create your own model context by subclassing ChatCompletionContext. Streaming mode: The assistant agent can be used in streaming mode by setting model_client_stream=True. In this mode, the on_messages_stream() and BaseChatAgent.run_stream() methods will also yield ModelClientStreamingChunkEvent messages as the model client produces chunks of response. The chunk messages will not be included in the final response’s inner messages. Parameters: name (str) – The name of the agent. model_client (ChatCompletionClient) – The model client to use for inference. tools (List[BaseTool[Any, Any] | Callable[..., Any] | Callable[..., Awaitable[Any]]] | None, optional) – The tools to register with the agent. workbench (Workbench | Sequence[Workbench] | None, optional) – The workbench or list of workbenches to use for the agent. Tools cannot be used when workbench is set and vice versa. handoffs (List[HandoffBase | str] | None, optional) – The handoff configurations for the agent, allowing it to transfer to other agents by responding with a HandoffMessage. The transfer is only executed when the team is in Swarm. If a handoff is a string, it should represent the target agent’s name. model_context (ChatCompletionContext | None, optional) – The model context for storing and retrieving LLMMessage. It can be preloaded with initial messages. The initial messages will be cleared when the agent is reset. description (str, optional) – The description of the agent. system_message (str, optional) – The system message for the model. If provided, it will be prepended to the messages in the model context when making an inference. Set to None to disable. model_client_stream (bool, optional) – If True, the model client will be used in streaming mode. on_messages_stream() and BaseChatAgent.run_stream() methods will also yield ModelClientStreamingChunkEvent messages as the model client produces chunks of response. Defaults to False. reflect_on_tool_use (bool, optional) – If True, the agent will make another model inference using the tool call and result to generate a response. If False, the tool call result will be returned as the response. By default, if output_content_type is set, this will be True; if output_content_type is not set, this will be False. output_content_type (type[BaseModel] | None, optional) – The output content type for StructuredMessage response as a Pydantic model. This will be used with the model client to generate structured output. If this is set, the agent will respond with a StructuredMessage instead of a TextMessage in the final response, unless reflect_on_tool_use is False and a tool call is made. output_content_type_format (str | None, optional) – (Experimental) The format string used for the content of a StructuredMessage response. max_tool_iterations (int, optional) – The maximum number of tool iterations to perform until the model stops making tool calls. Defaults to 1, which means the agent will only execute the tool calls made by the model once, and return the result as a ToolCallSummaryMessage, or a TextMessage or a StructuredMessage (when using structured output) in chat_message as the final response. As soon as the model stops making tool calls, the agent will stop executing tool calls and return the result as the final response. The value must be greater than or equal to 1. tool_call_summary_format (str, optional) – Static format string applied to each tool call result when composing the ToolCallSummaryMessage. Defaults to \"{result}\". Ignored if tool_call_summary_formatter is provided. When reflect_on_tool_use is False, the summaries for all tool calls are concatenated with a newline (’n’) and returned as the response. Placeholders available in the template: {tool_name}, {arguments}, {result}, {is_error}. tool_call_summary_formatter (Callable[[FunctionCall, FunctionExecutionResult], str] | None, optional) – Callable that receives the FunctionCall and its FunctionExecutionResult and returns the summary string. Overrides tool_call_summary_format when supplied and allows conditional logic — for example, emitting static string like \"Tool FooBar executed successfully.\" on success and a full payload (including all passed arguments etc.) only on failure. Limitation: The callable is not serializable; values provided via YAML/JSON configs are ignored. Note tool_call_summary_formatter is intended for in-code use only. It cannot currently be saved or restored via configuration files. memory (Sequence[Memory] | None, optional): The memory store to use for the agent. Defaults to None. metadata (Dict[str, str] | None, optional): Optional metadata for tracking. Raises: ValueError – If tool names are not unique. ValueError – If handoff names are not unique. ValueError – If handoff names are not unique from tool names. ValueError – If maximum number of tool iterations is less than 1. Examples Example 1: basic agent The following example demonstrates how to create an assistant agent with a model client and generate a response to a simple task. import asyncio from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_agentchat.agents import AssistantAgent async def main() -> None: model_client = OpenAIChatCompletionClient( model=\"gpt-4o\", # api_key = \"your_openai_api_key\" ) agent = AssistantAgent(name=\"assistant\", model_client=model_client) result = await agent.run(task=\"Name two cities in North America.\") print(result) asyncio.run(main()) Example 2: model client token streaming This example demonstrates how to create an assistant agent with a model client and generate a token stream by setting model_client_stream=True. import asyncio from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_agentchat.agents import AssistantAgent async def main() -> None: model_client = OpenAIChatCompletionClient( model=\"gpt-4o\", # api_key = \"your_openai_api_key\" ) agent = AssistantAgent( name=\"assistant\", model_client=model_client, model_client_stream=True, ) stream = agent.run_stream(task=\"Name two cities in North America.\") async for message in stream: print(message) asyncio.run(main()) source='user' models_usage=None metadata={} content='Name two cities in North America.' type='TextMessage' source='assistant' models_usage=None metadata={} content='Two' type='ModelClientStreamingChunkEvent' source='assistant' models_usage=None metadata={} content=' cities' type='ModelClientStreamingChunkEvent' source='assistant' models_usage=None metadata={} content=' in' type='ModelClientStreamingChunkEvent' source='assistant' models_usage=None metadata={} content=' North' type='ModelClientStreamingChunkEvent' source='assistant' models_usage=None metadata={} content=' America' type='ModelClientStreamingChunkEvent' source='assistant' models_usage=None metadata={} content=' are' type='ModelClientStreamingChunkEvent' source='assistant' models_usage=None metadata={} content=' New' type='ModelClientStreamingChunkEvent' source='assistant' models_usage=None metadata={} content=' York' type='ModelClientStreamingChunkEvent' source='assistant' models_usage=None metadata={} content=' City' type='ModelClientStreamingChunkEvent' source='assistant' models_usage=None metadata={} content=' and' type='ModelClientStreamingChunkEvent' source='assistant' models_usage=None metadata={} content=' Toronto' type='ModelClientStreamingChunkEvent' source='assistant' models_usage=None metadata={} content='.' type='ModelClientStreamingChunkEvent' source='assistant' models_usage=None metadata={} content=' TERMIN' type='ModelClientStreamingChunkEvent' source='assistant' models_usage=None metadata={} content='ATE' type='ModelClientStreamingChunkEvent' source='assistant' models_usage=RequestUsage(prompt_tokens=0, completion_tokens=0) metadata={} content='Two cities in North America are New York City and Toronto. TERMINATE' type='TextMessage' messages=[TextMessage(source='user', models_usage=None, metadata={}, content='Name two cities in North America.', type='TextMessage'), TextMessage(source='assistant', models_usage=RequestUsage(prompt_tokens=0, completion_tokens=0), metadata={}, content='Two cities in North America are New York City and Toronto. TERMINATE', type='TextMessage')] stop_reason=None Example 3: agent with tools The following example demonstrates how to create an assistant agent with a model client and a tool, generate a stream of messages for a task, and print the messages to the console using Console. The tool is a simple function that returns the current time. Under the hood, the function is wrapped in a FunctionTool and used with the agent’s model client. The doc string of the function is used as the tool description, the function name is used as the tool name, and the function signature including the type hints is used as the tool arguments. import asyncio from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.ui import Console async def get_current_time() -> str: return \"The current time is 12:00 PM.\" async def main() -> None: model_client = OpenAIChatCompletionClient( model=\"gpt-4o\", # api_key = \"your_openai_api_key\" ) agent = AssistantAgent(name=\"assistant\", model_client=model_client, tools=[get_current_time]) await Console(agent.run_stream(task=\"What is the current time?\")) asyncio.run(main()) Example 4: agent with max_tool_iterations The following example demonstrates how to use the max_tool_iterations parameter to control how many times the agent can execute tool calls in a single run. This is useful when you want the agent to perform multiple sequential tool operations to reach a goal. import asyncio from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.ui import Console # Global counter state counter = 0 def increment_counter() -> str: \"\"\"Increment the counter by 1 and return the current value.\"\"\" global counter counter += 1 return f\"Counter incremented to: {counter}\" def get_counter() -> str: \"\"\"Get the current counter value.\"\"\" global counter return f\"Current counter value: {counter}\" async def main() -> None: model_client = OpenAIChatCompletionClient( model=\"gpt-4o\", # api_key = \"your_openai_api_key\" ) # Create agent with max_tool_iterations=5 to allow multiple tool calls agent = AssistantAgent( name=\"assistant\", model_client=model_client, tools=[increment_counter, get_counter], max_tool_iterations=5, # Allow up to 5 tool call iterations reflect_on_tool_use=True, # Get a final summary after tool calls ) await Console(agent.run_stream(task=\"Increment the counter 3 times and then tell me the final value.\")) asyncio.run(main()) Example 5: agent with Model-Context Protocol (MCP) workbench The following example demonstrates how to create an assistant agent with a model client and an McpWorkbench for interacting with a Model-Context Protocol (MCP) server. import asyncio from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.ui import Console from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_ext.tools.mcp import StdioServerParams, McpWorkbench async def main() -> None: params = StdioServerParams( command=\"uvx\", args=[\"mcp-server-fetch\"], read_timeout_seconds=60, ) # You can also use `start()` and `stop()` to manage the session. async with McpWorkbench(server_params=params) as workbench: model_client = OpenAIChatCompletionClient(model=\"gpt-4.1-nano\") assistant = AssistantAgent( name=\"Assistant\", model_client=model_client, workbench=workbench, reflect_on_tool_use=True, ) await Console( assistant.run_stream(task=\"Go to https://github.com/microsoft/autogen and tell me what you see.\") ) asyncio.run(main()) Example 6: agent with structured output and tool The following example demonstrates how to create an assistant agent with a model client configured to use structured output and a tool. Note that you need to use FunctionTool to create the tool and the strict=True is required for structured output mode. Because the model is configured to use structured output, the output reflection response will be a JSON formatted string. import asyncio from typing import Literal from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.ui import Console from autogen_core.tools import FunctionTool from autogen_ext.models.openai import OpenAIChatCompletionClient from pydantic import BaseModel # Define the structured output format. class AgentResponse(BaseModel): thoughts: str response: Literal[\"happy\", \"sad\", \"neutral\"] # Define the function to be called as a tool. def sentiment_analysis(text: str) -> str: \"\"\"Given a text, return the sentiment.\"\"\" return \"happy\" if \"happy\" in text else \"sad\" if \"sad\" in text else \"neutral\" # Create a FunctionTool instance with `strict=True`, # which is required for structured output mode. tool = FunctionTool(sentiment_analysis, description=\"Sentiment Analysis\", strict=True) # Create an OpenAIChatCompletionClient instance that supports structured output. model_client = OpenAIChatCompletionClient( model=\"gpt-4o-mini\", ) # Create an AssistantAgent instance that uses the tool and model client. agent = AssistantAgent( name=\"assistant\", model_client=model_client, tools=[tool], system_message=\"Use the tool to analyze sentiment.\", output_content_type=AgentResponse, ) async def main() -> None: stream = agent.run_stream(task=\"I am happy today!\") await Console(stream) asyncio.run(main()) ---------- assistant ---------- [FunctionCall(id='call_tIZjAVyKEDuijbBwLY6RHV2p', arguments='{\"text\":\"I am happy today!\"}', name='sentiment_analysis')] ---------- assistant ---------- [FunctionExecutionResult(content='happy', call_id='call_tIZjAVyKEDuijbBwLY6RHV2p', is_error=False)] ---------- assistant ---------- {\"thoughts\":\"The user expresses a clear positive emotion by stating they are happy today, suggesting an upbeat mood.\",\"response\":\"happy\"} Example 7: agent with bounded model context The following example shows how to use a BufferedChatCompletionContext that only keeps the last 2 messages (1 user + 1 assistant). Bounded model context is useful when the model has a limit on the number of tokens it can process. import asyncio from autogen_agentchat.agents import AssistantAgent from autogen_core.model_context import BufferedChatCompletionContext from autogen_ext.models.openai import OpenAIChatCompletionClient async def main() -> None: # Create a model client. model_client = OpenAIChatCompletionClient( model=\"gpt-4o-mini\", # api_key = \"your_openai_api_key\" ) # Create a model context that only keeps the last 2 messages (1 user + 1 assistant). model_context = BufferedChatCompletionContext(buffer_size=2) # Create an AssistantAgent instance with the model client and context. agent = AssistantAgent( name=\"assistant\", model_client=model_client, model_context=model_context, system_message=\"You are a helpful assistant.\", ) result = await agent.run(task=\"Name two cities in North America.\") print(result.messages[-1].content) # type: ignore result = await agent.run(task=\"My favorite color is blue.\") print(result.messages[-1].content) # type: ignore result = await agent.run(task=\"Did I ask you any question?\") print(result.messages[-1].content) # type: ignore asyncio.run(main()) Two cities in North America are New York City and Toronto. That's great! Blue is often associated with calmness and serenity. Do you have a specific shade of blue that you like, or any particular reason why it's your favorite? No, you didn't ask a question. I apologize for any misunderstanding. If you have something specific you'd like to discuss or ask, feel free to let me know! Example 8: agent with memory The following example shows how to use a list-based memory with the assistant agent. The memory is preloaded with some initial content. Under the hood, the memory is used to update the model context before making an inference, using the update_context() method. import asyncio from autogen_agentchat.agents import AssistantAgent from autogen_core.memory import ListMemory, MemoryContent from autogen_ext.models.openai import OpenAIChatCompletionClient async def main() -> None: # Create a model client. model_client = OpenAIChatCompletionClient( model=\"gpt-4o-mini\", # api_key = \"your_openai_api_key\" ) # Create a list-based memory with some initial content. memory = ListMemory() await memory.add(MemoryContent(content=\"User likes pizza.\", mime_type=\"text/plain\")) await memory.add(MemoryContent(content=\"User dislikes cheese.\", mime_type=\"text/plain\")) # Create an AssistantAgent instance with the model client and memory. agent = AssistantAgent( name=\"assistant\", model_client=model_client, memory=[memory], system_message=\"You are a helpful assistant.\", ) result = await agent.run(task=\"What is a good dinner idea?\") print(result.messages[-1].content) # type: ignore asyncio.run(main()) How about making a delicious pizza without cheese? You can create a flavorful veggie pizza with a variety of toppings. Here's a quick idea: **Veggie Tomato Sauce Pizza** - Start with a pizza crust (store-bought or homemade). - Spread a layer of marinara or tomato sauce evenly over the crust. - Top with your favorite vegetables like bell peppers, mushrooms, onions, olives, and spinach. - Add some protein if you'd like, such as grilled chicken or pepperoni (ensure it's cheese-free). - Sprinkle with herbs like oregano and basil, and maybe a drizzle of olive oil. - Bake according to the crust instructions until the edges are golden and the veggies are cooked. Serve it with a side salad or some garlic bread to complete the meal! Enjoy your dinner! Example 9: agent with `o1-mini` The following example shows how to use o1-mini model with the assistant agent. import asyncio from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_agentchat.agents import AssistantAgent async def main() -> None: model_client = OpenAIChatCompletionClient( model=\"o1-mini\", # api_key = \"your_openai_api_key\" ) # The system message is not supported by the o1 series model. agent = AssistantAgent(name=\"assistant\", model_client=model_client, system_message=None) result = await agent.run(task=\"What is the capital of France?\") print(result.messages[-1].content) # type: ignore asyncio.run(main()) Note The o1-preview and o1-mini models do not support system message and function calling. So the system_message should be set to None and the tools and handoffs should not be set. See o1 beta limitations for more details. Example 10: agent using reasoning model with custom model context. The following example shows how to use a reasoning model (DeepSeek R1) with the assistant agent. The model context is used to filter out the thought field from the assistant message. import asyncio from typing import List from autogen_agentchat.agents import AssistantAgent from autogen_core.model_context import UnboundedChatCompletionContext from autogen_core.models import AssistantMessage, LLMMessage, ModelFamily from autogen_ext.models.ollama import OllamaChatCompletionClient class ReasoningModelContext(UnboundedChatCompletionContext): \"\"\"A model context for reasoning models.\"\"\" async def get_messages(self) -> List[LLMMessage]: messages = await super().get_messages() # Filter out thought field from AssistantMessage. messages_out: List[LLMMessage] = [] for message in messages: if isinstance(message, AssistantMessage): message.thought = None messages_out.append(message) return messages_out # Create an instance of the model client for DeepSeek R1 hosted locally on Ollama. model_client = OllamaChatCompletionClient( model=\"deepseek-r1:8b\", model_info={ \"vision\": False, \"function_calling\": False, \"json_output\": False, \"family\": ModelFamily.R1, \"structured_output\": True, }, ) agent = AssistantAgent( \"reasoning_agent\", model_client=model_client, model_context=ReasoningModelContext(), # Use the custom model context. ) async def run_reasoning_agent() -> None: result = await agent.run(task=\"What is the capital of France?\") print(result) asyncio.run(run_reasoning_agent()) For detailed examples and usage, see the Examples section below. component_version: ClassVar[int] = 2# The version of the component, if schema incompatibilities are introduced this should be updated. component_config_schema# alias of AssistantAgentConfig component_provider_override: ClassVar[str | None] = 'autogen_agentchat.agents.AssistantAgent'# Override the provider string for the component. This should be used to prevent internal module names being a part of the module name. property produced_message_types: Sequence[type[BaseChatMessage]]# Get the types of messages this agent can produce. Returns: Sequence of message types this agent can generate property model_context: ChatCompletionContext# Get the model context used by this agent. Returns: The chat completion context for this agent async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]# Process incoming messages and generate a response. Parameters: messages – Sequence of messages to process cancellation_token – Token for cancelling operation Returns: Response containing the agent’s reply async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None][source]# Process messages and stream the response. Parameters: messages – Sequence of messages to process cancellation_token – Token for cancelling operation Yields: Events, messages and final response during processing async on_reset(cancellation_token: CancellationToken) → None[source]# Reset the assistant agent to its initialization state. async save_state() → Mapping[str, Any][source]# Save the current state of the assistant agent. async load_state(state: Mapping[str, Any]) → None[source]# Load the state of the assistant agent class CodeExecutorAgent(name: str, code_executor: CodeExecutor, *, model_client: ChatCompletionClient | None = None, model_context: ChatCompletionContext | None = None, model_client_stream: bool = False, max_retries_on_error: int = 0, description: str | None = None, system_message: str | None = DEFAULT_SYSTEM_MESSAGE, sources: Sequence[str] | None = None, supported_languages: List[str] | None = None, approval_func: Callable[[ApprovalRequest], ApprovalResponse] | Callable[[ApprovalRequest], Awaitable[ApprovalResponse]] | None = None)[source]# Bases: BaseChatAgent, Component[CodeExecutorAgentConfig] (Experimental) An agent that generates and executes code snippets based on user instructions. Note This agent is experimental and may change in future releases. It is typically used within a team with another agent that generates code snippets to be executed or alone with model_client provided so that it can generate code based on user query, execute it and reflect on the code result. When used with model_client, it will generate code snippets using the model and execute them using the provided code_executor. The model will also reflect on the code execution results. The agent will yield the final reflection result from the model as the final response. When used without model_client, it will only execute code blocks found in TextMessage messages and returns the output of the code execution. Note Using AssistantAgent with PythonCodeExecutionTool is an alternative to this agent. However, the model for that agent will have to generate properly escaped code string as a parameter to the tool. Parameters: name (str) – The name of the agent. code_executor (CodeExecutor) – The code executor responsible for executing code received in messages (DockerCommandLineCodeExecutor recommended. See example below) model_client (ChatCompletionClient, optional) – The model client to use for inference and generating code. If not provided, the agent will only execute code blocks found in input messages. Currently, the model must support structured output mode, which is required for the automatic retry mechanism to work. model_client_stream (bool, optional) – If True, the model client will be used in streaming mode. on_messages_stream() and BaseChatAgent.run_stream() methods will also yield ModelClientStreamingChunkEvent messages as the model client produces chunks of response. Defaults to False. description (str, optional) – The description of the agent. If not provided, DEFAULT_AGENT_DESCRIPTION will be used. system_message (str, optional) – The system message for the model. If provided, it will be prepended to the messages in the model context when making an inference. Set to None to disable. Defaults to DEFAULT_SYSTEM_MESSAGE. This is only used if model_client is provided. sources (Sequence[str], optional) – Check only messages from the specified agents for the code to execute. This is useful when the agent is part of a group chat and you want to limit the code execution to messages from specific agents. If not provided, all messages will be checked for code blocks. This is only used if model_client is not provided. max_retries_on_error (int, optional) – The maximum number of retries on error. If the code execution fails, the agent will retry up to this number of times. If the code execution fails after this number of retries, the agent will yield a reflection result. supported_languages (List[str], optional) – List of programming languages that will be parsed and executed from agent response; others will be ignored. Defaults to DEFAULT_SUPPORTED_LANGUAGES. approval_func (Optional[Union[Callable[[ApprovalRequest], ApprovalResponse], Callable[[ApprovalRequest], Awaitable[ApprovalResponse]]]], optional) – A function that is called before each code execution to get approval. The function takes an ApprovalRequest containing the code to be executed and the current context, and returns an ApprovalResponse. The function can be either synchronous or asynchronous. If None (default), all code executions are automatically approved. If set, the agent cannot be serialized using dump_component(). Note It is recommended that the CodeExecutorAgent agent uses a Docker container to execute code. This ensures that model-generated code is executed in an isolated environment. To use Docker, your environment must have Docker installed and running. Follow the installation instructions for Docker. Note The code executor only processes code that is properly formatted in markdown code blocks using triple backticks. For example: ```python print(\"Hello World\") ``` # or ```sh echo \"Hello World\" ``` In this example, we show how to set up a CodeExecutorAgent agent that uses the DockerCommandLineCodeExecutor to execute code snippets in a Docker container. The work_dir parameter indicates where all executed files are first saved locally before being executed in the Docker container. import asyncio from autogen_agentchat.agents import CodeExecutorAgent, ApprovalRequest, ApprovalResponse from autogen_agentchat.messages import TextMessage from autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor from autogen_core import CancellationToken def simple_approval_func(request: ApprovalRequest) -> ApprovalResponse: \"\"\"Simple approval function that requests user input for code execution approval.\"\"\" print(\"Code execution approval requested:\") print(\"=\" * 50) print(request.code) print(\"=\" * 50) while True: user_input = input(\"Do you want to execute this code? (y/n): \").strip().lower() if user_input in ['y', 'yes']: return ApprovalResponse(approved=True, reason='Approved by user') elif user_input in ['n', 'no']: return ApprovalResponse(approved=False, reason='Denied by user') else: print(\"Please enter 'y' for yes or 'n' for no.\") async def run_code_executor_agent() -> None: # Create a code executor agent that uses a Docker container to execute code. code_executor = DockerCommandLineCodeExecutor(work_dir=\"coding\") await code_executor.start() code_executor_agent = CodeExecutorAgent( \"code_executor\", code_executor=code_executor, approval_func=simple_approval_func ) # Run the agent with a given code snippet. task = TextMessage( content='''Here is some code ```python print('Hello world') ``` ''', source=\"user\", ) response = await code_executor_agent.on_messages([task], CancellationToken()) print(response.chat_message) # Stop the code executor. await code_executor.stop() asyncio.run(run_code_executor_agent()) In this example, we show how to set up a CodeExecutorAgent agent that uses the DeviceRequest to expose a GPU to the container for cuda-accelerated code execution. import asyncio from autogen_agentchat.agents import CodeExecutorAgent from autogen_agentchat.messages import TextMessage from autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor from autogen_core import CancellationToken from docker.types import DeviceRequest async def run_code_executor_agent() -> None: # Create a code executor agent that uses a Docker container to execute code. code_executor = DockerCommandLineCodeExecutor( work_dir=\"coding\", device_requests=[DeviceRequest(count=-1, capabilities=[[\"gpu\"]])] ) await code_executor.start() code_executor_agent = CodeExecutorAgent(\"code_executor\", code_executor=code_executor) # Display the GPU information task = TextMessage( content='''Here is some code ```sh nvidia-smi ``` ''', source=\"user\", ) response = await code_executor_agent.on_messages([task], CancellationToken()) print(response.chat_message) # Stop the code executor. await code_executor.stop() asyncio.run(run_code_executor_agent()) In the following example, we show how to setup CodeExecutorAgent without model_client parameter for executing code blocks generated by other agents in a group chat using DockerCommandLineCodeExecutor import asyncio from autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_agentchat.agents import AssistantAgent, CodeExecutorAgent, ApprovalRequest, ApprovalResponse from autogen_agentchat.conditions import MaxMessageTermination from autogen_agentchat.teams import RoundRobinGroupChat from autogen_agentchat.ui import Console termination_condition = MaxMessageTermination(3) def group_chat_approval_func(request: ApprovalRequest) -> ApprovalResponse: \"\"\"Approval function for group chat that allows basic Python operations.\"\"\" # Allow common safe operations safe_operations = [\"print(\", \"import \", \"def \", \"class \", \"if \", \"for \", \"while \"] if any(op in request.code for op in safe_operations): return ApprovalResponse(approved=True, reason='Safe Python operation') # Deny file system operations in group chat dangerous_operations = [\"open(\", \"file(\", \"os.\", \"subprocess\", \"eval(\", \"exec(\"] if any(op in request.code for op in dangerous_operations): return ApprovalResponse(approved=False, reason='File system or dangerous operation not allowed') return ApprovalResponse(approved=True, reason='Operation approved') async def main() -> None: model_client = OpenAIChatCompletionClient(model=\"gpt-4o\") # define the Docker CLI Code Executor code_executor = DockerCommandLineCodeExecutor(work_dir=\"coding\") # start the execution container await code_executor.start() code_executor_agent = CodeExecutorAgent( \"code_executor_agent\", code_executor=code_executor, approval_func=group_chat_approval_func ) coder_agent = AssistantAgent(\"coder_agent\", model_client=model_client) groupchat = RoundRobinGroupChat( participants=[coder_agent, code_executor_agent], termination_condition=termination_condition ) task = \"Write python code to print Hello World!\" await Console(groupchat.run_stream(task=task)) # stop the execution container await code_executor.stop() asyncio.run(main()) ---------- user ---------- Write python code to print Hello World! ---------- coder_agent ---------- Certainly! Here's a simple Python code to print \"Hello World!\": ```python print(\"Hello World!\") ``` You can run this code in any Python environment to display the message. ---------- code_executor_agent ---------- Hello World! In the following example, we show how to setup CodeExecutorAgent with model_client that can generate its own code without the help of any other agent and executing it in DockerCommandLineCodeExecutor. It also demonstrates using a model-based approval function that reviews the code for safety before execution. import asyncio from autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_core.models import SystemMessage, UserMessage from autogen_agentchat.agents import CodeExecutorAgent, ApprovalRequest, ApprovalResponse from autogen_agentchat.conditions import TextMessageTermination from autogen_agentchat.ui import Console termination_condition = TextMessageTermination(\"code_executor_agent\") async def main() -> None: model_client = OpenAIChatCompletionClient(model=\"gpt-4o\") async def model_client_approval_func(request: ApprovalRequest) -> ApprovalResponse: instruction = \"Approve or reject the code in the last message based on whether it is dangerous or not. Use the following JSON format for your response: {approved: true/false, reason: 'your reason here'}\" response = await model_client.create( messages=[SystemMessage(content=instruction)] + request.context + [UserMessage(content=request.code, source=\"user\")], json_output=ApprovalResponse, ) assert isinstance(response.content, str) return ApprovalResponse.model_validate_json(response.content) # define the Docker CLI Code Executor code_executor = DockerCommandLineCodeExecutor(work_dir=\"coding\") # start the execution container await code_executor.start() code_executor_agent = CodeExecutorAgent( \"code_executor_agent\", code_executor=code_executor, model_client=model_client, approval_func=model_client_approval_func, ) task = \"Write python code to print Hello World!\" await Console(code_executor_agent.run_stream(task=task)) # stop the execution container await code_executor.stop() asyncio.run(main()) ---------- user ---------- Write python code to print Hello World! ---------- code_executor_agent ---------- Certainly! Here is a simple Python code to print \"Hello World!\" to the console: ```python print(\"Hello World!\") ``` Let's execute it to confirm the output. ---------- code_executor_agent ---------- Hello World! ---------- code_executor_agent ---------- The code has been executed successfully, and it printed \"Hello World!\" as expected. If you have any more requests or questions, feel free to ask! DEFAULT_TERMINAL_DESCRIPTION = 'A computer terminal that performs no other action than running Python scripts (provided to it quoted in ```python code blocks), or sh shell scripts (provided to it quoted in ```sh code blocks).'# DEFAULT_AGENT_DESCRIPTION = 'A Code Execution Agent that generates and executes Python and shell scripts based on user instructions. It ensures correctness, efficiency, and minimal errors while gracefully handling edge cases.'# DEFAULT_SYSTEM_MESSAGE = 'You are a Code Execution Agent. Your role is to generate and execute Python code and shell scripts based on user instructions, ensuring correctness, efficiency, and minimal errors. Handle edge cases gracefully. Python code should be provided in ```python code blocks, and sh shell scripts should be provided in ```sh code blocks for execution.'# NO_CODE_BLOCKS_FOUND_MESSAGE = 'No code blocks found in the thread. Please provide at least one markdown-encoded code block to execute (i.e., quoting code in ```python or ```sh code blocks).'# DEFAULT_SUPPORTED_LANGUAGES = ['python', 'sh']# component_config_schema# alias of CodeExecutorAgentConfig component_provider_override: ClassVar[str | None] = 'autogen_agentchat.agents.CodeExecutorAgent'# Override the provider string for the component. This should be used to prevent internal module names being a part of the module name. property produced_message_types: Sequence[type[BaseChatMessage]]# The types of messages that the code executor agent produces. property model_context: ChatCompletionContext# The model context in use by the agent. async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]# Handles incoming messages and returns a response. Note Agents are stateful and the messages passed to this method should be the new messages since the last call to this method. The agent should maintain its state between calls to this method. For example, if the agent needs to remember the previous messages to respond to the current message, it should store the previous messages in the agent state. async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None][source]# Process the incoming messages with the assistant agent and yield events/responses as they happen. async extract_code_blocks_from_messages(messages: Sequence[BaseChatMessage]) → List[CodeBlock][source]# async execute_code_block(code_blocks: List[CodeBlock], cancellation_token: CancellationToken) → CodeResult[source]# async on_reset(cancellation_token: CancellationToken) → None[source]# Its a no-op as the code executor agent has no mutable state. _to_config() → CodeExecutorAgentConfig[source]# Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance. Returns: T – The configuration of the component. classmethod _from_config(config: CodeExecutorAgentConfig) → Self[source]# Create a new instance of the component from a configuration object. Parameters: config (T) – The configuration object. Returns: Self – The new instance of the component. class SocietyOfMindAgent(name: str, team: Team, model_client: ChatCompletionClient, *, description: str = DEFAULT_DESCRIPTION, instruction: str = DEFAULT_INSTRUCTION, response_prompt: str = DEFAULT_RESPONSE_PROMPT, model_context: ChatCompletionContext | None = None)[source]# Bases: BaseChatAgent, Component[SocietyOfMindAgentConfig] An agent that uses an inner team of agents to generate responses. Each time the agent’s on_messages() or on_messages_stream() method is called, it runs the inner team of agents and then uses the model client to generate a response based on the inner team’s messages. Once the response is generated, the agent resets the inner team by calling Team.reset(). Limit context size sent to the model: You can limit the number of messages sent to the model by setting the model_context parameter to a BufferedChatCompletionContext. This will limit the number of recent messages sent to the model and can be useful when the model has a limit on the number of tokens it can process. You can also create your own model context by subclassing ChatCompletionContext. Parameters: name (str) – The name of the agent. team (Team) – The team of agents to use. model_client (ChatCompletionClient) – The model client to use for preparing responses. description (str, optional) – The description of the agent. instruction (str, optional) – The instruction to use when generating a response using the inner team’s messages. Defaults to DEFAULT_INSTRUCTION. It assumes the role of ‘system’. response_prompt (str, optional) – The response prompt to use when generating a response using the inner team’s messages. Defaults to DEFAULT_RESPONSE_PROMPT. It assumes the role of ‘system’. model_context (ChatCompletionContext | None, optional) – The model context for storing and retrieving LLMMessage. It can be preloaded with initial messages. The initial messages will be cleared when the agent is reset. Example: import asyncio from autogen_agentchat.ui import Console from autogen_agentchat.agents import AssistantAgent, SocietyOfMindAgent from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_agentchat.teams import RoundRobinGroupChat from autogen_agentchat.conditions import TextMentionTermination async def main() -> None: model_client = OpenAIChatCompletionClient(model=\"gpt-4o\") agent1 = AssistantAgent(\"assistant1\", model_client=model_client, system_message=\"You are a writer, write well.\") agent2 = AssistantAgent( \"assistant2\", model_client=model_client, system_message=\"You are an editor, provide critical feedback. Respond with 'APPROVE' if the text addresses all feedbacks.\", ) inner_termination = TextMentionTermination(\"APPROVE\") inner_team = RoundRobinGroupChat([agent1, agent2], termination_condition=inner_termination) society_of_mind_agent = SocietyOfMindAgent(\"society_of_mind\", team=inner_team, model_client=model_client) agent3 = AssistantAgent( \"assistant3\", model_client=model_client, system_message=\"Translate the text to Spanish.\" ) team = RoundRobinGroupChat([society_of_mind_agent, agent3], max_turns=2) stream = team.run_stream(task=\"Write a short story with a surprising ending.\") await Console(stream) asyncio.run(main()) component_config_schema# alias of SocietyOfMindAgentConfig component_provider_override: ClassVar[str | None] = 'autogen_agentchat.agents.SocietyOfMindAgent'# Override the provider string for the component. This should be used to prevent internal module names being a part of the module name. DEFAULT_INSTRUCTION = 'Earlier you were asked to fulfill a request. You and your team worked diligently to address that request. Here is a transcript of that conversation:'# The default instruction to use when generating a response using the inner team’s messages. The instruction will be prepended to the inner team’s messages when generating a response using the model. It assumes the role of ‘system’. Type: str DEFAULT_RESPONSE_PROMPT = 'Output a standalone response to the original request, without mentioning any of the intermediate discussion.'# The default response prompt to use when generating a response using the inner team’s messages. It assumes the role of ‘system’. Type: str DEFAULT_DESCRIPTION = 'An agent that uses an inner team of agents to generate responses.'# The default description for a SocietyOfMindAgent. Type: str property produced_message_types: Sequence[type[BaseChatMessage]]# The types of messages that the agent produces in the Response.chat_message field. They must be BaseChatMessage types. property model_context: ChatCompletionContext# The model context in use by the agent. async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]# Handles incoming messages and returns a response. Note Agents are stateful and the messages passed to this method should be the new messages since the last call to this method. The agent should maintain its state between calls to this method. For example, if the agent needs to remember the previous messages to respond to the current message, it should store the previous messages in the agent state. async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None][source]# Handles incoming messages and returns a stream of messages and and the final item is the response. The base implementation in BaseChatAgent simply calls on_messages() and yields the messages in the response. Note Agents are stateful and the messages passed to this method should be the new messages since the last call to this method. The agent should maintain its state between calls to this method. For example, if the agent needs to remember the previous messages to respond to the current message, it should store the previous messages in the agent state. async on_reset(cancellation_token: CancellationToken) → None[source]# Resets the agent to its initialization state. async save_state() → Mapping[str, Any][source]# Export state. Default implementation for stateless agents. async load_state(state: Mapping[str, Any]) → None[source]# Restore agent from saved state. Default implementation for stateless agents. _to_config() → SocietyOfMindAgentConfig[source]# Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance. Returns: T – The configuration of the component. classmethod _from_config(config: SocietyOfMindAgentConfig) → Self[source]# Create a new instance of the component from a configuration object. Parameters: config (T) – The configuration object. Returns: Self – The new instance of the component. class UserProxyAgent(name: str, *, description: str = 'A human user', input_func: Callable[[str], str] | Callable[[str, CancellationToken | None], Awaitable[str]] | None = None)[source]# Bases: BaseChatAgent, Component[UserProxyAgentConfig] An agent that can represent a human user through an input function. This agent can be used to represent a human user in a chat system by providing a custom input function. Note Using UserProxyAgent puts a running team in a temporary blocked state until the user responds. So it is important to time out the user input function and cancel using the CancellationToken if the user does not respond. The input function should also handle exceptions and return a default response if needed. For typical use cases that involve slow human responses, it is recommended to use termination conditions such as HandoffTermination or SourceMatchTermination to stop the running team and return the control to the application. You can run the team again with the user input. This way, the state of the team can be saved and restored when the user responds. See Human-in-the-loop for more information. Parameters: name (str) – The name of the agent. description (str, optional) – A description of the agent. input_func (Optional[Callable[[str], str]], Callable[[str, Optional[CancellationToken]], Awaitable[str]]) – A function that takes a prompt and returns a user input string. For examples of integrating with web and UI frameworks, see the following: FastAPI ChainLit Example Simple usage case: import asyncio from autogen_core import CancellationToken from autogen_agentchat.agents import UserProxyAgent from autogen_agentchat.messages import TextMessage async def simple_user_agent(): agent = UserProxyAgent(\"user_proxy\") response = await asyncio.create_task( agent.on_messages( [TextMessage(content=\"What is your name? \", source=\"user\")], cancellation_token=CancellationToken(), ) ) assert isinstance(response.chat_message, TextMessage) print(f\"Your name is {response.chat_message.content}\") Example Cancellable usage case: import asyncio from typing import Any from autogen_core import CancellationToken from autogen_agentchat.agents import UserProxyAgent from autogen_agentchat.messages import TextMessage token = CancellationToken() agent = UserProxyAgent(\"user_proxy\") async def timeout(delay: float): await asyncio.sleep(delay) def cancellation_callback(task: asyncio.Task[Any]): token.cancel() async def cancellable_user_agent(): try: timeout_task = asyncio.create_task(timeout(3)) timeout_task.add_done_callback(cancellation_callback) agent_task = asyncio.create_task( agent.on_messages( [TextMessage(content=\"What is your name? \", source=\"user\")], cancellation_token=token, ) ) response = await agent_task assert isinstance(response.chat_message, TextMessage) print(f\"Your name is {response.chat_message.content}\") except Exception as e: print(f\"Exception: {e}\") except BaseException as e: print(f\"BaseException: {e}\") component_type: ClassVar[ComponentType] = 'agent'# The logical type of the component. component_provider_override: ClassVar[str | None] = 'autogen_agentchat.agents.UserProxyAgent'# Override the provider string for the component. This should be used to prevent internal module names being a part of the module name. component_config_schema# alias of UserProxyAgentConfig class InputRequestContext[source]# Bases: object classmethod request_id() → str[source]# property produced_message_types: Sequence[type[BaseChatMessage]]# Message types this agent can produce. async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]# Handles incoming messages and returns a response. Note Agents are stateful and the messages passed to this method should be the new messages since the last call to this method. The agent should maintain its state between calls to this method. For example, if the agent needs to remember the previous messages to respond to the current message, it should store the previous messages in the agent state. async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None][source]# Handle incoming messages by requesting user input. async on_reset(cancellation_token: CancellationToken | None = None) → None[source]# Reset agent state. _to_config() → UserProxyAgentConfig[source]# Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance. Returns: T – The configuration of the component. classmethod _from_config(config: UserProxyAgentConfig) → Self[source]# Create a new instance of the component from a configuration object. Parameters: config (T) – The configuration object. Returns: Self – The new instance of the component. class MessageFilterAgent(name: str, wrapped_agent: BaseChatAgent, filter: MessageFilterConfig)[source]# Bases: BaseChatAgent, Component[MessageFilterAgentConfig] A wrapper agent that filters incoming messages before passing them to the inner agent. Warning This is an experimental feature, and the API will change in the future releases. This is useful in scenarios like multi-agent workflows where an agent should only process a subset of the full message history—for example, only the last message from each upstream agent, or only the first message from a specific source. Filtering is configured using MessageFilterConfig, which supports: - Filtering by message source (e.g., only messages from “user” or another agent) - Selecting the first N or last N messages from each source - If position is None, all messages from that source are included This agent is compatible with both direct message passing and team-based execution such as GraphFlow. Example >>> agent_a = MessageFilterAgent( ... name=\"A\", ... wrapped_agent=some_other_agent, ... filter=MessageFilterConfig( ... per_source=[ ... PerSourceFilter(source=\"user\", position=\"first\", count=1), ... PerSourceFilter(source=\"B\", position=\"last\", count=2), ... ] ... ), ... ) Example use case with Graph:Suppose you have a looping multi-agent graph: A → B → A → B → C. You want: - A to only see the user message and the last message from B - B to see the user message, last message from A, and its own prior responses (for reflection) - C to see the user message and the last message from B Wrap the agents like so: >>> agent_a = MessageFilterAgent( ... name=\"A\", ... wrapped_agent=agent_a_inner, ... filter=MessageFilterConfig( ... per_source=[ ... PerSourceFilter(source=\"user\", position=\"first\", count=1), ... PerSourceFilter(source=\"B\", position=\"last\", count=1), ... ] ... ), ... ) >>> agent_b = MessageFilterAgent( ... name=\"B\", ... wrapped_agent=agent_b_inner, ... filter=MessageFilterConfig( ... per_source=[ ... PerSourceFilter(source=\"user\", position=\"first\", count=1), ... PerSourceFilter(source=\"A\", position=\"last\", count=1), ... PerSourceFilter(source=\"B\", position=\"last\", count=10), ... ] ... ), ... ) >>> agent_c = MessageFilterAgent( ... name=\"C\", ... wrapped_agent=agent_c_inner, ... filter=MessageFilterConfig( ... per_source=[ ... PerSourceFilter(source=\"user\", position=\"first\", count=1), ... PerSourceFilter(source=\"B\", position=\"last\", count=1), ... ] ... ), ... ) Then define the graph: >>> graph = DiGraph( ... nodes={ ... \"A\": DiGraphNode(name=\"A\", edges=[DiGraphEdge(target=\"B\")]), ... \"B\": DiGraphNode( ... name=\"B\", ... edges=[ ... DiGraphEdge(target=\"C\", condition=\"exit\"), ... DiGraphEdge(target=\"A\", condition=\"loop\"), ... ], ... ), ... \"C\": DiGraphNode(name=\"C\", edges=[]), ... }, ... default_start_node=\"A\", ... ) This will ensure each agent sees only what is needed for its decision or action logic. component_config_schema# alias of MessageFilterAgentConfig component_provider_override: ClassVar[str | None] = 'autogen_agentchat.agents.MessageFilterAgent'# Override the provider string for the component. This should be used to prevent internal module names being a part of the module name. property produced_message_types: Sequence[type[BaseChatMessage]]# The types of messages that the agent produces in the Response.chat_message field. They must be BaseChatMessage types. async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]# Handles incoming messages and returns a response. Note Agents are stateful and the messages passed to this method should be the new messages since the last call to this method. The agent should maintain its state between calls to this method. For example, if the agent needs to remember the previous messages to respond to the current message, it should store the previous messages in the agent state. async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None][source]# Handles incoming messages and returns a stream of messages and and the final item is the response. The base implementation in BaseChatAgent simply calls on_messages() and yields the messages in the response. Note Agents are stateful and the messages passed to this method should be the new messages since the last call to this method. The agent should maintain its state between calls to this method. For example, if the agent needs to remember the previous messages to respond to the current message, it should store the previous messages in the agent state. async on_reset(cancellation_token: CancellationToken) → None[source]# Resets the agent to its initialization state. _to_config() → MessageFilterAgentConfig[source]# Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance. Returns: T – The configuration of the component. classmethod _from_config(config: MessageFilterAgentConfig) → MessageFilterAgent[source]# Create a new instance of the component from a configuration object. Parameters: config (T) – The configuration object. Returns: Self – The new instance of the component. pydantic model MessageFilterConfig[source]# Bases: BaseModel Show JSON schema{ \"title\": \"MessageFilterConfig\", \"type\": \"object\", \"properties\": { \"per_source\": { \"items\": { \"$ref\": \"#/$defs/PerSourceFilter\" }, \"title\": \"Per Source\", \"type\": \"array\" } }, \"$defs\": { \"PerSourceFilter\": { \"properties\": { \"source\": { \"title\": \"Source\", \"type\": \"string\" }, \"position\": { \"anyOf\": [ { \"enum\": [ \"first\", \"last\" ], \"type\": \"string\" }, { \"type\": \"null\" } ], \"default\": null, \"title\": \"Position\" }, \"count\": { \"anyOf\": [ { \"type\": \"integer\" }, { \"type\": \"null\" } ], \"default\": null, \"title\": \"Count\" } }, \"required\": [ \"source\" ], \"title\": \"PerSourceFilter\", \"type\": \"object\" } }, \"required\": [ \"per_source\" ] } Fields: per_source (List[autogen_agentchat.agents._message_filter_agent.PerSourceFilter]) field per_source: List[PerSourceFilter] [Required]# pydantic model PerSourceFilter[source]# Bases: BaseModel Show JSON schema{ \"title\": \"PerSourceFilter\", \"type\": \"object\", \"properties\": { \"source\": { \"title\": \"Source\", \"type\": \"string\" }, \"position\": { \"anyOf\": [ { \"enum\": [ \"first\", \"last\" ], \"type\": \"string\" }, { \"type\": \"null\" } ], \"default\": null, \"title\": \"Position\" }, \"count\": { \"anyOf\": [ { \"type\": \"integer\" }, { \"type\": \"null\" } ], \"default\": null, \"title\": \"Count\" } }, \"required\": [ \"source\" ] } Fields: count (int | None) position (Literal['first', 'last'] | None) source (str) field source: str [Required]# field position: Literal['first', 'last'] | None = None# field count: int | None = None# pydantic model ApprovalRequest[source]# Bases: BaseModel Request for approval of code execution. Show JSON schema{ \"title\": \"ApprovalRequest\", \"description\": \"Request for approval of code execution.\", \"type\": \"object\", \"properties\": { \"code\": { \"title\": \"Code\", \"type\": \"string\" }, \"context\": { \"items\": { \"discriminator\": { \"mapping\": { \"AssistantMessage\": \"#/$defs/AssistantMessage\", \"FunctionExecutionResultMessage\": \"#/$defs/FunctionExecutionResultMessage\", \"SystemMessage\": \"#/$defs/SystemMessage\", \"UserMessage\": \"#/$defs/UserMessage\" }, \"propertyName\": \"type\" }, \"oneOf\": [ { \"$ref\": \"#/$defs/SystemMessage\" }, { \"$ref\": \"#/$defs/UserMessage\" }, { \"$ref\": \"#/$defs/AssistantMessage\" }, { \"$ref\": \"#/$defs/FunctionExecutionResultMessage\" } ] }, \"title\": \"Context\", \"type\": \"array\" } }, \"$defs\": { \"AssistantMessage\": { \"description\": \"Assistant message are sampled from the language model.\", \"properties\": { \"content\": { \"anyOf\": [ { \"type\": \"string\" }, { \"items\": { \"$ref\": \"#/$defs/FunctionCall\" }, \"type\": \"array\" } ], \"title\": \"Content\" }, \"thought\": { \"anyOf\": [ { \"type\": \"string\" }, { \"type\": \"null\" } ], \"default\": null, \"title\": \"Thought\" }, \"source\": { \"title\": \"Source\", \"type\": \"string\" }, \"type\": { \"const\": \"AssistantMessage\", \"default\": \"AssistantMessage\", \"title\": \"Type\", \"type\": \"string\" } }, \"required\": [ \"content\", \"source\" ], \"title\": \"AssistantMessage\", \"type\": \"object\" }, \"FunctionCall\": { \"properties\": { \"id\": { \"title\": \"Id\", \"type\": \"string\" }, \"arguments\": { \"title\": \"Arguments\", \"type\": \"string\" }, \"name\": { \"title\": \"Name\", \"type\": \"string\" } }, \"required\": [ \"id\", \"arguments\", \"name\" ], \"title\": \"FunctionCall\", \"type\": \"object\" }, \"FunctionExecutionResult\": { \"description\": \"Function execution result contains the output of a function call.\", \"properties\": { \"content\": { \"title\": \"Content\", \"type\": \"string\" }, \"name\": { \"title\": \"Name\", \"type\": \"string\" }, \"call_id\": { \"title\": \"Call Id\", \"type\": \"string\" }, \"is_error\": { \"anyOf\": [ { \"type\": \"boolean\" }, { \"type\": \"null\" } ], \"default\": null, \"title\": \"Is Error\" } }, \"required\": [ \"content\", \"name\", \"call_id\" ], \"title\": \"FunctionExecutionResult\", \"type\": \"object\" }, \"FunctionExecutionResultMessage\": { \"description\": \"Function execution result message contains the output of multiple function calls.\", \"properties\": { \"content\": { \"items\": { \"$ref\": \"#/$defs/FunctionExecutionResult\" }, \"title\": \"Content\", \"type\": \"array\" }, \"type\": { \"const\": \"FunctionExecutionResultMessage\", \"default\": \"FunctionExecutionResultMessage\", \"title\": \"Type\", \"type\": \"string\" } }, \"required\": [ \"content\" ], \"title\": \"FunctionExecutionResultMessage\", \"type\": \"object\" }, \"SystemMessage\": { \"description\": \"System message contains instructions for the model coming from the developer.\\n\\n.. note::\\n\\n Open AI is moving away from using 'system' role in favor of 'developer' role.\\n See `Model Spec <https://cdn.openai.com/spec/model-spec-2024-05-08.html#definitions>`_ for more details.\\n However, the 'system' role is still allowed in their API and will be automatically converted to 'developer' role\\n on the server side.\\n So, you can use `SystemMessage` for developer messages.\", \"properties\": { \"content\": { \"title\": \"Content\", \"type\": \"string\" }, \"type\": { \"const\": \"SystemMessage\", \"default\": \"SystemMessage\", \"title\": \"Type\", \"type\": \"string\" } }, \"required\": [ \"content\" ], \"title\": \"SystemMessage\", \"type\": \"object\" }, \"UserMessage\": { \"description\": \"User message contains input from end users, or a catch-all for data provided to the model.\", \"properties\": { \"content\": { \"anyOf\": [ { \"type\": \"string\" }, { \"items\": { \"anyOf\": [ { \"type\": \"string\" }, {} ] }, \"type\": \"array\" } ], \"title\": \"Content\" }, \"source\": { \"title\": \"Source\", \"type\": \"string\" }, \"type\": { \"const\": \"UserMessage\", \"default\": \"UserMessage\", \"title\": \"Type\", \"type\": \"string\" } }, \"required\": [ \"content\", \"source\" ], \"title\": \"UserMessage\", \"type\": \"object\" } }, \"required\": [ \"code\", \"context\" ] } Fields: code (str) context (List[autogen_core.models._types.SystemMessage | autogen_core.models._types.UserMessage | autogen_core.models._types.AssistantMessage | autogen_core.models._types.FunctionExecutionResultMessage]) field code: str [Required]# field context: List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]] [Required]# pydantic model ApprovalResponse[source]# Bases: BaseModel Response to approval request. Show JSON schema{ \"title\": \"ApprovalResponse\", \"description\": \"Response to approval request.\", \"type\": \"object\", \"properties\": { \"approved\": { \"title\": \"Approved\", \"type\": \"boolean\" }, \"reason\": { \"title\": \"Reason\", \"type\": \"string\" } }, \"required\": [ \"approved\", \"reason\" ] } Fields: approved (bool) reason (str) field approved: bool [Required]# field reason: str [Required]# previous autogen_agentchat next autogen_agentchat.base",
      "code": "ChatAgent"
    },
    {
      "description": "Note The code executor only processes code that is properly formatted in markdown code blocks using triple backticks. For example: ```python print(\"Hello World\") ``` # or ```sh echo \"Hello World\" ```",
      "code": "```python\nprint(\"Hello World\")\n```\n\n# or\n\n```sh\necho \"Hello World\"\n```"
    },
    {
      "description": "The code executor only processes code that is properly formatted in markdown code blocks using triple backticks. For example:",
      "code": "```python\nprint(\"Hello World\")\n```\n\n# or\n\n```sh\necho \"Hello World\"\n```"
    },
    {
      "description": "Example:",
      "code": "import asyncio\nfrom autogen_agentchat.ui import Console\nfrom autogen_agentchat.agents import AssistantAgent, SocietyOfMindAgent\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.conditions import TextMentionTermination\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n    agent1 = AssistantAgent(\"assistant1\", model_client=model_client, system_message=\"You are a writer, write well.\")\n    agent2 = AssistantAgent(\n        \"assistant2\",\n        model_client=model_client,\n        system_message=\"You are an editor, provide critical feedback. Respond with 'APPROVE' if the text addresses all feedbacks.\",\n    )\n    inner_termination = TextMentionTermination(\"APPROVE\")\n    inner_team = RoundRobinGroupChat([agent1, agent2], termination_condition=inner_termination)\n\n    society_of_mind_agent = SocietyOfMindAgent(\"society_of_mind\", team=inner_team, model_client=model_client)\n\n    agent3 = AssistantAgent(\n        \"assistant3\", model_client=model_client, system_message=\"Translate the text to Spanish.\"\n    )\n    team = RoundRobinGroupChat([society_of_mind_agent, agent3], max_turns=2)\n\n    stream = team.run_stream(task=\"Write a short story with a surprising ending.\")\n    await Console(stream)\n\n\nasyncio.run(main())"
    }
  ],
  "links": [
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.agents.html",
    "https://microsoft.github.io/autogen/stable/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/extensions-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/reference/index.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.base.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.conditions.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.messages.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.state.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.teams.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.tools.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.ui.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.utils.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.code_executor.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.exceptions.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.logging.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.memory.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.model_context.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.models.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.tool_agent.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.tools.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.utils.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.cache_store.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.experimental.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.memory.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.teams.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.ui.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.azure.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.file_surfer.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.magentic_one.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.openai.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.video_surfer.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.web_surfer.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.auth.azure.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.cache_store.diskcache.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.cache_store.redis.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.azure.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.docker.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.docker_jupyter.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.jupyter.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.local.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.experimental.task_centric_memory.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.memory.canvas.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.memory.chromadb.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.memory.mem0.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.memory.redis.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.anthropic.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.azure.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.cache.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.llama_cpp.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.ollama.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.openai.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.replay.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.semantic_kernel.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.teams.magentic_one.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.azure.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.code_execution.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.graphrag.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.http.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.langchain.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.mcp.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.semantic_kernel.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.video_surfer.tools.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.web_surfer.playwright_controller.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.experimental.task_centric_memory.utils.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.anthropic.config.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.azure.config.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.ollama.config.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.openai.config.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.protos.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.protos.agent_worker_pb2.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.protos.agent_worker_pb2_grpc.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.protos.cloudevent_pb2.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.protos.cloudevent_pb2_grpc.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/human-in-the-loop.html"
  ]
}