{
  "url": "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/components/tools.html",
  "title": "Tools — AutoGen",
  "content": "Tools are code that can be executed by an agent to perform actions. A tool can be a simple function such as a calculator, or an API call to a third-party service such as stock price lookup or weather forecast. In the context of AI agents, tools are designed to be executed by agents in response to model-generated function calls.\n\nAutoGen provides the autogen_core.tools module with a suite of built-in tools and utilities for creating and running custom tools.\n\nOne of the built-in tools is the PythonCodeExecutionTool, which allows agents to execute Python code snippets.\n\nHere is how you create the tool and use it.\n\nThe DockerCommandLineCodeExecutor class is a built-in code executor that runs Python code snippets in a subprocess in the command line environment of a docker container. The PythonCodeExecutionTool class wraps the code executor and provides a simple interface to execute Python code snippets.\n\nExamples of other built-in tools\n\nLocalSearchTool and GlobalSearchTool for using GraphRAG.\n\nmcp_server_tools for using Model Context Protocol (MCP) servers as tools.\n\nHttpTool for making HTTP requests to REST APIs.\n\nLangChainToolAdapter for using LangChain tools.\n\nA tool can also be a simple Python function that performs a specific action. To create a custom function tool, you just need to create a Python function and use the FunctionTool class to wrap it.\n\nThe FunctionTool class uses descriptions and type annotations to inform the LLM when and how to use a given function. The description provides context about the function’s purpose and intended use cases, while type annotations inform the LLM about the expected parameters and return type.\n\nFor example, a simple tool to obtain the stock price of a company might look like this:\n\nIn AutoGen, every tool is a subclass of BaseTool, which automatically generates the JSON schema for the tool. For example, to get the JSON schema for the stock_price_tool, we can use the schema property.\n\nModel clients use the JSON schema of the tools to generate tool calls.\n\nHere is an example of how to use the FunctionTool class with a OpenAIChatCompletionClient. Other model client classes can be used in a similar way. See Model Clients for more details.\n\nWhat is actually going on under the hood of the call to the create method? The model client takes the list of tools and generates a JSON schema for the parameters of each tool. Then, it generates a request to the model API with the tool’s JSON schema and the other messages to obtain a result.\n\nMany models, such as OpenAI’s GPT-4o and Llama-3.2, are trained to produce tool calls in the form of structured JSON strings that conform to the JSON schema of the tool. AutoGen’s model clients then parse the model’s response and extract the tool call from the JSON string.\n\nThe result is a list of FunctionCall objects, which can be used to run the corresponding tools.\n\nWe use json.loads to parse the JSON string in the arguments field into a Python dictionary. The run_json() method takes the dictionary and runs the tool with the provided arguments.\n\nNow you can make another model client call to have the model generate a reflection on the result of the tool execution.\n\nThe result of the tool call is wrapped in a FunctionExecutionResult object, which contains the result of the tool execution and the ID of the tool that was called. The model client can use this information to generate a reflection on the result of the tool execution.\n\nPutting the model client and the tools together, you can create a tool-equipped agent that can use tools to perform actions, and reflect on the results of those actions.\n\nThe Core API is designed to be minimal and you need to build your own agent logic around model clients and tools. For “pre-built” agents that can use tools, please refer to the AgentChat API.\n\nWhen handling a user message, the ToolUseAgent class first use the model client to generate a list of function calls to the tools, and then run the tools and generate a reflection on the results of the tool execution. The reflection is then returned to the user as the agent’s response.\n\nTo run the agent, let’s create a runtime and register the agent with the runtime.\n\nThis example uses the OpenAIChatCompletionClient, for Azure OpenAI and other clients, see Model Clients. Let’s test the agent with a question about stock price.",
  "headings": [
    {
      "level": "h1",
      "text": "Tools#",
      "id": ""
    },
    {
      "level": "h2",
      "text": "Built-in Tools#",
      "id": ""
    },
    {
      "level": "h2",
      "text": "Custom Function Tools#",
      "id": ""
    },
    {
      "level": "h2",
      "text": "Calling Tools with Model Clients#",
      "id": ""
    },
    {
      "level": "h2",
      "text": "Tool-Equipped Agent#",
      "id": ""
    }
  ],
  "code_samples": [
    {
      "code": "from autogen_core import CancellationToken\nfrom autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor\nfrom autogen_ext.tools.code_execution import PythonCodeExecutionTool\n\n# Create the tool.\ncode_executor = DockerCommandLineCodeExecutor()\nawait code_executor.start()\ncode_execution_tool = PythonCodeExecutionTool(code_executor)\ncancellation_token = CancellationToken()\n\n# Use the tool directly without an agent.\ncode = \"print('Hello, world!')\"\nresult = await code_execution_tool.run_json({\"code\": code}, cancellation_token)\nprint(code_execution_tool.return_value_as_string(result))",
      "language": "python"
    },
    {
      "code": "from autogen_core import CancellationToken\nfrom autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor\nfrom autogen_ext.tools.code_execution import PythonCodeExecutionTool\n\n# Create the tool.\ncode_executor = DockerCommandLineCodeExecutor()\nawait code_executor.start()\ncode_execution_tool = PythonCodeExecutionTool(code_executor)\ncancellation_token = CancellationToken()\n\n# Use the tool directly without an agent.\ncode = \"print('Hello, world!')\"\nresult = await code_execution_tool.run_json({\"code\": code}, cancellation_token)\nprint(code_execution_tool.return_value_as_string(result))",
      "language": "python"
    },
    {
      "code": "Hello, world!",
      "language": "unknown"
    },
    {
      "code": "Hello, world!",
      "language": "unknown"
    },
    {
      "code": "import random\n\nfrom autogen_core import CancellationToken\nfrom autogen_core.tools import FunctionTool\nfrom typing_extensions import Annotated\n\n\nasync def get_stock_price(ticker: str, date: Annotated[str, \"Date in YYYY/MM/DD\"]) -> float:\n    # Returns a random stock price for demonstration purposes.\n    return random.uniform(10, 200)\n\n\n# Create a function tool.\nstock_price_tool = FunctionTool(get_stock_price, description=\"Get the stock price.\")\n\n# Run the tool.\ncancellation_token = CancellationToken()\nresult = await stock_price_tool.run_json({\"ticker\": \"AAPL\", \"date\": \"2021/01/01\"}, cancellation_token)\n\n# Print the result.\nprint(stock_price_tool.return_value_as_string(result))",
      "language": "python"
    },
    {
      "code": "import random\n\nfrom autogen_core import CancellationToken\nfrom autogen_core.tools import FunctionTool\nfrom typing_extensions import Annotated\n\n\nasync def get_stock_price(ticker: str, date: Annotated[str, \"Date in YYYY/MM/DD\"]) -> float:\n    # Returns a random stock price for demonstration purposes.\n    return random.uniform(10, 200)\n\n\n# Create a function tool.\nstock_price_tool = FunctionTool(get_stock_price, description=\"Get the stock price.\")\n\n# Run the tool.\ncancellation_token = CancellationToken()\nresult = await stock_price_tool.run_json({\"ticker\": \"AAPL\", \"date\": \"2021/01/01\"}, cancellation_token)\n\n# Print the result.\nprint(stock_price_tool.return_value_as_string(result))",
      "language": "python"
    },
    {
      "code": "143.83831971965762",
      "language": "unknown"
    },
    {
      "code": "143.83831971965762",
      "language": "unknown"
    },
    {
      "code": "stock_price_tool.schema",
      "language": "unknown"
    },
    {
      "code": "stock_price_tool.schema",
      "language": "unknown"
    },
    {
      "code": "{'name': 'get_stock_price',\n 'description': 'Get the stock price.',\n 'parameters': {'type': 'object',\n  'properties': {'ticker': {'description': 'ticker',\n    'title': 'Ticker',\n    'type': 'string'},\n   'date': {'description': 'Date in YYYY/MM/DD',\n    'title': 'Date',\n    'type': 'string'}},\n  'required': ['ticker', 'date'],\n  'additionalProperties': False},\n 'strict': False}",
      "language": "json"
    },
    {
      "code": "{'name': 'get_stock_price',\n 'description': 'Get the stock price.',\n 'parameters': {'type': 'object',\n  'properties': {'ticker': {'description': 'ticker',\n    'title': 'Ticker',\n    'type': 'string'},\n   'date': {'description': 'Date in YYYY/MM/DD',\n    'title': 'Date',\n    'type': 'string'}},\n  'required': ['ticker', 'date'],\n  'additionalProperties': False},\n 'strict': False}",
      "language": "json"
    },
    {
      "code": "import json\n\nfrom autogen_core.models import AssistantMessage, FunctionExecutionResult, FunctionExecutionResultMessage, UserMessage\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n# Create the OpenAI chat completion client. Using OPENAI_API_KEY from environment variable.\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n\n# Create a user message.\nuser_message = UserMessage(content=\"What is the stock price of AAPL on 2021/01/01?\", source=\"user\")\n\n# Run the chat completion with the stock_price_tool defined above.\ncancellation_token = CancellationToken()\ncreate_result = await model_client.create(\n    messages=[user_message], tools=[stock_price_tool], cancellation_token=cancellation_token\n)\ncreate_result.content",
      "language": "python"
    },
    {
      "code": "import json\n\nfrom autogen_core.models import AssistantMessage, FunctionExecutionResult, FunctionExecutionResultMessage, UserMessage\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n# Create the OpenAI chat completion client. Using OPENAI_API_KEY from environment variable.\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n\n# Create a user message.\nuser_message = UserMessage(content=\"What is the stock price of AAPL on 2021/01/01?\", source=\"user\")\n\n# Run the chat completion with the stock_price_tool defined above.\ncancellation_token = CancellationToken()\ncreate_result = await model_client.create(\n    messages=[user_message], tools=[stock_price_tool], cancellation_token=cancellation_token\n)\ncreate_result.content",
      "language": "python"
    },
    {
      "code": "[FunctionCall(id='call_tpJ5J1Xoxi84Sw4v0scH0qBM', arguments='{\"ticker\":\"AAPL\",\"date\":\"2021/01/01\"}', name='get_stock_price')]",
      "language": "json"
    },
    {
      "code": "[FunctionCall(id='call_tpJ5J1Xoxi84Sw4v0scH0qBM', arguments='{\"ticker\":\"AAPL\",\"date\":\"2021/01/01\"}', name='get_stock_price')]",
      "language": "json"
    },
    {
      "code": "assert isinstance(create_result.content, list)\narguments = json.loads(create_result.content[0].arguments)  # type: ignore\ntool_result = await stock_price_tool.run_json(arguments, cancellation_token)\ntool_result_str = stock_price_tool.return_value_as_string(tool_result)\ntool_result_str",
      "language": "csharp"
    },
    {
      "code": "assert isinstance(create_result.content, list)\narguments = json.loads(create_result.content[0].arguments)  # type: ignore\ntool_result = await stock_price_tool.run_json(arguments, cancellation_token)\ntool_result_str = stock_price_tool.return_value_as_string(tool_result)\ntool_result_str",
      "language": "csharp"
    },
    {
      "code": "'32.381250753393104'",
      "language": "unknown"
    },
    {
      "code": "'32.381250753393104'",
      "language": "unknown"
    },
    {
      "code": "# Create a function execution result\nexec_result = FunctionExecutionResult(\n    call_id=create_result.content[0].id,  # type: ignore\n    content=tool_result_str,\n    is_error=False,\n    name=stock_price_tool.name,\n)\n\n# Make another chat completion with the history and function execution result message.\nmessages = [\n    user_message,\n    AssistantMessage(content=create_result.content, source=\"assistant\"),  # assistant message with tool call\n    FunctionExecutionResultMessage(content=[exec_result]),  # function execution result message\n]\ncreate_result = await model_client.create(messages=messages, cancellation_token=cancellation_token)  # type: ignore\nprint(create_result.content)\nawait model_client.close()",
      "language": "python"
    },
    {
      "code": "# Create a function execution result\nexec_result = FunctionExecutionResult(\n    call_id=create_result.content[0].id,  # type: ignore\n    content=tool_result_str,\n    is_error=False,\n    name=stock_price_tool.name,\n)\n\n# Make another chat completion with the history and function execution result message.\nmessages = [\n    user_message,\n    AssistantMessage(content=create_result.content, source=\"assistant\"),  # assistant message with tool call\n    FunctionExecutionResultMessage(content=[exec_result]),  # function execution result message\n]\ncreate_result = await model_client.create(messages=messages, cancellation_token=cancellation_token)  # type: ignore\nprint(create_result.content)\nawait model_client.close()",
      "language": "python"
    },
    {
      "code": "The stock price of AAPL (Apple Inc.) on January 1, 2021, was approximately $32.38.",
      "language": "bash"
    },
    {
      "code": "The stock price of AAPL (Apple Inc.) on January 1, 2021, was approximately $32.38.",
      "language": "bash"
    },
    {
      "code": "import asyncio\nimport json\nfrom dataclasses import dataclass\nfrom typing import List\n\nfrom autogen_core import (\n    AgentId,\n    FunctionCall,\n    MessageContext,\n    RoutedAgent,\n    SingleThreadedAgentRuntime,\n    message_handler,\n)\nfrom autogen_core.models import (\n    ChatCompletionClient,\n    LLMMessage,\n    SystemMessage,\n    UserMessage,\n)\nfrom autogen_core.tools import FunctionTool, Tool\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n\n@dataclass\nclass Message:\n    content: str\n\n\nclass ToolUseAgent(RoutedAgent):\n    def __init__(self, model_client: ChatCompletionClient, tool_schema: List[Tool]) -> None:\n        super().__init__(\"An agent with tools\")\n        self._system_messages: List[LLMMessage] = [SystemMessage(content=\"You are a helpful AI assistant.\")]\n        self._model_client = model_client\n        self._tools = tool_schema\n\n    @message_handler\n    async def handle_user_message(self, message: Message, ctx: MessageContext) -> Message:\n        # Create a session of messages.\n        session: List[LLMMessage] = self._system_messages + [UserMessage(content=message.content, source=\"user\")]\n\n        # Run the chat completion with the tools.\n        create_result = await self._model_client.create(\n            messages=session,\n            tools=self._tools,\n            cancellation_token=ctx.cancellation_token,\n        )\n\n        # If there are no tool calls, return the result.\n        if isinstance(create_result.content, str):\n            return Message(content=create_result.content)\n        assert isinstance(create_result.content, list) and all(\n            isinstance(call, FunctionCall) for call in create_result.content\n        )\n\n        # Add the first model create result to the session.\n        session.append(AssistantMessage(content=create_result.content, source=\"assistant\"))\n\n        # Execute the tool calls.\n        results = await asyncio.gather(\n            *[self._execute_tool_call(call, ctx.cancellation_token) for call in create_result.content]\n        )\n\n        # Add the function execution results to the session.\n        session.append(FunctionExecutionResultMessage(content=results))\n\n        # Run the chat completion again to reflect on the history and function execution results.\n        create_result = await self._model_client.create(\n            messages=session,\n            cancellation_token=ctx.cancellation_token,\n        )\n        assert isinstance(create_result.content, str)\n\n        # Return the result as a message.\n        return Message(content=create_result.content)\n\n    async def _execute_tool_call(\n        self, call: FunctionCall, cancellation_token: CancellationToken\n    ) -> FunctionExecutionResult:\n        # Find the tool by name.\n        tool = next((tool for tool in self._tools if tool.name == call.name), None)\n        assert tool is not None\n\n        # Run the tool and capture the result.\n        try:\n            arguments = json.loads(call.arguments)\n            result = await tool.run_json(arguments, cancellation_token)\n            return FunctionExecutionResult(\n                call_id=call.id, content=tool.return_value_as_string(result), is_error=False, name=tool.name\n            )\n        except Exception as e:\n            return FunctionExecutionResult(call_id=call.id, content=str(e), is_error=True, name=tool.name)",
      "language": "python"
    },
    {
      "code": "import asyncio\nimport json\nfrom dataclasses import dataclass\nfrom typing import List\n\nfrom autogen_core import (\n    AgentId,\n    FunctionCall,\n    MessageContext,\n    RoutedAgent,\n    SingleThreadedAgentRuntime,\n    message_handler,\n)\nfrom autogen_core.models import (\n    ChatCompletionClient,\n    LLMMessage,\n    SystemMessage,\n    UserMessage,\n)\nfrom autogen_core.tools import FunctionTool, Tool\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n\n@dataclass\nclass Message:\n    content: str\n\n\nclass ToolUseAgent(RoutedAgent):\n    def __init__(self, model_client: ChatCompletionClient, tool_schema: List[Tool]) -> None:\n        super().__init__(\"An agent with tools\")\n        self._system_messages: List[LLMMessage] = [SystemMessage(content=\"You are a helpful AI assistant.\")]\n        self._model_client = model_client\n        self._tools = tool_schema\n\n    @message_handler\n    async def handle_user_message(self, message: Message, ctx: MessageContext) -> Message:\n        # Create a session of messages.\n        session: List[LLMMessage] = self._system_messages + [UserMessage(content=message.content, source=\"user\")]\n\n        # Run the chat completion with the tools.\n        create_result = await self._model_client.create(\n            messages=session,\n            tools=self._tools,\n            cancellation_token=ctx.cancellation_token,\n        )\n\n        # If there are no tool calls, return the result.\n        if isinstance(create_result.content, str):\n            return Message(content=create_result.content)\n        assert isinstance(create_result.content, list) and all(\n            isinstance(call, FunctionCall) for call in create_result.content\n        )\n\n        # Add the first model create result to the session.\n        session.append(AssistantMessage(content=create_result.content, source=\"assistant\"))\n\n        # Execute the tool calls.\n        results = await asyncio.gather(\n            *[self._execute_tool_call(call, ctx.cancellation_token) for call in create_result.content]\n        )\n\n        # Add the function execution results to the session.\n        session.append(FunctionExecutionResultMessage(content=results))\n\n        # Run the chat completion again to reflect on the history and function execution results.\n        create_result = await self._model_client.create(\n            messages=session,\n            cancellation_token=ctx.cancellation_token,\n        )\n        assert isinstance(create_result.content, str)\n\n        # Return the result as a message.\n        return Message(content=create_result.content)\n\n    async def _execute_tool_call(\n        self, call: FunctionCall, cancellation_token: CancellationToken\n    ) -> FunctionExecutionResult:\n        # Find the tool by name.\n        tool = next((tool for tool in self._tools if tool.name == call.name), None)\n        assert tool is not None\n\n        # Run the tool and capture the result.\n        try:\n            arguments = json.loads(call.arguments)\n            result = await tool.run_json(arguments, cancellation_token)\n            return FunctionExecutionResult(\n                call_id=call.id, content=tool.return_value_as_string(result), is_error=False, name=tool.name\n            )\n        except Exception as e:\n            return FunctionExecutionResult(call_id=call.id, content=str(e), is_error=True, name=tool.name)",
      "language": "python"
    },
    {
      "code": "# Create the model client.\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n# Create a runtime.\nruntime = SingleThreadedAgentRuntime()\n# Create the tools.\ntools: List[Tool] = [FunctionTool(get_stock_price, description=\"Get the stock price.\")]\n# Register the agents.\nawait ToolUseAgent.register(\n    runtime,\n    \"tool_use_agent\",\n    lambda: ToolUseAgent(\n        model_client=model_client,\n        tool_schema=tools,\n    ),\n)",
      "language": "yaml"
    },
    {
      "code": "# Create the model client.\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n# Create a runtime.\nruntime = SingleThreadedAgentRuntime()\n# Create the tools.\ntools: List[Tool] = [FunctionTool(get_stock_price, description=\"Get the stock price.\")]\n# Register the agents.\nawait ToolUseAgent.register(\n    runtime,\n    \"tool_use_agent\",\n    lambda: ToolUseAgent(\n        model_client=model_client,\n        tool_schema=tools,\n    ),\n)",
      "language": "yaml"
    },
    {
      "code": "AgentType(type='tool_use_agent')",
      "language": "unknown"
    },
    {
      "code": "AgentType(type='tool_use_agent')",
      "language": "unknown"
    },
    {
      "code": "# Start processing messages.\nruntime.start()\n# Send a direct message to the tool agent.\ntool_use_agent = AgentId(\"tool_use_agent\", \"default\")\nresponse = await runtime.send_message(Message(\"What is the stock price of NVDA on 2024/06/01?\"), tool_use_agent)\nprint(response.content)\n# Stop processing messages.\nawait runtime.stop()\nawait model_client.close()",
      "language": "python"
    },
    {
      "code": "# Start processing messages.\nruntime.start()\n# Send a direct message to the tool agent.\ntool_use_agent = AgentId(\"tool_use_agent\", \"default\")\nresponse = await runtime.send_message(Message(\"What is the stock price of NVDA on 2024/06/01?\"), tool_use_agent)\nprint(response.content)\n# Stop processing messages.\nawait runtime.stop()\nawait model_client.close()",
      "language": "python"
    },
    {
      "code": "The stock price of NVIDIA (NVDA) on June 1, 2024, was approximately $140.05.",
      "language": "bash"
    },
    {
      "code": "The stock price of NVIDIA (NVDA) on June 1, 2024, was approximately $140.05.",
      "language": "bash"
    }
  ],
  "patterns": [],
  "links": [
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/components/tools.html",
    "https://microsoft.github.io/autogen/stable/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/extensions-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/reference/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/installation.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/quickstart.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/core-concepts/agent-and-multi-agent-application.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/core-concepts/architecture.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/core-concepts/application-stack.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/core-concepts/agent-identity-and-lifecycle.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/core-concepts/topic-and-subscription.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/framework/agent-and-agent-runtime.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/framework/message-and-communication.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/framework/logging.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/framework/telemetry.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/framework/distributed-agent-runtime.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/framework/component-config.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/components/model-clients.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/components/model-context.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/components/workbench.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/components/command-line-code-executors.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/design-patterns/intro.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/design-patterns/concurrent-agents.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/design-patterns/sequential-workflow.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/design-patterns/group-chat.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/design-patterns/handoffs.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/design-patterns/mixture-of-agents.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/design-patterns/multi-agent-debate.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/design-patterns/reflection.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/design-patterns/code-execution-groupchat.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/cookbook/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/cookbook/azure-openai-with-aad-auth.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/cookbook/termination-with-intervention.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/cookbook/tool-use-with-intervention.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/cookbook/extracting-results-with-an-agent.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/cookbook/openai-assistant-agent.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/cookbook/langgraph-agent.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/cookbook/llamaindex-agent.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/cookbook/local-llms-ollama-litellm.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/cookbook/instrumenting.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/cookbook/topic-subscription-scenarios.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/cookbook/structured-output-agent.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/cookbook/llm-usage-logger.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/faqs.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.tools.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.code_execution.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.docker.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.graphrag.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.mcp.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.http.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.langchain.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.openai.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.models.html"
  ]
}