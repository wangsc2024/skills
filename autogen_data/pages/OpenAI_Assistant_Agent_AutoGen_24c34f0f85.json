{
  "url": "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/cookbook/openai-assistant-agent.html",
  "title": "OpenAI Assistant Agent — AutoGen",
  "content": "Open AI Assistant and Azure OpenAI Assistant are server-side APIs for building agents. They can be used to build agents in AutoGen. This cookbook demonstrates how to to use OpenAI Assistant to create an agent that can run code and Q&A over document.\n\nFirst, we need to specify the message protocol for the agent backed by OpenAI Assistant. The message protocol defines the structure of messages handled and published by the agent. For illustration, we define a simple message protocol of 4 message types: Message, Reset, UploadForCodeInterpreter and UploadForFileSearch.\n\nThe TextMessage message type is used to communicate with the agent. It has a content field that contains the message content, and a source field for the sender. The Reset message type is a control message that resets the memory of the agent. It has no fields. This is useful when we need to start a new conversation with the agent.\n\nThe UploadForCodeInterpreter message type is used to upload data files for the code interpreter and UploadForFileSearch message type is used to upload documents for file search. Both message types have a file_path field that contains the local path to the file to be uploaded.\n\nNext, we define the agent class. The agent class constructor has the following arguments: description, client, assistant_id, thread_id, and assistant_event_handler_factory. The client argument is the OpenAI async client object, and the assistant_event_handler_factory is for creating an assistant event handler to handle OpenAI Assistant events. This can be used to create streaming output from the assistant.\n\nThe agent class has the following message handlers:\n\nhandle_message: Handles the TextMessage message type, and sends back the response from the assistant.\n\nhandle_reset: Handles the Reset message type, and resets the memory of the assistant agent.\n\nhandle_upload_for_code_interpreter: Handles the UploadForCodeInterpreter message type, and uploads the file to the code interpreter.\n\nhandle_upload_for_file_search: Handles the UploadForFileSearch message type, and uploads the document to the file search.\n\nThe memory of the assistant is stored inside a thread, which is kept in the server side. The thread is referenced by the thread_id argument.\n\nThe agent class is a thin wrapper around the OpenAI Assistant API to implement the message protocol. More features, such as multi-modal message handling, can be added by extending the message protocol.\n\nThe assistant event handler provides call-backs for handling Assistant API specific events. This is useful for handling streaming output from the assistant and further user interface integration.\n\nFirst we need to use the openai client to create the actual assistant, thread, and vector store. Our AutoGen agent will be using these.\n\nThen, we create a runtime, and register an agent factory function for this agent with the runtime.\n\nLet’s turn on logging to see what’s happening under the hood.\n\nLet’s send a greeting message to the agent, and see the response streamed back.\n\nLet’s ask some math question to the agent, and see it uses the code interpreter to answer the question.\n\nLet’s get some data from Seattle Open Data portal. We will be using the City of Seattle Wage Data. Let’s download it first.\n\nLet’s send the file to the agent using an UploadForCodeInterpreter message.\n\nWe can now ask some questions about the data to the agent.\n\nLet’s try the Q&A over document feature. We first download Wikipedia page on the Third Anglo-Afghan War.\n\nSend the file to the agent using an UploadForFileSearch message.\n\nLet’s ask some questions about the document to the agent. Before asking, we reset the agent memory to start a new conversation.\n\nThat’s it! We have successfully built an agent backed by OpenAI Assistant.\n\nExtracting Results with an Agent\n\nUsing LangGraph-Backed Agent",
  "headings": [
    {
      "level": "h1",
      "text": "OpenAI Assistant Agent#",
      "id": ""
    },
    {
      "level": "h2",
      "text": "Message Protocol#",
      "id": ""
    },
    {
      "level": "h2",
      "text": "Defining the Agent#",
      "id": ""
    },
    {
      "level": "h2",
      "text": "Assistant Event Handler#",
      "id": ""
    },
    {
      "level": "h2",
      "text": "Using the Agent#",
      "id": ""
    },
    {
      "level": "h2",
      "text": "Assistant with Code Interpreter#",
      "id": ""
    },
    {
      "level": "h2",
      "text": "Assistant with File Search#",
      "id": ""
    }
  ],
  "code_samples": [
    {
      "code": "from dataclasses import dataclass\n\n\n@dataclass\nclass TextMessage:\n    content: str\n    source: str\n\n\n@dataclass\nclass Reset:\n    pass\n\n\n@dataclass\nclass UploadForCodeInterpreter:\n    file_path: str\n\n\n@dataclass\nclass UploadForFileSearch:\n    file_path: str\n    vector_store_id: str",
      "language": "python"
    },
    {
      "code": "from dataclasses import dataclass\n\n\n@dataclass\nclass TextMessage:\n    content: str\n    source: str\n\n\n@dataclass\nclass Reset:\n    pass\n\n\n@dataclass\nclass UploadForCodeInterpreter:\n    file_path: str\n\n\n@dataclass\nclass UploadForFileSearch:\n    file_path: str\n    vector_store_id: str",
      "language": "python"
    },
    {
      "code": "import asyncio\nimport os\nfrom typing import Any, Callable, List\n\nimport aiofiles\nfrom autogen_core import AgentId, MessageContext, RoutedAgent, message_handler\nfrom openai import AsyncAssistantEventHandler, AsyncClient\nfrom openai.types.beta.thread import ToolResources, ToolResourcesFileSearch\n\n\nclass OpenAIAssistantAgent(RoutedAgent):\n    \"\"\"An agent implementation that uses the OpenAI Assistant API to generate\n    responses.\n\n    Args:\n        description (str): The description of the agent.\n        client (openai.AsyncClient): The client to use for the OpenAI API.\n        assistant_id (str): The assistant ID to use for the OpenAI API.\n        thread_id (str): The thread ID to use for the OpenAI API.\n        assistant_event_handler_factory (Callable[[], AsyncAssistantEventHandler], optional):\n            A factory function to create an async assistant event handler. Defaults to None.\n            If provided, the agent will use the streaming mode with the event handler.\n            If not provided, the agent will use the blocking mode to generate responses.\n    \"\"\"\n\n    def __init__(\n        self,\n        description: str,\n        client: AsyncClient,\n        assistant_id: str,\n        thread_id: str,\n        assistant_event_handler_factory: Callable[[], AsyncAssistantEventHandler],\n    ) -> None:\n        super().__init__(description)\n        self._client = client\n        self._assistant_id = assistant_id\n        self._thread_id = thread_id\n        self._assistant_event_handler_factory = assistant_event_handler_factory\n\n    @message_handler\n    async def handle_message(self, message: TextMessage, ctx: MessageContext) -> TextMessage:\n        \"\"\"Handle a message. This method adds the message to the thread and publishes a response.\"\"\"\n        # Save the message to the thread.\n        await ctx.cancellation_token.link_future(\n            asyncio.ensure_future(\n                self._client.beta.threads.messages.create(\n                    thread_id=self._thread_id,\n                    content=message.content,\n                    role=\"user\",\n                    metadata={\"sender\": message.source},\n                )\n            )\n        )\n        # Generate a response.\n        async with self._client.beta.threads.runs.stream(\n            thread_id=self._thread_id,\n            assistant_id=self._assistant_id,\n            event_handler=self._assistant_event_handler_factory(),\n        ) as stream:\n            await ctx.cancellation_token.link_future(asyncio.ensure_future(stream.until_done()))\n\n        # Get the last message.\n        messages = await ctx.cancellation_token.link_future(\n            asyncio.ensure_future(self._client.beta.threads.messages.list(self._thread_id, order=\"desc\", limit=1))\n        )\n        last_message_content = messages.data[0].content\n\n        # Get the text content from the last message.\n        text_content = [content for content in last_message_content if content.type == \"text\"]\n        if not text_content:\n            raise ValueError(f\"Expected text content in the last message: {last_message_content}\")\n\n        return TextMessage(content=text_content[0].text.value, source=self.metadata[\"type\"])\n\n    @message_handler()\n    async def on_reset(self, message: Reset, ctx: MessageContext) -> None:\n        \"\"\"Handle a reset message. This method deletes all messages in the thread.\"\"\"\n        # Get all messages in this thread.\n        all_msgs: List[str] = []\n        while True:\n            if not all_msgs:\n                msgs = await ctx.cancellation_token.link_future(\n                    asyncio.ensure_future(self._client.beta.threads.messages.list(self._thread_id))\n                )\n            else:\n                msgs = await ctx.cancellation_token.link_future(\n                    asyncio.ensure_future(self._client.beta.threads.messages.list(self._thread_id, after=all_msgs[-1]))\n                )\n            for msg in msgs.data:\n                all_msgs.append(msg.id)\n            if not msgs.has_next_page():\n                break\n        # Delete all the messages.\n        for msg_id in all_msgs:\n            status = await ctx.cancellation_token.link_future(\n                asyncio.ensure_future(\n                    self._client.beta.threads.messages.delete(message_id=msg_id, thread_id=self._thread_id)\n                )\n            )\n            assert status.deleted is True\n\n    @message_handler()\n    async def on_upload_for_code_interpreter(self, message: UploadForCodeInterpreter, ctx: MessageContext) -> None:\n        \"\"\"Handle an upload for code interpreter. This method uploads a file and updates the thread with the file.\"\"\"\n        # Get the file content.\n        async with aiofiles.open(message.file_path, mode=\"rb\") as f:\n            file_content = await ctx.cancellation_token.link_future(asyncio.ensure_future(f.read()))\n        file_name = os.path.basename(message.file_path)\n        # Upload the file.\n        file = await ctx.cancellation_token.link_future(\n            asyncio.ensure_future(self._client.files.create(file=(file_name, file_content), purpose=\"assistants\"))\n        )\n        # Get existing file ids from tool resources.\n        thread = await ctx.cancellation_token.link_future(\n            asyncio.ensure_future(self._client.beta.threads.retrieve(thread_id=self._thread_id))\n        )\n        tool_resources: ToolResources = thread.tool_resources if thread.tool_resources else ToolResources()\n        assert tool_resources.code_interpreter is not None\n        if tool_resources.code_interpreter.file_ids:\n            file_ids = tool_resources.code_interpreter.file_ids\n        else:\n            file_ids = [file.id]\n        # Update thread with new file.\n        await ctx.cancellation_token.link_future(\n            asyncio.ensure_future(\n                self._client.beta.threads.update(\n                    thread_id=self._thread_id,\n                    tool_resources={\n                        \"code_interpreter\": {\"file_ids\": file_ids},\n                    },\n                )\n            )\n        )\n\n    @message_handler()\n    async def on_upload_for_file_search(self, message: UploadForFileSearch, ctx: MessageContext) -> None:\n        \"\"\"Handle an upload for file search. This method uploads a file and updates the vector store.\"\"\"\n        # Get the file content.\n        async with aiofiles.open(message.file_path, mode=\"rb\") as file:\n            file_content = await ctx.cancellation_token.link_future(asyncio.ensure_future(file.read()))\n        file_name = os.path.basename(message.file_path)\n        # Upload the file.\n        await ctx.cancellation_token.link_future(\n            asyncio.ensure_future(\n                self._client.vector_stores.file_batches.upload_and_poll(\n                    vector_store_id=message.vector_store_id,\n                    files=[(file_name, file_content)],\n                )\n            )\n        )",
      "language": "python"
    },
    {
      "code": "import asyncio\nimport os\nfrom typing import Any, Callable, List\n\nimport aiofiles\nfrom autogen_core import AgentId, MessageContext, RoutedAgent, message_handler\nfrom openai import AsyncAssistantEventHandler, AsyncClient\nfrom openai.types.beta.thread import ToolResources, ToolResourcesFileSearch\n\n\nclass OpenAIAssistantAgent(RoutedAgent):\n    \"\"\"An agent implementation that uses the OpenAI Assistant API to generate\n    responses.\n\n    Args:\n        description (str): The description of the agent.\n        client (openai.AsyncClient): The client to use for the OpenAI API.\n        assistant_id (str): The assistant ID to use for the OpenAI API.\n        thread_id (str): The thread ID to use for the OpenAI API.\n        assistant_event_handler_factory (Callable[[], AsyncAssistantEventHandler], optional):\n            A factory function to create an async assistant event handler. Defaults to None.\n            If provided, the agent will use the streaming mode with the event handler.\n            If not provided, the agent will use the blocking mode to generate responses.\n    \"\"\"\n\n    def __init__(\n        self,\n        description: str,\n        client: AsyncClient,\n        assistant_id: str,\n        thread_id: str,\n        assistant_event_handler_factory: Callable[[], AsyncAssistantEventHandler],\n    ) -> None:\n        super().__init__(description)\n        self._client = client\n        self._assistant_id = assistant_id\n        self._thread_id = thread_id\n        self._assistant_event_handler_factory = assistant_event_handler_factory\n\n    @message_handler\n    async def handle_message(self, message: TextMessage, ctx: MessageContext) -> TextMessage:\n        \"\"\"Handle a message. This method adds the message to the thread and publishes a response.\"\"\"\n        # Save the message to the thread.\n        await ctx.cancellation_token.link_future(\n            asyncio.ensure_future(\n                self._client.beta.threads.messages.create(\n                    thread_id=self._thread_id,\n                    content=message.content,\n                    role=\"user\",\n                    metadata={\"sender\": message.source},\n                )\n            )\n        )\n        # Generate a response.\n        async with self._client.beta.threads.runs.stream(\n            thread_id=self._thread_id,\n            assistant_id=self._assistant_id,\n            event_handler=self._assistant_event_handler_factory(),\n        ) as stream:\n            await ctx.cancellation_token.link_future(asyncio.ensure_future(stream.until_done()))\n\n        # Get the last message.\n        messages = await ctx.cancellation_token.link_future(\n            asyncio.ensure_future(self._client.beta.threads.messages.list(self._thread_id, order=\"desc\", limit=1))\n        )\n        last_message_content = messages.data[0].content\n\n        # Get the text content from the last message.\n        text_content = [content for content in last_message_content if content.type == \"text\"]\n        if not text_content:\n            raise ValueError(f\"Expected text content in the last message: {last_message_content}\")\n\n        return TextMessage(content=text_content[0].text.value, source=self.metadata[\"type\"])\n\n    @message_handler()\n    async def on_reset(self, message: Reset, ctx: MessageContext) -> None:\n        \"\"\"Handle a reset message. This method deletes all messages in the thread.\"\"\"\n        # Get all messages in this thread.\n        all_msgs: List[str] = []\n        while True:\n            if not all_msgs:\n                msgs = await ctx.cancellation_token.link_future(\n                    asyncio.ensure_future(self._client.beta.threads.messages.list(self._thread_id))\n                )\n            else:\n                msgs = await ctx.cancellation_token.link_future(\n                    asyncio.ensure_future(self._client.beta.threads.messages.list(self._thread_id, after=all_msgs[-1]))\n                )\n            for msg in msgs.data:\n                all_msgs.append(msg.id)\n            if not msgs.has_next_page():\n                break\n        # Delete all the messages.\n        for msg_id in all_msgs:\n            status = await ctx.cancellation_token.link_future(\n                asyncio.ensure_future(\n                    self._client.beta.threads.messages.delete(message_id=msg_id, thread_id=self._thread_id)\n                )\n            )\n            assert status.deleted is True\n\n    @message_handler()\n    async def on_upload_for_code_interpreter(self, message: UploadForCodeInterpreter, ctx: MessageContext) -> None:\n        \"\"\"Handle an upload for code interpreter. This method uploads a file and updates the thread with the file.\"\"\"\n        # Get the file content.\n        async with aiofiles.open(message.file_path, mode=\"rb\") as f:\n            file_content = await ctx.cancellation_token.link_future(asyncio.ensure_future(f.read()))\n        file_name = os.path.basename(message.file_path)\n        # Upload the file.\n        file = await ctx.cancellation_token.link_future(\n            asyncio.ensure_future(self._client.files.create(file=(file_name, file_content), purpose=\"assistants\"))\n        )\n        # Get existing file ids from tool resources.\n        thread = await ctx.cancellation_token.link_future(\n            asyncio.ensure_future(self._client.beta.threads.retrieve(thread_id=self._thread_id))\n        )\n        tool_resources: ToolResources = thread.tool_resources if thread.tool_resources else ToolResources()\n        assert tool_resources.code_interpreter is not None\n        if tool_resources.code_interpreter.file_ids:\n            file_ids = tool_resources.code_interpreter.file_ids\n        else:\n            file_ids = [file.id]\n        # Update thread with new file.\n        await ctx.cancellation_token.link_future(\n            asyncio.ensure_future(\n                self._client.beta.threads.update(\n                    thread_id=self._thread_id,\n                    tool_resources={\n                        \"code_interpreter\": {\"file_ids\": file_ids},\n                    },\n                )\n            )\n        )\n\n    @message_handler()\n    async def on_upload_for_file_search(self, message: UploadForFileSearch, ctx: MessageContext) -> None:\n        \"\"\"Handle an upload for file search. This method uploads a file and updates the vector store.\"\"\"\n        # Get the file content.\n        async with aiofiles.open(message.file_path, mode=\"rb\") as file:\n            file_content = await ctx.cancellation_token.link_future(asyncio.ensure_future(file.read()))\n        file_name = os.path.basename(message.file_path)\n        # Upload the file.\n        await ctx.cancellation_token.link_future(\n            asyncio.ensure_future(\n                self._client.vector_stores.file_batches.upload_and_poll(\n                    vector_store_id=message.vector_store_id,\n                    files=[(file_name, file_content)],\n                )\n            )\n        )",
      "language": "python"
    },
    {
      "code": "from openai import AsyncAssistantEventHandler, AsyncClient\nfrom openai.types.beta.threads import Message, Text, TextDelta\nfrom openai.types.beta.threads.runs import RunStep, RunStepDelta\nfrom typing_extensions import override\n\n\nclass EventHandler(AsyncAssistantEventHandler):\n    @override\n    async def on_text_delta(self, delta: TextDelta, snapshot: Text) -> None:\n        print(delta.value, end=\"\", flush=True)\n\n    @override\n    async def on_run_step_created(self, run_step: RunStep) -> None:\n        details = run_step.step_details\n        if details.type == \"tool_calls\":\n            for tool in details.tool_calls:\n                if tool.type == \"code_interpreter\":\n                    print(\"\\nGenerating code to interpret:\\n\\n```python\")\n\n    @override\n    async def on_run_step_done(self, run_step: RunStep) -> None:\n        details = run_step.step_details\n        if details.type == \"tool_calls\":\n            for tool in details.tool_calls:\n                if tool.type == \"code_interpreter\":\n                    print(\"\\n```\\nExecuting code...\")\n\n    @override\n    async def on_run_step_delta(self, delta: RunStepDelta, snapshot: RunStep) -> None:\n        details = delta.step_details\n        if details is not None and details.type == \"tool_calls\":\n            for tool in details.tool_calls or []:\n                if tool.type == \"code_interpreter\" and tool.code_interpreter and tool.code_interpreter.input:\n                    print(tool.code_interpreter.input, end=\"\", flush=True)\n\n    @override\n    async def on_message_created(self, message: Message) -> None:\n        print(f\"{'-'*80}\\nAssistant:\\n\")\n\n    @override\n    async def on_message_done(self, message: Message) -> None:\n        # print a citation to the file searched\n        if not message.content:\n            return\n        content = message.content[0]\n        if not content.type == \"text\":\n            return\n        text_content = content.text\n        annotations = text_content.annotations\n        citations: List[str] = []\n        for index, annotation in enumerate(annotations):\n            text_content.value = text_content.value.replace(annotation.text, f\"[{index}]\")\n            if file_citation := getattr(annotation, \"file_citation\", None):\n                client = AsyncClient()\n                cited_file = await client.files.retrieve(file_citation.file_id)\n                citations.append(f\"[{index}] {cited_file.filename}\")\n        if citations:\n            print(\"\\n\".join(citations))",
      "language": "python"
    },
    {
      "code": "from openai import AsyncAssistantEventHandler, AsyncClient\nfrom openai.types.beta.threads import Message, Text, TextDelta\nfrom openai.types.beta.threads.runs import RunStep, RunStepDelta\nfrom typing_extensions import override\n\n\nclass EventHandler(AsyncAssistantEventHandler):\n    @override\n    async def on_text_delta(self, delta: TextDelta, snapshot: Text) -> None:\n        print(delta.value, end=\"\", flush=True)\n\n    @override\n    async def on_run_step_created(self, run_step: RunStep) -> None:\n        details = run_step.step_details\n        if details.type == \"tool_calls\":\n            for tool in details.tool_calls:\n                if tool.type == \"code_interpreter\":\n                    print(\"\\nGenerating code to interpret:\\n\\n```python\")\n\n    @override\n    async def on_run_step_done(self, run_step: RunStep) -> None:\n        details = run_step.step_details\n        if details.type == \"tool_calls\":\n            for tool in details.tool_calls:\n                if tool.type == \"code_interpreter\":\n                    print(\"\\n```\\nExecuting code...\")\n\n    @override\n    async def on_run_step_delta(self, delta: RunStepDelta, snapshot: RunStep) -> None:\n        details = delta.step_details\n        if details is not None and details.type == \"tool_calls\":\n            for tool in details.tool_calls or []:\n                if tool.type == \"code_interpreter\" and tool.code_interpreter and tool.code_interpreter.input:\n                    print(tool.code_interpreter.input, end=\"\", flush=True)\n\n    @override\n    async def on_message_created(self, message: Message) -> None:\n        print(f\"{'-'*80}\\nAssistant:\\n\")\n\n    @override\n    async def on_message_done(self, message: Message) -> None:\n        # print a citation to the file searched\n        if not message.content:\n            return\n        content = message.content[0]\n        if not content.type == \"text\":\n            return\n        text_content = content.text\n        annotations = text_content.annotations\n        citations: List[str] = []\n        for index, annotation in enumerate(annotations):\n            text_content.value = text_content.value.replace(annotation.text, f\"[{index}]\")\n            if file_citation := getattr(annotation, \"file_citation\", None):\n                client = AsyncClient()\n                cited_file = await client.files.retrieve(file_citation.file_id)\n                citations.append(f\"[{index}] {cited_file.filename}\")\n        if citations:\n            print(\"\\n\".join(citations))",
      "language": "python"
    },
    {
      "code": "import openai\n\n# Create an assistant with code interpreter and file search tools.\noai_assistant = openai.beta.assistants.create(\n    model=\"gpt-4o-mini\",\n    description=\"An AI assistant that helps with everyday tasks.\",\n    instructions=\"Help the user with their task.\",\n    tools=[{\"type\": \"code_interpreter\"}, {\"type\": \"file_search\"}],\n)\n\n# Create a vector store to be used for file search.\nvector_store = openai.vector_stores.create()\n\n# Create a thread which is used as the memory for the assistant.\nthread = openai.beta.threads.create(\n    tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n)",
      "language": "json"
    },
    {
      "code": "import openai\n\n# Create an assistant with code interpreter and file search tools.\noai_assistant = openai.beta.assistants.create(\n    model=\"gpt-4o-mini\",\n    description=\"An AI assistant that helps with everyday tasks.\",\n    instructions=\"Help the user with their task.\",\n    tools=[{\"type\": \"code_interpreter\"}, {\"type\": \"file_search\"}],\n)\n\n# Create a vector store to be used for file search.\nvector_store = openai.vector_stores.create()\n\n# Create a thread which is used as the memory for the assistant.\nthread = openai.beta.threads.create(\n    tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n)",
      "language": "json"
    },
    {
      "code": "from autogen_core import SingleThreadedAgentRuntime\n\nruntime = SingleThreadedAgentRuntime()\nawait OpenAIAssistantAgent.register(\n    runtime,\n    \"assistant\",\n    lambda: OpenAIAssistantAgent(\n        description=\"OpenAI Assistant Agent\",\n        client=openai.AsyncClient(),\n        assistant_id=oai_assistant.id,\n        thread_id=thread.id,\n        assistant_event_handler_factory=lambda: EventHandler(),\n    ),\n)\nagent = AgentId(\"assistant\", \"default\")",
      "language": "python"
    },
    {
      "code": "from autogen_core import SingleThreadedAgentRuntime\n\nruntime = SingleThreadedAgentRuntime()\nawait OpenAIAssistantAgent.register(\n    runtime,\n    \"assistant\",\n    lambda: OpenAIAssistantAgent(\n        description=\"OpenAI Assistant Agent\",\n        client=openai.AsyncClient(),\n        assistant_id=oai_assistant.id,\n        thread_id=thread.id,\n        assistant_event_handler_factory=lambda: EventHandler(),\n    ),\n)\nagent = AgentId(\"assistant\", \"default\")",
      "language": "python"
    },
    {
      "code": "import logging\n\nlogging.basicConfig(level=logging.WARNING)\nlogging.getLogger(\"autogen_core\").setLevel(logging.DEBUG)",
      "language": "python"
    },
    {
      "code": "import logging\n\nlogging.basicConfig(level=logging.WARNING)\nlogging.getLogger(\"autogen_core\").setLevel(logging.DEBUG)",
      "language": "python"
    },
    {
      "code": "runtime.start()\nawait runtime.send_message(TextMessage(content=\"Hello, how are you today!\", source=\"user\"), agent)\nawait runtime.stop_when_idle()",
      "language": "csharp"
    },
    {
      "code": "runtime.start()\nawait runtime.send_message(TextMessage(content=\"Hello, how are you today!\", source=\"user\"), agent)\nawait runtime.stop_when_idle()",
      "language": "csharp"
    },
    {
      "code": "INFO:autogen_core:Sending message of type TextMessage to assistant: {'content': 'Hello, how are you today!', 'source': 'user'}\nINFO:autogen_core:Calling message handler for assistant:default with message type TextMessage sent by Unknown",
      "language": "yaml"
    },
    {
      "code": "INFO:autogen_core:Sending message of type TextMessage to assistant: {'content': 'Hello, how are you today!', 'source': 'user'}\nINFO:autogen_core:Calling message handler for assistant:default with message type TextMessage sent by Unknown",
      "language": "yaml"
    },
    {
      "code": "--------------------------------------------------------------------------------\nAssistant:\n\nHello! I'm here and ready to assist you. How can I help you today?",
      "language": "yaml"
    },
    {
      "code": "--------------------------------------------------------------------------------\nAssistant:\n\nHello! I'm here and ready to assist you. How can I help you today?",
      "language": "yaml"
    },
    {
      "code": "INFO:autogen_core:Resolving response with message type TextMessage for recipient None from assistant: {'content': \"Hello! I'm here and ready to assist you. How can I help you today?\", 'source': 'assistant'}",
      "language": "sql"
    },
    {
      "code": "INFO:autogen_core:Resolving response with message type TextMessage for recipient None from assistant: {'content': \"Hello! I'm here and ready to assist you. How can I help you today?\", 'source': 'assistant'}",
      "language": "sql"
    },
    {
      "code": "runtime.start()\nawait runtime.send_message(TextMessage(content=\"What is 1332322 x 123212?\", source=\"user\"), agent)\nawait runtime.stop_when_idle()",
      "language": "csharp"
    },
    {
      "code": "runtime.start()\nawait runtime.send_message(TextMessage(content=\"What is 1332322 x 123212?\", source=\"user\"), agent)\nawait runtime.stop_when_idle()",
      "language": "csharp"
    },
    {
      "code": "INFO:autogen_core:Sending message of type TextMessage to assistant: {'content': 'What is 1332322 x 123212?', 'source': 'user'}\nINFO:autogen_core:Calling message handler for assistant:default with message type TextMessage sent by Unknown",
      "language": "yaml"
    },
    {
      "code": "INFO:autogen_core:Sending message of type TextMessage to assistant: {'content': 'What is 1332322 x 123212?', 'source': 'user'}\nINFO:autogen_core:Calling message handler for assistant:default with message type TextMessage sent by Unknown",
      "language": "yaml"
    },
    {
      "code": "# Calculating the product of 1332322 and 123212\nresult = 1332322 * 123212\nresult\n```\nExecuting code...\n--------------------------------------------------------------------------------\nAssistant:\n\nThe product of 1,332,322 and 123,212 is 164,158,058,264.",
      "language": "yaml"
    },
    {
      "code": "# Calculating the product of 1332322 and 123212\nresult = 1332322 * 123212\nresult\n```\nExecuting code...\n--------------------------------------------------------------------------------\nAssistant:\n\nThe product of 1,332,322 and 123,212 is 164,158,058,264.",
      "language": "yaml"
    },
    {
      "code": "INFO:autogen_core:Resolving response with message type TextMessage for recipient None from assistant: {'content': 'The product of 1,332,322 and 123,212 is 164,158,058,264.', 'source': 'assistant'}",
      "language": "sql"
    },
    {
      "code": "INFO:autogen_core:Resolving response with message type TextMessage for recipient None from assistant: {'content': 'The product of 1,332,322 and 123,212 is 164,158,058,264.', 'source': 'assistant'}",
      "language": "sql"
    },
    {
      "code": "import requests\n\nresponse = requests.get(\"https://data.seattle.gov/resource/2khk-5ukd.csv\")\nwith open(\"seattle_city_wages.csv\", \"wb\") as file:\n    file.write(response.content)",
      "language": "python"
    },
    {
      "code": "import requests\n\nresponse = requests.get(\"https://data.seattle.gov/resource/2khk-5ukd.csv\")\nwith open(\"seattle_city_wages.csv\", \"wb\") as file:\n    file.write(response.content)",
      "language": "python"
    },
    {
      "code": "runtime.start()\nawait runtime.send_message(UploadForCodeInterpreter(file_path=\"seattle_city_wages.csv\"), agent)\nawait runtime.stop_when_idle()",
      "language": "csharp"
    },
    {
      "code": "runtime.start()\nawait runtime.send_message(UploadForCodeInterpreter(file_path=\"seattle_city_wages.csv\"), agent)\nawait runtime.stop_when_idle()",
      "language": "csharp"
    },
    {
      "code": "INFO:autogen_core:Sending message of type UploadForCodeInterpreter to assistant: {'file_path': 'seattle_city_wages.csv'}\nINFO:autogen_core:Calling message handler for assistant:default with message type UploadForCodeInterpreter sent by Unknown\nINFO:autogen_core:Resolving response with message type NoneType for recipient None from assistant: None",
      "language": "sql"
    },
    {
      "code": "INFO:autogen_core:Sending message of type UploadForCodeInterpreter to assistant: {'file_path': 'seattle_city_wages.csv'}\nINFO:autogen_core:Calling message handler for assistant:default with message type UploadForCodeInterpreter sent by Unknown\nINFO:autogen_core:Resolving response with message type NoneType for recipient None from assistant: None",
      "language": "sql"
    },
    {
      "code": "runtime.start()\nawait runtime.send_message(TextMessage(content=\"Take a look at the uploaded CSV file.\", source=\"user\"), agent)\nawait runtime.stop_when_idle()",
      "language": "csharp"
    },
    {
      "code": "runtime.start()\nawait runtime.send_message(TextMessage(content=\"Take a look at the uploaded CSV file.\", source=\"user\"), agent)\nawait runtime.stop_when_idle()",
      "language": "csharp"
    },
    {
      "code": "INFO:autogen_core:Sending message of type TextMessage to assistant: {'content': 'Take a look at the uploaded CSV file.', 'source': 'user'}\nINFO:autogen_core:Calling message handler for assistant:default with message type TextMessage sent by Unknown",
      "language": "yaml"
    },
    {
      "code": "INFO:autogen_core:Sending message of type TextMessage to assistant: {'content': 'Take a look at the uploaded CSV file.', 'source': 'user'}\nINFO:autogen_core:Calling message handler for assistant:default with message type TextMessage sent by Unknown",
      "language": "yaml"
    },
    {
      "code": "import pandas as pd\n\n# Load the uploaded CSV file to examine its contents\nfile_path = '/mnt/data/file-oEvRiyGyHc2jZViKyDqL8aoh'\ncsv_data = pd.read_csv(file_path)\n\n# Display the first few rows of the dataframe to understand its structure\ncsv_data.head()\n```\nExecuting code...\n--------------------------------------------------------------------------------\nAssistant:\n\nThe uploaded CSV file contains the following columns:\n\n1. **department**: The department in which the individual works.\n2. **last_name**: The last name of the employee.\n3. **first_name**: The first name of the employee.\n4. **job_title**: The job title of the employee.\n5. **hourly_rate**: The hourly rate for the employee's position.\n\nHere are the first few entries from the file:\n\n| department                     | last_name | first_name | job_title                          | hourly_rate |\n|--------------------------------|-----------|------------|------------------------------------|-------------|\n| Police Department              | Aagard    | Lori       | Pol Capt-Precinct                 | 112.70      |\n| Police Department              | Aakervik  | Dag        | Pol Ofcr-Detective                | 75.61       |\n| Seattle City Light             | Aaltonen  | Evan       | Pwrline Clear Tree Trimmer        | 53.06       |\n| Seattle Public Utilities       | Aar       | Abdimallik | Civil Engrng Spec,Sr               | 64.43       |\n| Seattle Dept of Transportation | Abad      | Abigail    | Admin Spec II-BU                  | 37.40       |\n\nIf you need any specific analysis or information from this data, please let me know!",
      "language": "yaml"
    },
    {
      "code": "import pandas as pd\n\n# Load the uploaded CSV file to examine its contents\nfile_path = '/mnt/data/file-oEvRiyGyHc2jZViKyDqL8aoh'\ncsv_data = pd.read_csv(file_path)\n\n# Display the first few rows of the dataframe to understand its structure\ncsv_data.head()\n```\nExecuting code...\n--------------------------------------------------------------------------------\nAssistant:\n\nThe uploaded CSV file contains the following columns:\n\n1. **department**: The department in which the individual works.\n2. **last_name**: The last name of the employee.\n3. **first_name**: The first name of the employee.\n4. **job_title**: The job title of the employee.\n5. **hourly_rate**: The hourly rate for the employee's position.\n\nHere are the first few entries from the file:\n\n| department                     | last_name | first_name | job_title                          | hourly_rate |\n|--------------------------------|-----------|------------|------------------------------------|-------------|\n| Police Department              | Aagard    | Lori       | Pol Capt-Precinct                 | 112.70      |\n| Police Department              | Aakervik  | Dag        | Pol Ofcr-Detective                | 75.61       |\n| Seattle City Light             | Aaltonen  | Evan       | Pwrline Clear Tree Trimmer        | 53.06       |\n| Seattle Public Utilities       | Aar       | Abdimallik | Civil Engrng Spec,Sr               | 64.43       |\n| Seattle Dept of Transportation | Abad      | Abigail    | Admin Spec II-BU                  | 37.40       |\n\nIf you need any specific analysis or information from this data, please let me know!",
      "language": "yaml"
    },
    {
      "code": "INFO:autogen_core:Resolving response with message type TextMessage for recipient None from assistant: {'content': \"The uploaded CSV file contains the following columns:\\n\\n1. **department**: The department in which the individual works.\\n2. **last_name**: The last name of the employee.\\n3. **first_name**: The first name of the employee.\\n4. **job_title**: The job title of the employee.\\n5. **hourly_rate**: The hourly rate for the employee's position.\\n\\nHere are the first few entries from the file:\\n\\n| department                     | last_name | first_name | job_title                          | hourly_rate |\\n|--------------------------------|-----------|------------|------------------------------------|-------------|\\n| Police Department              | Aagard    | Lori       | Pol Capt-Precinct                 | 112.70      |\\n| Police Department              | Aakervik  | Dag        | Pol Ofcr-Detective                | 75.61       |\\n| Seattle City Light             | Aaltonen  | Evan       | Pwrline Clear Tree Trimmer        | 53.06       |\\n| Seattle Public Utilities       | Aar       | Abdimallik | Civil Engrng Spec,Sr               | 64.43       |\\n| Seattle Dept of Transportation | Abad      | Abigail    | Admin Spec II-BU                  | 37.40       |\\n\\nIf you need any specific analysis or information from this data, please let me know!\", 'source': 'assistant'}",
      "language": "yaml"
    },
    {
      "code": "INFO:autogen_core:Resolving response with message type TextMessage for recipient None from assistant: {'content': \"The uploaded CSV file contains the following columns:\\n\\n1. **department**: The department in which the individual works.\\n2. **last_name**: The last name of the employee.\\n3. **first_name**: The first name of the employee.\\n4. **job_title**: The job title of the employee.\\n5. **hourly_rate**: The hourly rate for the employee's position.\\n\\nHere are the first few entries from the file:\\n\\n| department                     | last_name | first_name | job_title                          | hourly_rate |\\n|--------------------------------|-----------|------------|------------------------------------|-------------|\\n| Police Department              | Aagard    | Lori       | Pol Capt-Precinct                 | 112.70      |\\n| Police Department              | Aakervik  | Dag        | Pol Ofcr-Detective                | 75.61       |\\n| Seattle City Light             | Aaltonen  | Evan       | Pwrline Clear Tree Trimmer        | 53.06       |\\n| Seattle Public Utilities       | Aar       | Abdimallik | Civil Engrng Spec,Sr               | 64.43       |\\n| Seattle Dept of Transportation | Abad      | Abigail    | Admin Spec II-BU                  | 37.40       |\\n\\nIf you need any specific analysis or information from this data, please let me know!\", 'source': 'assistant'}",
      "language": "yaml"
    },
    {
      "code": "runtime.start()\nawait runtime.send_message(TextMessage(content=\"What are the top-10 salaries?\", source=\"user\"), agent)\nawait runtime.stop_when_idle()",
      "language": "csharp"
    },
    {
      "code": "runtime.start()\nawait runtime.send_message(TextMessage(content=\"What are the top-10 salaries?\", source=\"user\"), agent)\nawait runtime.stop_when_idle()",
      "language": "csharp"
    },
    {
      "code": "INFO:autogen_core:Sending message of type TextMessage to assistant: {'content': 'What are the top-10 salaries?', 'source': 'user'}\nINFO:autogen_core:Calling message handler for assistant:default with message type TextMessage sent by Unknown",
      "language": "yaml"
    },
    {
      "code": "INFO:autogen_core:Sending message of type TextMessage to assistant: {'content': 'What are the top-10 salaries?', 'source': 'user'}\nINFO:autogen_core:Calling message handler for assistant:default with message type TextMessage sent by Unknown",
      "language": "yaml"
    },
    {
      "code": "# Sorting the data by hourly_rate in descending order and selecting the top 10 salaries\ntop_10_salaries = csv_data[['first_name', 'last_name', 'job_title', 'hourly_rate']].sort_values(by='hourly_rate', ascending=False).head(10)\ntop_10_salaries.reset_index(drop=True, inplace=True)\ntop_10_salaries\n```\nExecuting code...\n--------------------------------------------------------------------------------\nAssistant:\n\nHere are the top 10 salaries based on the hourly rates from the CSV file:\n\n| First Name | Last Name | Job Title                          | Hourly Rate |\n|------------|-----------|------------------------------------|-------------|\n| Eric       | Barden    | Executive4                        | 139.61      |\n| Idris      | Beauregard| Executive3                        | 115.90      |\n| Lori       | Aagard    | Pol Capt-Precinct                 | 112.70      |\n| Krista     | Bair      | Pol Capt-Precinct                 | 108.74      |\n| Amy        | Bannister | Fire Chief, Dep Adm-80 Hrs        | 104.07      |\n| Ginger     | Armbruster| Executive2                        | 102.42      |\n| William    | Andersen  | Executive2                        | 102.42      |\n| Valarie    | Anderson  | Executive2                        | 102.42      |\n| Paige      | Alderete  | Executive2                        | 102.42      |\n| Kathryn    | Aisenberg | Executive2                        | 100.65      |\n\nIf you need any further details or analysis, let me know!",
      "language": "yaml"
    },
    {
      "code": "# Sorting the data by hourly_rate in descending order and selecting the top 10 salaries\ntop_10_salaries = csv_data[['first_name', 'last_name', 'job_title', 'hourly_rate']].sort_values(by='hourly_rate', ascending=False).head(10)\ntop_10_salaries.reset_index(drop=True, inplace=True)\ntop_10_salaries\n```\nExecuting code...\n--------------------------------------------------------------------------------\nAssistant:\n\nHere are the top 10 salaries based on the hourly rates from the CSV file:\n\n| First Name | Last Name | Job Title                          | Hourly Rate |\n|------------|-----------|------------------------------------|-------------|\n| Eric       | Barden    | Executive4                        | 139.61      |\n| Idris      | Beauregard| Executive3                        | 115.90      |\n| Lori       | Aagard    | Pol Capt-Precinct                 | 112.70      |\n| Krista     | Bair      | Pol Capt-Precinct                 | 108.74      |\n| Amy        | Bannister | Fire Chief, Dep Adm-80 Hrs        | 104.07      |\n| Ginger     | Armbruster| Executive2                        | 102.42      |\n| William    | Andersen  | Executive2                        | 102.42      |\n| Valarie    | Anderson  | Executive2                        | 102.42      |\n| Paige      | Alderete  | Executive2                        | 102.42      |\n| Kathryn    | Aisenberg | Executive2                        | 100.65      |\n\nIf you need any further details or analysis, let me know!",
      "language": "yaml"
    },
    {
      "code": "INFO:autogen_core:Resolving response with message type TextMessage for recipient None from assistant: {'content': 'Here are the top 10 salaries based on the hourly rates from the CSV file:\\n\\n| First Name | Last Name | Job Title                          | Hourly Rate |\\n|------------|-----------|------------------------------------|-------------|\\n| Eric       | Barden    | Executive4                        | 139.61      |\\n| Idris      | Beauregard| Executive3                        | 115.90      |\\n| Lori       | Aagard    | Pol Capt-Precinct                 | 112.70      |\\n| Krista     | Bair      | Pol Capt-Precinct                 | 108.74      |\\n| Amy        | Bannister | Fire Chief, Dep Adm-80 Hrs        | 104.07      |\\n| Ginger     | Armbruster| Executive2                        | 102.42      |\\n| William    | Andersen  | Executive2                        | 102.42      |\\n| Valarie    | Anderson  | Executive2                        | 102.42      |\\n| Paige      | Alderete  | Executive2                        | 102.42      |\\n| Kathryn    | Aisenberg | Executive2                        | 100.65      |\\n\\nIf you need any further details or analysis, let me know!', 'source': 'assistant'}",
      "language": "yaml"
    },
    {
      "code": "INFO:autogen_core:Resolving response with message type TextMessage for recipient None from assistant: {'content': 'Here are the top 10 salaries based on the hourly rates from the CSV file:\\n\\n| First Name | Last Name | Job Title                          | Hourly Rate |\\n|------------|-----------|------------------------------------|-------------|\\n| Eric       | Barden    | Executive4                        | 139.61      |\\n| Idris      | Beauregard| Executive3                        | 115.90      |\\n| Lori       | Aagard    | Pol Capt-Precinct                 | 112.70      |\\n| Krista     | Bair      | Pol Capt-Precinct                 | 108.74      |\\n| Amy        | Bannister | Fire Chief, Dep Adm-80 Hrs        | 104.07      |\\n| Ginger     | Armbruster| Executive2                        | 102.42      |\\n| William    | Andersen  | Executive2                        | 102.42      |\\n| Valarie    | Anderson  | Executive2                        | 102.42      |\\n| Paige      | Alderete  | Executive2                        | 102.42      |\\n| Kathryn    | Aisenberg | Executive2                        | 100.65      |\\n\\nIf you need any further details or analysis, let me know!', 'source': 'assistant'}",
      "language": "yaml"
    },
    {
      "code": "response = requests.get(\"https://en.wikipedia.org/wiki/Third_Anglo-Afghan_War\")\nwith open(\"third_anglo_afghan_war.html\", \"wb\") as file:\n    file.write(response.content)",
      "language": "typescript"
    },
    {
      "code": "response = requests.get(\"https://en.wikipedia.org/wiki/Third_Anglo-Afghan_War\")\nwith open(\"third_anglo_afghan_war.html\", \"wb\") as file:\n    file.write(response.content)",
      "language": "typescript"
    },
    {
      "code": "runtime.start()\nawait runtime.send_message(\n    UploadForFileSearch(file_path=\"third_anglo_afghan_war.html\", vector_store_id=vector_store.id), agent\n)\nawait runtime.stop_when_idle()",
      "language": "csharp"
    },
    {
      "code": "runtime.start()\nawait runtime.send_message(\n    UploadForFileSearch(file_path=\"third_anglo_afghan_war.html\", vector_store_id=vector_store.id), agent\n)\nawait runtime.stop_when_idle()",
      "language": "csharp"
    },
    {
      "code": "INFO:autogen_core:Sending message of type UploadForFileSearch to assistant: {'file_path': 'third_anglo_afghan_war.html', 'vector_store_id': 'vs_h3xxPbJFnd1iZ9WdjsQwNdrp'}\nINFO:autogen_core:Calling message handler for assistant:default with message type UploadForFileSearch sent by Unknown\nINFO:autogen_core:Resolving response with message type NoneType for recipient None from assistant: None",
      "language": "sql"
    },
    {
      "code": "INFO:autogen_core:Sending message of type UploadForFileSearch to assistant: {'file_path': 'third_anglo_afghan_war.html', 'vector_store_id': 'vs_h3xxPbJFnd1iZ9WdjsQwNdrp'}\nINFO:autogen_core:Calling message handler for assistant:default with message type UploadForFileSearch sent by Unknown\nINFO:autogen_core:Resolving response with message type NoneType for recipient None from assistant: None",
      "language": "sql"
    },
    {
      "code": "runtime.start()\nawait runtime.send_message(Reset(), agent)\nawait runtime.send_message(\n    TextMessage(\n        content=\"When and where was the treaty of Rawalpindi signed? Answer using the document provided.\", source=\"user\"\n    ),\n    agent,\n)\nawait runtime.stop_when_idle()",
      "language": "julia"
    },
    {
      "code": "runtime.start()\nawait runtime.send_message(Reset(), agent)\nawait runtime.send_message(\n    TextMessage(\n        content=\"When and where was the treaty of Rawalpindi signed? Answer using the document provided.\", source=\"user\"\n    ),\n    agent,\n)\nawait runtime.stop_when_idle()",
      "language": "julia"
    },
    {
      "code": "INFO:autogen_core:Sending message of type Reset to assistant: {}\nINFO:autogen_core:Calling message handler for assistant:default with message type Reset sent by Unknown\nINFO:autogen_core:Resolving response with message type NoneType for recipient None from assistant: None\nINFO:autogen_core:Sending message of type TextMessage to assistant: {'content': 'When and where was the treaty of Rawalpindi signed? Answer using the document provided.', 'source': 'user'}\nINFO:autogen_core:Calling message handler for assistant:default with message type TextMessage sent by Unknown",
      "language": "sql"
    },
    {
      "code": "INFO:autogen_core:Sending message of type Reset to assistant: {}\nINFO:autogen_core:Calling message handler for assistant:default with message type Reset sent by Unknown\nINFO:autogen_core:Resolving response with message type NoneType for recipient None from assistant: None\nINFO:autogen_core:Sending message of type TextMessage to assistant: {'content': 'When and where was the treaty of Rawalpindi signed? Answer using the document provided.', 'source': 'user'}\nINFO:autogen_core:Calling message handler for assistant:default with message type TextMessage sent by Unknown",
      "language": "sql"
    },
    {
      "code": "--------------------------------------------------------------------------------\nAssistant:\n\nThe Treaty of Rawalpindi was signed on **8 August 1919**. The location of the signing was in **Rawalpindi**, which is in present-day Pakistan【6:0†source】.",
      "language": "yaml"
    },
    {
      "code": "--------------------------------------------------------------------------------\nAssistant:\n\nThe Treaty of Rawalpindi was signed on **8 August 1919**. The location of the signing was in **Rawalpindi**, which is in present-day Pakistan【6:0†source】.",
      "language": "yaml"
    },
    {
      "code": "INFO:autogen_core:Resolving response with message type TextMessage for recipient None from assistant: {'content': 'The Treaty of Rawalpindi was signed on **8 August 1919**. The location of the signing was in **Rawalpindi**, which is in present-day Pakistan【6:0†source】.', 'source': 'assistant'}",
      "language": "sql"
    },
    {
      "code": "INFO:autogen_core:Resolving response with message type TextMessage for recipient None from assistant: {'content': 'The Treaty of Rawalpindi was signed on **8 August 1919**. The location of the signing was in **Rawalpindi**, which is in present-day Pakistan【6:0†source】.', 'source': 'assistant'}",
      "language": "sql"
    },
    {
      "code": "[0] third_anglo_afghan_war.html",
      "language": "json"
    },
    {
      "code": "[0] third_anglo_afghan_war.html",
      "language": "json"
    }
  ],
  "patterns": [],
  "links": [
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/cookbook/openai-assistant-agent.html",
    "https://microsoft.github.io/autogen/stable/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/extensions-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/reference/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/installation.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/quickstart.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/core-concepts/agent-and-multi-agent-application.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/core-concepts/architecture.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/core-concepts/application-stack.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/core-concepts/agent-identity-and-lifecycle.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/core-concepts/topic-and-subscription.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/framework/agent-and-agent-runtime.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/framework/message-and-communication.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/framework/logging.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/framework/telemetry.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/framework/distributed-agent-runtime.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/framework/component-config.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/components/model-clients.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/components/model-context.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/components/tools.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/components/workbench.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/components/command-line-code-executors.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/design-patterns/intro.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/design-patterns/concurrent-agents.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/design-patterns/sequential-workflow.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/design-patterns/group-chat.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/design-patterns/handoffs.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/design-patterns/mixture-of-agents.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/design-patterns/multi-agent-debate.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/design-patterns/reflection.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/design-patterns/code-execution-groupchat.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/cookbook/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/cookbook/azure-openai-with-aad-auth.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/cookbook/termination-with-intervention.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/cookbook/tool-use-with-intervention.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/cookbook/extracting-results-with-an-agent.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/cookbook/langgraph-agent.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/cookbook/llamaindex-agent.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/cookbook/local-llms-ollama-litellm.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/cookbook/instrumenting.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/cookbook/topic-subscription-scenarios.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/cookbook/structured-output-agent.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/cookbook/llm-usage-logger.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/faqs.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.html"
  ]
}