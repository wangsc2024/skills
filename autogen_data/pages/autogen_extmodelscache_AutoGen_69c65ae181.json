{
  "url": "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.cache.html",
  "title": "autogen_ext.models.cache — AutoGen",
  "content": "Bases: ChatCompletionClient, Component[ChatCompletionCacheConfig]\n\nA wrapper around a ChatCompletionClient that caches creation results from an underlying client. Cache hits do not contribute to token usage of the original client.\n\nLets use caching on disk with openai client as an example. First install autogen-ext with the required packages:\n\nFor streaming with Redis caching:\n\nYou can now use the cached_client as you would the original client, but with caching enabled.\n\nclient (ChatCompletionClient) – The original ChatCompletionClient to wrap.\n\nstore (CacheStore) – A store object that implements get and set methods. The user is responsible for managing the store’s lifecycle & clearing it (if needed). Defaults to using in-memory cache.\n\nThe logical type of the component.\n\nOverride the provider string for the component. This should be used to prevent internal module names being a part of the module name.\n\nalias of ChatCompletionCacheConfig\n\nCached version of ChatCompletionClient.create. If the result of a call to create has been cached, it will be returned immediately without invoking the underlying client.\n\nNOTE: cancellation_token is ignored for cached results.\n\nCached version of ChatCompletionClient.create_stream. If the result of a call to create_stream has been cached, it will be returned without streaming from the underlying client.\n\nNOTE: cancellation_token is ignored for cached results.\n\nDump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.\n\nT – The configuration of the component.\n\nCreate a new instance of the component from a configuration object.\n\nconfig (T) – The configuration object.\n\nSelf – The new instance of the component.\n\nautogen_ext.models.azure\n\nautogen_ext.models.llama_cpp",
  "headings": [
    {
      "level": "h1",
      "text": "autogen_ext.models.cache#",
      "id": ""
    }
  ],
  "code_samples": [
    {
      "code": "pip install -U \"autogen-ext[openai, diskcache]\"",
      "language": "unknown"
    },
    {
      "code": "pip install -U \"autogen-ext[openai, diskcache]\"",
      "language": "unknown"
    },
    {
      "code": "import asyncio\nimport tempfile\n\nfrom autogen_core.models import UserMessage\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_ext.models.cache import ChatCompletionCache, CHAT_CACHE_VALUE_TYPE\nfrom autogen_ext.cache_store.diskcache import DiskCacheStore\nfrom diskcache import Cache\n\n\nasync def main():\n    with tempfile.TemporaryDirectory() as tmpdirname:\n        # Initialize the original client\n        openai_model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n        # Then initialize the CacheStore, in this case with diskcache.Cache.\n        # You can also use redis like:\n        # from autogen_ext.cache_store.redis import RedisStore\n        # import redis\n        # redis_instance = redis.Redis()\n        # cache_store = RedisCacheStore[CHAT_CACHE_VALUE_TYPE](redis_instance)\n        cache_store = DiskCacheStore[CHAT_CACHE_VALUE_TYPE](Cache(tmpdirname))\n        cache_client = ChatCompletionCache(openai_model_client, cache_store)\n\n        response = await cache_client.create([UserMessage(content=\"Hello, how are you?\", source=\"user\")])\n        print(response)  # Should print response from OpenAI\n        response = await cache_client.create([UserMessage(content=\"Hello, how are you?\", source=\"user\")])\n        print(response)  # Should print cached response\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nimport tempfile\n\nfrom autogen_core.models import UserMessage\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_ext.models.cache import ChatCompletionCache, CHAT_CACHE_VALUE_TYPE\nfrom autogen_ext.cache_store.diskcache import DiskCacheStore\nfrom diskcache import Cache\n\n\nasync def main():\n    with tempfile.TemporaryDirectory() as tmpdirname:\n        # Initialize the original client\n        openai_model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n        # Then initialize the CacheStore, in this case with diskcache.Cache.\n        # You can also use redis like:\n        # from autogen_ext.cache_store.redis import RedisStore\n        # import redis\n        # redis_instance = redis.Redis()\n        # cache_store = RedisCacheStore[CHAT_CACHE_VALUE_TYPE](redis_instance)\n        cache_store = DiskCacheStore[CHAT_CACHE_VALUE_TYPE](Cache(tmpdirname))\n        cache_client = ChatCompletionCache(openai_model_client, cache_store)\n\n        response = await cache_client.create([UserMessage(content=\"Hello, how are you?\", source=\"user\")])\n        print(response)  # Should print response from OpenAI\n        response = await cache_client.create([UserMessage(content=\"Hello, how are you?\", source=\"user\")])\n        print(response)  # Should print cached response\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\n\nfrom autogen_core.models import UserMessage\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_ext.models.cache import ChatCompletionCache, CHAT_CACHE_VALUE_TYPE\nfrom autogen_ext.cache_store.redis import RedisStore\nimport redis\n\n\nasync def main():\n    # Initialize the original client\n    openai_model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n    # Initialize Redis cache store\n    redis_instance = redis.Redis()\n    cache_store = RedisStore[CHAT_CACHE_VALUE_TYPE](redis_instance)\n    cache_client = ChatCompletionCache(openai_model_client, cache_store)\n\n    response = await cache_client.create([UserMessage(content=\"Hello, how are you?\", source=\"user\")])\n    print(response)  # Should print response from OpenAI\n    response = await cache_client.create([UserMessage(content=\"Hello, how are you?\", source=\"user\")])\n    print(response)  # Should print cached response\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\n\nfrom autogen_core.models import UserMessage\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_ext.models.cache import ChatCompletionCache, CHAT_CACHE_VALUE_TYPE\nfrom autogen_ext.cache_store.redis import RedisStore\nimport redis\n\n\nasync def main():\n    # Initialize the original client\n    openai_model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n    # Initialize Redis cache store\n    redis_instance = redis.Redis()\n    cache_store = RedisStore[CHAT_CACHE_VALUE_TYPE](redis_instance)\n    cache_client = ChatCompletionCache(openai_model_client, cache_store)\n\n    response = await cache_client.create([UserMessage(content=\"Hello, how are you?\", source=\"user\")])\n    print(response)  # Should print response from OpenAI\n    response = await cache_client.create([UserMessage(content=\"Hello, how are you?\", source=\"user\")])\n    print(response)  # Should print cached response\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\n\nfrom autogen_core.models import UserMessage, CreateResult\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_ext.models.cache import ChatCompletionCache, CHAT_CACHE_VALUE_TYPE\nfrom autogen_ext.cache_store.redis import RedisStore\nimport redis\n\n\nasync def main():\n    # Initialize the original client\n    openai_model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n    # Initialize Redis cache store\n    redis_instance = redis.Redis()\n    cache_store = RedisStore[CHAT_CACHE_VALUE_TYPE](redis_instance)\n    cache_client = ChatCompletionCache(openai_model_client, cache_store)\n\n    # First streaming call\n    async for chunk in cache_client.create_stream(\n        [UserMessage(content=\"List all countries in Africa\", source=\"user\")]\n    ):\n        if isinstance(chunk, CreateResult):\n            print(\"\\n\")\n            print(\"Cached: \", chunk.cached)  # Should print False\n        else:\n            print(chunk, end=\"\")\n\n    # Second streaming call (cached)\n    async for chunk in cache_client.create_stream(\n        [UserMessage(content=\"List all countries in Africa\", source=\"user\")]\n    ):\n        if isinstance(chunk, CreateResult):\n            print(\"\\n\")\n            print(\"Cached: \", chunk.cached)  # Should print True\n        else:\n            print(chunk, end=\"\")\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\n\nfrom autogen_core.models import UserMessage, CreateResult\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_ext.models.cache import ChatCompletionCache, CHAT_CACHE_VALUE_TYPE\nfrom autogen_ext.cache_store.redis import RedisStore\nimport redis\n\n\nasync def main():\n    # Initialize the original client\n    openai_model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n    # Initialize Redis cache store\n    redis_instance = redis.Redis()\n    cache_store = RedisStore[CHAT_CACHE_VALUE_TYPE](redis_instance)\n    cache_client = ChatCompletionCache(openai_model_client, cache_store)\n\n    # First streaming call\n    async for chunk in cache_client.create_stream(\n        [UserMessage(content=\"List all countries in Africa\", source=\"user\")]\n    ):\n        if isinstance(chunk, CreateResult):\n            print(\"\\n\")\n            print(\"Cached: \", chunk.cached)  # Should print False\n        else:\n            print(chunk, end=\"\")\n\n    # Second streaming call (cached)\n    async for chunk in cache_client.create_stream(\n        [UserMessage(content=\"List all countries in Africa\", source=\"user\")]\n    ):\n        if isinstance(chunk, CreateResult):\n            print(\"\\n\")\n            print(\"Cached: \", chunk.cached)  # Should print True\n        else:\n            print(chunk, end=\"\")\n\n\nasyncio.run(main())",
      "language": "python"
    }
  ],
  "patterns": [
    {
      "description": "API Reference autogen_ext.models.cache autogen_ext.models.cache# class ChatCompletionCache(client: ChatCompletionClient, store: CacheStore[CreateResult | List[str | CreateResult]] | None = None)[source]# Bases: ChatCompletionClient, Component[ChatCompletionCacheConfig] A wrapper around a ChatCompletionClient that caches creation results from an underlying client. Cache hits do not contribute to token usage of the original client. Typical Usage: Lets use caching on disk with openai client as an example. First install autogen-ext with the required packages: pip install -U \"autogen-ext[openai, diskcache]\" And use it as: import asyncio import tempfile from autogen_core.models import UserMessage from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_ext.models.cache import ChatCompletionCache, CHAT_CACHE_VALUE_TYPE from autogen_ext.cache_store.diskcache import DiskCacheStore from diskcache import Cache async def main(): with tempfile.TemporaryDirectory() as tmpdirname: # Initialize the original client openai_model_client = OpenAIChatCompletionClient(model=\"gpt-4o\") # Then initialize the CacheStore, in this case with diskcache.Cache. # You can also use redis like: # from autogen_ext.cache_store.redis import RedisStore # import redis # redis_instance = redis.Redis() # cache_store = RedisCacheStore[CHAT_CACHE_VALUE_TYPE](redis_instance) cache_store = DiskCacheStore[CHAT_CACHE_VALUE_TYPE](Cache(tmpdirname)) cache_client = ChatCompletionCache(openai_model_client, cache_store) response = await cache_client.create([UserMessage(content=\"Hello, how are you?\", source=\"user\")]) print(response) # Should print response from OpenAI response = await cache_client.create([UserMessage(content=\"Hello, how are you?\", source=\"user\")]) print(response) # Should print cached response asyncio.run(main()) For Redis caching: import asyncio from autogen_core.models import UserMessage from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_ext.models.cache import ChatCompletionCache, CHAT_CACHE_VALUE_TYPE from autogen_ext.cache_store.redis import RedisStore import redis async def main(): # Initialize the original client openai_model_client = OpenAIChatCompletionClient(model=\"gpt-4o\") # Initialize Redis cache store redis_instance = redis.Redis() cache_store = RedisStore[CHAT_CACHE_VALUE_TYPE](redis_instance) cache_client = ChatCompletionCache(openai_model_client, cache_store) response = await cache_client.create([UserMessage(content=\"Hello, how are you?\", source=\"user\")]) print(response) # Should print response from OpenAI response = await cache_client.create([UserMessage(content=\"Hello, how are you?\", source=\"user\")]) print(response) # Should print cached response asyncio.run(main()) For streaming with Redis caching: import asyncio from autogen_core.models import UserMessage, CreateResult from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_ext.models.cache import ChatCompletionCache, CHAT_CACHE_VALUE_TYPE from autogen_ext.cache_store.redis import RedisStore import redis async def main(): # Initialize the original client openai_model_client = OpenAIChatCompletionClient(model=\"gpt-4o\") # Initialize Redis cache store redis_instance = redis.Redis() cache_store = RedisStore[CHAT_CACHE_VALUE_TYPE](redis_instance) cache_client = ChatCompletionCache(openai_model_client, cache_store) # First streaming call async for chunk in cache_client.create_stream( [UserMessage(content=\"List all countries in Africa\", source=\"user\")] ): if isinstance(chunk, CreateResult): print(\"\\n\") print(\"Cached: \", chunk.cached) # Should print False else: print(chunk, end=\"\") # Second streaming call (cached) async for chunk in cache_client.create_stream( [UserMessage(content=\"List all countries in Africa\", source=\"user\")] ): if isinstance(chunk, CreateResult): print(\"\\n\") print(\"Cached: \", chunk.cached) # Should print True else: print(chunk, end=\"\") asyncio.run(main()) You can now use the cached_client as you would the original client, but with caching enabled. Parameters: client (ChatCompletionClient) – The original ChatCompletionClient to wrap. store (CacheStore) – A store object that implements get and set methods. The user is responsible for managing the store’s lifecycle & clearing it (if needed). Defaults to using in-memory cache. component_type: ClassVar[ComponentType] = 'chat_completion_cache'# The logical type of the component. component_provider_override: ClassVar[str | None] = 'autogen_ext.models.cache.ChatCompletionCache'# Override the provider string for the component. This should be used to prevent internal module names being a part of the module name. component_config_schema# alias of ChatCompletionCacheConfig async create(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], tool_choice: Tool | Literal['auto', 'required', 'none'] = 'auto', json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → CreateResult[source]# Cached version of ChatCompletionClient.create. If the result of a call to create has been cached, it will be returned immediately without invoking the underlying client. NOTE: cancellation_token is ignored for cached results. create_stream(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], tool_choice: Tool | Literal['auto', 'required', 'none'] = 'auto', json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → AsyncGenerator[str | CreateResult, None][source]# Cached version of ChatCompletionClient.create_stream. If the result of a call to create_stream has been cached, it will be returned without streaming from the underlying client. NOTE: cancellation_token is ignored for cached results. async close() → None[source]# actual_usage() → RequestUsage[source]# count_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]# property capabilities: ModelCapabilities# property model_info: ModelInfo# remaining_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]# total_usage() → RequestUsage[source]# _to_config() → ChatCompletionCacheConfig[source]# Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance. Returns: T – The configuration of the component. classmethod _from_config(config: ChatCompletionCacheConfig) → Self[source]# Create a new instance of the component from a configuration object. Parameters: config (T) – The configuration object. Returns: Self – The new instance of the component. previous autogen_ext.models.azure next autogen_ext.models.llama_cpp On this page ChatCompletionCache ChatCompletionCache.component_type ChatCompletionCache.component_provider_override ChatCompletionCache.component_config_schema ChatCompletionCache.create() ChatCompletionCache.create_stream() ChatCompletionCache.close() ChatCompletionCache.actual_usage() ChatCompletionCache.count_tokens() ChatCompletionCache.capabilities ChatCompletionCache.model_info ChatCompletionCache.remaining_tokens() ChatCompletionCache.total_usage() ChatCompletionCache._to_config() ChatCompletionCache._from_config() Edit on GitHub Show Source",
      "code": "ChatCompletionClient"
    },
    {
      "description": "API Reference autogen_ext.models.cache autogen_ext.models.cache# class ChatCompletionCache(client: ChatCompletionClient, store: CacheStore[CreateResult | List[str | CreateResult]] | None = None)[source]# Bases: ChatCompletionClient, Component[ChatCompletionCacheConfig] A wrapper around a ChatCompletionClient that caches creation results from an underlying client. Cache hits do not contribute to token usage of the original client. Typical Usage: Lets use caching on disk with openai client as an example. First install autogen-ext with the required packages: pip install -U \"autogen-ext[openai, diskcache]\" And use it as: import asyncio import tempfile from autogen_core.models import UserMessage from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_ext.models.cache import ChatCompletionCache, CHAT_CACHE_VALUE_TYPE from autogen_ext.cache_store.diskcache import DiskCacheStore from diskcache import Cache async def main(): with tempfile.TemporaryDirectory() as tmpdirname: # Initialize the original client openai_model_client = OpenAIChatCompletionClient(model=\"gpt-4o\") # Then initialize the CacheStore, in this case with diskcache.Cache. # You can also use redis like: # from autogen_ext.cache_store.redis import RedisStore # import redis # redis_instance = redis.Redis() # cache_store = RedisCacheStore[CHAT_CACHE_VALUE_TYPE](redis_instance) cache_store = DiskCacheStore[CHAT_CACHE_VALUE_TYPE](Cache(tmpdirname)) cache_client = ChatCompletionCache(openai_model_client, cache_store) response = await cache_client.create([UserMessage(content=\"Hello, how are you?\", source=\"user\")]) print(response) # Should print response from OpenAI response = await cache_client.create([UserMessage(content=\"Hello, how are you?\", source=\"user\")]) print(response) # Should print cached response asyncio.run(main()) For Redis caching: import asyncio from autogen_core.models import UserMessage from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_ext.models.cache import ChatCompletionCache, CHAT_CACHE_VALUE_TYPE from autogen_ext.cache_store.redis import RedisStore import redis async def main(): # Initialize the original client openai_model_client = OpenAIChatCompletionClient(model=\"gpt-4o\") # Initialize Redis cache store redis_instance = redis.Redis() cache_store = RedisStore[CHAT_CACHE_VALUE_TYPE](redis_instance) cache_client = ChatCompletionCache(openai_model_client, cache_store) response = await cache_client.create([UserMessage(content=\"Hello, how are you?\", source=\"user\")]) print(response) # Should print response from OpenAI response = await cache_client.create([UserMessage(content=\"Hello, how are you?\", source=\"user\")]) print(response) # Should print cached response asyncio.run(main()) For streaming with Redis caching: import asyncio from autogen_core.models import UserMessage, CreateResult from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_ext.models.cache import ChatCompletionCache, CHAT_CACHE_VALUE_TYPE from autogen_ext.cache_store.redis import RedisStore import redis async def main(): # Initialize the original client openai_model_client = OpenAIChatCompletionClient(model=\"gpt-4o\") # Initialize Redis cache store redis_instance = redis.Redis() cache_store = RedisStore[CHAT_CACHE_VALUE_TYPE](redis_instance) cache_client = ChatCompletionCache(openai_model_client, cache_store) # First streaming call async for chunk in cache_client.create_stream( [UserMessage(content=\"List all countries in Africa\", source=\"user\")] ): if isinstance(chunk, CreateResult): print(\"\\n\") print(\"Cached: \", chunk.cached) # Should print False else: print(chunk, end=\"\") # Second streaming call (cached) async for chunk in cache_client.create_stream( [UserMessage(content=\"List all countries in Africa\", source=\"user\")] ): if isinstance(chunk, CreateResult): print(\"\\n\") print(\"Cached: \", chunk.cached) # Should print True else: print(chunk, end=\"\") asyncio.run(main()) You can now use the cached_client as you would the original client, but with caching enabled. Parameters: client (ChatCompletionClient) – The original ChatCompletionClient to wrap. store (CacheStore) – A store object that implements get and set methods. The user is responsible for managing the store’s lifecycle & clearing it (if needed). Defaults to using in-memory cache. component_type: ClassVar[ComponentType] = 'chat_completion_cache'# The logical type of the component. component_provider_override: ClassVar[str | None] = 'autogen_ext.models.cache.ChatCompletionCache'# Override the provider string for the component. This should be used to prevent internal module names being a part of the module name. component_config_schema# alias of ChatCompletionCacheConfig async create(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], tool_choice: Tool | Literal['auto', 'required', 'none'] = 'auto', json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → CreateResult[source]# Cached version of ChatCompletionClient.create. If the result of a call to create has been cached, it will be returned immediately without invoking the underlying client. NOTE: cancellation_token is ignored for cached results. create_stream(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], tool_choice: Tool | Literal['auto', 'required', 'none'] = 'auto', json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → AsyncGenerator[str | CreateResult, None][source]# Cached version of ChatCompletionClient.create_stream. If the result of a call to create_stream has been cached, it will be returned without streaming from the underlying client. NOTE: cancellation_token is ignored for cached results. async close() → None[source]# actual_usage() → RequestUsage[source]# count_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]# property capabilities: ModelCapabilities# property model_info: ModelInfo# remaining_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]# total_usage() → RequestUsage[source]# _to_config() → ChatCompletionCacheConfig[source]# Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance. Returns: T – The configuration of the component. classmethod _from_config(config: ChatCompletionCacheConfig) → Self[source]# Create a new instance of the component from a configuration object. Parameters: config (T) – The configuration object. Returns: Self – The new instance of the component. previous autogen_ext.models.azure next autogen_ext.models.llama_cpp",
      "code": "ChatCompletionClient"
    },
    {
      "description": "Typical Usage:",
      "code": "pip install -U \"autogen-ext[openai, diskcache]\""
    }
  ],
  "links": [
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.cache.html",
    "https://microsoft.github.io/autogen/stable/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/extensions-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/reference/index.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.agents.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.base.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.conditions.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.messages.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.state.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.teams.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.tools.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.ui.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.utils.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.code_executor.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.exceptions.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.logging.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.memory.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.model_context.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.models.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.tool_agent.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.tools.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.utils.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.cache_store.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.experimental.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.memory.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.teams.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.ui.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.azure.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.file_surfer.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.magentic_one.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.openai.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.video_surfer.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.web_surfer.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.auth.azure.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.cache_store.diskcache.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.cache_store.redis.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.azure.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.docker.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.docker_jupyter.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.jupyter.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.local.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.experimental.task_centric_memory.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.memory.canvas.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.memory.chromadb.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.memory.mem0.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.memory.redis.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.anthropic.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.azure.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.llama_cpp.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.ollama.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.openai.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.replay.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.semantic_kernel.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.teams.magentic_one.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.azure.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.code_execution.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.graphrag.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.http.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.langchain.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.mcp.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.semantic_kernel.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.video_surfer.tools.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.web_surfer.playwright_controller.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.experimental.task_centric_memory.utils.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.anthropic.config.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.azure.config.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.ollama.config.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.openai.config.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.protos.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.protos.agent_worker_pb2.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.protos.agent_worker_pb2_grpc.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.protos.cloudevent_pb2.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.protos.cloudevent_pb2_grpc.html"
  ]
}