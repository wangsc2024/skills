{
  "url": "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/agents.html",
  "title": "Agents — AutoGen",
  "content": "AutoGen AgentChat provides a set of preset Agents, each with variations in how an agent might respond to messages. All agents share the following attributes and methods:\n\nname: The unique name of the agent.\n\ndescription: The description of the agent in text.\n\nrun: The method that runs the agent given a task as a string or a list of messages, and returns a TaskResult. Agents are expected to be stateful and this method is expected to be called with new messages, not complete history.\n\nrun_stream: Same as run() but returns an iterator of messages that subclass BaseAgentEvent or BaseChatMessage followed by a TaskResult as the last item.\n\nSee autogen_agentchat.messages for more information on AgentChat message types.\n\nAssistantAgent is a built-in agent that uses a language model and has the ability to use tools.\n\nAssistantAgent is a “kitchen sink” agent for prototyping and educational purpose – it is very general. Make sure you read the documentation and implementation to understand the design choices. Once you fully understand the design, you may want to implement your own agent. See Custom Agent.\n\nWe can use the run() method to get the agent run on a given task.\n\nThe call to the run() method returns a TaskResult with the list of messages in the messages attribute, which stores the agent’s “thought process” as well as the final response.\n\nIt is important to note that run() will update the internal state of the agent – it will add the messages to the agent’s message history. You can also call run() without a task to get the agent to generate responses given its current state.\n\nUnlike in v0.2 AgentChat, the tools are executed by the same agent directly within the same call to run(). By default, the agent will return the result of the tool call as the final response.\n\nThe AssistantAgent can handle multi-modal input by providing the input as a MultiModalMessage.\n\nWe can also stream each message as it is generated by the agent by using the run_stream() method, and use Console to print the messages as they appear to the console.\n\nThe run_stream() method returns an asynchronous generator that yields each message generated by the agent, followed by a TaskResult as the last item.\n\nFrom the messages, you can observe that the assistant agent utilized the web_search tool to gather information and responded based on the search results.\n\nLarge Language Models (LLMs) are typically limited to generating text or code responses. However, many complex tasks benefit from the ability to use external tools that perform specific actions, such as fetching data from APIs or databases.\n\nTo address this limitation, modern LLMs can now accept a list of available tool schemas (descriptions of tools and their arguments) and generate a tool call message. This capability is known as Tool Calling or Function Calling and is becoming a popular pattern in building intelligent agent-based applications. Refer to the documentation from OpenAI and Anthropic for more information about tool calling in LLMs.\n\nIn AgentChat, the AssistantAgent can use tools to perform specific actions. The web_search tool is one such tool that allows the assistant agent to search the web for information. A single custom tool can be a Python function or a subclass of the BaseTool.\n\nOn the other hand, a Workbench is a collection of tools that share state and resources.\n\nFor how to use model clients directly with tools and workbench, refer to the Tools and Workbench sections in the Core User Guide.\n\nBy default, when AssistantAgent executes a tool, it will return the tool’s output as a string in ToolCallSummaryMessage in its response. If your tool does not return a well-formed string in natural language, you can add a reflection step to have the model summarize the tool’s output, by setting the reflect_on_tool_use=True parameter in the AssistantAgent constructor.\n\nAutoGen Extension provides a set of built-in tools that can be used with the Assistant Agent. Head over to the API documentation for all the available tools under the autogen_ext.tools namespace. For example, you can find the following tools:\n\ngraphrag: Tools for using GraphRAG index.\n\nhttp: Tools for making HTTP requests.\n\nlangchain: Adaptor for using LangChain tools.\n\nmcp: Tools and workbench for using Model Chat Protocol (MCP) servers.\n\nThe AssistantAgent automatically converts a Python function into a FunctionTool which can be used as a tool by the agent and automatically generates the tool schema from the function signature and docstring.\n\nThe web_search_func tool is an example of a function tool. The schema is automatically generated.\n\nThe AssistantAgent can also use tools that are served from a Model Context Protocol (MCP) server using McpWorkbench().\n\nAny BaseChatAgent can be used as a tool by wrapping it in a AgentTool. This allows for a dynamic, model-driven multi-agent workflow where the agent can call other agents as tools to solve tasks.\n\nSome models support parallel tool calls, which can be useful for tasks that require multiple tools to be called simultaneously. By default, if the model client produces multiple tool calls, AssistantAgent will call the tools in parallel.\n\nYou may want to disable parallel tool calls when the tools have side effects that may interfere with each other, or, when agent behavior needs to be consistent across different models. This should be done at the model client level.\n\nWhen using AgentTool or TeamTool, you must disable parallel tool calls to avoid concurrency issues. These tools cannot run concurrently as agents and teams maintain internal state that would conflict with parallel execution.\n\nFor OpenAIChatCompletionClient and AzureOpenAIChatCompletionClient, set parallel_tool_calls=False to disable parallel tool calls.\n\nOne model call followed by one tool call or parallel tool calls is a single tool iteration. By default, the AssistantAgent will execute at most one iteration.\n\nThe agent can be configured to execute multiple iterations until the model stops generating tool calls or the maximum number of iterations is reached. You can control the maximum number of iterations by setting the max_tool_iterations parameter in the AssistantAgent constructor.\n\nStructured output allows models to return structured JSON text with pre-defined schema provided by the application. Different from JSON-mode, the schema can be provided as a Pydantic BaseModel class, which can also be used to validate the output.\n\nOnce you specify the base model class in the output_content_type parameter of the AssistantAgent constructor, the agent will respond with a StructuredMessage whose content’s type is the type of the base model class.\n\nThis way, you can integrate agent’s response directly into your application and use the model’s output as a structured object.\n\nWhen the output_content_type is set, it by default requires the agent to reflect on the tool use and return the a structured output message based on the tool call result. You can disable this behavior by setting reflect_on_tool_use=False explictly.\n\nStructured output is also useful for incorporating Chain-of-Thought reasoning in the agent’s responses. See the example below for how to use structured output with the assistant agent.\n\nYou can stream the tokens generated by the model client by setting model_client_stream=True. This will cause the agent to yield ModelClientStreamingChunkEvent messages in run_stream().\n\nThe underlying model API must support streaming tokens for this to work. Please check with your model provider to see if this is supported.\n\nYou can see the streaming chunks in the output above. The chunks are generated by the model client and are yielded by the agent as they are received. The final response, the concatenation of all the chunks, is yielded right after the last chunk.\n\nAssistantAgent has a model_context parameter that can be used to pass in a ChatCompletionContext object. This allows the agent to use different model contexts, such as BufferedChatCompletionContext to limit the context sent to the model.\n\nBy default, AssistantAgent uses the UnboundedChatCompletionContext which sends the full conversation history to the model. To limit the context to the last n messages, you can use the BufferedChatCompletionContext. To limit the context by token count, you can use the TokenLimitedChatCompletionContext.\n\nThe following preset agents are available:\n\nUserProxyAgent: An agent that takes user input returns it as responses.\n\nCodeExecutorAgent: An agent that can execute code.\n\nOpenAIAssistantAgent: An agent that is backed by an OpenAI Assistant, with ability to use custom tools.\n\nMultimodalWebSurfer: A multi-modal agent that can search the web and visit web pages for information.\n\nFileSurfer: An agent that can search and browse local files for information.\n\nVideoSurfer: An agent that can watch videos for information.\n\nHaving explored the usage of the AssistantAgent, we can now proceed to the next section to learn about the teams feature in AgentChat.",
  "headings": [
    {
      "level": "h1",
      "text": "Agents#",
      "id": ""
    },
    {
      "level": "h2",
      "text": "Assistant Agent#",
      "id": ""
    },
    {
      "level": "h2",
      "text": "Getting Result#",
      "id": ""
    },
    {
      "level": "h2",
      "text": "Multi-Modal Input#",
      "id": ""
    },
    {
      "level": "h2",
      "text": "Streaming Messages#",
      "id": ""
    },
    {
      "level": "h2",
      "text": "Using Tools and Workbench#",
      "id": ""
    },
    {
      "level": "h3",
      "text": "Built-in Tools and Workbench#",
      "id": ""
    },
    {
      "level": "h3",
      "text": "Function Tool#",
      "id": ""
    },
    {
      "level": "h3",
      "text": "Model Context Protocol (MCP) Workbench#",
      "id": ""
    },
    {
      "level": "h3",
      "text": "Agent as a Tool#",
      "id": ""
    },
    {
      "level": "h3",
      "text": "Parallel Tool Calls#",
      "id": ""
    },
    {
      "level": "h3",
      "text": "Tool Iterations#",
      "id": ""
    },
    {
      "level": "h2",
      "text": "Structured Output#",
      "id": ""
    },
    {
      "level": "h2",
      "text": "Streaming Tokens#",
      "id": ""
    },
    {
      "level": "h2",
      "text": "Using Model Context#",
      "id": ""
    },
    {
      "level": "h2",
      "text": "Other Preset Agents#",
      "id": ""
    },
    {
      "level": "h2",
      "text": "Next Step#",
      "id": ""
    }
  ],
  "code_samples": [
    {
      "code": "from autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.messages import StructuredMessage\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient",
      "language": "sql"
    },
    {
      "code": "from autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.messages import StructuredMessage\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient",
      "language": "sql"
    },
    {
      "code": "# Define a tool that searches the web for information.\n# For simplicity, we will use a mock function here that returns a static string.\nasync def web_search(query: str) -> str:\n    \"\"\"Find information on the web\"\"\"\n    return \"AutoGen is a programming framework for building multi-agent applications.\"\n\n\n# Create an agent that uses the OpenAI GPT-4o model.\nmodel_client = OpenAIChatCompletionClient(\n    model=\"gpt-4.1-nano\",\n    # api_key=\"YOUR_API_KEY\",\n)\nagent = AssistantAgent(\n    name=\"assistant\",\n    model_client=model_client,\n    tools=[web_search],\n    system_message=\"Use tools to solve tasks.\",\n)",
      "language": "python"
    },
    {
      "code": "# Define a tool that searches the web for information.\n# For simplicity, we will use a mock function here that returns a static string.\nasync def web_search(query: str) -> str:\n    \"\"\"Find information on the web\"\"\"\n    return \"AutoGen is a programming framework for building multi-agent applications.\"\n\n\n# Create an agent that uses the OpenAI GPT-4o model.\nmodel_client = OpenAIChatCompletionClient(\n    model=\"gpt-4.1-nano\",\n    # api_key=\"YOUR_API_KEY\",\n)\nagent = AssistantAgent(\n    name=\"assistant\",\n    model_client=model_client,\n    tools=[web_search],\n    system_message=\"Use tools to solve tasks.\",\n)",
      "language": "python"
    },
    {
      "code": "# Use asyncio.run(agent.run(...)) when running in a script.\nresult = await agent.run(task=\"Find information on AutoGen\")\nprint(result.messages)",
      "language": "python"
    },
    {
      "code": "# Use asyncio.run(agent.run(...)) when running in a script.\nresult = await agent.run(task=\"Find information on AutoGen\")\nprint(result.messages)",
      "language": "python"
    },
    {
      "code": "[TextMessage(source='user', models_usage=None, metadata={}, content='Find information on AutoGen', type='TextMessage'), ToolCallRequestEvent(source='assistant', models_usage=RequestUsage(prompt_tokens=61, completion_tokens=16), metadata={}, content=[FunctionCall(id='call_703i17OLXfztkuioUbkESnea', arguments='{\"query\":\"AutoGen\"}', name='web_search')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='assistant', models_usage=None, metadata={}, content=[FunctionExecutionResult(content='AutoGen is a programming framework for building multi-agent applications.', name='web_search', call_id='call_703i17OLXfztkuioUbkESnea', is_error=False)], type='ToolCallExecutionEvent'), ToolCallSummaryMessage(source='assistant', models_usage=None, metadata={}, content='AutoGen is a programming framework for building multi-agent applications.', type='ToolCallSummaryMessage')]",
      "language": "json"
    },
    {
      "code": "[TextMessage(source='user', models_usage=None, metadata={}, content='Find information on AutoGen', type='TextMessage'), ToolCallRequestEvent(source='assistant', models_usage=RequestUsage(prompt_tokens=61, completion_tokens=16), metadata={}, content=[FunctionCall(id='call_703i17OLXfztkuioUbkESnea', arguments='{\"query\":\"AutoGen\"}', name='web_search')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='assistant', models_usage=None, metadata={}, content=[FunctionExecutionResult(content='AutoGen is a programming framework for building multi-agent applications.', name='web_search', call_id='call_703i17OLXfztkuioUbkESnea', is_error=False)], type='ToolCallExecutionEvent'), ToolCallSummaryMessage(source='assistant', models_usage=None, metadata={}, content='AutoGen is a programming framework for building multi-agent applications.', type='ToolCallSummaryMessage')]",
      "language": "json"
    },
    {
      "code": "from io import BytesIO\n\nimport PIL\nimport requests\nfrom autogen_agentchat.messages import MultiModalMessage\nfrom autogen_core import Image\n\n# Create a multi-modal message with random image and text.\npil_image = PIL.Image.open(BytesIO(requests.get(\"https://picsum.photos/300/200\").content))\nimg = Image(pil_image)\nmulti_modal_message = MultiModalMessage(content=[\"Can you describe the content of this image?\", img], source=\"user\")\nimg",
      "language": "python"
    },
    {
      "code": "from io import BytesIO\n\nimport PIL\nimport requests\nfrom autogen_agentchat.messages import MultiModalMessage\nfrom autogen_core import Image\n\n# Create a multi-modal message with random image and text.\npil_image = PIL.Image.open(BytesIO(requests.get(\"https://picsum.photos/300/200\").content))\nimg = Image(pil_image)\nmulti_modal_message = MultiModalMessage(content=[\"Can you describe the content of this image?\", img], source=\"user\")\nimg",
      "language": "python"
    },
    {
      "code": "# Use asyncio.run(...) when running in a script.\nresult = await agent.run(task=multi_modal_message)\nprint(result.messages[-1].content)  # type: ignore",
      "language": "python"
    },
    {
      "code": "# Use asyncio.run(...) when running in a script.\nresult = await agent.run(task=multi_modal_message)\nprint(result.messages[-1].content)  # type: ignore",
      "language": "python"
    },
    {
      "code": "The image depicts a scenic mountain landscape under a clear blue sky. There are several rugged mountain peaks in the background, with some clouds scattered across the sky. In the valley below, there is a body of water, possibly a lake or river, surrounded by greenery. The overall scene conveys a sense of natural beauty and tranquility.",
      "language": "unknown"
    },
    {
      "code": "The image depicts a scenic mountain landscape under a clear blue sky. There are several rugged mountain peaks in the background, with some clouds scattered across the sky. In the valley below, there is a body of water, possibly a lake or river, surrounded by greenery. The overall scene conveys a sense of natural beauty and tranquility.",
      "language": "unknown"
    },
    {
      "code": "async def assistant_run_stream() -> None:\n    # Option 1: read each message from the stream (as shown in the previous example).\n    # async for message in agent.run_stream(task=\"Find information on AutoGen\"):\n    #     print(message)\n\n    # Option 2: use Console to print all messages as they appear.\n    await Console(\n        agent.run_stream(task=\"Find information on AutoGen\"),\n        output_stats=True,  # Enable stats printing.\n    )\n\n\n# Use asyncio.run(assistant_run_stream()) when running in a script.\nawait assistant_run_stream()",
      "language": "python"
    },
    {
      "code": "async def assistant_run_stream() -> None:\n    # Option 1: read each message from the stream (as shown in the previous example).\n    # async for message in agent.run_stream(task=\"Find information on AutoGen\"):\n    #     print(message)\n\n    # Option 2: use Console to print all messages as they appear.\n    await Console(\n        agent.run_stream(task=\"Find information on AutoGen\"),\n        output_stats=True,  # Enable stats printing.\n    )\n\n\n# Use asyncio.run(assistant_run_stream()) when running in a script.\nawait assistant_run_stream()",
      "language": "python"
    },
    {
      "code": "---------- TextMessage (user) ----------\nFind information on AutoGen\n---------- ToolCallRequestEvent (assistant) ----------\n[FunctionCall(id='call_HOTRhOzXCBm0zSqZCFbHD7YP', arguments='{\"query\":\"AutoGen\"}', name='web_search')]\n[Prompt tokens: 61, Completion tokens: 16]\n---------- ToolCallExecutionEvent (assistant) ----------\n[FunctionExecutionResult(content='AutoGen is a programming framework for building multi-agent applications.', name='web_search', call_id='call_HOTRhOzXCBm0zSqZCFbHD7YP', is_error=False)]\n---------- ToolCallSummaryMessage (assistant) ----------\nAutoGen is a programming framework for building multi-agent applications.\n---------- Summary ----------\nNumber of messages: 4\nFinish reason: None\nTotal prompt tokens: 61\nTotal completion tokens: 16\nDuration: 0.52 seconds",
      "language": "json"
    },
    {
      "code": "---------- TextMessage (user) ----------\nFind information on AutoGen\n---------- ToolCallRequestEvent (assistant) ----------\n[FunctionCall(id='call_HOTRhOzXCBm0zSqZCFbHD7YP', arguments='{\"query\":\"AutoGen\"}', name='web_search')]\n[Prompt tokens: 61, Completion tokens: 16]\n---------- ToolCallExecutionEvent (assistant) ----------\n[FunctionExecutionResult(content='AutoGen is a programming framework for building multi-agent applications.', name='web_search', call_id='call_HOTRhOzXCBm0zSqZCFbHD7YP', is_error=False)]\n---------- ToolCallSummaryMessage (assistant) ----------\nAutoGen is a programming framework for building multi-agent applications.\n---------- Summary ----------\nNumber of messages: 4\nFinish reason: None\nTotal prompt tokens: 61\nTotal completion tokens: 16\nDuration: 0.52 seconds",
      "language": "json"
    },
    {
      "code": "from autogen_core.tools import FunctionTool\n\n\n# Define a tool using a Python function.\nasync def web_search_func(query: str) -> str:\n    \"\"\"Find information on the web\"\"\"\n    return \"AutoGen is a programming framework for building multi-agent applications.\"\n\n\n# This step is automatically performed inside the AssistantAgent if the tool is a Python function.\nweb_search_function_tool = FunctionTool(web_search_func, description=\"Find information on the web\")\n# The schema is provided to the model during AssistantAgent's on_messages call.\nweb_search_function_tool.schema",
      "language": "python"
    },
    {
      "code": "from autogen_core.tools import FunctionTool\n\n\n# Define a tool using a Python function.\nasync def web_search_func(query: str) -> str:\n    \"\"\"Find information on the web\"\"\"\n    return \"AutoGen is a programming framework for building multi-agent applications.\"\n\n\n# This step is automatically performed inside the AssistantAgent if the tool is a Python function.\nweb_search_function_tool = FunctionTool(web_search_func, description=\"Find information on the web\")\n# The schema is provided to the model during AssistantAgent's on_messages call.\nweb_search_function_tool.schema",
      "language": "python"
    },
    {
      "code": "{'name': 'web_search_func',\n 'description': 'Find information on the web',\n 'parameters': {'type': 'object',\n  'properties': {'query': {'description': 'query',\n    'title': 'Query',\n    'type': 'string'}},\n  'required': ['query'],\n  'additionalProperties': False},\n 'strict': False}",
      "language": "json"
    },
    {
      "code": "{'name': 'web_search_func',\n 'description': 'Find information on the web',\n 'parameters': {'type': 'object',\n  'properties': {'query': {'description': 'query',\n    'title': 'Query',\n    'type': 'string'}},\n  'required': ['query'],\n  'additionalProperties': False},\n 'strict': False}",
      "language": "json"
    },
    {
      "code": "from autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.messages import TextMessage\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_ext.tools.mcp import McpWorkbench, StdioServerParams\n\n# Get the fetch tool from mcp-server-fetch.\nfetch_mcp_server = StdioServerParams(command=\"uvx\", args=[\"mcp-server-fetch\"])\n\n# Create an MCP workbench which provides a session to the mcp server.\nasync with McpWorkbench(fetch_mcp_server) as workbench:  # type: ignore\n    # Create an agent that can use the fetch tool.\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4.1-nano\")\n    fetch_agent = AssistantAgent(\n        name=\"fetcher\", model_client=model_client, workbench=workbench, reflect_on_tool_use=True\n    )\n\n    # Let the agent fetch the content of a URL and summarize it.\n    result = await fetch_agent.run(task=\"Summarize the content of https://en.wikipedia.org/wiki/Seattle\")\n    assert isinstance(result.messages[-1], TextMessage)\n    print(result.messages[-1].content)\n\n    # Close the connection to the model client.\n    await model_client.close()",
      "language": "python"
    },
    {
      "code": "from autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.messages import TextMessage\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_ext.tools.mcp import McpWorkbench, StdioServerParams\n\n# Get the fetch tool from mcp-server-fetch.\nfetch_mcp_server = StdioServerParams(command=\"uvx\", args=[\"mcp-server-fetch\"])\n\n# Create an MCP workbench which provides a session to the mcp server.\nasync with McpWorkbench(fetch_mcp_server) as workbench:  # type: ignore\n    # Create an agent that can use the fetch tool.\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4.1-nano\")\n    fetch_agent = AssistantAgent(\n        name=\"fetcher\", model_client=model_client, workbench=workbench, reflect_on_tool_use=True\n    )\n\n    # Let the agent fetch the content of a URL and summarize it.\n    result = await fetch_agent.run(task=\"Summarize the content of https://en.wikipedia.org/wiki/Seattle\")\n    assert isinstance(result.messages[-1], TextMessage)\n    print(result.messages[-1].content)\n\n    # Close the connection to the model client.\n    await model_client.close()",
      "language": "python"
    },
    {
      "code": "Seattle is a major city located in the state of Washington, United States. It was founded on November 13, 1851, and incorporated as a town on January 14, 1865, and later as a city on December 2, 1869. The city is named after Chief Seattle. It covers an area of approximately 142 square miles, with a population of around 737,000 as of the 2020 Census, and an estimated 755,078 residents in 2023. Seattle is known by nicknames such as The Emerald City, Jet City, and Rain City, and has mottos including The City of Flowers and The City of Goodwill. The city operates under a mayor–council government system, with Bruce Harrell serving as mayor. Key landmarks include the Space Needle, Pike Place Market, Amazon Spheres, and the Seattle Great Wheel. It is situated on the U.S. West Coast, with a diverse urban and metropolitan area that extends to a population of over 4 million in the greater metropolitan region.",
      "language": "gdscript"
    },
    {
      "code": "Seattle is a major city located in the state of Washington, United States. It was founded on November 13, 1851, and incorporated as a town on January 14, 1865, and later as a city on December 2, 1869. The city is named after Chief Seattle. It covers an area of approximately 142 square miles, with a population of around 737,000 as of the 2020 Census, and an estimated 755,078 residents in 2023. Seattle is known by nicknames such as The Emerald City, Jet City, and Rain City, and has mottos including The City of Flowers and The City of Goodwill. The city operates under a mayor–council government system, with Bruce Harrell serving as mayor. Key landmarks include the Space Needle, Pike Place Market, Amazon Spheres, and the Seattle Great Wheel. It is situated on the U.S. West Coast, with a diverse urban and metropolitan area that extends to a population of over 4 million in the greater metropolitan region.",
      "language": "gdscript"
    },
    {
      "code": "model_client_no_parallel_tool_call = OpenAIChatCompletionClient(\n    model=\"gpt-4o\",\n    parallel_tool_calls=False,  # type: ignore\n)\nagent_no_parallel_tool_call = AssistantAgent(\n    name=\"assistant\",\n    model_client=model_client_no_parallel_tool_call,\n    tools=[web_search],\n    system_message=\"Use tools to solve tasks.\",\n)",
      "language": "unknown"
    },
    {
      "code": "model_client_no_parallel_tool_call = OpenAIChatCompletionClient(\n    model=\"gpt-4o\",\n    parallel_tool_calls=False,  # type: ignore\n)\nagent_no_parallel_tool_call = AssistantAgent(\n    name=\"assistant\",\n    model_client=model_client_no_parallel_tool_call,\n    tools=[web_search],\n    system_message=\"Use tools to solve tasks.\",\n)",
      "language": "unknown"
    },
    {
      "code": "agent_loop = AssistantAgent(\n    name=\"assistant_loop\",\n    model_client=model_client_no_parallel_tool_call,\n    tools=[web_search],\n    system_message=\"Use tools to solve tasks.\",\n    max_tool_iterations=10,  # At most 10 iterations of tool calls before stopping the loop.\n)",
      "language": "unknown"
    },
    {
      "code": "agent_loop = AssistantAgent(\n    name=\"assistant_loop\",\n    model_client=model_client_no_parallel_tool_call,\n    tools=[web_search],\n    system_message=\"Use tools to solve tasks.\",\n    max_tool_iterations=10,  # At most 10 iterations of tool calls before stopping the loop.\n)",
      "language": "unknown"
    },
    {
      "code": "from typing import Literal\n\nfrom pydantic import BaseModel\n\n\n# The response format for the agent as a Pydantic base model.\nclass AgentResponse(BaseModel):\n    thoughts: str\n    response: Literal[\"happy\", \"sad\", \"neutral\"]\n\n\n# Create an agent that uses the OpenAI GPT-4o model.\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\nagent = AssistantAgent(\n    \"assistant\",\n    model_client=model_client,\n    system_message=\"Categorize the input as happy, sad, or neutral following the JSON format.\",\n    # Define the output content type of the agent.\n    output_content_type=AgentResponse,\n)\n\nresult = await Console(agent.run_stream(task=\"I am happy.\"))\n\n# Check the last message in the result, validate its type, and print the thoughts and response.\nassert isinstance(result.messages[-1], StructuredMessage)\nassert isinstance(result.messages[-1].content, AgentResponse)\nprint(\"Thought: \", result.messages[-1].content.thoughts)\nprint(\"Response: \", result.messages[-1].content.response)\nawait model_client.close()",
      "language": "python"
    },
    {
      "code": "from typing import Literal\n\nfrom pydantic import BaseModel\n\n\n# The response format for the agent as a Pydantic base model.\nclass AgentResponse(BaseModel):\n    thoughts: str\n    response: Literal[\"happy\", \"sad\", \"neutral\"]\n\n\n# Create an agent that uses the OpenAI GPT-4o model.\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\nagent = AssistantAgent(\n    \"assistant\",\n    model_client=model_client,\n    system_message=\"Categorize the input as happy, sad, or neutral following the JSON format.\",\n    # Define the output content type of the agent.\n    output_content_type=AgentResponse,\n)\n\nresult = await Console(agent.run_stream(task=\"I am happy.\"))\n\n# Check the last message in the result, validate its type, and print the thoughts and response.\nassert isinstance(result.messages[-1], StructuredMessage)\nassert isinstance(result.messages[-1].content, AgentResponse)\nprint(\"Thought: \", result.messages[-1].content.thoughts)\nprint(\"Response: \", result.messages[-1].content.response)\nawait model_client.close()",
      "language": "python"
    },
    {
      "code": "---------- user ----------\nI am happy.",
      "language": "yaml"
    },
    {
      "code": "---------- user ----------\nI am happy.",
      "language": "yaml"
    },
    {
      "code": "---------- assistant ----------\n{\n  \"thoughts\": \"The user explicitly states they are happy.\",\n  \"response\": \"happy\"\n}\nThought:  The user explicitly states they are happy.\nResponse:  happy",
      "language": "json"
    },
    {
      "code": "---------- assistant ----------\n{\n  \"thoughts\": \"The user explicitly states they are happy.\",\n  \"response\": \"happy\"\n}\nThought:  The user explicitly states they are happy.\nResponse:  happy",
      "language": "json"
    },
    {
      "code": "model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\nstreaming_assistant = AssistantAgent(\n    name=\"assistant\",\n    model_client=model_client,\n    system_message=\"You are a helpful assistant.\",\n    model_client_stream=True,  # Enable streaming tokens.\n)\n\n# Use an async function and asyncio.run() in a script.\nasync for message in streaming_assistant.run_stream(task=\"Name two cities in South America\"):  # type: ignore\n    print(message)",
      "language": "javascript"
    },
    {
      "code": "model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\nstreaming_assistant = AssistantAgent(\n    name=\"assistant\",\n    model_client=model_client,\n    system_message=\"You are a helpful assistant.\",\n    model_client_stream=True,  # Enable streaming tokens.\n)\n\n# Use an async function and asyncio.run() in a script.\nasync for message in streaming_assistant.run_stream(task=\"Name two cities in South America\"):  # type: ignore\n    print(message)",
      "language": "javascript"
    },
    {
      "code": "source='user' models_usage=None metadata={} content='Name two cities in South America' type='TextMessage'\nsource='assistant' models_usage=None metadata={} content='Two' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content=' cities' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content=' in' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content=' South' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content=' America' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content=' are' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content=' Buenos' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content=' Aires' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content=' in' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content=' Argentina' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content=' and' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content=' São' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content=' Paulo' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content=' in' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content=' Brazil' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content='.' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=RequestUsage(prompt_tokens=0, completion_tokens=0) metadata={} content='Two cities in South America are Buenos Aires in Argentina and São Paulo in Brazil.' type='TextMessage'\nmessages=[TextMessage(source='user', models_usage=None, metadata={}, content='Name two cities in South America', type='TextMessage'), TextMessage(source='assistant', models_usage=RequestUsage(prompt_tokens=0, completion_tokens=0), metadata={}, content='Two cities in South America are Buenos Aires in Argentina and São Paulo in Brazil.', type='TextMessage')] stop_reason=None",
      "language": "rust"
    },
    {
      "code": "source='user' models_usage=None metadata={} content='Name two cities in South America' type='TextMessage'\nsource='assistant' models_usage=None metadata={} content='Two' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content=' cities' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content=' in' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content=' South' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content=' America' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content=' are' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content=' Buenos' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content=' Aires' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content=' in' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content=' Argentina' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content=' and' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content=' São' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content=' Paulo' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content=' in' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content=' Brazil' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=None metadata={} content='.' type='ModelClientStreamingChunkEvent'\nsource='assistant' models_usage=RequestUsage(prompt_tokens=0, completion_tokens=0) metadata={} content='Two cities in South America are Buenos Aires in Argentina and São Paulo in Brazil.' type='TextMessage'\nmessages=[TextMessage(source='user', models_usage=None, metadata={}, content='Name two cities in South America', type='TextMessage'), TextMessage(source='assistant', models_usage=RequestUsage(prompt_tokens=0, completion_tokens=0), metadata={}, content='Two cities in South America are Buenos Aires in Argentina and São Paulo in Brazil.', type='TextMessage')] stop_reason=None",
      "language": "rust"
    },
    {
      "code": "from autogen_core.model_context import BufferedChatCompletionContext\n\n# Create an agent that uses only the last 5 messages in the context to generate responses.\nagent = AssistantAgent(\n    name=\"assistant\",\n    model_client=model_client,\n    tools=[web_search],\n    system_message=\"Use tools to solve tasks.\",\n    model_context=BufferedChatCompletionContext(buffer_size=5),  # Only use the last 5 messages in the context.\n)",
      "language": "sql"
    },
    {
      "code": "from autogen_core.model_context import BufferedChatCompletionContext\n\n# Create an agent that uses only the last 5 messages in the context to generate responses.\nagent = AssistantAgent(\n    name=\"assistant\",\n    model_client=model_client,\n    tools=[web_search],\n    system_message=\"Use tools to solve tasks.\",\n    model_context=BufferedChatCompletionContext(buffer_size=5),  # Only use the last 5 messages in the context.\n)",
      "language": "sql"
    }
  ],
  "patterns": [],
  "links": [
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/agents.html",
    "https://microsoft.github.io/autogen/stable/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/extensions-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/reference/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/installation.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/quickstart.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/migration-guide.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/models.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/messages.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/teams.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/human-in-the-loop.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/termination.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/state.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/custom-agents.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/selector-group-chat.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/swarm.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/magentic-one.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/graph-flow.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/memory.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/logging.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/serialize-components.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tracing.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/examples/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/examples/travel-planning.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/examples/company-research.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/examples/literature-review.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.agents.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.base.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.messages.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.ui.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.tools.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/components/tools.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/components/workbench.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.graphrag.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.http.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.langchain.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.mcp.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.tools.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.openai.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.model_context.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.openai.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.web_surfer.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.file_surfer.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.video_surfer.html"
  ]
}