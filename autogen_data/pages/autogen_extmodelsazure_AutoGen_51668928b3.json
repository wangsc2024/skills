{
  "url": "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.azure.html",
  "title": "autogen_ext.models.azure — AutoGen",
  "content": "Bases: ChatCompletionClient\n\nChat completion client for models hosted on Azure AI Foundry or GitHub Models. See here for more info.\n\nendpoint (str) – The endpoint to use. Required.\n\ncredential (union, AzureKeyCredential, AsyncTokenCredential) – The credentials to use. Required\n\nmodel_info (ModelInfo) – The model family and capabilities of the model. Required.\n\nmodel (str) – The name of the model. Required if model is hosted on GitHub Models.\n\nfrequency_penalty – (optional,float)\n\npresence_penalty – (optional,float)\n\ntemperature – (optional,float)\n\ntop_p – (optional,float)\n\nmax_tokens – (optional,int)\n\nresponse_format – (optional, literal[“text”, “json_object”])\n\nstop – (optional,List[str])\n\ntools – (optional,List[ChatCompletionsToolDefinition])\n\ntool_choice – (optional,Union[str, ChatCompletionsToolChoicePreset, ChatCompletionsNamedToolChoice]])\n\nseed – (optional,int)\n\nmodel_extras – (optional,Dict[str, Any])\n\nTo use this client, you must install the azure extra:\n\nThe following code snippet shows how to use the client with GitHub Models:\n\nTo use streaming, you can use the create_stream method:\n\nCreates a single response from the model.\n\nmessages (Sequence[LLMMessage]) – The messages to send to the model.\n\ntools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to [].\n\ntool_choice (Tool | Literal[\"auto\", \"required\", \"none\"], optional) – A single Tool object to force the model to use, “auto” to let the model choose any available tool, “required” to force tool usage, or “none” to disable tool usage. Defaults to “auto”.\n\njson_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither. Defaults to None. If set to a Pydantic BaseModel type, it will be used as the output type for structured output. If set to a boolean, it will be used to determine whether to use JSON mode or not. If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.\n\nextra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}.\n\ncancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None.\n\nCreateResult – The result of the model call.\n\nCreates a stream of string chunks from the model ending with a CreateResult.\n\nmessages (Sequence[LLMMessage]) – The messages to send to the model.\n\ntools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to [].\n\ntool_choice (Tool | Literal[\"auto\", \"required\", \"none\"], optional) – A single Tool object to force the model to use, “auto” to let the model choose any available tool, “required” to force tool usage, or “none” to disable tool usage. Defaults to “auto”.\n\njson_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither. Defaults to None. If set to a Pydantic BaseModel type, it will be used as the output type for structured output. If set to a boolean, it will be used to determine whether to use JSON mode or not. If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.\n\nWhether to use JSON mode, structured output, or neither. Defaults to None. If set to a Pydantic BaseModel type, it will be used as the output type for structured output. If set to a boolean, it will be used to determine whether to use JSON mode or not. If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.\n\nextra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}.\n\ncancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None.\n\nAsyncGenerator[Union[str, CreateResult], None] – A generator that yields string chunks and ends with a CreateResult.\n\nautogen_ext.models.anthropic\n\nautogen_ext.models.cache",
  "headings": [
    {
      "level": "h1",
      "text": "autogen_ext.models.azure#",
      "id": ""
    }
  ],
  "code_samples": [
    {
      "code": "pip install \"autogen-ext[azure]\"",
      "language": "unknown"
    },
    {
      "code": "pip install \"autogen-ext[azure]\"",
      "language": "unknown"
    },
    {
      "code": "import asyncio\nimport os\nfrom azure.core.credentials import AzureKeyCredential\nfrom autogen_ext.models.azure import AzureAIChatCompletionClient\nfrom autogen_core.models import UserMessage\n\n\nasync def main():\n    client = AzureAIChatCompletionClient(\n        model=\"Phi-4\",\n        endpoint=\"https://models.github.ai/inference\",\n        # To authenticate with the model you will need to generate a personal access token (PAT) in your GitHub settings.\n        # Create your PAT token by following instructions here: https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens\n        credential=AzureKeyCredential(os.environ[\"GITHUB_TOKEN\"]),\n        model_info={\n            \"json_output\": False,\n            \"function_calling\": False,\n            \"vision\": False,\n            \"family\": \"unknown\",\n            \"structured_output\": False,\n        },\n    )\n\n    result = await client.create([UserMessage(content=\"What is the capital of France?\", source=\"user\")])\n    print(result)\n\n    # Close the client.\n    await client.close()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nimport os\nfrom azure.core.credentials import AzureKeyCredential\nfrom autogen_ext.models.azure import AzureAIChatCompletionClient\nfrom autogen_core.models import UserMessage\n\n\nasync def main():\n    client = AzureAIChatCompletionClient(\n        model=\"Phi-4\",\n        endpoint=\"https://models.github.ai/inference\",\n        # To authenticate with the model you will need to generate a personal access token (PAT) in your GitHub settings.\n        # Create your PAT token by following instructions here: https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens\n        credential=AzureKeyCredential(os.environ[\"GITHUB_TOKEN\"]),\n        model_info={\n            \"json_output\": False,\n            \"function_calling\": False,\n            \"vision\": False,\n            \"family\": \"unknown\",\n            \"structured_output\": False,\n        },\n    )\n\n    result = await client.create([UserMessage(content=\"What is the capital of France?\", source=\"user\")])\n    print(result)\n\n    # Close the client.\n    await client.close()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nimport os\n\nfrom autogen_core.models import UserMessage\nfrom autogen_ext.models.azure import AzureAIChatCompletionClient\nfrom azure.core.credentials import AzureKeyCredential\n\n\nasync def main():\n    client = AzureAIChatCompletionClient(\n        model=\"Phi-4\",\n        endpoint=\"https://models.github.ai/inference\",\n        # To authenticate with the model you will need to generate a personal access token (PAT) in your GitHub settings.\n        # Create your PAT token by following instructions here: https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens\n        credential=AzureKeyCredential(os.environ[\"GITHUB_TOKEN\"]),\n        model_info={\n            \"json_output\": False,\n            \"function_calling\": False,\n            \"vision\": False,\n            \"family\": \"unknown\",\n            \"structured_output\": False,\n        },\n    )\n\n    # Create a stream.\n    stream = client.create_stream([UserMessage(content=\"Write a poem about the ocean\", source=\"user\")])\n    async for chunk in stream:\n        print(chunk, end=\"\", flush=True)\n    print()\n\n    # Close the client.\n    await client.close()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nimport os\n\nfrom autogen_core.models import UserMessage\nfrom autogen_ext.models.azure import AzureAIChatCompletionClient\nfrom azure.core.credentials import AzureKeyCredential\n\n\nasync def main():\n    client = AzureAIChatCompletionClient(\n        model=\"Phi-4\",\n        endpoint=\"https://models.github.ai/inference\",\n        # To authenticate with the model you will need to generate a personal access token (PAT) in your GitHub settings.\n        # Create your PAT token by following instructions here: https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens\n        credential=AzureKeyCredential(os.environ[\"GITHUB_TOKEN\"]),\n        model_info={\n            \"json_output\": False,\n            \"function_calling\": False,\n            \"vision\": False,\n            \"family\": \"unknown\",\n            \"structured_output\": False,\n        },\n    )\n\n    # Create a stream.\n    stream = client.create_stream([UserMessage(content=\"Write a poem about the ocean\", source=\"user\")])\n    async for chunk in stream:\n        print(chunk, end=\"\", flush=True)\n    print()\n\n    # Close the client.\n    await client.close()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())",
      "language": "python"
    }
  ],
  "patterns": [
    {
      "description": "API Reference autogen_ext.models.azure autogen_ext.models.azure# class AzureAIChatCompletionClient(**kwargs: Unpack)[source]# Bases: ChatCompletionClient Chat completion client for models hosted on Azure AI Foundry or GitHub Models. See here for more info. Parameters: endpoint (str) – The endpoint to use. Required. credential (union, AzureKeyCredential, AsyncTokenCredential) – The credentials to use. Required model_info (ModelInfo) – The model family and capabilities of the model. Required. model (str) – The name of the model. Required if model is hosted on GitHub Models. frequency_penalty – (optional,float) presence_penalty – (optional,float) temperature – (optional,float) top_p – (optional,float) max_tokens – (optional,int) response_format – (optional, literal[“text”, “json_object”]) stop – (optional,List[str]) tools – (optional,List[ChatCompletionsToolDefinition]) tool_choice – (optional,Union[str, ChatCompletionsToolChoicePreset, ChatCompletionsNamedToolChoice]]) seed – (optional,int) model_extras – (optional,Dict[str, Any]) To use this client, you must install the azure extra: pip install \"autogen-ext[azure]\" The following code snippet shows how to use the client with GitHub Models: import asyncio import os from azure.core.credentials import AzureKeyCredential from autogen_ext.models.azure import AzureAIChatCompletionClient from autogen_core.models import UserMessage async def main(): client = AzureAIChatCompletionClient( model=\"Phi-4\", endpoint=\"https://models.github.ai/inference\", # To authenticate with the model you will need to generate a personal access token (PAT) in your GitHub settings. # Create your PAT token by following instructions here: https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens credential=AzureKeyCredential(os.environ[\"GITHUB_TOKEN\"]), model_info={ \"json_output\": False, \"function_calling\": False, \"vision\": False, \"family\": \"unknown\", \"structured_output\": False, }, ) result = await client.create([UserMessage(content=\"What is the capital of France?\", source=\"user\")]) print(result) # Close the client. await client.close() if __name__ == \"__main__\": asyncio.run(main()) To use streaming, you can use the create_stream method: import asyncio import os from autogen_core.models import UserMessage from autogen_ext.models.azure import AzureAIChatCompletionClient from azure.core.credentials import AzureKeyCredential async def main(): client = AzureAIChatCompletionClient( model=\"Phi-4\", endpoint=\"https://models.github.ai/inference\", # To authenticate with the model you will need to generate a personal access token (PAT) in your GitHub settings. # Create your PAT token by following instructions here: https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens credential=AzureKeyCredential(os.environ[\"GITHUB_TOKEN\"]), model_info={ \"json_output\": False, \"function_calling\": False, \"vision\": False, \"family\": \"unknown\", \"structured_output\": False, }, ) # Create a stream. stream = client.create_stream([UserMessage(content=\"Write a poem about the ocean\", source=\"user\")]) async for chunk in stream: print(chunk, end=\"\", flush=True) print() # Close the client. await client.close() if __name__ == \"__main__\": asyncio.run(main()) add_usage(usage: RequestUsage) → None[source]# async create(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], tool_choice: Tool | Literal['auto', 'required', 'none'] = 'auto', json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → CreateResult[source]# Creates a single response from the model. Parameters: messages (Sequence[LLMMessage]) – The messages to send to the model. tools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to []. tool_choice (Tool | Literal[\"auto\", \"required\", \"none\"], optional) – A single Tool object to force the model to use, “auto” to let the model choose any available tool, “required” to force tool usage, or “none” to disable tool usage. Defaults to “auto”. json_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither. Defaults to None. If set to a Pydantic BaseModel type, it will be used as the output type for structured output. If set to a boolean, it will be used to determine whether to use JSON mode or not. If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt. extra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}. cancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None. Returns: CreateResult – The result of the model call. async create_stream(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], tool_choice: Tool | Literal['auto', 'required', 'none'] = 'auto', json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → AsyncGenerator[str | CreateResult, None][source]# Creates a stream of string chunks from the model ending with a CreateResult. Parameters: messages (Sequence[LLMMessage]) – The messages to send to the model. tools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to []. tool_choice (Tool | Literal[\"auto\", \"required\", \"none\"], optional) – A single Tool object to force the model to use, “auto” to let the model choose any available tool, “required” to force tool usage, or “none” to disable tool usage. Defaults to “auto”. json_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither. Defaults to None. If set to a Pydantic BaseModel type, it will be used as the output type for structured output. If set to a boolean, it will be used to determine whether to use JSON mode or not. If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt. extra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}. cancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None. Returns: AsyncGenerator[Union[str, CreateResult], None] – A generator that yields string chunks and ends with a CreateResult. async close() → None[source]# actual_usage() → RequestUsage[source]# total_usage() → RequestUsage[source]# count_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]# remaining_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]# property model_info: ModelInfo# property capabilities: ModelInfo# class AzureAIChatCompletionClientConfig[source]# Bases: dict endpoint: str# credential: AzureKeyCredential | AsyncTokenCredential# model_info: ModelInfo# frequency_penalty: float | None# presence_penalty: float | None# temperature: float | None# top_p: float | None# max_tokens: int | None# response_format: Literal['text', 'json_object'] | None# stop: List[str] | None# tools: List[ChatCompletionsToolDefinition] | None# tool_choice: str | ChatCompletionsToolChoicePreset | ChatCompletionsNamedToolChoice | None# seed: int | None# model: str | None# model_extras: Dict[str, Any] | None# previous autogen_ext.models.anthropic next autogen_ext.models.cache On this page AzureAIChatCompletionClient AzureAIChatCompletionClient.add_usage() AzureAIChatCompletionClient.create() AzureAIChatCompletionClient.create_stream() AzureAIChatCompletionClient.close() AzureAIChatCompletionClient.actual_usage() AzureAIChatCompletionClient.total_usage() AzureAIChatCompletionClient.count_tokens() AzureAIChatCompletionClient.remaining_tokens() AzureAIChatCompletionClient.model_info AzureAIChatCompletionClient.capabilities AzureAIChatCompletionClientConfig AzureAIChatCompletionClientConfig.endpoint AzureAIChatCompletionClientConfig.credential AzureAIChatCompletionClientConfig.model_info AzureAIChatCompletionClientConfig.frequency_penalty AzureAIChatCompletionClientConfig.presence_penalty AzureAIChatCompletionClientConfig.temperature AzureAIChatCompletionClientConfig.top_p AzureAIChatCompletionClientConfig.max_tokens AzureAIChatCompletionClientConfig.response_format AzureAIChatCompletionClientConfig.stop AzureAIChatCompletionClientConfig.tools AzureAIChatCompletionClientConfig.tool_choice AzureAIChatCompletionClientConfig.seed AzureAIChatCompletionClientConfig.model AzureAIChatCompletionClientConfig.model_extras Edit on GitHub Show Source",
      "code": "ChatCompletionClient"
    },
    {
      "description": "API Reference autogen_ext.models.azure autogen_ext.models.azure# class AzureAIChatCompletionClient(**kwargs: Unpack)[source]# Bases: ChatCompletionClient Chat completion client for models hosted on Azure AI Foundry or GitHub Models. See here for more info. Parameters: endpoint (str) – The endpoint to use. Required. credential (union, AzureKeyCredential, AsyncTokenCredential) – The credentials to use. Required model_info (ModelInfo) – The model family and capabilities of the model. Required. model (str) – The name of the model. Required if model is hosted on GitHub Models. frequency_penalty – (optional,float) presence_penalty – (optional,float) temperature – (optional,float) top_p – (optional,float) max_tokens – (optional,int) response_format – (optional, literal[“text”, “json_object”]) stop – (optional,List[str]) tools – (optional,List[ChatCompletionsToolDefinition]) tool_choice – (optional,Union[str, ChatCompletionsToolChoicePreset, ChatCompletionsNamedToolChoice]]) seed – (optional,int) model_extras – (optional,Dict[str, Any]) To use this client, you must install the azure extra: pip install \"autogen-ext[azure]\" The following code snippet shows how to use the client with GitHub Models: import asyncio import os from azure.core.credentials import AzureKeyCredential from autogen_ext.models.azure import AzureAIChatCompletionClient from autogen_core.models import UserMessage async def main(): client = AzureAIChatCompletionClient( model=\"Phi-4\", endpoint=\"https://models.github.ai/inference\", # To authenticate with the model you will need to generate a personal access token (PAT) in your GitHub settings. # Create your PAT token by following instructions here: https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens credential=AzureKeyCredential(os.environ[\"GITHUB_TOKEN\"]), model_info={ \"json_output\": False, \"function_calling\": False, \"vision\": False, \"family\": \"unknown\", \"structured_output\": False, }, ) result = await client.create([UserMessage(content=\"What is the capital of France?\", source=\"user\")]) print(result) # Close the client. await client.close() if __name__ == \"__main__\": asyncio.run(main()) To use streaming, you can use the create_stream method: import asyncio import os from autogen_core.models import UserMessage from autogen_ext.models.azure import AzureAIChatCompletionClient from azure.core.credentials import AzureKeyCredential async def main(): client = AzureAIChatCompletionClient( model=\"Phi-4\", endpoint=\"https://models.github.ai/inference\", # To authenticate with the model you will need to generate a personal access token (PAT) in your GitHub settings. # Create your PAT token by following instructions here: https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens credential=AzureKeyCredential(os.environ[\"GITHUB_TOKEN\"]), model_info={ \"json_output\": False, \"function_calling\": False, \"vision\": False, \"family\": \"unknown\", \"structured_output\": False, }, ) # Create a stream. stream = client.create_stream([UserMessage(content=\"Write a poem about the ocean\", source=\"user\")]) async for chunk in stream: print(chunk, end=\"\", flush=True) print() # Close the client. await client.close() if __name__ == \"__main__\": asyncio.run(main()) add_usage(usage: RequestUsage) → None[source]# async create(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], tool_choice: Tool | Literal['auto', 'required', 'none'] = 'auto', json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → CreateResult[source]# Creates a single response from the model. Parameters: messages (Sequence[LLMMessage]) – The messages to send to the model. tools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to []. tool_choice (Tool | Literal[\"auto\", \"required\", \"none\"], optional) – A single Tool object to force the model to use, “auto” to let the model choose any available tool, “required” to force tool usage, or “none” to disable tool usage. Defaults to “auto”. json_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither. Defaults to None. If set to a Pydantic BaseModel type, it will be used as the output type for structured output. If set to a boolean, it will be used to determine whether to use JSON mode or not. If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt. extra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}. cancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None. Returns: CreateResult – The result of the model call. async create_stream(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], tool_choice: Tool | Literal['auto', 'required', 'none'] = 'auto', json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → AsyncGenerator[str | CreateResult, None][source]# Creates a stream of string chunks from the model ending with a CreateResult. Parameters: messages (Sequence[LLMMessage]) – The messages to send to the model. tools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to []. tool_choice (Tool | Literal[\"auto\", \"required\", \"none\"], optional) – A single Tool object to force the model to use, “auto” to let the model choose any available tool, “required” to force tool usage, or “none” to disable tool usage. Defaults to “auto”. json_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither. Defaults to None. If set to a Pydantic BaseModel type, it will be used as the output type for structured output. If set to a boolean, it will be used to determine whether to use JSON mode or not. If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt. extra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}. cancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None. Returns: AsyncGenerator[Union[str, CreateResult], None] – A generator that yields string chunks and ends with a CreateResult. async close() → None[source]# actual_usage() → RequestUsage[source]# total_usage() → RequestUsage[source]# count_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]# remaining_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]# property model_info: ModelInfo# property capabilities: ModelInfo# class AzureAIChatCompletionClientConfig[source]# Bases: dict endpoint: str# credential: AzureKeyCredential | AsyncTokenCredential# model_info: ModelInfo# frequency_penalty: float | None# presence_penalty: float | None# temperature: float | None# top_p: float | None# max_tokens: int | None# response_format: Literal['text', 'json_object'] | None# stop: List[str] | None# tools: List[ChatCompletionsToolDefinition] | None# tool_choice: str | ChatCompletionsToolChoicePreset | ChatCompletionsNamedToolChoice | None# seed: int | None# model: str | None# model_extras: Dict[str, Any] | None# previous autogen_ext.models.anthropic next autogen_ext.models.cache",
      "code": "ChatCompletionClient"
    }
  ],
  "links": [
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.azure.html",
    "https://microsoft.github.io/autogen/stable/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/extensions-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/reference/index.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.agents.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.base.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.conditions.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.messages.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.state.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.teams.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.tools.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.ui.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.utils.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.code_executor.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.exceptions.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.logging.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.memory.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.model_context.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.models.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.tool_agent.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.tools.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.utils.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.cache_store.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.experimental.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.memory.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.teams.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.ui.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.azure.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.file_surfer.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.magentic_one.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.openai.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.video_surfer.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.web_surfer.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.auth.azure.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.cache_store.diskcache.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.cache_store.redis.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.azure.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.docker.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.docker_jupyter.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.jupyter.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.local.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.experimental.task_centric_memory.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.memory.canvas.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.memory.chromadb.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.memory.mem0.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.memory.redis.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.anthropic.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.cache.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.llama_cpp.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.ollama.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.openai.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.replay.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.semantic_kernel.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.teams.magentic_one.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.azure.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.code_execution.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.graphrag.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.http.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.langchain.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.mcp.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.semantic_kernel.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.video_surfer.tools.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.web_surfer.playwright_controller.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.experimental.task_centric_memory.utils.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.anthropic.config.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.azure.config.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.ollama.config.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.openai.config.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.protos.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.protos.agent_worker_pb2.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.protos.agent_worker_pb2_grpc.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.protos.cloudevent_pb2.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.protos.cloudevent_pb2_grpc.html"
  ]
}