{
  "url": "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.teams.magentic_one.html",
  "title": "autogen_ext.teams.magentic_one — AutoGen",
  "content": "Bases: MagenticOneGroupChat\n\nMagenticOne is a specialized group chat class that integrates various agents such as FileSurfer, WebSurfer, Coder, and Executor to solve complex tasks. To read more about the science behind Magentic-One, see the full blog post: Magentic-One: A Generalist Multi-Agent System for Solving Complex Tasks and the references below.\n\nclient (ChatCompletionClient) – The client used for model interactions.\n\nhil_mode (bool) – Optional; If set to True, adds the UserProxyAgent to the list of agents.\n\ninput_func (InputFuncType | None) – Optional; Function to use for user input in human-in-the-loop mode.\n\ncode_executor (CodeExecutor | None) – Optional; Code executor to use. If None, will use Docker if available, otherwise local executor.\n\napproval_func (ApprovalFuncType | None) – Optional; Function to approve code execution before running. If None, code will execute without approval.\n\nUsing Magentic-One involves interacting with a digital world designed for humans, which carries inherent risks. To minimize these risks, consider the following precautions:\n\nUse Containers: Run all tasks in docker containers to isolate the agents and prevent direct system attacks.\n\nVirtual Environment: Use a virtual environment to run the agents and prevent them from accessing sensitive data.\n\nMonitor Logs: Closely monitor logs during and after execution to detect and mitigate risky behavior.\n\nHuman Oversight: Run the examples with a human in the loop to supervise the agents and prevent unintended consequences.\n\nLimit Access: Restrict the agents’ access to the internet and other resources to prevent unauthorized actions.\n\nSafeguard Data: Ensure that the agents do not have access to sensitive data or resources that could be compromised. Do not share sensitive information with the agents.\n\nBe aware that agents may occasionally attempt risky actions, such as recruiting humans for help or accepting cookie agreements without human involvement. Always ensure agents are monitored and operate within a controlled environment to prevent unintended consequences. Moreover, be cautious that Magentic-One may be susceptible to prompt injection attacks from webpages.\n\nMagentic-One is a generalist multi-agent system for solving open-ended web and file-based tasks across a variety of domains. It represents a significant step towards developing agents that can complete tasks that people encounter in their work and personal lives.\n\nMagentic-One work is based on a multi-agent architecture where a lead Orchestrator agent is responsible for high-level planning, directing other agents, and tracking task progress. The Orchestrator begins by creating a plan to tackle the task, gathering needed facts and educated guesses in a Task Ledger that is maintained. At each step of its plan, the Orchestrator creates a Progress Ledger where it self-reflects on task progress and checks whether the task is completed. If the task is not yet completed, it assigns one of Magentic-One’s other agents a subtask to complete. After the assigned agent completes its subtask, the Orchestrator updates the Progress Ledger and continues in this way until the task is complete. If the Orchestrator finds that progress is not being made for enough steps, it can update the Task Ledger and create a new plan.\n\nOverall, Magentic-One consists of the following agents:\n\nOrchestrator: The lead agent responsible for task decomposition and planning, directing other agents in executing subtasks, tracking overall progress, and taking corrective actions as needed.\n\nWebSurfer: An LLM-based agent proficient in commanding and managing the state of a Chromium-based web browser. It performs actions on the browser and reports on the new state of the web page.\n\nFileSurfer: An LLM-based agent that commands a markdown-based file preview application to read local files of most types. It can also perform common navigation tasks such as listing the contents of directories and navigating a folder structure.\n\nCoder: An LLM-based agent specialized in writing code, analyzing information collected from other agents, or creating new artifacts.\n\nComputerTerminal: Provides the team with access to a console shell where the Coder’s programs can be executed, and where new programming libraries can be installed.\n\nTogether, Magentic-One’s agents provide the Orchestrator with the tools and capabilities needed to solve a broad variety of open-ended problems, as well as the ability to autonomously adapt to, and act in, dynamic and ever-changing web and file-system environments.\n\nautogen_ext.runtimes.grpc\n\nautogen_ext.tools.azure",
  "headings": [
    {
      "level": "h1",
      "text": "autogen_ext.teams.magentic_one#",
      "id": ""
    }
  ],
  "code_samples": [
    {
      "code": "pip install \"autogen-ext[magentic-one]\"",
      "language": "unknown"
    },
    {
      "code": "pip install \"autogen-ext[magentic-one]\"",
      "language": "unknown"
    },
    {
      "code": "# Autonomously complete a coding task:\nimport asyncio\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_ext.teams.magentic_one import MagenticOne\nfrom autogen_agentchat.ui import Console\n\n\nasync def example_usage():\n    client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n    m1 = MagenticOne(client=client)  # Uses DockerCommandLineCodeExecutor by default\n    task = \"Write a Python script to fetch data from an API.\"\n    result = await Console(m1.run_stream(task=task))\n    print(result)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(example_usage())",
      "language": "python"
    },
    {
      "code": "# Autonomously complete a coding task:\nimport asyncio\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_ext.teams.magentic_one import MagenticOne\nfrom autogen_agentchat.ui import Console\n\n\nasync def example_usage():\n    client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n    m1 = MagenticOne(client=client)  # Uses DockerCommandLineCodeExecutor by default\n    task = \"Write a Python script to fetch data from an API.\"\n    result = await Console(m1.run_stream(task=task))\n    print(result)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(example_usage())",
      "language": "python"
    },
    {
      "code": "# Enable human-in-the-loop mode with explicit Docker executor and code approval\nimport asyncio\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_ext.teams.magentic_one import MagenticOne\nfrom autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor\nfrom autogen_agentchat.ui import Console\nfrom autogen_agentchat.agents import ApprovalRequest, ApprovalResponse\n\n\ndef user_input_func(prompt: str) -> str:\n    \"\"\"Custom input function for user interaction.\"\"\"\n    return input(prompt)\n\n\ndef approval_func(request: ApprovalRequest) -> ApprovalResponse:\n    \"\"\"Simple approval function that requests user input.\"\"\"\n    print(f\"Code to execute:\\n{request.code}\")\n    user_input = input(\"Do you approve this code execution? (y/n): \").strip().lower()\n    if user_input == 'y':\n        return ApprovalResponse(approved=True, reason=\"User approved the code execution\")\n    else:\n        return ApprovalResponse(approved=False, reason=\"User denied the code execution\")\n\n\nasync def example_usage_hil():\n    client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n    # Explicitly specify Docker code executor for better security\n    async with DockerCommandLineCodeExecutor() as code_executor:\n        m1 = MagenticOne(\n            client=client,\n            hil_mode=True,\n            input_func=user_input_func,\n            code_executor=code_executor,\n            approval_func=approval_func\n        )\n        task = \"Write a Python script to fetch data from an API.\"\n        result = await Console(m1.run_stream(task=task))\n        print(result)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(example_usage_hil())",
      "language": "python"
    },
    {
      "code": "# Enable human-in-the-loop mode with explicit Docker executor and code approval\nimport asyncio\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_ext.teams.magentic_one import MagenticOne\nfrom autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor\nfrom autogen_agentchat.ui import Console\nfrom autogen_agentchat.agents import ApprovalRequest, ApprovalResponse\n\n\ndef user_input_func(prompt: str) -> str:\n    \"\"\"Custom input function for user interaction.\"\"\"\n    return input(prompt)\n\n\ndef approval_func(request: ApprovalRequest) -> ApprovalResponse:\n    \"\"\"Simple approval function that requests user input.\"\"\"\n    print(f\"Code to execute:\\n{request.code}\")\n    user_input = input(\"Do you approve this code execution? (y/n): \").strip().lower()\n    if user_input == 'y':\n        return ApprovalResponse(approved=True, reason=\"User approved the code execution\")\n    else:\n        return ApprovalResponse(approved=False, reason=\"User denied the code execution\")\n\n\nasync def example_usage_hil():\n    client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n    # Explicitly specify Docker code executor for better security\n    async with DockerCommandLineCodeExecutor() as code_executor:\n        m1 = MagenticOne(\n            client=client,\n            hil_mode=True,\n            input_func=user_input_func,\n            code_executor=code_executor,\n            approval_func=approval_func\n        )\n        task = \"Write a Python script to fetch data from an API.\"\n        result = await Console(m1.run_stream(task=task))\n        print(result)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(example_usage_hil())",
      "language": "python"
    },
    {
      "code": "# Enable code execution approval without human-in-the-loop mode\nimport asyncio\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_ext.teams.magentic_one import MagenticOne\nfrom autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor\nfrom autogen_agentchat.ui import Console\nfrom autogen_agentchat.agents import ApprovalRequest, ApprovalResponse\n\n\ndef approval_func(request: ApprovalRequest) -> ApprovalResponse:\n    \"\"\"Simple approval function that requests user input.\"\"\"\n    print(f\"Code to execute:\\n{request.code}\")\n    user_input = input(\"Do you approve this code execution? (y/n): \").strip().lower()\n    if user_input == 'y':\n        return ApprovalResponse(approved=True, reason=\"User approved the code execution\")\n    else:\n        return ApprovalResponse(approved=False, reason=\"User denied the code execution\")\n\n\nasync def example_usage_with_approval():\n    client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n    # Use approval_func for code approval only (hil_mode=False)\n    async with DockerCommandLineCodeExecutor() as code_executor:\n        m1 = MagenticOne(\n            client=client,\n            hil_mode=False,  # No human-in-the-loop for general conversation\n            code_executor=code_executor,\n            approval_func=approval_func  # But still ask for code execution approval\n        )\n        task = \"Write a Python script to fetch data from an API.\"\n        result = await Console(m1.run_stream(task=task))\n        print(result)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(example_usage_with_approval())",
      "language": "python"
    },
    {
      "code": "# Enable code execution approval without human-in-the-loop mode\nimport asyncio\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_ext.teams.magentic_one import MagenticOne\nfrom autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor\nfrom autogen_agentchat.ui import Console\nfrom autogen_agentchat.agents import ApprovalRequest, ApprovalResponse\n\n\ndef approval_func(request: ApprovalRequest) -> ApprovalResponse:\n    \"\"\"Simple approval function that requests user input.\"\"\"\n    print(f\"Code to execute:\\n{request.code}\")\n    user_input = input(\"Do you approve this code execution? (y/n): \").strip().lower()\n    if user_input == 'y':\n        return ApprovalResponse(approved=True, reason=\"User approved the code execution\")\n    else:\n        return ApprovalResponse(approved=False, reason=\"User denied the code execution\")\n\n\nasync def example_usage_with_approval():\n    client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n    # Use approval_func for code approval only (hil_mode=False)\n    async with DockerCommandLineCodeExecutor() as code_executor:\n        m1 = MagenticOne(\n            client=client,\n            hil_mode=False,  # No human-in-the-loop for general conversation\n            code_executor=code_executor,\n            approval_func=approval_func  # But still ask for code execution approval\n        )\n        task = \"Write a Python script to fetch data from an API.\"\n        result = await Console(m1.run_stream(task=task))\n        print(result)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(example_usage_with_approval())",
      "language": "python"
    },
    {
      "code": "@article{fourney2024magentic,\n    title={Magentic-one: A generalist multi-agent system for solving complex tasks},\n    author={Fourney, Adam and Bansal, Gagan and Mozannar, Hussein and Tan, Cheng and Salinas, Eduardo and Niedtner, Friederike and Proebsting, Grace and Bassman, Griffin and Gerrits, Jack and Alber, Jacob and others},\n    journal={arXiv preprint arXiv:2411.04468},\n    year={2024},\n    url={https://arxiv.org/abs/2411.04468}\n}",
      "language": "css"
    },
    {
      "code": "@article{fourney2024magentic,\n    title={Magentic-one: A generalist multi-agent system for solving complex tasks},\n    author={Fourney, Adam and Bansal, Gagan and Mozannar, Hussein and Tan, Cheng and Salinas, Eduardo and Niedtner, Friederike and Proebsting, Grace and Bassman, Griffin and Gerrits, Jack and Alber, Jacob and others},\n    journal={arXiv preprint arXiv:2411.04468},\n    year={2024},\n    url={https://arxiv.org/abs/2411.04468}\n}",
      "language": "css"
    }
  ],
  "patterns": [],
  "links": [
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.teams.magentic_one.html",
    "https://microsoft.github.io/autogen/stable/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/extensions-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/reference/index.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.agents.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.base.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.conditions.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.messages.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.state.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.teams.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.tools.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.ui.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.utils.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.code_executor.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.exceptions.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.logging.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.memory.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.model_context.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.models.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.tool_agent.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.tools.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.utils.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.cache_store.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.experimental.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.memory.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.teams.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.ui.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.azure.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.file_surfer.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.magentic_one.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.openai.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.video_surfer.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.web_surfer.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.auth.azure.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.cache_store.diskcache.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.cache_store.redis.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.azure.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.docker.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.docker_jupyter.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.jupyter.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.local.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.experimental.task_centric_memory.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.memory.canvas.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.memory.chromadb.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.memory.mem0.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.memory.redis.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.anthropic.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.azure.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.cache.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.llama_cpp.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.ollama.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.openai.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.replay.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.semantic_kernel.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.azure.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.code_execution.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.graphrag.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.http.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.langchain.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.mcp.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.semantic_kernel.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.video_surfer.tools.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.web_surfer.playwright_controller.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.experimental.task_centric_memory.utils.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.anthropic.config.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.azure.config.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.ollama.config.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.openai.config.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.protos.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.protos.agent_worker_pb2.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.protos.agent_worker_pb2_grpc.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.protos.cloudevent_pb2.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.protos.cloudevent_pb2_grpc.html"
  ]
}