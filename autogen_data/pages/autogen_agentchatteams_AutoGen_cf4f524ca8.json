{
  "url": "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.teams.html",
  "title": "autogen_agentchat.teams — AutoGen",
  "content": "This module provides implementation of various pre-defined multi-agent teams. Each team inherits from the BaseGroupChat class.\n\nBases: Team, ABC, ComponentBase[BaseModel]\n\nThe base class for group chat teams.\n\nIn a group chat team, participants share context by publishing their messages to all other participants.\n\nIf an ChatAgent is a participant, the BaseChatMessage from the agent response’s chat_message will be published to other participants in the group chat.\n\nIf a Team is a participant, the BaseChatMessage from the team result’ messages will be published to other participants in the group chat.\n\nTo implement a group chat team, first create a subclass of BaseGroupChatManager and then create a subclass of BaseGroupChat that uses the group chat manager.\n\nThis base class provides the mapping between the agents of the AgentChat API and the agent runtime of the Core API, and handles high-level features like running, pausing, resuming, and resetting the team.\n\nThe logical type of the component.\n\nThe name of the group chat team.\n\nA description of the group chat team.\n\nRun the team and return the result. The base implementation uses run_stream() to run the team and then returns the final result. Once the team is stopped, the termination condition is reset.\n\ntask (str | BaseChatMessage | Sequence[BaseChatMessage] | None) – The task to run the team with. Can be a string, a single BaseChatMessage , or a list of BaseChatMessage.\n\ncancellation_token (CancellationToken | None) – The cancellation token to kill the task immediately. Setting the cancellation token potentially put the team in an inconsistent state, and it may not reset the termination condition. To gracefully stop the team, use ExternalTermination instead.\n\nresult – The result of the task as TaskResult. The result contains the messages produced by the team and the stop reason.\n\nExample using the RoundRobinGroupChat team:\n\nExample using the CancellationToken to cancel the task:\n\nRun the team and produces a stream of messages and the final result of the type TaskResult as the last item in the stream. Once the team is stopped, the termination condition is reset.\n\nIf an agent produces ModelClientStreamingChunkEvent, the message will be yielded in the stream but it will not be included in the messages.\n\ntask (str | BaseChatMessage | Sequence[BaseChatMessage] | None) – The task to run the team with. Can be a string, a single BaseChatMessage , or a list of BaseChatMessage.\n\ncancellation_token (CancellationToken | None) – The cancellation token to kill the task immediately. Setting the cancellation token potentially put the team in an inconsistent state, and it may not reset the termination condition. To gracefully stop the team, use ExternalTermination instead.\n\noutput_task_messages (bool) – Whether to include task messages in the output stream. Defaults to True for backward compatibility.\n\nstream – an AsyncGenerator that yields BaseAgentEvent, BaseChatMessage, and the final result TaskResult as the last item in the stream.\n\nExample using the RoundRobinGroupChat team:\n\nExample using the CancellationToken to cancel the task:\n\nReset the team and its participants to their initial state.\n\nThe team must be stopped before it can be reset.\n\nRuntimeError – If the team has not been initialized or is currently running.\n\nExample using the RoundRobinGroupChat team:\n\nPause its participants when the team is running by calling their on_pause() method via direct RPC calls.\n\nThis is an experimental feature introduced in v0.4.9 and may subject to change or removal in the future.\n\nThe team must be initialized before it can be paused.\n\nDifferent from termination, pausing the team does not cause the run() or run_stream() method to return. It calls the on_pause() method on each participant, and if the participant does not implement the method, it will be a no-op.\n\nIt is the responsibility of the agent class to handle the pause and ensure that the agent can be resumed later. Make sure to implement the on_pause() method in your agent class for custom pause behavior. By default, the agent will not do anything when called.\n\nRuntimeError – If the team has not been initialized. Exceptions from the participants when calling their implementations of on_pause are propagated to this method and raised.\n\nResume its participants when the team is running and paused by calling their on_resume() method via direct RPC calls.\n\nThis is an experimental feature introduced in v0.4.9 and may subject to change or removal in the future.\n\nThe team must be initialized before it can be resumed.\n\nDifferent from termination and restart with a new task, resuming the team does not cause the run() or run_stream() method to return. It calls the on_resume() method on each participant, and if the participant does not implement the method, it will be a no-op.\n\nIt is the responsibility of the agent class to handle the resume and ensure that the agent continues from where it was paused. Make sure to implement the on_resume() method in your agent class for custom resume behavior.\n\nRuntimeError – If the team has not been initialized. Exceptions from the participants when calling their implementations of on_resume method are propagated to this method and raised.\n\nSave the state of the group chat team.\n\nThe state is saved by calling the agent_save_state() method on each participant and the group chat manager with their internal agent ID. The state is returned as a nested dictionary: a dictionary with key agent_states, which is a dictionary the agent names as keys and the state as values.\n\nStarting v0.4.9, the state is using the agent name as the key instead of the agent ID, and the team_id field is removed from the state. This is to allow the state to be portable across different teams and runtimes. States saved with the old format may not be compatible with the new format in the future.\n\nWhen calling save_state() on a team while it is running, the state may not be consistent and may result in an unexpected state. It is recommended to call this method when the team is not running or after it is stopped.\n\nLoad an external state and overwrite the current state of the group chat team.\n\nThe state is loaded by calling the agent_load_state() method on each participant and the group chat manager with their internal agent ID. See save_state() for the expected format of the state.\n\nBases: BaseGroupChat, Component[RoundRobinGroupChatConfig]\n\nA team that runs a group chat with participants taking turns in a round-robin fashion to publish a message to all.\n\nIf an ChatAgent is a participant, the BaseChatMessage from the agent response’s chat_message will be published to other participants in the group chat.\n\nIf a Team is a participant, the BaseChatMessage from the team result’ messages will be published to other participants in the group chat.\n\nIf a single participant is in the team, the participant will be the only speaker.\n\nparticipants (List[ChatAgent | Team]) – The participants in the group chat.\n\nname (str | None, optional) – The name of the group chat, using DEFAULT_NAME if not provided. The name is used by a parent team to identify this group chat so it must be unique within the parent team.\n\ndescription (str | None, optional) – The description of the group chat, using DEFAULT_DESCRIPTION if not provided.\n\ntermination_condition (TerminationCondition, optional) – The termination condition for the group chat. Defaults to None. Without a termination condition, the group chat will run indefinitely.\n\nmax_turns (int, optional) – The maximum number of turns in the group chat before stopping. Defaults to None, meaning no limit.\n\ncustom_message_types (List[type[BaseAgentEvent | BaseChatMessage]], optional) – A list of custom message types that will be used in the group chat. If you are using custom message types or your agents produces custom message types, you need to specify them here. Make sure your custom message types are subclasses of BaseAgentEvent or BaseChatMessage.\n\nemit_team_events (bool, optional) – Whether to emit team events through BaseGroupChat.run_stream(). Defaults to False.\n\nValueError – If no participants are provided or if participant names are not unique.\n\nA team with one participant with tools:\n\nA team with multiple participants:\n\nA team of user proxy and a nested team of writer and reviewer agents:\n\nalias of RoundRobinGroupChatConfig\n\nOverride the provider string for the component. This should be used to prevent internal module names being a part of the module name.\n\nDump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.\n\nT – The configuration of the component.\n\nCreate a new instance of the component from a configuration object.\n\nconfig (T) – The configuration object.\n\nSelf – The new instance of the component.\n\nBases: BaseGroupChat, Component[SelectorGroupChatConfig]\n\nA group chat team that have participants takes turn to publish a message to all, using a ChatCompletion model to select the next speaker after each message.\n\nIf an ChatAgent is a participant, the BaseChatMessage from the agent response’s chat_message will be published to other participants in the group chat.\n\nIf a Team is a participant, the BaseChatMessage from the team result’ messages will be published to other participants in the group chat.\n\nparticipants (List[ChatAgent | Team]) – The participants in the group chat, must have unique names and at least two participants.\n\nmodel_client (ChatCompletionClient) – The ChatCompletion model client used to select the next speaker.\n\nname (str | None, optional) – The name of the group chat, using DEFAULT_NAME if not provided. The name is used by a parent team to identify this group chat so it must be unique within the parent team.\n\ndescription (str | None, optional) – The description of the group chat, using DEFAULT_DESCRIPTION if not provided.\n\ntermination_condition (TerminationCondition, optional) – The termination condition for the group chat. Defaults to None. Without a termination condition, the group chat will run indefinitely.\n\nmax_turns (int, optional) – The maximum number of turns in the group chat before stopping. Defaults to None, meaning no limit.\n\nselector_prompt (str, optional) – The prompt template to use for selecting the next speaker. Available fields: ‘{roles}’, ‘{participants}’, and ‘{history}’. {participants} is the names of candidates for selection. The format is [“<name1>”, “<name2>”, …]. {roles} is a newline-separated list of names and descriptions of the candidate agents. The format for each line is: “<name> : <description>”. {history} is the conversation history formatted as a double newline separated of names and message content. The format for each message is: “<name> : <message content>”.\n\nallow_repeated_speaker (bool, optional) – Whether to include the previous speaker in the list of candidates to be selected for the next turn. Defaults to False. The model may still select the previous speaker – a warning will be logged if this happens.\n\nmax_selector_attempts (int, optional) – The maximum number of attempts to select a speaker using the model. Defaults to 3. If the model fails to select a speaker after the maximum number of attempts, the previous speaker will be used if available, otherwise the first participant will be used.\n\nselector_func (Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], str | None], Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], Awaitable[str | None]], optional) – A custom selector function that takes the conversation history and returns the name of the next speaker. If provided, this function will be used to override the model to select the next speaker. If the function returns None, the model will be used to select the next speaker. NOTE: selector_func is not serializable and will be ignored during serialization and deserialization process.\n\ncandidate_func (Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], List[str]], Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], Awaitable[List[str]]], optional) – A custom function that takes the conversation history and returns a filtered list of candidates for the next speaker selection using model. If the function returns an empty list or None, SelectorGroupChat will raise a ValueError. This function is only used if selector_func is not set. The allow_repeated_speaker will be ignored if set.\n\ncustom_message_types (List[type[BaseAgentEvent | BaseChatMessage]], optional) – A list of custom message types that will be used in the group chat. If you are using custom message types or your agents produces custom message types, you need to specify them here. Make sure your custom message types are subclasses of BaseAgentEvent or BaseChatMessage.\n\nemit_team_events (bool, optional) – Whether to emit team events through BaseGroupChat.run_stream(). Defaults to False.\n\nmodel_client_streaming (bool, optional) – Whether to use streaming for the model client. (This is useful for reasoning models like QwQ). Defaults to False.\n\nmodel_context (ChatCompletionContext | None, optional) – The model context for storing and retrieving LLMMessage. It can be preloaded with initial messages. Messages stored in model context will be used for speaker selection. The initial messages will be cleared when the team is reset.\n\nValueError – If the number of participants is less than two or if the selector prompt is invalid.\n\nA team with multiple participants:\n\nA team with a custom selector function:\n\nA team with custom model context:\n\nalias of SelectorGroupChatConfig\n\nOverride the provider string for the component. This should be used to prevent internal module names being a part of the module name.\n\nDump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.\n\nT – The configuration of the component.\n\nCreate a new instance of the component from a configuration object.\n\nconfig (T) – The configuration object.\n\nSelf – The new instance of the component.\n\nBases: BaseGroupChat, Component[SwarmConfig]\n\nA group chat team that selects the next speaker based on handoff message only.\n\nThe first participant in the list of participants is the initial speaker. The next speaker is selected based on the HandoffMessage message sent by the current speaker. If no handoff message is sent, the current speaker continues to be the speaker.\n\nUnlike RoundRobinGroupChat and SelectorGroupChat, this group chat team does not support inner teams as participants.\n\nparticipants (List[ChatAgent]) – The agents participating in the group chat. The first agent in the list is the initial speaker.\n\nname (str | None, optional) – The name of the group chat, using DEFAULT_NAME if not provided. The name is used by a parent team to identify this group chat so it must be unique within the parent team.\n\ndescription (str | None, optional) – The description of the group chat, using DEFAULT_DESCRIPTION if not provided.\n\ntermination_condition (TerminationCondition, optional) – The termination condition for the group chat. Defaults to None. Without a termination condition, the group chat will run indefinitely.\n\nmax_turns (int, optional) – The maximum number of turns in the group chat before stopping. Defaults to None, meaning no limit.\n\ncustom_message_types (List[type[BaseAgentEvent | BaseChatMessage]], optional) – A list of custom message types that will be used in the group chat. If you are using custom message types or your agents produces custom message types, you need to specify them here. Make sure your custom message types are subclasses of BaseAgentEvent or BaseChatMessage.\n\nemit_team_events (bool, optional) – Whether to emit team events through BaseGroupChat.run_stream(). Defaults to False.\n\nUsing the HandoffTermination for human-in-the-loop handoff:\n\nOverride the provider string for the component. This should be used to prevent internal module names being a part of the module name.\n\nDump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.\n\nT – The configuration of the component.\n\nCreate a new instance of the component from a configuration object.\n\nconfig (T) – The configuration object.\n\nSelf – The new instance of the component.\n\nBases: BaseGroupChat, Component[MagenticOneGroupChatConfig]\n\nA team that runs a group chat with participants managed by the MagenticOneOrchestrator.\n\nThe orchestrator handles the conversation flow, ensuring that the task is completed efficiently by managing the participants’ interactions.\n\nThe orchestrator is based on the Magentic-One architecture, which is a generalist multi-agent system for solving complex tasks (see references below).\n\nUnlike RoundRobinGroupChat and SelectorGroupChat, the MagenticOneGroupChat does not support using team as participant.\n\nparticipants (List[ChatAgent]) – The participants in the group chat.\n\nmodel_client (ChatCompletionClient) – The model client used for generating responses.\n\ntermination_condition (TerminationCondition, optional) – The termination condition for the group chat. Defaults to None. Without a termination condition, the group chat will run based on the orchestrator logic or until the maximum number of turns is reached.\n\nmax_turns (int, optional) – The maximum number of turns in the group chat before stopping. Defaults to 20.\n\nmax_stalls (int, optional) – The maximum number of stalls allowed before re-planning. Defaults to 3.\n\nfinal_answer_prompt (str, optional) – The LLM prompt used to generate the final answer or response from the team’s transcript. A default (sensible for GPT-4o class models) is provided.\n\ncustom_message_types (List[type[BaseAgentEvent | BaseChatMessage]], optional) – A list of custom message types that will be used in the group chat. If you are using custom message types or your agents produces custom message types, you need to specify them here. Make sure your custom message types are subclasses of BaseAgentEvent or BaseChatMessage.\n\nemit_team_events (bool, optional) – Whether to emit team events through BaseGroupChat.run_stream(). Defaults to False.\n\nValueError – In orchestration logic if progress ledger does not have required keys or if next speaker is not valid.\n\nMagenticOneGroupChat with one assistant agent:\n\nIf you use the MagenticOneGroupChat in your work, please cite the following paper:\n\nalias of MagenticOneGroupChatConfig\n\nOverride the provider string for the component. This should be used to prevent internal module names being a part of the module name.\n\nDump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.\n\nT – The configuration of the component.\n\nCreate a new instance of the component from a configuration object.\n\nconfig (T) – The configuration object.\n\nSelf – The new instance of the component.\n\nA fluent builder for constructing DiGraph execution graphs used in GraphFlow.\n\nThis is an experimental feature, and the API will change in the future releases.\n\nThis utility provides a convenient way to programmatically build a graph of agent interactions, including complex execution flows such as:\n\nConditional branching\n\nCyclic loops with safe exits\n\nEach node in the graph represents an agent. Edges define execution paths between agents, and can optionally be conditioned on message content using callable functions.\n\nThe builder is compatible with the Graph runner and supports both standard and filtered agents.\n\nAdd an agent node to the graph.\n\nConnect two nodes optionally with a condition.\n\nAdd multiple conditional edges from a source.\n\nDefine the default start node (optional).\n\nGenerate a validated DiGraph.\n\nReturn the list of added agents.\n\nAdd a node to the graph and register its agent.\n\nAdd a directed edge from source to target, optionally with a condition.\n\nsource – Source node (agent name or agent object)\n\ntarget – Target node (agent name or agent object)\n\ncondition – Optional condition for edge activation. If string, activates when substring is found in message. If callable, activates when function returns True for the message.\n\nSelf for method chaining\n\nValueError – If source or target node doesn’t exist in the builder\n\nAdd multiple conditional edges from a source node based on keyword checks.\n\nThis method interface will be changed in the future to support callable conditions. Please use add_edge if you need to specify custom conditions.\n\nsource – Source node (agent name or agent object)\n\ncondition_to_target – Mapping from condition strings to target nodes Each key is a keyword that will be checked in the message content Each value is the target node to activate when condition is met For each key (keyword), a lambda will be created that checks if the keyword is in the message text.\n\nMapping from condition strings to target nodes Each key is a keyword that will be checked in the message content Each value is the target node to activate when condition is met\n\nFor each key (keyword), a lambda will be created that checks if the keyword is in the message text.\n\nSelf for method chaining\n\nSet the default start node of the graph.\n\nBuild and validate the DiGraph.\n\nReturn the list of agents in the builder, in insertion order.\n\nDefines a directed graph structure with nodes and edges. GraphFlow uses this to determine execution order and conditions.\n\nThis is an experimental feature, and the API will change in the future releases.\n\nShow JSON schema{ \"title\": \"DiGraph\", \"type\": \"object\", \"properties\": { \"nodes\": { \"default\": null, \"title\": \"Nodes\" }, \"default_start_node\": { \"anyOf\": [ { \"type\": \"string\" }, { \"type\": \"null\" } ], \"default\": null, \"title\": \"Default Start Node\" } } }\n\ndefault_start_node (str | None)\n\nnodes (Dict[str, autogen_agentchat.teams._group_chat._graph._digraph_group_chat.DiGraphNode])\n\nCompute a mapping of each node to its parent nodes.\n\nReturn the nodes that have no incoming edges (entry points).\n\nReturn nodes that have no outgoing edges (final output nodes).\n\nCheck if the graph has any cycles and validate that each cycle has at least one conditional edge.\n\nbool – True if there is at least one cycle and all cycles have an exit condition. False if there are no cycles.\n\nValueError – If there is a cycle without any conditional edge.\n\nIndicates if the graph has at least one cycle (with valid exit conditions).\n\nValidate graph structure and execution rules.\n\nGet the remaining map that tracks how many edges point to each target node with each activation group.\n\nDictionary mapping target nodes to their activation groups and remaining counts\n\nThis function is meant to behave like a BaseModel method to initialise private attributes.\n\nIt takes context as an argument since that’s what pydantic-core passes when calling it.\n\nself – The BaseModel instance.\n\ncontext – The context.\n\nRepresents a node (agent) in a DiGraph, with its outgoing edges and activation type.\n\nThis is an experimental feature, and the API will change in the future releases.\n\nShow JSON schema{ \"title\": \"DiGraphNode\", \"type\": \"object\", \"properties\": { \"name\": { \"title\": \"Name\", \"type\": \"string\" }, \"edges\": { \"default\": null, \"title\": \"Edges\" }, \"activation\": { \"default\": \"all\", \"enum\": [ \"all\", \"any\" ], \"title\": \"Activation\", \"type\": \"string\" } }, \"required\": [ \"name\" ] }\n\nactivation (Literal['all', 'any'])\n\nedges (List[autogen_agentchat.teams._group_chat._graph._digraph_group_chat.DiGraphEdge])\n\nRepresents a directed edge in a DiGraph, with an optional execution condition.\n\nThis is an experimental feature, and the API will change in the future releases.\n\nIf the condition is a callable, it will not be serialized in the model.\n\nShow JSON schema{ \"title\": \"DiGraphEdge\", \"type\": \"object\", \"properties\": { \"target\": { \"title\": \"Target\", \"type\": \"string\" }, \"condition\": { \"anyOf\": [ { \"type\": \"string\" }, { \"type\": \"null\" } ], \"default\": null, \"title\": \"Condition\" }, \"condition_function\": { \"default\": null, \"title\": \"Condition Function\" }, \"activation_group\": { \"default\": \"\", \"title\": \"Activation Group\", \"type\": \"string\" }, \"activation_condition\": { \"default\": \"all\", \"enum\": [ \"all\", \"any\" ], \"title\": \"Activation Condition\", \"type\": \"string\" } }, \"required\": [ \"target\" ] }\n\nactivation_condition (Literal['all', 'any'])\n\nactivation_group (str)\n\ncondition (str | Callable[[autogen_agentchat.messages.BaseChatMessage], bool] | None)\n\ncondition_function (Callable[[autogen_agentchat.messages.BaseChatMessage], bool] | None)\n\n_validate_condition » all fields\n\n(Experimental) Condition to execute this edge. If None, the edge is unconditional. If a string, the edge is conditional on the presence of that string in the last agent chat message. If a callable, the edge is conditional on the callable returning True when given the last message.\n\nGroup identifier for forward dependencies.\n\nWhen multiple edges point to the same target node, they are grouped by this field. This allows distinguishing between different cycles or dependency patterns.\n\nExample: In a graph containing a cycle like A->B->C->B, the two edges pointing to B (A->B and C->B) can be in different activation groups to control how B is activated. Defaults to the target node name if not specified.\n\nDetermines how forward dependencies within the same activation_group are evaluated.\n\n“all”: All edges in this activation group must be satisfied before the target node can execute\n\n“any”: Any single edge in this activation group being satisfied allows the target node to execute\n\nThis is used to handle complex dependency patterns in cyclic graphs where multiple paths can lead to the same target node.\n\nCheck if the edge condition is satisfied for the given message.\n\nmessage – The message to check the condition against.\n\nTrue if condition is satisfied (None condition always returns True)\n\nBases: BaseGroupChat, Component[GraphFlowConfig]\n\nA team that runs a group chat following a Directed Graph execution pattern.\n\nThis is an experimental feature, and the API will change in the future releases.\n\nThis group chat executes agents based on a directed graph (DiGraph) structure, allowing complex workflows such as sequential execution, parallel fan-out, conditional branching, join patterns, and loops with explicit exit conditions.\n\nThe execution order is determined by the edges defined in the DiGraph. Each node in the graph corresponds to an agent, and edges define the flow of messages between agents. Nodes can be configured to activate when:\n\nAll parent nodes have completed (activation=”all”) → default\n\nAny parent node completes (activation=”any”)\n\nConditional branching is supported using edge conditions, where the next agent(s) are selected based on content in the chat history. Loops are permitted as long as there is a condition that eventually exits the loop.\n\nUse the DiGraphBuilder class to create a DiGraph easily. It provides a fluent API for adding nodes and edges, setting entry points, and validating the graph structure. See the DiGraphBuilder documentation for more details. The GraphFlow class is designed to be used with the DiGraphBuilder for creating complex workflows.\n\nWhen using callable conditions in edges, they will not be serialized when calling dump_component(). This will be addressed in future releases.\n\nparticipants (List[ChatAgent]) – The participants in the group chat.\n\ntermination_condition (TerminationCondition, optional) – Termination condition for the chat.\n\nmax_turns (int, optional) – Maximum number of turns before forcing termination.\n\ngraph (DiGraph) – Directed execution graph defining node flow and conditions.\n\nValueError – If participant names are not unique, or if graph validation fails (e.g., cycles without exit).\n\nSequential Flow: A → B → C\n\nParallel Fan-out: A → (B, C)\n\nConditional Branching: A → B (if ‘yes’) or C (otherwise)\n\nLoop with exit condition: A → B → C (if ‘APPROVE’) or A (otherwise)\n\nalias of GraphFlowConfig\n\nOverride the provider string for the component. This should be used to prevent internal module names being a part of the module name.\n\nautogen_agentchat.state\n\nautogen_agentchat.tools",
  "headings": [
    {
      "level": "h1",
      "text": "autogen_agentchat.teams#",
      "id": ""
    }
  ],
  "code_samples": [
    {
      "code": "import asyncio\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.conditions import MaxMessageTermination\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n    agent1 = AssistantAgent(\"Assistant1\", model_client=model_client)\n    agent2 = AssistantAgent(\"Assistant2\", model_client=model_client)\n    termination = MaxMessageTermination(3)\n    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)\n\n    result = await team.run(task=\"Count from 1 to 10, respond one at a time.\")\n    print(result)\n\n    # Run the team again without a task to continue the previous task.\n    result = await team.run()\n    print(result)\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.conditions import MaxMessageTermination\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n    agent1 = AssistantAgent(\"Assistant1\", model_client=model_client)\n    agent2 = AssistantAgent(\"Assistant2\", model_client=model_client)\n    termination = MaxMessageTermination(3)\n    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)\n\n    result = await team.run(task=\"Count from 1 to 10, respond one at a time.\")\n    print(result)\n\n    # Run the team again without a task to continue the previous task.\n    result = await team.run()\n    print(result)\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.conditions import MaxMessageTermination\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_core import CancellationToken\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n    agent1 = AssistantAgent(\"Assistant1\", model_client=model_client)\n    agent2 = AssistantAgent(\"Assistant2\", model_client=model_client)\n    termination = MaxMessageTermination(3)\n    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)\n\n    cancellation_token = CancellationToken()\n\n    # Create a task to run the team in the background.\n    run_task = asyncio.create_task(\n        team.run(\n            task=\"Count from 1 to 10, respond one at a time.\",\n            cancellation_token=cancellation_token,\n        )\n    )\n\n    # Wait for 1 second and then cancel the task.\n    await asyncio.sleep(1)\n    cancellation_token.cancel()\n\n    # This will raise a cancellation error.\n    await run_task\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.conditions import MaxMessageTermination\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_core import CancellationToken\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n    agent1 = AssistantAgent(\"Assistant1\", model_client=model_client)\n    agent2 = AssistantAgent(\"Assistant2\", model_client=model_client)\n    termination = MaxMessageTermination(3)\n    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)\n\n    cancellation_token = CancellationToken()\n\n    # Create a task to run the team in the background.\n    run_task = asyncio.create_task(\n        team.run(\n            task=\"Count from 1 to 10, respond one at a time.\",\n            cancellation_token=cancellation_token,\n        )\n    )\n\n    # Wait for 1 second and then cancel the task.\n    await asyncio.sleep(1)\n    cancellation_token.cancel()\n\n    # This will raise a cancellation error.\n    await run_task\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.conditions import MaxMessageTermination\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n    agent1 = AssistantAgent(\"Assistant1\", model_client=model_client)\n    agent2 = AssistantAgent(\"Assistant2\", model_client=model_client)\n    termination = MaxMessageTermination(3)\n    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)\n\n    stream = team.run_stream(task=\"Count from 1 to 10, respond one at a time.\")\n    async for message in stream:\n        print(message)\n\n    # Run the team again without a task to continue the previous task.\n    stream = team.run_stream()\n    async for message in stream:\n        print(message)\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.conditions import MaxMessageTermination\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n    agent1 = AssistantAgent(\"Assistant1\", model_client=model_client)\n    agent2 = AssistantAgent(\"Assistant2\", model_client=model_client)\n    termination = MaxMessageTermination(3)\n    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)\n\n    stream = team.run_stream(task=\"Count from 1 to 10, respond one at a time.\")\n    async for message in stream:\n        print(message)\n\n    # Run the team again without a task to continue the previous task.\n    stream = team.run_stream()\n    async for message in stream:\n        print(message)\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.conditions import MaxMessageTermination\nfrom autogen_agentchat.ui import Console\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_core import CancellationToken\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n    agent1 = AssistantAgent(\"Assistant1\", model_client=model_client)\n    agent2 = AssistantAgent(\"Assistant2\", model_client=model_client)\n    termination = MaxMessageTermination(3)\n    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)\n\n    cancellation_token = CancellationToken()\n\n    # Create a task to run the team in the background.\n    run_task = asyncio.create_task(\n        Console(\n            team.run_stream(\n                task=\"Count from 1 to 10, respond one at a time.\",\n                cancellation_token=cancellation_token,\n            )\n        )\n    )\n\n    # Wait for 1 second and then cancel the task.\n    await asyncio.sleep(1)\n    cancellation_token.cancel()\n\n    # This will raise a cancellation error.\n    await run_task\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.conditions import MaxMessageTermination\nfrom autogen_agentchat.ui import Console\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_core import CancellationToken\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n    agent1 = AssistantAgent(\"Assistant1\", model_client=model_client)\n    agent2 = AssistantAgent(\"Assistant2\", model_client=model_client)\n    termination = MaxMessageTermination(3)\n    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)\n\n    cancellation_token = CancellationToken()\n\n    # Create a task to run the team in the background.\n    run_task = asyncio.create_task(\n        Console(\n            team.run_stream(\n                task=\"Count from 1 to 10, respond one at a time.\",\n                cancellation_token=cancellation_token,\n            )\n        )\n    )\n\n    # Wait for 1 second and then cancel the task.\n    await asyncio.sleep(1)\n    cancellation_token.cancel()\n\n    # This will raise a cancellation error.\n    await run_task\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.conditions import MaxMessageTermination\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n    agent1 = AssistantAgent(\"Assistant1\", model_client=model_client)\n    agent2 = AssistantAgent(\"Assistant2\", model_client=model_client)\n    termination = MaxMessageTermination(3)\n    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)\n    stream = team.run_stream(task=\"Count from 1 to 10, respond one at a time.\")\n    async for message in stream:\n        print(message)\n\n    # Reset the team.\n    await team.reset()\n    stream = team.run_stream(task=\"Count from 1 to 10, respond one at a time.\")\n    async for message in stream:\n        print(message)\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.conditions import MaxMessageTermination\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n    agent1 = AssistantAgent(\"Assistant1\", model_client=model_client)\n    agent2 = AssistantAgent(\"Assistant2\", model_client=model_client)\n    termination = MaxMessageTermination(3)\n    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)\n    stream = team.run_stream(task=\"Count from 1 to 10, respond one at a time.\")\n    async for message in stream:\n        print(message)\n\n    # Reset the team.\n    await team.reset()\n    stream = team.run_stream(task=\"Count from 1 to 10, respond one at a time.\")\n    async for message in stream:\n        print(message)\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "{\n    \"agent_states\": {\n        \"agent1\": ...,\n        \"agent2\": ...,\n        \"RoundRobinGroupChatManager\": ...\n    }\n}",
      "language": "json"
    },
    {
      "code": "{\n    \"agent_states\": {\n        \"agent1\": ...,\n        \"agent2\": ...,\n        \"RoundRobinGroupChatManager\": ...\n    }\n}",
      "language": "json"
    },
    {
      "code": "import asyncio\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.conditions import TextMentionTermination\nfrom autogen_agentchat.ui import Console\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n    async def get_weather(location: str) -> str:\n        return f\"The weather in {location} is sunny.\"\n\n    assistant = AssistantAgent(\n        \"Assistant\",\n        model_client=model_client,\n        tools=[get_weather],\n    )\n    termination = TextMentionTermination(\"TERMINATE\")\n    team = RoundRobinGroupChat([assistant], termination_condition=termination)\n    await Console(team.run_stream(task=\"What's the weather in New York?\"))\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.conditions import TextMentionTermination\nfrom autogen_agentchat.ui import Console\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n    async def get_weather(location: str) -> str:\n        return f\"The weather in {location} is sunny.\"\n\n    assistant = AssistantAgent(\n        \"Assistant\",\n        model_client=model_client,\n        tools=[get_weather],\n    )\n    termination = TextMentionTermination(\"TERMINATE\")\n    team = RoundRobinGroupChat([assistant], termination_condition=termination)\n    await Console(team.run_stream(task=\"What's the weather in New York?\"))\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.conditions import TextMentionTermination\nfrom autogen_agentchat.ui import Console\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n    agent1 = AssistantAgent(\"Assistant1\", model_client=model_client)\n    agent2 = AssistantAgent(\"Assistant2\", model_client=model_client)\n    termination = TextMentionTermination(\"TERMINATE\")\n    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)\n    await Console(team.run_stream(task=\"Tell me some jokes.\"))\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.conditions import TextMentionTermination\nfrom autogen_agentchat.ui import Console\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n    agent1 = AssistantAgent(\"Assistant1\", model_client=model_client)\n    agent2 = AssistantAgent(\"Assistant2\", model_client=model_client)\n    termination = TextMentionTermination(\"TERMINATE\")\n    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)\n    await Console(team.run_stream(task=\"Tell me some jokes.\"))\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\n\nfrom autogen_agentchat.agents import UserProxyAgent, AssistantAgent\nfrom autogen_agentchat.conditions import TextMentionTermination, MaxMessageTermination\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4.1-nano\")\n\n    writer = AssistantAgent(\n        \"writer\", model_client=model_client, system_message=\"You are a writer.\", model_client_stream=True\n    )\n\n    reviewer = AssistantAgent(\n        \"reviewer\",\n        model_client=model_client,\n        system_message=\"Provide feedback to the input and suggest improvements.\",\n        model_client_stream=True,\n    )\n\n    # NOTE: you can skip input by pressing Enter.\n    user_proxy = UserProxyAgent(\"user_proxy\")\n\n    # Maximum 1 round of review and revision.\n    inner_termination = MaxMessageTermination(max_messages=4)\n\n    # The outter-loop termination condition that will terminate the team when the user types \"exit\".\n    outter_termination = TextMentionTermination(\"exit\", sources=[\"user_proxy\"])\n\n    team = RoundRobinGroupChat(\n        [\n            # For each turn, the writer writes a summary and the reviewer reviews it.\n            RoundRobinGroupChat([writer, reviewer], termination_condition=inner_termination),\n            # The user proxy gets user input once the writer and reviewer have finished their actions.\n            user_proxy,\n        ],\n        termination_condition=outter_termination,\n    )\n    # Start the team and wait for it to terminate.\n    await Console(team.run_stream(task=\"Write a short essay about the impact of AI on society.\"))\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\n\nfrom autogen_agentchat.agents import UserProxyAgent, AssistantAgent\nfrom autogen_agentchat.conditions import TextMentionTermination, MaxMessageTermination\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4.1-nano\")\n\n    writer = AssistantAgent(\n        \"writer\", model_client=model_client, system_message=\"You are a writer.\", model_client_stream=True\n    )\n\n    reviewer = AssistantAgent(\n        \"reviewer\",\n        model_client=model_client,\n        system_message=\"Provide feedback to the input and suggest improvements.\",\n        model_client_stream=True,\n    )\n\n    # NOTE: you can skip input by pressing Enter.\n    user_proxy = UserProxyAgent(\"user_proxy\")\n\n    # Maximum 1 round of review and revision.\n    inner_termination = MaxMessageTermination(max_messages=4)\n\n    # The outter-loop termination condition that will terminate the team when the user types \"exit\".\n    outter_termination = TextMentionTermination(\"exit\", sources=[\"user_proxy\"])\n\n    team = RoundRobinGroupChat(\n        [\n            # For each turn, the writer writes a summary and the reviewer reviews it.\n            RoundRobinGroupChat([writer, reviewer], termination_condition=inner_termination),\n            # The user proxy gets user input once the writer and reviewer have finished their actions.\n            user_proxy,\n        ],\n        termination_condition=outter_termination,\n    )\n    # Start the team and wait for it to terminate.\n    await Console(team.run_stream(task=\"Write a short essay about the impact of AI on society.\"))\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.teams import SelectorGroupChat\nfrom autogen_agentchat.conditions import TextMentionTermination\nfrom autogen_agentchat.ui import Console\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n    async def lookup_hotel(location: str) -> str:\n        return f\"Here are some hotels in {location}: hotel1, hotel2, hotel3.\"\n\n    async def lookup_flight(origin: str, destination: str) -> str:\n        return f\"Here are some flights from {origin} to {destination}: flight1, flight2, flight3.\"\n\n    async def book_trip() -> str:\n        return \"Your trip is booked!\"\n\n    travel_advisor = AssistantAgent(\n        \"Travel_Advisor\",\n        model_client,\n        tools=[book_trip],\n        description=\"Helps with travel planning.\",\n    )\n    hotel_agent = AssistantAgent(\n        \"Hotel_Agent\",\n        model_client,\n        tools=[lookup_hotel],\n        description=\"Helps with hotel booking.\",\n    )\n    flight_agent = AssistantAgent(\n        \"Flight_Agent\",\n        model_client,\n        tools=[lookup_flight],\n        description=\"Helps with flight booking.\",\n    )\n    termination = TextMentionTermination(\"TERMINATE\")\n    team = SelectorGroupChat(\n        [travel_advisor, hotel_agent, flight_agent],\n        model_client=model_client,\n        termination_condition=termination,\n    )\n    await Console(team.run_stream(task=\"Book a 3-day trip to new york.\"))\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.teams import SelectorGroupChat\nfrom autogen_agentchat.conditions import TextMentionTermination\nfrom autogen_agentchat.ui import Console\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n    async def lookup_hotel(location: str) -> str:\n        return f\"Here are some hotels in {location}: hotel1, hotel2, hotel3.\"\n\n    async def lookup_flight(origin: str, destination: str) -> str:\n        return f\"Here are some flights from {origin} to {destination}: flight1, flight2, flight3.\"\n\n    async def book_trip() -> str:\n        return \"Your trip is booked!\"\n\n    travel_advisor = AssistantAgent(\n        \"Travel_Advisor\",\n        model_client,\n        tools=[book_trip],\n        description=\"Helps with travel planning.\",\n    )\n    hotel_agent = AssistantAgent(\n        \"Hotel_Agent\",\n        model_client,\n        tools=[lookup_hotel],\n        description=\"Helps with hotel booking.\",\n    )\n    flight_agent = AssistantAgent(\n        \"Flight_Agent\",\n        model_client,\n        tools=[lookup_flight],\n        description=\"Helps with flight booking.\",\n    )\n    termination = TextMentionTermination(\"TERMINATE\")\n    team = SelectorGroupChat(\n        [travel_advisor, hotel_agent, flight_agent],\n        model_client=model_client,\n        termination_condition=termination,\n    )\n    await Console(team.run_stream(task=\"Book a 3-day trip to new york.\"))\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom typing import Sequence\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.teams import SelectorGroupChat\nfrom autogen_agentchat.conditions import TextMentionTermination\nfrom autogen_agentchat.ui import Console\nfrom autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n    def check_calculation(x: int, y: int, answer: int) -> str:\n        if x + y == answer:\n            return \"Correct!\"\n        else:\n            return \"Incorrect!\"\n\n    agent1 = AssistantAgent(\n        \"Agent1\",\n        model_client,\n        description=\"For calculation\",\n        system_message=\"Calculate the sum of two numbers\",\n    )\n    agent2 = AssistantAgent(\n        \"Agent2\",\n        model_client,\n        tools=[check_calculation],\n        description=\"For checking calculation\",\n        system_message=\"Check the answer and respond with 'Correct!' or 'Incorrect!'\",\n    )\n\n    def selector_func(messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> str | None:\n        if len(messages) == 1 or messages[-1].to_text() == \"Incorrect!\":\n            return \"Agent1\"\n        if messages[-1].source == \"Agent1\":\n            return \"Agent2\"\n        return None\n\n    termination = TextMentionTermination(\"Correct!\")\n    team = SelectorGroupChat(\n        [agent1, agent2],\n        model_client=model_client,\n        selector_func=selector_func,\n        termination_condition=termination,\n    )\n\n    await Console(team.run_stream(task=\"What is 1 + 1?\"))\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom typing import Sequence\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.teams import SelectorGroupChat\nfrom autogen_agentchat.conditions import TextMentionTermination\nfrom autogen_agentchat.ui import Console\nfrom autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n    def check_calculation(x: int, y: int, answer: int) -> str:\n        if x + y == answer:\n            return \"Correct!\"\n        else:\n            return \"Incorrect!\"\n\n    agent1 = AssistantAgent(\n        \"Agent1\",\n        model_client,\n        description=\"For calculation\",\n        system_message=\"Calculate the sum of two numbers\",\n    )\n    agent2 = AssistantAgent(\n        \"Agent2\",\n        model_client,\n        tools=[check_calculation],\n        description=\"For checking calculation\",\n        system_message=\"Check the answer and respond with 'Correct!' or 'Incorrect!'\",\n    )\n\n    def selector_func(messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> str | None:\n        if len(messages) == 1 or messages[-1].to_text() == \"Incorrect!\":\n            return \"Agent1\"\n        if messages[-1].source == \"Agent1\":\n            return \"Agent2\"\n        return None\n\n    termination = TextMentionTermination(\"Correct!\")\n    team = SelectorGroupChat(\n        [agent1, agent2],\n        model_client=model_client,\n        selector_func=selector_func,\n        termination_condition=termination,\n    )\n\n    await Console(team.run_stream(task=\"What is 1 + 1?\"))\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\n\nfrom autogen_core.model_context import BufferedChatCompletionContext\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.conditions import TextMentionTermination\nfrom autogen_agentchat.teams import SelectorGroupChat\nfrom autogen_agentchat.ui import Console\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n    model_context = BufferedChatCompletionContext(buffer_size=5)\n\n    async def lookup_hotel(location: str) -> str:\n        return f\"Here are some hotels in {location}: hotel1, hotel2, hotel3.\"\n\n    async def lookup_flight(origin: str, destination: str) -> str:\n        return f\"Here are some flights from {origin} to {destination}: flight1, flight2, flight3.\"\n\n    async def book_trip() -> str:\n        return \"Your trip is booked!\"\n\n    travel_advisor = AssistantAgent(\n        \"Travel_Advisor\",\n        model_client,\n        tools=[book_trip],\n        description=\"Helps with travel planning.\",\n    )\n    hotel_agent = AssistantAgent(\n        \"Hotel_Agent\",\n        model_client,\n        tools=[lookup_hotel],\n        description=\"Helps with hotel booking.\",\n    )\n    flight_agent = AssistantAgent(\n        \"Flight_Agent\",\n        model_client,\n        tools=[lookup_flight],\n        description=\"Helps with flight booking.\",\n    )\n    termination = TextMentionTermination(\"TERMINATE\")\n    team = SelectorGroupChat(\n        [travel_advisor, hotel_agent, flight_agent],\n        model_client=model_client,\n        termination_condition=termination,\n        model_context=model_context,\n    )\n    await Console(team.run_stream(task=\"Book a 3-day trip to new york.\"))\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\n\nfrom autogen_core.model_context import BufferedChatCompletionContext\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.conditions import TextMentionTermination\nfrom autogen_agentchat.teams import SelectorGroupChat\nfrom autogen_agentchat.ui import Console\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n    model_context = BufferedChatCompletionContext(buffer_size=5)\n\n    async def lookup_hotel(location: str) -> str:\n        return f\"Here are some hotels in {location}: hotel1, hotel2, hotel3.\"\n\n    async def lookup_flight(origin: str, destination: str) -> str:\n        return f\"Here are some flights from {origin} to {destination}: flight1, flight2, flight3.\"\n\n    async def book_trip() -> str:\n        return \"Your trip is booked!\"\n\n    travel_advisor = AssistantAgent(\n        \"Travel_Advisor\",\n        model_client,\n        tools=[book_trip],\n        description=\"Helps with travel planning.\",\n    )\n    hotel_agent = AssistantAgent(\n        \"Hotel_Agent\",\n        model_client,\n        tools=[lookup_hotel],\n        description=\"Helps with hotel booking.\",\n    )\n    flight_agent = AssistantAgent(\n        \"Flight_Agent\",\n        model_client,\n        tools=[lookup_flight],\n        description=\"Helps with flight booking.\",\n    )\n    termination = TextMentionTermination(\"TERMINATE\")\n    team = SelectorGroupChat(\n        [travel_advisor, hotel_agent, flight_agent],\n        model_client=model_client,\n        termination_condition=termination,\n        model_context=model_context,\n    )\n    await Console(team.run_stream(task=\"Book a 3-day trip to new york.\"))\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.teams import Swarm\nfrom autogen_agentchat.conditions import MaxMessageTermination\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n    agent1 = AssistantAgent(\n        \"Alice\",\n        model_client=model_client,\n        handoffs=[\"Bob\"],\n        system_message=\"You are Alice and you only answer questions about yourself.\",\n    )\n    agent2 = AssistantAgent(\n        \"Bob\", model_client=model_client, system_message=\"You are Bob and your birthday is on 1st January.\"\n    )\n\n    termination = MaxMessageTermination(3)\n    team = Swarm([agent1, agent2], termination_condition=termination)\n\n    stream = team.run_stream(task=\"What is bob's birthday?\")\n    async for message in stream:\n        print(message)\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.teams import Swarm\nfrom autogen_agentchat.conditions import MaxMessageTermination\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n    agent1 = AssistantAgent(\n        \"Alice\",\n        model_client=model_client,\n        handoffs=[\"Bob\"],\n        system_message=\"You are Alice and you only answer questions about yourself.\",\n    )\n    agent2 = AssistantAgent(\n        \"Bob\", model_client=model_client, system_message=\"You are Bob and your birthday is on 1st January.\"\n    )\n\n    termination = MaxMessageTermination(3)\n    team = Swarm([agent1, agent2], termination_condition=termination)\n\n    stream = team.run_stream(task=\"What is bob's birthday?\")\n    async for message in stream:\n        print(message)\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.teams import Swarm\nfrom autogen_agentchat.conditions import HandoffTermination, MaxMessageTermination\nfrom autogen_agentchat.ui import Console\nfrom autogen_agentchat.messages import HandoffMessage\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n    agent = AssistantAgent(\n        \"Alice\",\n        model_client=model_client,\n        handoffs=[\"user\"],\n        system_message=\"You are Alice and you only answer questions about yourself, ask the user for help if needed.\",\n    )\n    termination = HandoffTermination(target=\"user\") | MaxMessageTermination(3)\n    team = Swarm([agent], termination_condition=termination)\n\n    # Start the conversation.\n    await Console(team.run_stream(task=\"What is bob's birthday?\"))\n\n    # Resume with user feedback.\n    await Console(\n        team.run_stream(\n            task=HandoffMessage(source=\"user\", target=\"Alice\", content=\"Bob's birthday is on 1st January.\")\n        )\n    )\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.teams import Swarm\nfrom autogen_agentchat.conditions import HandoffTermination, MaxMessageTermination\nfrom autogen_agentchat.ui import Console\nfrom autogen_agentchat.messages import HandoffMessage\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n    agent = AssistantAgent(\n        \"Alice\",\n        model_client=model_client,\n        handoffs=[\"user\"],\n        system_message=\"You are Alice and you only answer questions about yourself, ask the user for help if needed.\",\n    )\n    termination = HandoffTermination(target=\"user\") | MaxMessageTermination(3)\n    team = Swarm([agent], termination_condition=termination)\n\n    # Start the conversation.\n    await Console(team.run_stream(task=\"What is bob's birthday?\"))\n\n    # Resume with user feedback.\n    await Console(\n        team.run_stream(\n            task=HandoffMessage(source=\"user\", target=\"Alice\", content=\"Bob's birthday is on 1st January.\")\n        )\n    )\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.teams import MagenticOneGroupChat\nfrom autogen_agentchat.ui import Console\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n    assistant = AssistantAgent(\n        \"Assistant\",\n        model_client=model_client,\n    )\n    team = MagenticOneGroupChat([assistant], model_client=model_client)\n    await Console(team.run_stream(task=\"Provide a different proof to Fermat last theorem\"))\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.teams import MagenticOneGroupChat\nfrom autogen_agentchat.ui import Console\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n    assistant = AssistantAgent(\n        \"Assistant\",\n        model_client=model_client,\n    )\n    team = MagenticOneGroupChat([assistant], model_client=model_client)\n    await Console(team.run_stream(task=\"Provide a different proof to Fermat last theorem\"))\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "@article{fourney2024magentic,\n    title={Magentic-one: A generalist multi-agent system for solving complex tasks},\n    author={Fourney, Adam and Bansal, Gagan and Mozannar, Hussein and Tan, Cheng and Salinas, Eduardo and Niedtner, Friederike and Proebsting, Grace and Bassman, Griffin and Gerrits, Jack and Alber, Jacob and others},\n    journal={arXiv preprint arXiv:2411.04468},\n    year={2024}\n}",
      "language": "css"
    },
    {
      "code": "@article{fourney2024magentic,\n    title={Magentic-one: A generalist multi-agent system for solving complex tasks},\n    author={Fourney, Adam and Bansal, Gagan and Mozannar, Hussein and Tan, Cheng and Salinas, Eduardo and Niedtner, Friederike and Proebsting, Grace and Bassman, Griffin and Gerrits, Jack and Alber, Jacob and others},\n    journal={arXiv preprint arXiv:2411.04468},\n    year={2024}\n}",
      "language": "css"
    },
    {
      "code": ">>> builder = GraphBuilder()\n>>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)\n>>> builder.add_edge(agent_a, agent_b).add_edge(agent_b, agent_c)\n>>> team = Graph(\n...     participants=builder.get_participants(),\n...     graph=builder.build(),\n...     termination_condition=MaxMessageTermination(5),\n... )",
      "language": "unknown"
    },
    {
      "code": ">>> builder = GraphBuilder()\n>>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)\n>>> builder.add_edge(agent_a, agent_b).add_edge(agent_b, agent_c)\n>>> team = Graph(\n...     participants=builder.get_participants(),\n...     graph=builder.build(),\n...     termination_condition=MaxMessageTermination(5),\n... )",
      "language": "unknown"
    },
    {
      "code": ">>> builder = GraphBuilder()\n>>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)\n>>> builder.add_edge(agent_a, agent_b).add_edge(agent_a, agent_c)",
      "language": "unknown"
    },
    {
      "code": ">>> builder = GraphBuilder()\n>>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)\n>>> builder.add_edge(agent_a, agent_b).add_edge(agent_a, agent_c)",
      "language": "unknown"
    },
    {
      "code": ">>> builder = GraphBuilder()\n>>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)\n>>> # Add conditional edges using keyword check\n>>> builder.add_edge(agent_a, agent_b, condition=\"keyword1\")\n>>> builder.add_edge(agent_a, agent_c, condition=\"keyword2\")",
      "language": "julia"
    },
    {
      "code": ">>> builder = GraphBuilder()\n>>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)\n>>> # Add conditional edges using keyword check\n>>> builder.add_edge(agent_a, agent_b, condition=\"keyword1\")\n>>> builder.add_edge(agent_a, agent_c, condition=\"keyword2\")",
      "language": "julia"
    },
    {
      "code": ">>> builder = GraphBuilder()\n>>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)\n>>> # Add condition strings to check in messages\n>>> builder.add_edge(agent_a, agent_b, condition=\"big\")\n>>> builder.add_edge(agent_a, agent_c, condition=\"small\")",
      "language": "unknown"
    },
    {
      "code": ">>> builder = GraphBuilder()\n>>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)\n>>> # Add condition strings to check in messages\n>>> builder.add_edge(agent_a, agent_b, condition=\"big\")\n>>> builder.add_edge(agent_a, agent_c, condition=\"small\")",
      "language": "unknown"
    },
    {
      "code": ">>> builder = GraphBuilder()\n>>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)\n>>> builder.add_edge(agent_a, agent_b)\n>> # Add a loop back to agent A\n>>> builder.add_edge(agent_b, agent_a, condition=lambda msg: \"loop\" in msg.to_model_text())\n>>> # Add exit condition to break the loop\n>>> builder.add_edge(agent_b, agent_c, condition=lambda msg: \"loop\" not in msg.to_model_text())",
      "language": "json"
    },
    {
      "code": ">>> builder = GraphBuilder()\n>>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)\n>>> builder.add_edge(agent_a, agent_b)\n>> # Add a loop back to agent A\n>>> builder.add_edge(agent_b, agent_a, condition=lambda msg: \"loop\" in msg.to_model_text())\n>>> # Add exit condition to break the loop\n>>> builder.add_edge(agent_b, agent_c, condition=lambda msg: \"loop\" not in msg.to_model_text())",
      "language": "json"
    },
    {
      "code": ">>> builder = GraphBuilder()\n>>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)\n>>> builder.add_edge(agent_a, agent_b)\n>>> builder.add_edge(agent_b, agent_c)\n>>> builder.add_edge(agent_c, agent_b, activation_group=\"loop_back\")",
      "language": "unknown"
    },
    {
      "code": ">>> builder = GraphBuilder()\n>>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)\n>>> builder.add_edge(agent_a, agent_b)\n>>> builder.add_edge(agent_b, agent_c)\n>>> builder.add_edge(agent_c, agent_b, activation_group=\"loop_back\")",
      "language": "unknown"
    },
    {
      "code": ">>> builder = GraphBuilder()\n>>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c1).add_node(agent_c2).add_node(agent_e)\n>>> builder.add_edge(agent_a, agent_b)\n>>> builder.add_edge(agent_b, agent_c1)\n>>> builder.add_edge(agent_b, agent_c2)\n>>> builder.add_edge(agent_b, agent_e, condition=\"exit\")\n>>> builder.add_edge(agent_c1, agent_b, activation_group=\"loop_back_group\", activation_condition=\"any\")\n>>> builder.add_edge(agent_c2, agent_b, activation_group=\"loop_back_group\", activation_condition=\"any\")",
      "language": "unknown"
    },
    {
      "code": ">>> builder = GraphBuilder()\n>>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c1).add_node(agent_c2).add_node(agent_e)\n>>> builder.add_edge(agent_a, agent_b)\n>>> builder.add_edge(agent_b, agent_c1)\n>>> builder.add_edge(agent_b, agent_c2)\n>>> builder.add_edge(agent_b, agent_e, condition=\"exit\")\n>>> builder.add_edge(agent_c1, agent_b, activation_group=\"loop_back_group\", activation_condition=\"any\")\n>>> builder.add_edge(agent_c2, agent_b, activation_group=\"loop_back_group\", activation_condition=\"any\")",
      "language": "unknown"
    },
    {
      "code": "{\n   \"title\": \"DiGraph\",\n   \"type\": \"object\",\n   \"properties\": {\n      \"nodes\": {\n         \"default\": null,\n         \"title\": \"Nodes\"\n      },\n      \"default_start_node\": {\n         \"anyOf\": [\n            {\n               \"type\": \"string\"\n            },\n            {\n               \"type\": \"null\"\n            }\n         ],\n         \"default\": null,\n         \"title\": \"Default Start Node\"\n      }\n   }\n}",
      "language": "json"
    },
    {
      "code": "{\n   \"title\": \"DiGraph\",\n   \"type\": \"object\",\n   \"properties\": {\n      \"nodes\": {\n         \"default\": null,\n         \"title\": \"Nodes\"\n      },\n      \"default_start_node\": {\n         \"anyOf\": [\n            {\n               \"type\": \"string\"\n            },\n            {\n               \"type\": \"null\"\n            }\n         ],\n         \"default\": null,\n         \"title\": \"Default Start Node\"\n      }\n   }\n}",
      "language": "json"
    },
    {
      "code": "{\n   \"title\": \"DiGraphNode\",\n   \"type\": \"object\",\n   \"properties\": {\n      \"name\": {\n         \"title\": \"Name\",\n         \"type\": \"string\"\n      },\n      \"edges\": {\n         \"default\": null,\n         \"title\": \"Edges\"\n      },\n      \"activation\": {\n         \"default\": \"all\",\n         \"enum\": [\n            \"all\",\n            \"any\"\n         ],\n         \"title\": \"Activation\",\n         \"type\": \"string\"\n      }\n   },\n   \"required\": [\n      \"name\"\n   ]\n}",
      "language": "json"
    },
    {
      "code": "{\n   \"title\": \"DiGraphNode\",\n   \"type\": \"object\",\n   \"properties\": {\n      \"name\": {\n         \"title\": \"Name\",\n         \"type\": \"string\"\n      },\n      \"edges\": {\n         \"default\": null,\n         \"title\": \"Edges\"\n      },\n      \"activation\": {\n         \"default\": \"all\",\n         \"enum\": [\n            \"all\",\n            \"any\"\n         ],\n         \"title\": \"Activation\",\n         \"type\": \"string\"\n      }\n   },\n   \"required\": [\n      \"name\"\n   ]\n}",
      "language": "json"
    },
    {
      "code": "{\n   \"title\": \"DiGraphEdge\",\n   \"type\": \"object\",\n   \"properties\": {\n      \"target\": {\n         \"title\": \"Target\",\n         \"type\": \"string\"\n      },\n      \"condition\": {\n         \"anyOf\": [\n            {\n               \"type\": \"string\"\n            },\n            {\n               \"type\": \"null\"\n            }\n         ],\n         \"default\": null,\n         \"title\": \"Condition\"\n      },\n      \"condition_function\": {\n         \"default\": null,\n         \"title\": \"Condition Function\"\n      },\n      \"activation_group\": {\n         \"default\": \"\",\n         \"title\": \"Activation Group\",\n         \"type\": \"string\"\n      },\n      \"activation_condition\": {\n         \"default\": \"all\",\n         \"enum\": [\n            \"all\",\n            \"any\"\n         ],\n         \"title\": \"Activation Condition\",\n         \"type\": \"string\"\n      }\n   },\n   \"required\": [\n      \"target\"\n   ]\n}",
      "language": "json"
    },
    {
      "code": "{\n   \"title\": \"DiGraphEdge\",\n   \"type\": \"object\",\n   \"properties\": {\n      \"target\": {\n         \"title\": \"Target\",\n         \"type\": \"string\"\n      },\n      \"condition\": {\n         \"anyOf\": [\n            {\n               \"type\": \"string\"\n            },\n            {\n               \"type\": \"null\"\n            }\n         ],\n         \"default\": null,\n         \"title\": \"Condition\"\n      },\n      \"condition_function\": {\n         \"default\": null,\n         \"title\": \"Condition Function\"\n      },\n      \"activation_group\": {\n         \"default\": \"\",\n         \"title\": \"Activation Group\",\n         \"type\": \"string\"\n      },\n      \"activation_condition\": {\n         \"default\": \"all\",\n         \"enum\": [\n            \"all\",\n            \"any\"\n         ],\n         \"title\": \"Activation Condition\",\n         \"type\": \"string\"\n      }\n   },\n   \"required\": [\n      \"target\"\n   ]\n}",
      "language": "json"
    },
    {
      "code": "import asyncio\n\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.conditions import MaxMessageTermination\nfrom autogen_agentchat.teams import DiGraphBuilder, GraphFlow\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n\nasync def main():\n    # Initialize agents with OpenAI model clients.\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4.1-nano\")\n    agent_a = AssistantAgent(\"A\", model_client=model_client, system_message=\"You are a helpful assistant.\")\n    agent_b = AssistantAgent(\"B\", model_client=model_client, system_message=\"Translate input to Chinese.\")\n    agent_c = AssistantAgent(\"C\", model_client=model_client, system_message=\"Translate input to English.\")\n\n    # Create a directed graph with sequential flow A -> B -> C.\n    builder = DiGraphBuilder()\n    builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)\n    builder.add_edge(agent_a, agent_b).add_edge(agent_b, agent_c)\n    graph = builder.build()\n\n    # Create a GraphFlow team with the directed graph.\n    team = GraphFlow(\n        participants=[agent_a, agent_b, agent_c],\n        graph=graph,\n        termination_condition=MaxMessageTermination(5),\n    )\n\n    # Run the team and print the events.\n    async for event in team.run_stream(task=\"Write a short story about a cat.\"):\n        print(event)\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\n\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.conditions import MaxMessageTermination\nfrom autogen_agentchat.teams import DiGraphBuilder, GraphFlow\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n\nasync def main():\n    # Initialize agents with OpenAI model clients.\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4.1-nano\")\n    agent_a = AssistantAgent(\"A\", model_client=model_client, system_message=\"You are a helpful assistant.\")\n    agent_b = AssistantAgent(\"B\", model_client=model_client, system_message=\"Translate input to Chinese.\")\n    agent_c = AssistantAgent(\"C\", model_client=model_client, system_message=\"Translate input to English.\")\n\n    # Create a directed graph with sequential flow A -> B -> C.\n    builder = DiGraphBuilder()\n    builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)\n    builder.add_edge(agent_a, agent_b).add_edge(agent_b, agent_c)\n    graph = builder.build()\n\n    # Create a GraphFlow team with the directed graph.\n    team = GraphFlow(\n        participants=[agent_a, agent_b, agent_c],\n        graph=graph,\n        termination_condition=MaxMessageTermination(5),\n    )\n\n    # Run the team and print the events.\n    async for event in team.run_stream(task=\"Write a short story about a cat.\"):\n        print(event)\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\n\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.conditions import MaxMessageTermination\nfrom autogen_agentchat.teams import DiGraphBuilder, GraphFlow\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n\nasync def main():\n    # Initialize agents with OpenAI model clients.\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4.1-nano\")\n    agent_a = AssistantAgent(\"A\", model_client=model_client, system_message=\"You are a helpful assistant.\")\n    agent_b = AssistantAgent(\"B\", model_client=model_client, system_message=\"Translate input to Chinese.\")\n    agent_c = AssistantAgent(\"C\", model_client=model_client, system_message=\"Translate input to Japanese.\")\n\n    # Create a directed graph with fan-out flow A -> (B, C).\n    builder = DiGraphBuilder()\n    builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)\n    builder.add_edge(agent_a, agent_b).add_edge(agent_a, agent_c)\n    graph = builder.build()\n\n    # Create a GraphFlow team with the directed graph.\n    team = GraphFlow(\n        participants=[agent_a, agent_b, agent_c],\n        graph=graph,\n        termination_condition=MaxMessageTermination(5),\n    )\n\n    # Run the team and print the events.\n    async for event in team.run_stream(task=\"Write a short story about a cat.\"):\n        print(event)\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\n\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.conditions import MaxMessageTermination\nfrom autogen_agentchat.teams import DiGraphBuilder, GraphFlow\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n\nasync def main():\n    # Initialize agents with OpenAI model clients.\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4.1-nano\")\n    agent_a = AssistantAgent(\"A\", model_client=model_client, system_message=\"You are a helpful assistant.\")\n    agent_b = AssistantAgent(\"B\", model_client=model_client, system_message=\"Translate input to Chinese.\")\n    agent_c = AssistantAgent(\"C\", model_client=model_client, system_message=\"Translate input to Japanese.\")\n\n    # Create a directed graph with fan-out flow A -> (B, C).\n    builder = DiGraphBuilder()\n    builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)\n    builder.add_edge(agent_a, agent_b).add_edge(agent_a, agent_c)\n    graph = builder.build()\n\n    # Create a GraphFlow team with the directed graph.\n    team = GraphFlow(\n        participants=[agent_a, agent_b, agent_c],\n        graph=graph,\n        termination_condition=MaxMessageTermination(5),\n    )\n\n    # Run the team and print the events.\n    async for event in team.run_stream(task=\"Write a short story about a cat.\"):\n        print(event)\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\n\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.conditions import MaxMessageTermination\nfrom autogen_agentchat.teams import DiGraphBuilder, GraphFlow\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n\nasync def main():\n    # Initialize agents with OpenAI model clients.\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4.1-nano\")\n    agent_a = AssistantAgent(\n        \"A\",\n        model_client=model_client,\n        system_message=\"Detect if the input is in Chinese. If it is, say 'yes', else say 'no', and nothing else.\",\n    )\n    agent_b = AssistantAgent(\"B\", model_client=model_client, system_message=\"Translate input to English.\")\n    agent_c = AssistantAgent(\"C\", model_client=model_client, system_message=\"Translate input to Chinese.\")\n\n    # Create a directed graph with conditional branching flow A -> B (\"yes\"), A -> C (otherwise).\n    builder = DiGraphBuilder()\n    builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)\n    # Create conditions as callables that check the message content.\n    builder.add_edge(agent_a, agent_b, condition=lambda msg: \"yes\" in msg.to_model_text())\n    builder.add_edge(agent_a, agent_c, condition=lambda msg: \"yes\" not in msg.to_model_text())\n    graph = builder.build()\n\n    # Create a GraphFlow team with the directed graph.\n    team = GraphFlow(\n        participants=[agent_a, agent_b, agent_c],\n        graph=graph,\n        termination_condition=MaxMessageTermination(5),\n    )\n\n    # Run the team and print the events.\n    async for event in team.run_stream(task=\"AutoGen is a framework for building AI agents.\"):\n        print(event)\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\n\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.conditions import MaxMessageTermination\nfrom autogen_agentchat.teams import DiGraphBuilder, GraphFlow\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n\nasync def main():\n    # Initialize agents with OpenAI model clients.\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4.1-nano\")\n    agent_a = AssistantAgent(\n        \"A\",\n        model_client=model_client,\n        system_message=\"Detect if the input is in Chinese. If it is, say 'yes', else say 'no', and nothing else.\",\n    )\n    agent_b = AssistantAgent(\"B\", model_client=model_client, system_message=\"Translate input to English.\")\n    agent_c = AssistantAgent(\"C\", model_client=model_client, system_message=\"Translate input to Chinese.\")\n\n    # Create a directed graph with conditional branching flow A -> B (\"yes\"), A -> C (otherwise).\n    builder = DiGraphBuilder()\n    builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)\n    # Create conditions as callables that check the message content.\n    builder.add_edge(agent_a, agent_b, condition=lambda msg: \"yes\" in msg.to_model_text())\n    builder.add_edge(agent_a, agent_c, condition=lambda msg: \"yes\" not in msg.to_model_text())\n    graph = builder.build()\n\n    # Create a GraphFlow team with the directed graph.\n    team = GraphFlow(\n        participants=[agent_a, agent_b, agent_c],\n        graph=graph,\n        termination_condition=MaxMessageTermination(5),\n    )\n\n    # Run the team and print the events.\n    async for event in team.run_stream(task=\"AutoGen is a framework for building AI agents.\"):\n        print(event)\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\n\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.conditions import MaxMessageTermination\nfrom autogen_agentchat.teams import DiGraphBuilder, GraphFlow\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n\nasync def main():\n    # Initialize agents with OpenAI model clients.\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4.1\")\n    agent_a = AssistantAgent(\n        \"A\",\n        model_client=model_client,\n        system_message=\"You are a helpful assistant.\",\n    )\n    agent_b = AssistantAgent(\n        \"B\",\n        model_client=model_client,\n        system_message=\"Provide feedback on the input, if your feedback has been addressed, \"\n        \"say 'APPROVE', otherwise provide a reason for rejection.\",\n    )\n    agent_c = AssistantAgent(\n        \"C\", model_client=model_client, system_message=\"Translate the final product to Korean.\"\n    )\n\n    # Create a loop graph with conditional exit: A -> B -> C (\"APPROVE\"), B -> A (otherwise).\n    builder = DiGraphBuilder()\n    builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)\n    builder.add_edge(agent_a, agent_b)\n\n    # Create conditional edges using strings\n    builder.add_edge(agent_b, agent_c, condition=lambda msg: \"APPROVE\" in msg.to_model_text())\n    builder.add_edge(agent_b, agent_a, condition=lambda msg: \"APPROVE\" not in msg.to_model_text())\n\n    builder.set_entry_point(agent_a)\n    graph = builder.build()\n\n    # Create a GraphFlow team with the directed graph.\n    team = GraphFlow(\n        participants=[agent_a, agent_b, agent_c],\n        graph=graph,\n        termination_condition=MaxMessageTermination(20),  # Max 20 messages to avoid infinite loop.\n    )\n\n    # Run the team and print the events.\n    async for event in team.run_stream(task=\"Write a short poem about AI Agents.\"):\n        print(event)\n\n\nasyncio.run(main())",
      "language": "python"
    },
    {
      "code": "import asyncio\n\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.conditions import MaxMessageTermination\nfrom autogen_agentchat.teams import DiGraphBuilder, GraphFlow\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n\nasync def main():\n    # Initialize agents with OpenAI model clients.\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4.1\")\n    agent_a = AssistantAgent(\n        \"A\",\n        model_client=model_client,\n        system_message=\"You are a helpful assistant.\",\n    )\n    agent_b = AssistantAgent(\n        \"B\",\n        model_client=model_client,\n        system_message=\"Provide feedback on the input, if your feedback has been addressed, \"\n        \"say 'APPROVE', otherwise provide a reason for rejection.\",\n    )\n    agent_c = AssistantAgent(\n        \"C\", model_client=model_client, system_message=\"Translate the final product to Korean.\"\n    )\n\n    # Create a loop graph with conditional exit: A -> B -> C (\"APPROVE\"), B -> A (otherwise).\n    builder = DiGraphBuilder()\n    builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)\n    builder.add_edge(agent_a, agent_b)\n\n    # Create conditional edges using strings\n    builder.add_edge(agent_b, agent_c, condition=lambda msg: \"APPROVE\" in msg.to_model_text())\n    builder.add_edge(agent_b, agent_a, condition=lambda msg: \"APPROVE\" not in msg.to_model_text())\n\n    builder.set_entry_point(agent_a)\n    graph = builder.build()\n\n    # Create a GraphFlow team with the directed graph.\n    team = GraphFlow(\n        participants=[agent_a, agent_b, agent_c],\n        graph=graph,\n        termination_condition=MaxMessageTermination(20),  # Max 20 messages to avoid infinite loop.\n    )\n\n    # Run the team and print the events.\n    async for event in team.run_stream(task=\"Write a short poem about AI Agents.\"):\n        print(event)\n\n\nasyncio.run(main())",
      "language": "python"
    }
  ],
  "patterns": [
    {
      "description": "API Reference autogen_agentchat.teams autogen_agentchat.teams# This module provides implementation of various pre-defined multi-agent teams. Each team inherits from the BaseGroupChat class. class BaseGroupChat(name: str, description: str, participants: List[ChatAgent | Team], group_chat_manager_name: str, group_chat_manager_class: type[SequentialRoutedAgent], termination_condition: TerminationCondition | None = None, max_turns: int | None = None, runtime: AgentRuntime | None = None, custom_message_types: List[type[BaseAgentEvent | BaseChatMessage]] | None = None, emit_team_events: bool = False)[source]# Bases: Team, ABC, ComponentBase[BaseModel] The base class for group chat teams. In a group chat team, participants share context by publishing their messages to all other participants. If an ChatAgent is a participant, the BaseChatMessage from the agent response’s chat_message will be published to other participants in the group chat. If a Team is a participant, the BaseChatMessage from the team result’ messages will be published to other participants in the group chat. To implement a group chat team, first create a subclass of BaseGroupChatManager and then create a subclass of BaseGroupChat that uses the group chat manager. This base class provides the mapping between the agents of the AgentChat API and the agent runtime of the Core API, and handles high-level features like running, pausing, resuming, and resetting the team. component_type: ClassVar[ComponentType] = 'team'# The logical type of the component. property name: str# The name of the group chat team. property description: str# A description of the group chat team. async run(*, task: str | BaseChatMessage | Sequence[BaseChatMessage] | None = None, cancellation_token: CancellationToken | None = None, output_task_messages: bool = True) → TaskResult[source]# Run the team and return the result. The base implementation uses run_stream() to run the team and then returns the final result. Once the team is stopped, the termination condition is reset. Parameters: task (str | BaseChatMessage | Sequence[BaseChatMessage] | None) – The task to run the team with. Can be a string, a single BaseChatMessage , or a list of BaseChatMessage. cancellation_token (CancellationToken | None) – The cancellation token to kill the task immediately. Setting the cancellation token potentially put the team in an inconsistent state, and it may not reset the termination condition. To gracefully stop the team, use ExternalTermination instead. Returns: result – The result of the task as TaskResult. The result contains the messages produced by the team and the stop reason. Example using the RoundRobinGroupChat team: import asyncio from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.conditions import MaxMessageTermination from autogen_agentchat.teams import RoundRobinGroupChat from autogen_ext.models.openai import OpenAIChatCompletionClient async def main() -> None: model_client = OpenAIChatCompletionClient(model=\"gpt-4o\") agent1 = AssistantAgent(\"Assistant1\", model_client=model_client) agent2 = AssistantAgent(\"Assistant2\", model_client=model_client) termination = MaxMessageTermination(3) team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination) result = await team.run(task=\"Count from 1 to 10, respond one at a time.\") print(result) # Run the team again without a task to continue the previous task. result = await team.run() print(result) asyncio.run(main()) Example using the CancellationToken to cancel the task: import asyncio from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.conditions import MaxMessageTermination from autogen_agentchat.teams import RoundRobinGroupChat from autogen_core import CancellationToken from autogen_ext.models.openai import OpenAIChatCompletionClient async def main() -> None: model_client = OpenAIChatCompletionClient(model=\"gpt-4o\") agent1 = AssistantAgent(\"Assistant1\", model_client=model_client) agent2 = AssistantAgent(\"Assistant2\", model_client=model_client) termination = MaxMessageTermination(3) team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination) cancellation_token = CancellationToken() # Create a task to run the team in the background. run_task = asyncio.create_task( team.run( task=\"Count from 1 to 10, respond one at a time.\", cancellation_token=cancellation_token, ) ) # Wait for 1 second and then cancel the task. await asyncio.sleep(1) cancellation_token.cancel() # This will raise a cancellation error. await run_task asyncio.run(main()) async run_stream(*, task: str | BaseChatMessage | Sequence[BaseChatMessage] | None = None, cancellation_token: CancellationToken | None = None, output_task_messages: bool = True) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | TaskResult, None][source]# Run the team and produces a stream of messages and the final result of the type TaskResult as the last item in the stream. Once the team is stopped, the termination condition is reset. Note If an agent produces ModelClientStreamingChunkEvent, the message will be yielded in the stream but it will not be included in the messages. Parameters: task (str | BaseChatMessage | Sequence[BaseChatMessage] | None) – The task to run the team with. Can be a string, a single BaseChatMessage , or a list of BaseChatMessage. cancellation_token (CancellationToken | None) – The cancellation token to kill the task immediately. Setting the cancellation token potentially put the team in an inconsistent state, and it may not reset the termination condition. To gracefully stop the team, use ExternalTermination instead. output_task_messages (bool) – Whether to include task messages in the output stream. Defaults to True for backward compatibility. Returns: stream – an AsyncGenerator that yields BaseAgentEvent, BaseChatMessage, and the final result TaskResult as the last item in the stream. Example using the RoundRobinGroupChat team: import asyncio from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.conditions import MaxMessageTermination from autogen_agentchat.teams import RoundRobinGroupChat from autogen_ext.models.openai import OpenAIChatCompletionClient async def main() -> None: model_client = OpenAIChatCompletionClient(model=\"gpt-4o\") agent1 = AssistantAgent(\"Assistant1\", model_client=model_client) agent2 = AssistantAgent(\"Assistant2\", model_client=model_client) termination = MaxMessageTermination(3) team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination) stream = team.run_stream(task=\"Count from 1 to 10, respond one at a time.\") async for message in stream: print(message) # Run the team again without a task to continue the previous task. stream = team.run_stream() async for message in stream: print(message) asyncio.run(main()) Example using the CancellationToken to cancel the task: import asyncio from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.conditions import MaxMessageTermination from autogen_agentchat.ui import Console from autogen_agentchat.teams import RoundRobinGroupChat from autogen_core import CancellationToken from autogen_ext.models.openai import OpenAIChatCompletionClient async def main() -> None: model_client = OpenAIChatCompletionClient(model=\"gpt-4o\") agent1 = AssistantAgent(\"Assistant1\", model_client=model_client) agent2 = AssistantAgent(\"Assistant2\", model_client=model_client) termination = MaxMessageTermination(3) team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination) cancellation_token = CancellationToken() # Create a task to run the team in the background. run_task = asyncio.create_task( Console( team.run_stream( task=\"Count from 1 to 10, respond one at a time.\", cancellation_token=cancellation_token, ) ) ) # Wait for 1 second and then cancel the task. await asyncio.sleep(1) cancellation_token.cancel() # This will raise a cancellation error. await run_task asyncio.run(main()) async reset() → None[source]# Reset the team and its participants to their initial state. The team must be stopped before it can be reset. Raises: RuntimeError – If the team has not been initialized or is currently running. Example using the RoundRobinGroupChat team: import asyncio from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.conditions import MaxMessageTermination from autogen_agentchat.teams import RoundRobinGroupChat from autogen_ext.models.openai import OpenAIChatCompletionClient async def main() -> None: model_client = OpenAIChatCompletionClient(model=\"gpt-4o\") agent1 = AssistantAgent(\"Assistant1\", model_client=model_client) agent2 = AssistantAgent(\"Assistant2\", model_client=model_client) termination = MaxMessageTermination(3) team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination) stream = team.run_stream(task=\"Count from 1 to 10, respond one at a time.\") async for message in stream: print(message) # Reset the team. await team.reset() stream = team.run_stream(task=\"Count from 1 to 10, respond one at a time.\") async for message in stream: print(message) asyncio.run(main()) async pause() → None[source]# Pause its participants when the team is running by calling their on_pause() method via direct RPC calls. Attention This is an experimental feature introduced in v0.4.9 and may subject to change or removal in the future. The team must be initialized before it can be paused. Different from termination, pausing the team does not cause the run() or run_stream() method to return. It calls the on_pause() method on each participant, and if the participant does not implement the method, it will be a no-op. Note It is the responsibility of the agent class to handle the pause and ensure that the agent can be resumed later. Make sure to implement the on_pause() method in your agent class for custom pause behavior. By default, the agent will not do anything when called. Raises: RuntimeError – If the team has not been initialized. Exceptions from the participants when calling their implementations of on_pause are propagated to this method and raised. async resume() → None[source]# Resume its participants when the team is running and paused by calling their on_resume() method via direct RPC calls. Attention This is an experimental feature introduced in v0.4.9 and may subject to change or removal in the future. The team must be initialized before it can be resumed. Different from termination and restart with a new task, resuming the team does not cause the run() or run_stream() method to return. It calls the on_resume() method on each participant, and if the participant does not implement the method, it will be a no-op. Note It is the responsibility of the agent class to handle the resume and ensure that the agent continues from where it was paused. Make sure to implement the on_resume() method in your agent class for custom resume behavior. Raises: RuntimeError – If the team has not been initialized. Exceptions from the participants when calling their implementations of on_resume method are propagated to this method and raised. async save_state() → Mapping[str, Any][source]# Save the state of the group chat team. The state is saved by calling the agent_save_state() method on each participant and the group chat manager with their internal agent ID. The state is returned as a nested dictionary: a dictionary with key agent_states, which is a dictionary the agent names as keys and the state as values. { \"agent_states\": { \"agent1\": ..., \"agent2\": ..., \"RoundRobinGroupChatManager\": ... } } Note Starting v0.4.9, the state is using the agent name as the key instead of the agent ID, and the team_id field is removed from the state. This is to allow the state to be portable across different teams and runtimes. States saved with the old format may not be compatible with the new format in the future. Caution When calling save_state() on a team while it is running, the state may not be consistent and may result in an unexpected state. It is recommended to call this method when the team is not running or after it is stopped. async load_state(state: Mapping[str, Any]) → None[source]# Load an external state and overwrite the current state of the group chat team. The state is loaded by calling the agent_load_state() method on each participant and the group chat manager with their internal agent ID. See save_state() for the expected format of the state. class RoundRobinGroupChat(participants: List[ChatAgent | Team], *, name: str | None = None, description: str | None = None, termination_condition: TerminationCondition | None = None, max_turns: int | None = None, runtime: AgentRuntime | None = None, custom_message_types: List[type[BaseAgentEvent | BaseChatMessage]] | None = None, emit_team_events: bool = False)[source]# Bases: BaseGroupChat, Component[RoundRobinGroupChatConfig] A team that runs a group chat with participants taking turns in a round-robin fashion to publish a message to all. If an ChatAgent is a participant, the BaseChatMessage from the agent response’s chat_message will be published to other participants in the group chat. If a Team is a participant, the BaseChatMessage from the team result’ messages will be published to other participants in the group chat. If a single participant is in the team, the participant will be the only speaker. Parameters: participants (List[ChatAgent | Team]) – The participants in the group chat. name (str | None, optional) – The name of the group chat, using DEFAULT_NAME if not provided. The name is used by a parent team to identify this group chat so it must be unique within the parent team. description (str | None, optional) – The description of the group chat, using DEFAULT_DESCRIPTION if not provided. termination_condition (TerminationCondition, optional) – The termination condition for the group chat. Defaults to None. Without a termination condition, the group chat will run indefinitely. max_turns (int, optional) – The maximum number of turns in the group chat before stopping. Defaults to None, meaning no limit. custom_message_types (List[type[BaseAgentEvent | BaseChatMessage]], optional) – A list of custom message types that will be used in the group chat. If you are using custom message types or your agents produces custom message types, you need to specify them here. Make sure your custom message types are subclasses of BaseAgentEvent or BaseChatMessage. emit_team_events (bool, optional) – Whether to emit team events through BaseGroupChat.run_stream(). Defaults to False. Raises: ValueError – If no participants are provided or if participant names are not unique. Examples A team with one participant with tools: import asyncio from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.teams import RoundRobinGroupChat from autogen_agentchat.conditions import TextMentionTermination from autogen_agentchat.ui import Console async def main() -> None: model_client = OpenAIChatCompletionClient(model=\"gpt-4o\") async def get_weather(location: str) -> str: return f\"The weather in {location} is sunny.\" assistant = AssistantAgent( \"Assistant\", model_client=model_client, tools=[get_weather], ) termination = TextMentionTermination(\"TERMINATE\") team = RoundRobinGroupChat([assistant], termination_condition=termination) await Console(team.run_stream(task=\"What's the weather in New York?\")) asyncio.run(main()) A team with multiple participants: import asyncio from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.teams import RoundRobinGroupChat from autogen_agentchat.conditions import TextMentionTermination from autogen_agentchat.ui import Console async def main() -> None: model_client = OpenAIChatCompletionClient(model=\"gpt-4o\") agent1 = AssistantAgent(\"Assistant1\", model_client=model_client) agent2 = AssistantAgent(\"Assistant2\", model_client=model_client) termination = TextMentionTermination(\"TERMINATE\") team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination) await Console(team.run_stream(task=\"Tell me some jokes.\")) asyncio.run(main()) A team of user proxy and a nested team of writer and reviewer agents: import asyncio from autogen_agentchat.agents import UserProxyAgent, AssistantAgent from autogen_agentchat.conditions import TextMentionTermination, MaxMessageTermination from autogen_agentchat.teams import RoundRobinGroupChat from autogen_agentchat.ui import Console from autogen_ext.models.openai import OpenAIChatCompletionClient async def main() -> None: model_client = OpenAIChatCompletionClient(model=\"gpt-4.1-nano\") writer = AssistantAgent( \"writer\", model_client=model_client, system_message=\"You are a writer.\", model_client_stream=True ) reviewer = AssistantAgent( \"reviewer\", model_client=model_client, system_message=\"Provide feedback to the input and suggest improvements.\", model_client_stream=True, ) # NOTE: you can skip input by pressing Enter. user_proxy = UserProxyAgent(\"user_proxy\") # Maximum 1 round of review and revision. inner_termination = MaxMessageTermination(max_messages=4) # The outter-loop termination condition that will terminate the team when the user types \"exit\". outter_termination = TextMentionTermination(\"exit\", sources=[\"user_proxy\"]) team = RoundRobinGroupChat( [ # For each turn, the writer writes a summary and the reviewer reviews it. RoundRobinGroupChat([writer, reviewer], termination_condition=inner_termination), # The user proxy gets user input once the writer and reviewer have finished their actions. user_proxy, ], termination_condition=outter_termination, ) # Start the team and wait for it to terminate. await Console(team.run_stream(task=\"Write a short essay about the impact of AI on society.\")) asyncio.run(main()) component_config_schema# alias of RoundRobinGroupChatConfig component_provider_override: ClassVar[str | None] = 'autogen_agentchat.teams.RoundRobinGroupChat'# Override the provider string for the component. This should be used to prevent internal module names being a part of the module name. DEFAULT_NAME = 'RoundRobinGroupChat'# DEFAULT_DESCRIPTION = 'A team of agents.'# _to_config() → RoundRobinGroupChatConfig[source]# Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance. Returns: T – The configuration of the component. classmethod _from_config(config: RoundRobinGroupChatConfig) → Self[source]# Create a new instance of the component from a configuration object. Parameters: config (T) – The configuration object. Returns: Self – The new instance of the component. class SelectorGroupChat(participants: List[ChatAgent | Team], model_client: ChatCompletionClient, *, name: str | None = None, description: str | None = None, termination_condition: TerminationCondition | None = None, max_turns: int | None = None, runtime: AgentRuntime | None = None, selector_prompt: str = 'You are in a role play game. The following roles are available:\\n{roles}.\\nRead the following conversation. Then select the next role from {participants} to play. Only return the role.\\n\\n{history}\\n\\nRead the above conversation. Then select the next role from {participants} to play. Only return the role.\\n', allow_repeated_speaker: bool = False, max_selector_attempts: int = 3, selector_func: Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], str | None] | Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], Awaitable[str | None]] | None = None, candidate_func: Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], List[str]] | Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], Awaitable[List[str]]] | None = None, custom_message_types: List[type[BaseAgentEvent | BaseChatMessage]] | None = None, emit_team_events: bool = False, model_client_streaming: bool = False, model_context: ChatCompletionContext | None = None)[source]# Bases: BaseGroupChat, Component[SelectorGroupChatConfig] A group chat team that have participants takes turn to publish a message to all, using a ChatCompletion model to select the next speaker after each message. If an ChatAgent is a participant, the BaseChatMessage from the agent response’s chat_message will be published to other participants in the group chat. If a Team is a participant, the BaseChatMessage from the team result’ messages will be published to other participants in the group chat. Parameters: participants (List[ChatAgent | Team]) – The participants in the group chat, must have unique names and at least two participants. model_client (ChatCompletionClient) – The ChatCompletion model client used to select the next speaker. name (str | None, optional) – The name of the group chat, using DEFAULT_NAME if not provided. The name is used by a parent team to identify this group chat so it must be unique within the parent team. description (str | None, optional) – The description of the group chat, using DEFAULT_DESCRIPTION if not provided. termination_condition (TerminationCondition, optional) – The termination condition for the group chat. Defaults to None. Without a termination condition, the group chat will run indefinitely. max_turns (int, optional) – The maximum number of turns in the group chat before stopping. Defaults to None, meaning no limit. selector_prompt (str, optional) – The prompt template to use for selecting the next speaker. Available fields: ‘{roles}’, ‘{participants}’, and ‘{history}’. {participants} is the names of candidates for selection. The format is [“<name1>”, “<name2>”, …]. {roles} is a newline-separated list of names and descriptions of the candidate agents. The format for each line is: “<name> : <description>”. {history} is the conversation history formatted as a double newline separated of names and message content. The format for each message is: “<name> : <message content>”. allow_repeated_speaker (bool, optional) – Whether to include the previous speaker in the list of candidates to be selected for the next turn. Defaults to False. The model may still select the previous speaker – a warning will be logged if this happens. max_selector_attempts (int, optional) – The maximum number of attempts to select a speaker using the model. Defaults to 3. If the model fails to select a speaker after the maximum number of attempts, the previous speaker will be used if available, otherwise the first participant will be used. selector_func (Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], str | None], Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], Awaitable[str | None]], optional) – A custom selector function that takes the conversation history and returns the name of the next speaker. If provided, this function will be used to override the model to select the next speaker. If the function returns None, the model will be used to select the next speaker. NOTE: selector_func is not serializable and will be ignored during serialization and deserialization process. candidate_func (Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], List[str]], Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], Awaitable[List[str]]], optional) – A custom function that takes the conversation history and returns a filtered list of candidates for the next speaker selection using model. If the function returns an empty list or None, SelectorGroupChat will raise a ValueError. This function is only used if selector_func is not set. The allow_repeated_speaker will be ignored if set. custom_message_types (List[type[BaseAgentEvent | BaseChatMessage]], optional) – A list of custom message types that will be used in the group chat. If you are using custom message types or your agents produces custom message types, you need to specify them here. Make sure your custom message types are subclasses of BaseAgentEvent or BaseChatMessage. emit_team_events (bool, optional) – Whether to emit team events through BaseGroupChat.run_stream(). Defaults to False. model_client_streaming (bool, optional) – Whether to use streaming for the model client. (This is useful for reasoning models like QwQ). Defaults to False. model_context (ChatCompletionContext | None, optional) – The model context for storing and retrieving LLMMessage. It can be preloaded with initial messages. Messages stored in model context will be used for speaker selection. The initial messages will be cleared when the team is reset. Raises: ValueError – If the number of participants is less than two or if the selector prompt is invalid. Examples: A team with multiple participants: import asyncio from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.teams import SelectorGroupChat from autogen_agentchat.conditions import TextMentionTermination from autogen_agentchat.ui import Console async def main() -> None: model_client = OpenAIChatCompletionClient(model=\"gpt-4o\") async def lookup_hotel(location: str) -> str: return f\"Here are some hotels in {location}: hotel1, hotel2, hotel3.\" async def lookup_flight(origin: str, destination: str) -> str: return f\"Here are some flights from {origin} to {destination}: flight1, flight2, flight3.\" async def book_trip() -> str: return \"Your trip is booked!\" travel_advisor = AssistantAgent( \"Travel_Advisor\", model_client, tools=[book_trip], description=\"Helps with travel planning.\", ) hotel_agent = AssistantAgent( \"Hotel_Agent\", model_client, tools=[lookup_hotel], description=\"Helps with hotel booking.\", ) flight_agent = AssistantAgent( \"Flight_Agent\", model_client, tools=[lookup_flight], description=\"Helps with flight booking.\", ) termination = TextMentionTermination(\"TERMINATE\") team = SelectorGroupChat( [travel_advisor, hotel_agent, flight_agent], model_client=model_client, termination_condition=termination, ) await Console(team.run_stream(task=\"Book a 3-day trip to new york.\")) asyncio.run(main()) A team with a custom selector function: import asyncio from typing import Sequence from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.teams import SelectorGroupChat from autogen_agentchat.conditions import TextMentionTermination from autogen_agentchat.ui import Console from autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage async def main() -> None: model_client = OpenAIChatCompletionClient(model=\"gpt-4o\") def check_calculation(x: int, y: int, answer: int) -> str: if x + y == answer: return \"Correct!\" else: return \"Incorrect!\" agent1 = AssistantAgent( \"Agent1\", model_client, description=\"For calculation\", system_message=\"Calculate the sum of two numbers\", ) agent2 = AssistantAgent( \"Agent2\", model_client, tools=[check_calculation], description=\"For checking calculation\", system_message=\"Check the answer and respond with 'Correct!' or 'Incorrect!'\", ) def selector_func(messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> str | None: if len(messages) == 1 or messages[-1].to_text() == \"Incorrect!\": return \"Agent1\" if messages[-1].source == \"Agent1\": return \"Agent2\" return None termination = TextMentionTermination(\"Correct!\") team = SelectorGroupChat( [agent1, agent2], model_client=model_client, selector_func=selector_func, termination_condition=termination, ) await Console(team.run_stream(task=\"What is 1 + 1?\")) asyncio.run(main()) A team with custom model context: import asyncio from autogen_core.model_context import BufferedChatCompletionContext from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.conditions import TextMentionTermination from autogen_agentchat.teams import SelectorGroupChat from autogen_agentchat.ui import Console async def main() -> None: model_client = OpenAIChatCompletionClient(model=\"gpt-4o\") model_context = BufferedChatCompletionContext(buffer_size=5) async def lookup_hotel(location: str) -> str: return f\"Here are some hotels in {location}: hotel1, hotel2, hotel3.\" async def lookup_flight(origin: str, destination: str) -> str: return f\"Here are some flights from {origin} to {destination}: flight1, flight2, flight3.\" async def book_trip() -> str: return \"Your trip is booked!\" travel_advisor = AssistantAgent( \"Travel_Advisor\", model_client, tools=[book_trip], description=\"Helps with travel planning.\", ) hotel_agent = AssistantAgent( \"Hotel_Agent\", model_client, tools=[lookup_hotel], description=\"Helps with hotel booking.\", ) flight_agent = AssistantAgent( \"Flight_Agent\", model_client, tools=[lookup_flight], description=\"Helps with flight booking.\", ) termination = TextMentionTermination(\"TERMINATE\") team = SelectorGroupChat( [travel_advisor, hotel_agent, flight_agent], model_client=model_client, termination_condition=termination, model_context=model_context, ) await Console(team.run_stream(task=\"Book a 3-day trip to new york.\")) asyncio.run(main()) component_config_schema# alias of SelectorGroupChatConfig component_provider_override: ClassVar[str | None] = 'autogen_agentchat.teams.SelectorGroupChat'# Override the provider string for the component. This should be used to prevent internal module names being a part of the module name. DEFAULT_NAME = 'SelectorGroupChat'# DEFAULT_DESCRIPTION = 'A team of agents.'# _to_config() → SelectorGroupChatConfig[source]# Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance. Returns: T – The configuration of the component. classmethod _from_config(config: SelectorGroupChatConfig) → Self[source]# Create a new instance of the component from a configuration object. Parameters: config (T) – The configuration object. Returns: Self – The new instance of the component. class Swarm(participants: List[ChatAgent], *, name: str | None = None, description: str | None = None, termination_condition: TerminationCondition | None = None, max_turns: int | None = None, runtime: AgentRuntime | None = None, custom_message_types: List[type[BaseAgentEvent | BaseChatMessage]] | None = None, emit_team_events: bool = False)[source]# Bases: BaseGroupChat, Component[SwarmConfig] A group chat team that selects the next speaker based on handoff message only. The first participant in the list of participants is the initial speaker. The next speaker is selected based on the HandoffMessage message sent by the current speaker. If no handoff message is sent, the current speaker continues to be the speaker. Note Unlike RoundRobinGroupChat and SelectorGroupChat, this group chat team does not support inner teams as participants. Parameters: participants (List[ChatAgent]) – The agents participating in the group chat. The first agent in the list is the initial speaker. name (str | None, optional) – The name of the group chat, using DEFAULT_NAME if not provided. The name is used by a parent team to identify this group chat so it must be unique within the parent team. description (str | None, optional) – The description of the group chat, using DEFAULT_DESCRIPTION if not provided. termination_condition (TerminationCondition, optional) – The termination condition for the group chat. Defaults to None. Without a termination condition, the group chat will run indefinitely. max_turns (int, optional) – The maximum number of turns in the group chat before stopping. Defaults to None, meaning no limit. custom_message_types (List[type[BaseAgentEvent | BaseChatMessage]], optional) – A list of custom message types that will be used in the group chat. If you are using custom message types or your agents produces custom message types, you need to specify them here. Make sure your custom message types are subclasses of BaseAgentEvent or BaseChatMessage. emit_team_events (bool, optional) – Whether to emit team events through BaseGroupChat.run_stream(). Defaults to False. Basic example: import asyncio from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.teams import Swarm from autogen_agentchat.conditions import MaxMessageTermination async def main() -> None: model_client = OpenAIChatCompletionClient(model=\"gpt-4o\") agent1 = AssistantAgent( \"Alice\", model_client=model_client, handoffs=[\"Bob\"], system_message=\"You are Alice and you only answer questions about yourself.\", ) agent2 = AssistantAgent( \"Bob\", model_client=model_client, system_message=\"You are Bob and your birthday is on 1st January.\" ) termination = MaxMessageTermination(3) team = Swarm([agent1, agent2], termination_condition=termination) stream = team.run_stream(task=\"What is bob's birthday?\") async for message in stream: print(message) asyncio.run(main()) Using the HandoffTermination for human-in-the-loop handoff: import asyncio from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.teams import Swarm from autogen_agentchat.conditions import HandoffTermination, MaxMessageTermination from autogen_agentchat.ui import Console from autogen_agentchat.messages import HandoffMessage async def main() -> None: model_client = OpenAIChatCompletionClient(model=\"gpt-4o\") agent = AssistantAgent( \"Alice\", model_client=model_client, handoffs=[\"user\"], system_message=\"You are Alice and you only answer questions about yourself, ask the user for help if needed.\", ) termination = HandoffTermination(target=\"user\") | MaxMessageTermination(3) team = Swarm([agent], termination_condition=termination) # Start the conversation. await Console(team.run_stream(task=\"What is bob's birthday?\")) # Resume with user feedback. await Console( team.run_stream( task=HandoffMessage(source=\"user\", target=\"Alice\", content=\"Bob's birthday is on 1st January.\") ) ) asyncio.run(main()) component_config_schema# alias of SwarmConfig component_provider_override: ClassVar[str | None] = 'autogen_agentchat.teams.Swarm'# Override the provider string for the component. This should be used to prevent internal module names being a part of the module name. DEFAULT_NAME = 'Swarm'# DEFAULT_DESCRIPTION = 'A team of agents.'# _to_config() → SwarmConfig[source]# Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance. Returns: T – The configuration of the component. classmethod _from_config(config: SwarmConfig) → Swarm[source]# Create a new instance of the component from a configuration object. Parameters: config (T) – The configuration object. Returns: Self – The new instance of the component. class MagenticOneGroupChat(participants: List[ChatAgent], model_client: ChatCompletionClient, *, name: str | None = None, description: str | None = None, termination_condition: TerminationCondition | None = None, max_turns: int | None = 20, runtime: AgentRuntime | None = None, max_stalls: int = 3, final_answer_prompt: str = ORCHESTRATOR_FINAL_ANSWER_PROMPT, custom_message_types: List[type[BaseAgentEvent | BaseChatMessage]] | None = None, emit_team_events: bool = False)[source]# Bases: BaseGroupChat, Component[MagenticOneGroupChatConfig] A team that runs a group chat with participants managed by the MagenticOneOrchestrator. The orchestrator handles the conversation flow, ensuring that the task is completed efficiently by managing the participants’ interactions. The orchestrator is based on the Magentic-One architecture, which is a generalist multi-agent system for solving complex tasks (see references below). Unlike RoundRobinGroupChat and SelectorGroupChat, the MagenticOneGroupChat does not support using team as participant. Parameters: participants (List[ChatAgent]) – The participants in the group chat. model_client (ChatCompletionClient) – The model client used for generating responses. termination_condition (TerminationCondition, optional) – The termination condition for the group chat. Defaults to None. Without a termination condition, the group chat will run based on the orchestrator logic or until the maximum number of turns is reached. max_turns (int, optional) – The maximum number of turns in the group chat before stopping. Defaults to 20. max_stalls (int, optional) – The maximum number of stalls allowed before re-planning. Defaults to 3. final_answer_prompt (str, optional) – The LLM prompt used to generate the final answer or response from the team’s transcript. A default (sensible for GPT-4o class models) is provided. custom_message_types (List[type[BaseAgentEvent | BaseChatMessage]], optional) – A list of custom message types that will be used in the group chat. If you are using custom message types or your agents produces custom message types, you need to specify them here. Make sure your custom message types are subclasses of BaseAgentEvent or BaseChatMessage. emit_team_events (bool, optional) – Whether to emit team events through BaseGroupChat.run_stream(). Defaults to False. Raises: ValueError – In orchestration logic if progress ledger does not have required keys or if next speaker is not valid. Examples: MagenticOneGroupChat with one assistant agent: import asyncio from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.teams import MagenticOneGroupChat from autogen_agentchat.ui import Console async def main() -> None: model_client = OpenAIChatCompletionClient(model=\"gpt-4o\") assistant = AssistantAgent( \"Assistant\", model_client=model_client, ) team = MagenticOneGroupChat([assistant], model_client=model_client) await Console(team.run_stream(task=\"Provide a different proof to Fermat last theorem\")) asyncio.run(main()) References If you use the MagenticOneGroupChat in your work, please cite the following paper: @article{fourney2024magentic, title={Magentic-one: A generalist multi-agent system for solving complex tasks}, author={Fourney, Adam and Bansal, Gagan and Mozannar, Hussein and Tan, Cheng and Salinas, Eduardo and Niedtner, Friederike and Proebsting, Grace and Bassman, Griffin and Gerrits, Jack and Alber, Jacob and others}, journal={arXiv preprint arXiv:2411.04468}, year={2024} } component_config_schema# alias of MagenticOneGroupChatConfig component_provider_override: ClassVar[str | None] = 'autogen_agentchat.teams.MagenticOneGroupChat'# Override the provider string for the component. This should be used to prevent internal module names being a part of the module name. DEFAULT_NAME = 'MagenticOneGroupChat'# DEFAULT_DESCRIPTION = 'A team of agents.'# _to_config() → MagenticOneGroupChatConfig[source]# Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance. Returns: T – The configuration of the component. classmethod _from_config(config: MagenticOneGroupChatConfig) → Self[source]# Create a new instance of the component from a configuration object. Parameters: config (T) – The configuration object. Returns: Self – The new instance of the component. class DiGraphBuilder[source]# Bases: object A fluent builder for constructing DiGraph execution graphs used in GraphFlow. Warning This is an experimental feature, and the API will change in the future releases. This utility provides a convenient way to programmatically build a graph of agent interactions, including complex execution flows such as: Sequential chains Parallel fan-outs Conditional branching Cyclic loops with safe exits Each node in the graph represents an agent. Edges define execution paths between agents, and can optionally be conditioned on message content using callable functions. The builder is compatible with the Graph runner and supports both standard and filtered agents. - add_node(agent, activation) Add an agent node to the graph. - add_edge(source, target, condition) Connect two nodes optionally with a condition. - add_conditional_edges(source, condition_to_target) Add multiple conditional edges from a source. - set_entry_point(agent) Define the default start node (optional). - build() Generate a validated DiGraph. - get_participants() Return the list of added agents. Example — Sequential Flow A → B → C:>>> builder = GraphBuilder() >>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c) >>> builder.add_edge(agent_a, agent_b).add_edge(agent_b, agent_c) >>> team = Graph( ... participants=builder.get_participants(), ... graph=builder.build(), ... termination_condition=MaxMessageTermination(5), ... ) Example — Parallel Fan-out A → (B, C):>>> builder = GraphBuilder() >>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c) >>> builder.add_edge(agent_a, agent_b).add_edge(agent_a, agent_c) Example — Conditional Branching A → B or A → C:>>> builder = GraphBuilder() >>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c) >>> # Add conditional edges using keyword check >>> builder.add_edge(agent_a, agent_b, condition=\"keyword1\") >>> builder.add_edge(agent_a, agent_c, condition=\"keyword2\") Example — Using Custom String Conditions:>>> builder = GraphBuilder() >>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c) >>> # Add condition strings to check in messages >>> builder.add_edge(agent_a, agent_b, condition=\"big\") >>> builder.add_edge(agent_a, agent_c, condition=\"small\") Example — Loop: A → B → A or B → C:>>> builder = GraphBuilder() >>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c) >>> builder.add_edge(agent_a, agent_b) >> # Add a loop back to agent A >>> builder.add_edge(agent_b, agent_a, condition=lambda msg: \"loop\" in msg.to_model_text()) >>> # Add exit condition to break the loop >>> builder.add_edge(agent_b, agent_c, condition=lambda msg: \"loop\" not in msg.to_model_text()) Example — Loop with multiple paths to the same node: A → B → C → B:>>> builder = GraphBuilder() >>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c) >>> builder.add_edge(agent_a, agent_b) >>> builder.add_edge(agent_b, agent_c) >>> builder.add_edge(agent_c, agent_b, activation_group=\"loop_back\") Example — Loop with multiple paths to the same node with any activation condition: A → B → (C1, C2) → B → E(exit):>>> builder = GraphBuilder() >>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c1).add_node(agent_c2).add_node(agent_e) >>> builder.add_edge(agent_a, agent_b) >>> builder.add_edge(agent_b, agent_c1) >>> builder.add_edge(agent_b, agent_c2) >>> builder.add_edge(agent_b, agent_e, condition=\"exit\") >>> builder.add_edge(agent_c1, agent_b, activation_group=\"loop_back_group\", activation_condition=\"any\") >>> builder.add_edge(agent_c2, agent_b, activation_group=\"loop_back_group\", activation_condition=\"any\") add_node(agent: ChatAgent, activation: Literal['all', 'any'] = 'all') → DiGraphBuilder[source]# Add a node to the graph and register its agent. add_edge(source: str | ChatAgent, target: str | ChatAgent, condition: str | Callable[[BaseChatMessage], bool] | None = None, activation_group: str | None = None, activation_condition: Literal['all', 'any'] | None = None) → DiGraphBuilder[source]# Add a directed edge from source to target, optionally with a condition. Parameters: source – Source node (agent name or agent object) target – Target node (agent name or agent object) condition – Optional condition for edge activation. If string, activates when substring is found in message. If callable, activates when function returns True for the message. Returns: Self for method chaining Raises: ValueError – If source or target node doesn’t exist in the builder add_conditional_edges(source: str | ChatAgent, condition_to_target: Dict[str, str | ChatAgent]) → DiGraphBuilder[source]# Add multiple conditional edges from a source node based on keyword checks. Warning This method interface will be changed in the future to support callable conditions. Please use add_edge if you need to specify custom conditions. Parameters: source – Source node (agent name or agent object) condition_to_target – Mapping from condition strings to target nodes Each key is a keyword that will be checked in the message content Each value is the target node to activate when condition is met For each key (keyword), a lambda will be created that checks if the keyword is in the message text. Returns: Self for method chaining set_entry_point(name: str | ChatAgent) → DiGraphBuilder[source]# Set the default start node of the graph. build() → DiGraph[source]# Build and validate the DiGraph. get_participants() → list[ChatAgent][source]# Return the list of agents in the builder, in insertion order. pydantic model DiGraph[source]# Bases: BaseModel Defines a directed graph structure with nodes and edges. GraphFlow uses this to determine execution order and conditions. Warning This is an experimental feature, and the API will change in the future releases. Show JSON schema{ \"title\": \"DiGraph\", \"type\": \"object\", \"properties\": { \"nodes\": { \"default\": null, \"title\": \"Nodes\" }, \"default_start_node\": { \"anyOf\": [ { \"type\": \"string\" }, { \"type\": \"null\" } ], \"default\": null, \"title\": \"Default Start Node\" } } } Fields: default_start_node (str | None) nodes (Dict[str, autogen_agentchat.teams._group_chat._graph._digraph_group_chat.DiGraphNode]) field nodes: Dict[str, DiGraphNode] [Required]# field default_start_node: str | None = None# get_parents() → Dict[str, List[str]][source]# Compute a mapping of each node to its parent nodes. get_start_nodes() → Set[str][source]# Return the nodes that have no incoming edges (entry points). get_leaf_nodes() → Set[str][source]# Return nodes that have no outgoing edges (final output nodes). has_cycles_with_exit() → bool[source]# Check if the graph has any cycles and validate that each cycle has at least one conditional edge. Returns: bool – True if there is at least one cycle and all cycles have an exit condition. False if there are no cycles. Raises: ValueError – If there is a cycle without any conditional edge. get_has_cycles() → bool[source]# Indicates if the graph has at least one cycle (with valid exit conditions). graph_validate() → None[source]# Validate graph structure and execution rules. get_remaining_map() → Dict[str, Dict[str, int]][source]# Get the remaining map that tracks how many edges point to each target node with each activation group. Returns: Dictionary mapping target nodes to their activation groups and remaining counts model_post_init(context: Any, /) → None# This function is meant to behave like a BaseModel method to initialise private attributes. It takes context as an argument since that’s what pydantic-core passes when calling it. Parameters: self – The BaseModel instance. context – The context. pydantic model DiGraphNode[source]# Bases: BaseModel Represents a node (agent) in a DiGraph, with its outgoing edges and activation type. Warning This is an experimental feature, and the API will change in the future releases. Show JSON schema{ \"title\": \"DiGraphNode\", \"type\": \"object\", \"properties\": { \"name\": { \"title\": \"Name\", \"type\": \"string\" }, \"edges\": { \"default\": null, \"title\": \"Edges\" }, \"activation\": { \"default\": \"all\", \"enum\": [ \"all\", \"any\" ], \"title\": \"Activation\", \"type\": \"string\" } }, \"required\": [ \"name\" ] } Fields: activation (Literal['all', 'any']) edges (List[autogen_agentchat.teams._group_chat._graph._digraph_group_chat.DiGraphEdge]) name (str) field name: str [Required]# field edges: List[DiGraphEdge] = []# field activation: Literal['all', 'any'] = 'all'# pydantic model DiGraphEdge[source]# Bases: BaseModel Represents a directed edge in a DiGraph, with an optional execution condition. Warning This is an experimental feature, and the API will change in the future releases. Warning If the condition is a callable, it will not be serialized in the model. Show JSON schema{ \"title\": \"DiGraphEdge\", \"type\": \"object\", \"properties\": { \"target\": { \"title\": \"Target\", \"type\": \"string\" }, \"condition\": { \"anyOf\": [ { \"type\": \"string\" }, { \"type\": \"null\" } ], \"default\": null, \"title\": \"Condition\" }, \"condition_function\": { \"default\": null, \"title\": \"Condition Function\" }, \"activation_group\": { \"default\": \"\", \"title\": \"Activation Group\", \"type\": \"string\" }, \"activation_condition\": { \"default\": \"all\", \"enum\": [ \"all\", \"any\" ], \"title\": \"Activation Condition\", \"type\": \"string\" } }, \"required\": [ \"target\" ] } Fields: activation_condition (Literal['all', 'any']) activation_group (str) condition (str | Callable[[autogen_agentchat.messages.BaseChatMessage], bool] | None) condition_function (Callable[[autogen_agentchat.messages.BaseChatMessage], bool] | None) target (str) Validators: _validate_condition » all fields field target: str [Required]# Validated by: _validate_condition field condition: str | Callable[[BaseChatMessage], bool] | None = None# (Experimental) Condition to execute this edge. If None, the edge is unconditional. If a string, the edge is conditional on the presence of that string in the last agent chat message. If a callable, the edge is conditional on the callable returning True when given the last message. Validated by: _validate_condition field condition_function: Callable[[BaseChatMessage], bool] | None = None# Validated by: _validate_condition field activation_group: str = ''# Group identifier for forward dependencies. When multiple edges point to the same target node, they are grouped by this field. This allows distinguishing between different cycles or dependency patterns. Example: In a graph containing a cycle like A->B->C->B, the two edges pointing to B (A->B and C->B) can be in different activation groups to control how B is activated. Defaults to the target node name if not specified. Validated by: _validate_condition field activation_condition: Literal['all', 'any'] = 'all'# Determines how forward dependencies within the same activation_group are evaluated. “all”: All edges in this activation group must be satisfied before the target node can execute “any”: Any single edge in this activation group being satisfied allows the target node to execute This is used to handle complex dependency patterns in cyclic graphs where multiple paths can lead to the same target node. Validated by: _validate_condition check_condition(message: BaseChatMessage) → bool[source]# Check if the edge condition is satisfied for the given message. Parameters: message – The message to check the condition against. Returns: True if condition is satisfied (None condition always returns True) False otherwise. class GraphFlow(participants: List[ChatAgent], graph: DiGraph, *, name: str | None = None, description: str | None = None, termination_condition: TerminationCondition | None = None, max_turns: int | None = None, runtime: AgentRuntime | None = None, custom_message_types: List[type[BaseAgentEvent | BaseChatMessage]] | None = None)[source]# Bases: BaseGroupChat, Component[GraphFlowConfig] A team that runs a group chat following a Directed Graph execution pattern. Warning This is an experimental feature, and the API will change in the future releases. This group chat executes agents based on a directed graph (DiGraph) structure, allowing complex workflows such as sequential execution, parallel fan-out, conditional branching, join patterns, and loops with explicit exit conditions. The execution order is determined by the edges defined in the DiGraph. Each node in the graph corresponds to an agent, and edges define the flow of messages between agents. Nodes can be configured to activate when: All parent nodes have completed (activation=”all”) → default Any parent node completes (activation=”any”) Conditional branching is supported using edge conditions, where the next agent(s) are selected based on content in the chat history. Loops are permitted as long as there is a condition that eventually exits the loop. Note Use the DiGraphBuilder class to create a DiGraph easily. It provides a fluent API for adding nodes and edges, setting entry points, and validating the graph structure. See the DiGraphBuilder documentation for more details. The GraphFlow class is designed to be used with the DiGraphBuilder for creating complex workflows. Warning When using callable conditions in edges, they will not be serialized when calling dump_component(). This will be addressed in future releases. Parameters: participants (List[ChatAgent]) – The participants in the group chat. termination_condition (TerminationCondition, optional) – Termination condition for the chat. max_turns (int, optional) – Maximum number of turns before forcing termination. graph (DiGraph) – Directed execution graph defining node flow and conditions. Raises: ValueError – If participant names are not unique, or if graph validation fails (e.g., cycles without exit). Examples Sequential Flow: A → B → C import asyncio from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.conditions import MaxMessageTermination from autogen_agentchat.teams import DiGraphBuilder, GraphFlow from autogen_ext.models.openai import OpenAIChatCompletionClient async def main(): # Initialize agents with OpenAI model clients. model_client = OpenAIChatCompletionClient(model=\"gpt-4.1-nano\") agent_a = AssistantAgent(\"A\", model_client=model_client, system_message=\"You are a helpful assistant.\") agent_b = AssistantAgent(\"B\", model_client=model_client, system_message=\"Translate input to Chinese.\") agent_c = AssistantAgent(\"C\", model_client=model_client, system_message=\"Translate input to English.\") # Create a directed graph with sequential flow A -> B -> C. builder = DiGraphBuilder() builder.add_node(agent_a).add_node(agent_b).add_node(agent_c) builder.add_edge(agent_a, agent_b).add_edge(agent_b, agent_c) graph = builder.build() # Create a GraphFlow team with the directed graph. team = GraphFlow( participants=[agent_a, agent_b, agent_c], graph=graph, termination_condition=MaxMessageTermination(5), ) # Run the team and print the events. async for event in team.run_stream(task=\"Write a short story about a cat.\"): print(event) asyncio.run(main()) Parallel Fan-out: A → (B, C) import asyncio from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.conditions import MaxMessageTermination from autogen_agentchat.teams import DiGraphBuilder, GraphFlow from autogen_ext.models.openai import OpenAIChatCompletionClient async def main(): # Initialize agents with OpenAI model clients. model_client = OpenAIChatCompletionClient(model=\"gpt-4.1-nano\") agent_a = AssistantAgent(\"A\", model_client=model_client, system_message=\"You are a helpful assistant.\") agent_b = AssistantAgent(\"B\", model_client=model_client, system_message=\"Translate input to Chinese.\") agent_c = AssistantAgent(\"C\", model_client=model_client, system_message=\"Translate input to Japanese.\") # Create a directed graph with fan-out flow A -> (B, C). builder = DiGraphBuilder() builder.add_node(agent_a).add_node(agent_b).add_node(agent_c) builder.add_edge(agent_a, agent_b).add_edge(agent_a, agent_c) graph = builder.build() # Create a GraphFlow team with the directed graph. team = GraphFlow( participants=[agent_a, agent_b, agent_c], graph=graph, termination_condition=MaxMessageTermination(5), ) # Run the team and print the events. async for event in team.run_stream(task=\"Write a short story about a cat.\"): print(event) asyncio.run(main()) Conditional Branching: A → B (if ‘yes’) or C (otherwise) import asyncio from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.conditions import MaxMessageTermination from autogen_agentchat.teams import DiGraphBuilder, GraphFlow from autogen_ext.models.openai import OpenAIChatCompletionClient async def main(): # Initialize agents with OpenAI model clients. model_client = OpenAIChatCompletionClient(model=\"gpt-4.1-nano\") agent_a = AssistantAgent( \"A\", model_client=model_client, system_message=\"Detect if the input is in Chinese. If it is, say 'yes', else say 'no', and nothing else.\", ) agent_b = AssistantAgent(\"B\", model_client=model_client, system_message=\"Translate input to English.\") agent_c = AssistantAgent(\"C\", model_client=model_client, system_message=\"Translate input to Chinese.\") # Create a directed graph with conditional branching flow A -> B (\"yes\"), A -> C (otherwise). builder = DiGraphBuilder() builder.add_node(agent_a).add_node(agent_b).add_node(agent_c) # Create conditions as callables that check the message content. builder.add_edge(agent_a, agent_b, condition=lambda msg: \"yes\" in msg.to_model_text()) builder.add_edge(agent_a, agent_c, condition=lambda msg: \"yes\" not in msg.to_model_text()) graph = builder.build() # Create a GraphFlow team with the directed graph. team = GraphFlow( participants=[agent_a, agent_b, agent_c], graph=graph, termination_condition=MaxMessageTermination(5), ) # Run the team and print the events. async for event in team.run_stream(task=\"AutoGen is a framework for building AI agents.\"): print(event) asyncio.run(main()) Loop with exit condition: A → B → C (if ‘APPROVE’) or A (otherwise) import asyncio from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.conditions import MaxMessageTermination from autogen_agentchat.teams import DiGraphBuilder, GraphFlow from autogen_ext.models.openai import OpenAIChatCompletionClient async def main(): # Initialize agents with OpenAI model clients. model_client = OpenAIChatCompletionClient(model=\"gpt-4.1\") agent_a = AssistantAgent( \"A\", model_client=model_client, system_message=\"You are a helpful assistant.\", ) agent_b = AssistantAgent( \"B\", model_client=model_client, system_message=\"Provide feedback on the input, if your feedback has been addressed, \" \"say 'APPROVE', otherwise provide a reason for rejection.\", ) agent_c = AssistantAgent( \"C\", model_client=model_client, system_message=\"Translate the final product to Korean.\" ) # Create a loop graph with conditional exit: A -> B -> C (\"APPROVE\"), B -> A (otherwise). builder = DiGraphBuilder() builder.add_node(agent_a).add_node(agent_b).add_node(agent_c) builder.add_edge(agent_a, agent_b) # Create conditional edges using strings builder.add_edge(agent_b, agent_c, condition=lambda msg: \"APPROVE\" in msg.to_model_text()) builder.add_edge(agent_b, agent_a, condition=lambda msg: \"APPROVE\" not in msg.to_model_text()) builder.set_entry_point(agent_a) graph = builder.build() # Create a GraphFlow team with the directed graph. team = GraphFlow( participants=[agent_a, agent_b, agent_c], graph=graph, termination_condition=MaxMessageTermination(20), # Max 20 messages to avoid infinite loop. ) # Run the team and print the events. async for event in team.run_stream(task=\"Write a short poem about AI Agents.\"): print(event) asyncio.run(main()) component_config_schema# alias of GraphFlowConfig component_provider_override: ClassVar[str | None] = 'autogen_agentchat.teams.GraphFlow'# Override the provider string for the component. This should be used to prevent internal module names being a part of the module name. DEFAULT_NAME = 'GraphFlow'# DEFAULT_DESCRIPTION = 'A team of agents'# previous autogen_agentchat.state next autogen_agentchat.tools On this page BaseGroupChat BaseGroupChat.component_type BaseGroupChat.name BaseGroupChat.description BaseGroupChat.run() BaseGroupChat.run_stream() BaseGroupChat.reset() BaseGroupChat.pause() BaseGroupChat.resume() BaseGroupChat.save_state() BaseGroupChat.load_state() RoundRobinGroupChat RoundRobinGroupChat.component_config_schema RoundRobinGroupChat.component_provider_override RoundRobinGroupChat.DEFAULT_NAME RoundRobinGroupChat.DEFAULT_DESCRIPTION RoundRobinGroupChat._to_config() RoundRobinGroupChat._from_config() SelectorGroupChat SelectorGroupChat.component_config_schema SelectorGroupChat.component_provider_override SelectorGroupChat.DEFAULT_NAME SelectorGroupChat.DEFAULT_DESCRIPTION SelectorGroupChat._to_config() SelectorGroupChat._from_config() Swarm Swarm.component_config_schema Swarm.component_provider_override Swarm.DEFAULT_NAME Swarm.DEFAULT_DESCRIPTION Swarm._to_config() Swarm._from_config() MagenticOneGroupChat MagenticOneGroupChat.component_config_schema MagenticOneGroupChat.component_provider_override MagenticOneGroupChat.DEFAULT_NAME MagenticOneGroupChat.DEFAULT_DESCRIPTION MagenticOneGroupChat._to_config() MagenticOneGroupChat._from_config() DiGraphBuilder DiGraphBuilder.add_node() DiGraphBuilder.add_edge() DiGraphBuilder.add_conditional_edges() DiGraphBuilder.set_entry_point() DiGraphBuilder.build() DiGraphBuilder.get_participants() DiGraph DiGraph.nodes DiGraph.default_start_node DiGraph.get_parents() DiGraph.get_start_nodes() DiGraph.get_leaf_nodes() DiGraph.has_cycles_with_exit() DiGraph.get_has_cycles() DiGraph.graph_validate() DiGraph.get_remaining_map() DiGraph.model_post_init() DiGraphNode DiGraphNode.name DiGraphNode.edges DiGraphNode.activation DiGraphEdge DiGraphEdge.target DiGraphEdge.condition DiGraphEdge.condition_function DiGraphEdge.activation_group DiGraphEdge.activation_condition DiGraphEdge.check_condition() GraphFlow GraphFlow.component_config_schema GraphFlow.component_provider_override GraphFlow.DEFAULT_NAME GraphFlow.DEFAULT_DESCRIPTION Edit on GitHub Show Source",
      "code": "Team"
    },
    {
      "description": "API Reference autogen_agentchat.teams autogen_agentchat.teams# This module provides implementation of various pre-defined multi-agent teams. Each team inherits from the BaseGroupChat class. class BaseGroupChat(name: str, description: str, participants: List[ChatAgent | Team], group_chat_manager_name: str, group_chat_manager_class: type[SequentialRoutedAgent], termination_condition: TerminationCondition | None = None, max_turns: int | None = None, runtime: AgentRuntime | None = None, custom_message_types: List[type[BaseAgentEvent | BaseChatMessage]] | None = None, emit_team_events: bool = False)[source]# Bases: Team, ABC, ComponentBase[BaseModel] The base class for group chat teams. In a group chat team, participants share context by publishing their messages to all other participants. If an ChatAgent is a participant, the BaseChatMessage from the agent response’s chat_message will be published to other participants in the group chat. If a Team is a participant, the BaseChatMessage from the team result’ messages will be published to other participants in the group chat. To implement a group chat team, first create a subclass of BaseGroupChatManager and then create a subclass of BaseGroupChat that uses the group chat manager. This base class provides the mapping between the agents of the AgentChat API and the agent runtime of the Core API, and handles high-level features like running, pausing, resuming, and resetting the team. component_type: ClassVar[ComponentType] = 'team'# The logical type of the component. property name: str# The name of the group chat team. property description: str# A description of the group chat team. async run(*, task: str | BaseChatMessage | Sequence[BaseChatMessage] | None = None, cancellation_token: CancellationToken | None = None, output_task_messages: bool = True) → TaskResult[source]# Run the team and return the result. The base implementation uses run_stream() to run the team and then returns the final result. Once the team is stopped, the termination condition is reset. Parameters: task (str | BaseChatMessage | Sequence[BaseChatMessage] | None) – The task to run the team with. Can be a string, a single BaseChatMessage , or a list of BaseChatMessage. cancellation_token (CancellationToken | None) – The cancellation token to kill the task immediately. Setting the cancellation token potentially put the team in an inconsistent state, and it may not reset the termination condition. To gracefully stop the team, use ExternalTermination instead. Returns: result – The result of the task as TaskResult. The result contains the messages produced by the team and the stop reason. Example using the RoundRobinGroupChat team: import asyncio from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.conditions import MaxMessageTermination from autogen_agentchat.teams import RoundRobinGroupChat from autogen_ext.models.openai import OpenAIChatCompletionClient async def main() -> None: model_client = OpenAIChatCompletionClient(model=\"gpt-4o\") agent1 = AssistantAgent(\"Assistant1\", model_client=model_client) agent2 = AssistantAgent(\"Assistant2\", model_client=model_client) termination = MaxMessageTermination(3) team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination) result = await team.run(task=\"Count from 1 to 10, respond one at a time.\") print(result) # Run the team again without a task to continue the previous task. result = await team.run() print(result) asyncio.run(main()) Example using the CancellationToken to cancel the task: import asyncio from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.conditions import MaxMessageTermination from autogen_agentchat.teams import RoundRobinGroupChat from autogen_core import CancellationToken from autogen_ext.models.openai import OpenAIChatCompletionClient async def main() -> None: model_client = OpenAIChatCompletionClient(model=\"gpt-4o\") agent1 = AssistantAgent(\"Assistant1\", model_client=model_client) agent2 = AssistantAgent(\"Assistant2\", model_client=model_client) termination = MaxMessageTermination(3) team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination) cancellation_token = CancellationToken() # Create a task to run the team in the background. run_task = asyncio.create_task( team.run( task=\"Count from 1 to 10, respond one at a time.\", cancellation_token=cancellation_token, ) ) # Wait for 1 second and then cancel the task. await asyncio.sleep(1) cancellation_token.cancel() # This will raise a cancellation error. await run_task asyncio.run(main()) async run_stream(*, task: str | BaseChatMessage | Sequence[BaseChatMessage] | None = None, cancellation_token: CancellationToken | None = None, output_task_messages: bool = True) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | TaskResult, None][source]# Run the team and produces a stream of messages and the final result of the type TaskResult as the last item in the stream. Once the team is stopped, the termination condition is reset. Note If an agent produces ModelClientStreamingChunkEvent, the message will be yielded in the stream but it will not be included in the messages. Parameters: task (str | BaseChatMessage | Sequence[BaseChatMessage] | None) – The task to run the team with. Can be a string, a single BaseChatMessage , or a list of BaseChatMessage. cancellation_token (CancellationToken | None) – The cancellation token to kill the task immediately. Setting the cancellation token potentially put the team in an inconsistent state, and it may not reset the termination condition. To gracefully stop the team, use ExternalTermination instead. output_task_messages (bool) – Whether to include task messages in the output stream. Defaults to True for backward compatibility. Returns: stream – an AsyncGenerator that yields BaseAgentEvent, BaseChatMessage, and the final result TaskResult as the last item in the stream. Example using the RoundRobinGroupChat team: import asyncio from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.conditions import MaxMessageTermination from autogen_agentchat.teams import RoundRobinGroupChat from autogen_ext.models.openai import OpenAIChatCompletionClient async def main() -> None: model_client = OpenAIChatCompletionClient(model=\"gpt-4o\") agent1 = AssistantAgent(\"Assistant1\", model_client=model_client) agent2 = AssistantAgent(\"Assistant2\", model_client=model_client) termination = MaxMessageTermination(3) team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination) stream = team.run_stream(task=\"Count from 1 to 10, respond one at a time.\") async for message in stream: print(message) # Run the team again without a task to continue the previous task. stream = team.run_stream() async for message in stream: print(message) asyncio.run(main()) Example using the CancellationToken to cancel the task: import asyncio from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.conditions import MaxMessageTermination from autogen_agentchat.ui import Console from autogen_agentchat.teams import RoundRobinGroupChat from autogen_core import CancellationToken from autogen_ext.models.openai import OpenAIChatCompletionClient async def main() -> None: model_client = OpenAIChatCompletionClient(model=\"gpt-4o\") agent1 = AssistantAgent(\"Assistant1\", model_client=model_client) agent2 = AssistantAgent(\"Assistant2\", model_client=model_client) termination = MaxMessageTermination(3) team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination) cancellation_token = CancellationToken() # Create a task to run the team in the background. run_task = asyncio.create_task( Console( team.run_stream( task=\"Count from 1 to 10, respond one at a time.\", cancellation_token=cancellation_token, ) ) ) # Wait for 1 second and then cancel the task. await asyncio.sleep(1) cancellation_token.cancel() # This will raise a cancellation error. await run_task asyncio.run(main()) async reset() → None[source]# Reset the team and its participants to their initial state. The team must be stopped before it can be reset. Raises: RuntimeError – If the team has not been initialized or is currently running. Example using the RoundRobinGroupChat team: import asyncio from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.conditions import MaxMessageTermination from autogen_agentchat.teams import RoundRobinGroupChat from autogen_ext.models.openai import OpenAIChatCompletionClient async def main() -> None: model_client = OpenAIChatCompletionClient(model=\"gpt-4o\") agent1 = AssistantAgent(\"Assistant1\", model_client=model_client) agent2 = AssistantAgent(\"Assistant2\", model_client=model_client) termination = MaxMessageTermination(3) team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination) stream = team.run_stream(task=\"Count from 1 to 10, respond one at a time.\") async for message in stream: print(message) # Reset the team. await team.reset() stream = team.run_stream(task=\"Count from 1 to 10, respond one at a time.\") async for message in stream: print(message) asyncio.run(main()) async pause() → None[source]# Pause its participants when the team is running by calling their on_pause() method via direct RPC calls. Attention This is an experimental feature introduced in v0.4.9 and may subject to change or removal in the future. The team must be initialized before it can be paused. Different from termination, pausing the team does not cause the run() or run_stream() method to return. It calls the on_pause() method on each participant, and if the participant does not implement the method, it will be a no-op. Note It is the responsibility of the agent class to handle the pause and ensure that the agent can be resumed later. Make sure to implement the on_pause() method in your agent class for custom pause behavior. By default, the agent will not do anything when called. Raises: RuntimeError – If the team has not been initialized. Exceptions from the participants when calling their implementations of on_pause are propagated to this method and raised. async resume() → None[source]# Resume its participants when the team is running and paused by calling their on_resume() method via direct RPC calls. Attention This is an experimental feature introduced in v0.4.9 and may subject to change or removal in the future. The team must be initialized before it can be resumed. Different from termination and restart with a new task, resuming the team does not cause the run() or run_stream() method to return. It calls the on_resume() method on each participant, and if the participant does not implement the method, it will be a no-op. Note It is the responsibility of the agent class to handle the resume and ensure that the agent continues from where it was paused. Make sure to implement the on_resume() method in your agent class for custom resume behavior. Raises: RuntimeError – If the team has not been initialized. Exceptions from the participants when calling their implementations of on_resume method are propagated to this method and raised. async save_state() → Mapping[str, Any][source]# Save the state of the group chat team. The state is saved by calling the agent_save_state() method on each participant and the group chat manager with their internal agent ID. The state is returned as a nested dictionary: a dictionary with key agent_states, which is a dictionary the agent names as keys and the state as values. { \"agent_states\": { \"agent1\": ..., \"agent2\": ..., \"RoundRobinGroupChatManager\": ... } } Note Starting v0.4.9, the state is using the agent name as the key instead of the agent ID, and the team_id field is removed from the state. This is to allow the state to be portable across different teams and runtimes. States saved with the old format may not be compatible with the new format in the future. Caution When calling save_state() on a team while it is running, the state may not be consistent and may result in an unexpected state. It is recommended to call this method when the team is not running or after it is stopped. async load_state(state: Mapping[str, Any]) → None[source]# Load an external state and overwrite the current state of the group chat team. The state is loaded by calling the agent_load_state() method on each participant and the group chat manager with their internal agent ID. See save_state() for the expected format of the state. class RoundRobinGroupChat(participants: List[ChatAgent | Team], *, name: str | None = None, description: str | None = None, termination_condition: TerminationCondition | None = None, max_turns: int | None = None, runtime: AgentRuntime | None = None, custom_message_types: List[type[BaseAgentEvent | BaseChatMessage]] | None = None, emit_team_events: bool = False)[source]# Bases: BaseGroupChat, Component[RoundRobinGroupChatConfig] A team that runs a group chat with participants taking turns in a round-robin fashion to publish a message to all. If an ChatAgent is a participant, the BaseChatMessage from the agent response’s chat_message will be published to other participants in the group chat. If a Team is a participant, the BaseChatMessage from the team result’ messages will be published to other participants in the group chat. If a single participant is in the team, the participant will be the only speaker. Parameters: participants (List[ChatAgent | Team]) – The participants in the group chat. name (str | None, optional) – The name of the group chat, using DEFAULT_NAME if not provided. The name is used by a parent team to identify this group chat so it must be unique within the parent team. description (str | None, optional) – The description of the group chat, using DEFAULT_DESCRIPTION if not provided. termination_condition (TerminationCondition, optional) – The termination condition for the group chat. Defaults to None. Without a termination condition, the group chat will run indefinitely. max_turns (int, optional) – The maximum number of turns in the group chat before stopping. Defaults to None, meaning no limit. custom_message_types (List[type[BaseAgentEvent | BaseChatMessage]], optional) – A list of custom message types that will be used in the group chat. If you are using custom message types or your agents produces custom message types, you need to specify them here. Make sure your custom message types are subclasses of BaseAgentEvent or BaseChatMessage. emit_team_events (bool, optional) – Whether to emit team events through BaseGroupChat.run_stream(). Defaults to False. Raises: ValueError – If no participants are provided or if participant names are not unique. Examples A team with one participant with tools: import asyncio from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.teams import RoundRobinGroupChat from autogen_agentchat.conditions import TextMentionTermination from autogen_agentchat.ui import Console async def main() -> None: model_client = OpenAIChatCompletionClient(model=\"gpt-4o\") async def get_weather(location: str) -> str: return f\"The weather in {location} is sunny.\" assistant = AssistantAgent( \"Assistant\", model_client=model_client, tools=[get_weather], ) termination = TextMentionTermination(\"TERMINATE\") team = RoundRobinGroupChat([assistant], termination_condition=termination) await Console(team.run_stream(task=\"What's the weather in New York?\")) asyncio.run(main()) A team with multiple participants: import asyncio from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.teams import RoundRobinGroupChat from autogen_agentchat.conditions import TextMentionTermination from autogen_agentchat.ui import Console async def main() -> None: model_client = OpenAIChatCompletionClient(model=\"gpt-4o\") agent1 = AssistantAgent(\"Assistant1\", model_client=model_client) agent2 = AssistantAgent(\"Assistant2\", model_client=model_client) termination = TextMentionTermination(\"TERMINATE\") team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination) await Console(team.run_stream(task=\"Tell me some jokes.\")) asyncio.run(main()) A team of user proxy and a nested team of writer and reviewer agents: import asyncio from autogen_agentchat.agents import UserProxyAgent, AssistantAgent from autogen_agentchat.conditions import TextMentionTermination, MaxMessageTermination from autogen_agentchat.teams import RoundRobinGroupChat from autogen_agentchat.ui import Console from autogen_ext.models.openai import OpenAIChatCompletionClient async def main() -> None: model_client = OpenAIChatCompletionClient(model=\"gpt-4.1-nano\") writer = AssistantAgent( \"writer\", model_client=model_client, system_message=\"You are a writer.\", model_client_stream=True ) reviewer = AssistantAgent( \"reviewer\", model_client=model_client, system_message=\"Provide feedback to the input and suggest improvements.\", model_client_stream=True, ) # NOTE: you can skip input by pressing Enter. user_proxy = UserProxyAgent(\"user_proxy\") # Maximum 1 round of review and revision. inner_termination = MaxMessageTermination(max_messages=4) # The outter-loop termination condition that will terminate the team when the user types \"exit\". outter_termination = TextMentionTermination(\"exit\", sources=[\"user_proxy\"]) team = RoundRobinGroupChat( [ # For each turn, the writer writes a summary and the reviewer reviews it. RoundRobinGroupChat([writer, reviewer], termination_condition=inner_termination), # The user proxy gets user input once the writer and reviewer have finished their actions. user_proxy, ], termination_condition=outter_termination, ) # Start the team and wait for it to terminate. await Console(team.run_stream(task=\"Write a short essay about the impact of AI on society.\")) asyncio.run(main()) component_config_schema# alias of RoundRobinGroupChatConfig component_provider_override: ClassVar[str | None] = 'autogen_agentchat.teams.RoundRobinGroupChat'# Override the provider string for the component. This should be used to prevent internal module names being a part of the module name. DEFAULT_NAME = 'RoundRobinGroupChat'# DEFAULT_DESCRIPTION = 'A team of agents.'# _to_config() → RoundRobinGroupChatConfig[source]# Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance. Returns: T – The configuration of the component. classmethod _from_config(config: RoundRobinGroupChatConfig) → Self[source]# Create a new instance of the component from a configuration object. Parameters: config (T) – The configuration object. Returns: Self – The new instance of the component. class SelectorGroupChat(participants: List[ChatAgent | Team], model_client: ChatCompletionClient, *, name: str | None = None, description: str | None = None, termination_condition: TerminationCondition | None = None, max_turns: int | None = None, runtime: AgentRuntime | None = None, selector_prompt: str = 'You are in a role play game. The following roles are available:\\n{roles}.\\nRead the following conversation. Then select the next role from {participants} to play. Only return the role.\\n\\n{history}\\n\\nRead the above conversation. Then select the next role from {participants} to play. Only return the role.\\n', allow_repeated_speaker: bool = False, max_selector_attempts: int = 3, selector_func: Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], str | None] | Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], Awaitable[str | None]] | None = None, candidate_func: Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], List[str]] | Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], Awaitable[List[str]]] | None = None, custom_message_types: List[type[BaseAgentEvent | BaseChatMessage]] | None = None, emit_team_events: bool = False, model_client_streaming: bool = False, model_context: ChatCompletionContext | None = None)[source]# Bases: BaseGroupChat, Component[SelectorGroupChatConfig] A group chat team that have participants takes turn to publish a message to all, using a ChatCompletion model to select the next speaker after each message. If an ChatAgent is a participant, the BaseChatMessage from the agent response’s chat_message will be published to other participants in the group chat. If a Team is a participant, the BaseChatMessage from the team result’ messages will be published to other participants in the group chat. Parameters: participants (List[ChatAgent | Team]) – The participants in the group chat, must have unique names and at least two participants. model_client (ChatCompletionClient) – The ChatCompletion model client used to select the next speaker. name (str | None, optional) – The name of the group chat, using DEFAULT_NAME if not provided. The name is used by a parent team to identify this group chat so it must be unique within the parent team. description (str | None, optional) – The description of the group chat, using DEFAULT_DESCRIPTION if not provided. termination_condition (TerminationCondition, optional) – The termination condition for the group chat. Defaults to None. Without a termination condition, the group chat will run indefinitely. max_turns (int, optional) – The maximum number of turns in the group chat before stopping. Defaults to None, meaning no limit. selector_prompt (str, optional) – The prompt template to use for selecting the next speaker. Available fields: ‘{roles}’, ‘{participants}’, and ‘{history}’. {participants} is the names of candidates for selection. The format is [“<name1>”, “<name2>”, …]. {roles} is a newline-separated list of names and descriptions of the candidate agents. The format for each line is: “<name> : <description>”. {history} is the conversation history formatted as a double newline separated of names and message content. The format for each message is: “<name> : <message content>”. allow_repeated_speaker (bool, optional) – Whether to include the previous speaker in the list of candidates to be selected for the next turn. Defaults to False. The model may still select the previous speaker – a warning will be logged if this happens. max_selector_attempts (int, optional) – The maximum number of attempts to select a speaker using the model. Defaults to 3. If the model fails to select a speaker after the maximum number of attempts, the previous speaker will be used if available, otherwise the first participant will be used. selector_func (Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], str | None], Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], Awaitable[str | None]], optional) – A custom selector function that takes the conversation history and returns the name of the next speaker. If provided, this function will be used to override the model to select the next speaker. If the function returns None, the model will be used to select the next speaker. NOTE: selector_func is not serializable and will be ignored during serialization and deserialization process. candidate_func (Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], List[str]], Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], Awaitable[List[str]]], optional) – A custom function that takes the conversation history and returns a filtered list of candidates for the next speaker selection using model. If the function returns an empty list or None, SelectorGroupChat will raise a ValueError. This function is only used if selector_func is not set. The allow_repeated_speaker will be ignored if set. custom_message_types (List[type[BaseAgentEvent | BaseChatMessage]], optional) – A list of custom message types that will be used in the group chat. If you are using custom message types or your agents produces custom message types, you need to specify them here. Make sure your custom message types are subclasses of BaseAgentEvent or BaseChatMessage. emit_team_events (bool, optional) – Whether to emit team events through BaseGroupChat.run_stream(). Defaults to False. model_client_streaming (bool, optional) – Whether to use streaming for the model client. (This is useful for reasoning models like QwQ). Defaults to False. model_context (ChatCompletionContext | None, optional) – The model context for storing and retrieving LLMMessage. It can be preloaded with initial messages. Messages stored in model context will be used for speaker selection. The initial messages will be cleared when the team is reset. Raises: ValueError – If the number of participants is less than two or if the selector prompt is invalid. Examples: A team with multiple participants: import asyncio from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.teams import SelectorGroupChat from autogen_agentchat.conditions import TextMentionTermination from autogen_agentchat.ui import Console async def main() -> None: model_client = OpenAIChatCompletionClient(model=\"gpt-4o\") async def lookup_hotel(location: str) -> str: return f\"Here are some hotels in {location}: hotel1, hotel2, hotel3.\" async def lookup_flight(origin: str, destination: str) -> str: return f\"Here are some flights from {origin} to {destination}: flight1, flight2, flight3.\" async def book_trip() -> str: return \"Your trip is booked!\" travel_advisor = AssistantAgent( \"Travel_Advisor\", model_client, tools=[book_trip], description=\"Helps with travel planning.\", ) hotel_agent = AssistantAgent( \"Hotel_Agent\", model_client, tools=[lookup_hotel], description=\"Helps with hotel booking.\", ) flight_agent = AssistantAgent( \"Flight_Agent\", model_client, tools=[lookup_flight], description=\"Helps with flight booking.\", ) termination = TextMentionTermination(\"TERMINATE\") team = SelectorGroupChat( [travel_advisor, hotel_agent, flight_agent], model_client=model_client, termination_condition=termination, ) await Console(team.run_stream(task=\"Book a 3-day trip to new york.\")) asyncio.run(main()) A team with a custom selector function: import asyncio from typing import Sequence from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.teams import SelectorGroupChat from autogen_agentchat.conditions import TextMentionTermination from autogen_agentchat.ui import Console from autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage async def main() -> None: model_client = OpenAIChatCompletionClient(model=\"gpt-4o\") def check_calculation(x: int, y: int, answer: int) -> str: if x + y == answer: return \"Correct!\" else: return \"Incorrect!\" agent1 = AssistantAgent( \"Agent1\", model_client, description=\"For calculation\", system_message=\"Calculate the sum of two numbers\", ) agent2 = AssistantAgent( \"Agent2\", model_client, tools=[check_calculation], description=\"For checking calculation\", system_message=\"Check the answer and respond with 'Correct!' or 'Incorrect!'\", ) def selector_func(messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> str | None: if len(messages) == 1 or messages[-1].to_text() == \"Incorrect!\": return \"Agent1\" if messages[-1].source == \"Agent1\": return \"Agent2\" return None termination = TextMentionTermination(\"Correct!\") team = SelectorGroupChat( [agent1, agent2], model_client=model_client, selector_func=selector_func, termination_condition=termination, ) await Console(team.run_stream(task=\"What is 1 + 1?\")) asyncio.run(main()) A team with custom model context: import asyncio from autogen_core.model_context import BufferedChatCompletionContext from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.conditions import TextMentionTermination from autogen_agentchat.teams import SelectorGroupChat from autogen_agentchat.ui import Console async def main() -> None: model_client = OpenAIChatCompletionClient(model=\"gpt-4o\") model_context = BufferedChatCompletionContext(buffer_size=5) async def lookup_hotel(location: str) -> str: return f\"Here are some hotels in {location}: hotel1, hotel2, hotel3.\" async def lookup_flight(origin: str, destination: str) -> str: return f\"Here are some flights from {origin} to {destination}: flight1, flight2, flight3.\" async def book_trip() -> str: return \"Your trip is booked!\" travel_advisor = AssistantAgent( \"Travel_Advisor\", model_client, tools=[book_trip], description=\"Helps with travel planning.\", ) hotel_agent = AssistantAgent( \"Hotel_Agent\", model_client, tools=[lookup_hotel], description=\"Helps with hotel booking.\", ) flight_agent = AssistantAgent( \"Flight_Agent\", model_client, tools=[lookup_flight], description=\"Helps with flight booking.\", ) termination = TextMentionTermination(\"TERMINATE\") team = SelectorGroupChat( [travel_advisor, hotel_agent, flight_agent], model_client=model_client, termination_condition=termination, model_context=model_context, ) await Console(team.run_stream(task=\"Book a 3-day trip to new york.\")) asyncio.run(main()) component_config_schema# alias of SelectorGroupChatConfig component_provider_override: ClassVar[str | None] = 'autogen_agentchat.teams.SelectorGroupChat'# Override the provider string for the component. This should be used to prevent internal module names being a part of the module name. DEFAULT_NAME = 'SelectorGroupChat'# DEFAULT_DESCRIPTION = 'A team of agents.'# _to_config() → SelectorGroupChatConfig[source]# Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance. Returns: T – The configuration of the component. classmethod _from_config(config: SelectorGroupChatConfig) → Self[source]# Create a new instance of the component from a configuration object. Parameters: config (T) – The configuration object. Returns: Self – The new instance of the component. class Swarm(participants: List[ChatAgent], *, name: str | None = None, description: str | None = None, termination_condition: TerminationCondition | None = None, max_turns: int | None = None, runtime: AgentRuntime | None = None, custom_message_types: List[type[BaseAgentEvent | BaseChatMessage]] | None = None, emit_team_events: bool = False)[source]# Bases: BaseGroupChat, Component[SwarmConfig] A group chat team that selects the next speaker based on handoff message only. The first participant in the list of participants is the initial speaker. The next speaker is selected based on the HandoffMessage message sent by the current speaker. If no handoff message is sent, the current speaker continues to be the speaker. Note Unlike RoundRobinGroupChat and SelectorGroupChat, this group chat team does not support inner teams as participants. Parameters: participants (List[ChatAgent]) – The agents participating in the group chat. The first agent in the list is the initial speaker. name (str | None, optional) – The name of the group chat, using DEFAULT_NAME if not provided. The name is used by a parent team to identify this group chat so it must be unique within the parent team. description (str | None, optional) – The description of the group chat, using DEFAULT_DESCRIPTION if not provided. termination_condition (TerminationCondition, optional) – The termination condition for the group chat. Defaults to None. Without a termination condition, the group chat will run indefinitely. max_turns (int, optional) – The maximum number of turns in the group chat before stopping. Defaults to None, meaning no limit. custom_message_types (List[type[BaseAgentEvent | BaseChatMessage]], optional) – A list of custom message types that will be used in the group chat. If you are using custom message types or your agents produces custom message types, you need to specify them here. Make sure your custom message types are subclasses of BaseAgentEvent or BaseChatMessage. emit_team_events (bool, optional) – Whether to emit team events through BaseGroupChat.run_stream(). Defaults to False. Basic example: import asyncio from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.teams import Swarm from autogen_agentchat.conditions import MaxMessageTermination async def main() -> None: model_client = OpenAIChatCompletionClient(model=\"gpt-4o\") agent1 = AssistantAgent( \"Alice\", model_client=model_client, handoffs=[\"Bob\"], system_message=\"You are Alice and you only answer questions about yourself.\", ) agent2 = AssistantAgent( \"Bob\", model_client=model_client, system_message=\"You are Bob and your birthday is on 1st January.\" ) termination = MaxMessageTermination(3) team = Swarm([agent1, agent2], termination_condition=termination) stream = team.run_stream(task=\"What is bob's birthday?\") async for message in stream: print(message) asyncio.run(main()) Using the HandoffTermination for human-in-the-loop handoff: import asyncio from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.teams import Swarm from autogen_agentchat.conditions import HandoffTermination, MaxMessageTermination from autogen_agentchat.ui import Console from autogen_agentchat.messages import HandoffMessage async def main() -> None: model_client = OpenAIChatCompletionClient(model=\"gpt-4o\") agent = AssistantAgent( \"Alice\", model_client=model_client, handoffs=[\"user\"], system_message=\"You are Alice and you only answer questions about yourself, ask the user for help if needed.\", ) termination = HandoffTermination(target=\"user\") | MaxMessageTermination(3) team = Swarm([agent], termination_condition=termination) # Start the conversation. await Console(team.run_stream(task=\"What is bob's birthday?\")) # Resume with user feedback. await Console( team.run_stream( task=HandoffMessage(source=\"user\", target=\"Alice\", content=\"Bob's birthday is on 1st January.\") ) ) asyncio.run(main()) component_config_schema# alias of SwarmConfig component_provider_override: ClassVar[str | None] = 'autogen_agentchat.teams.Swarm'# Override the provider string for the component. This should be used to prevent internal module names being a part of the module name. DEFAULT_NAME = 'Swarm'# DEFAULT_DESCRIPTION = 'A team of agents.'# _to_config() → SwarmConfig[source]# Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance. Returns: T – The configuration of the component. classmethod _from_config(config: SwarmConfig) → Swarm[source]# Create a new instance of the component from a configuration object. Parameters: config (T) – The configuration object. Returns: Self – The new instance of the component. class MagenticOneGroupChat(participants: List[ChatAgent], model_client: ChatCompletionClient, *, name: str | None = None, description: str | None = None, termination_condition: TerminationCondition | None = None, max_turns: int | None = 20, runtime: AgentRuntime | None = None, max_stalls: int = 3, final_answer_prompt: str = ORCHESTRATOR_FINAL_ANSWER_PROMPT, custom_message_types: List[type[BaseAgentEvent | BaseChatMessage]] | None = None, emit_team_events: bool = False)[source]# Bases: BaseGroupChat, Component[MagenticOneGroupChatConfig] A team that runs a group chat with participants managed by the MagenticOneOrchestrator. The orchestrator handles the conversation flow, ensuring that the task is completed efficiently by managing the participants’ interactions. The orchestrator is based on the Magentic-One architecture, which is a generalist multi-agent system for solving complex tasks (see references below). Unlike RoundRobinGroupChat and SelectorGroupChat, the MagenticOneGroupChat does not support using team as participant. Parameters: participants (List[ChatAgent]) – The participants in the group chat. model_client (ChatCompletionClient) – The model client used for generating responses. termination_condition (TerminationCondition, optional) – The termination condition for the group chat. Defaults to None. Without a termination condition, the group chat will run based on the orchestrator logic or until the maximum number of turns is reached. max_turns (int, optional) – The maximum number of turns in the group chat before stopping. Defaults to 20. max_stalls (int, optional) – The maximum number of stalls allowed before re-planning. Defaults to 3. final_answer_prompt (str, optional) – The LLM prompt used to generate the final answer or response from the team’s transcript. A default (sensible for GPT-4o class models) is provided. custom_message_types (List[type[BaseAgentEvent | BaseChatMessage]], optional) – A list of custom message types that will be used in the group chat. If you are using custom message types or your agents produces custom message types, you need to specify them here. Make sure your custom message types are subclasses of BaseAgentEvent or BaseChatMessage. emit_team_events (bool, optional) – Whether to emit team events through BaseGroupChat.run_stream(). Defaults to False. Raises: ValueError – In orchestration logic if progress ledger does not have required keys or if next speaker is not valid. Examples: MagenticOneGroupChat with one assistant agent: import asyncio from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.teams import MagenticOneGroupChat from autogen_agentchat.ui import Console async def main() -> None: model_client = OpenAIChatCompletionClient(model=\"gpt-4o\") assistant = AssistantAgent( \"Assistant\", model_client=model_client, ) team = MagenticOneGroupChat([assistant], model_client=model_client) await Console(team.run_stream(task=\"Provide a different proof to Fermat last theorem\")) asyncio.run(main()) References If you use the MagenticOneGroupChat in your work, please cite the following paper: @article{fourney2024magentic, title={Magentic-one: A generalist multi-agent system for solving complex tasks}, author={Fourney, Adam and Bansal, Gagan and Mozannar, Hussein and Tan, Cheng and Salinas, Eduardo and Niedtner, Friederike and Proebsting, Grace and Bassman, Griffin and Gerrits, Jack and Alber, Jacob and others}, journal={arXiv preprint arXiv:2411.04468}, year={2024} } component_config_schema# alias of MagenticOneGroupChatConfig component_provider_override: ClassVar[str | None] = 'autogen_agentchat.teams.MagenticOneGroupChat'# Override the provider string for the component. This should be used to prevent internal module names being a part of the module name. DEFAULT_NAME = 'MagenticOneGroupChat'# DEFAULT_DESCRIPTION = 'A team of agents.'# _to_config() → MagenticOneGroupChatConfig[source]# Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance. Returns: T – The configuration of the component. classmethod _from_config(config: MagenticOneGroupChatConfig) → Self[source]# Create a new instance of the component from a configuration object. Parameters: config (T) – The configuration object. Returns: Self – The new instance of the component. class DiGraphBuilder[source]# Bases: object A fluent builder for constructing DiGraph execution graphs used in GraphFlow. Warning This is an experimental feature, and the API will change in the future releases. This utility provides a convenient way to programmatically build a graph of agent interactions, including complex execution flows such as: Sequential chains Parallel fan-outs Conditional branching Cyclic loops with safe exits Each node in the graph represents an agent. Edges define execution paths between agents, and can optionally be conditioned on message content using callable functions. The builder is compatible with the Graph runner and supports both standard and filtered agents. - add_node(agent, activation) Add an agent node to the graph. - add_edge(source, target, condition) Connect two nodes optionally with a condition. - add_conditional_edges(source, condition_to_target) Add multiple conditional edges from a source. - set_entry_point(agent) Define the default start node (optional). - build() Generate a validated DiGraph. - get_participants() Return the list of added agents. Example — Sequential Flow A → B → C:>>> builder = GraphBuilder() >>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c) >>> builder.add_edge(agent_a, agent_b).add_edge(agent_b, agent_c) >>> team = Graph( ... participants=builder.get_participants(), ... graph=builder.build(), ... termination_condition=MaxMessageTermination(5), ... ) Example — Parallel Fan-out A → (B, C):>>> builder = GraphBuilder() >>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c) >>> builder.add_edge(agent_a, agent_b).add_edge(agent_a, agent_c) Example — Conditional Branching A → B or A → C:>>> builder = GraphBuilder() >>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c) >>> # Add conditional edges using keyword check >>> builder.add_edge(agent_a, agent_b, condition=\"keyword1\") >>> builder.add_edge(agent_a, agent_c, condition=\"keyword2\") Example — Using Custom String Conditions:>>> builder = GraphBuilder() >>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c) >>> # Add condition strings to check in messages >>> builder.add_edge(agent_a, agent_b, condition=\"big\") >>> builder.add_edge(agent_a, agent_c, condition=\"small\") Example — Loop: A → B → A or B → C:>>> builder = GraphBuilder() >>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c) >>> builder.add_edge(agent_a, agent_b) >> # Add a loop back to agent A >>> builder.add_edge(agent_b, agent_a, condition=lambda msg: \"loop\" in msg.to_model_text()) >>> # Add exit condition to break the loop >>> builder.add_edge(agent_b, agent_c, condition=lambda msg: \"loop\" not in msg.to_model_text()) Example — Loop with multiple paths to the same node: A → B → C → B:>>> builder = GraphBuilder() >>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c) >>> builder.add_edge(agent_a, agent_b) >>> builder.add_edge(agent_b, agent_c) >>> builder.add_edge(agent_c, agent_b, activation_group=\"loop_back\") Example — Loop with multiple paths to the same node with any activation condition: A → B → (C1, C2) → B → E(exit):>>> builder = GraphBuilder() >>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c1).add_node(agent_c2).add_node(agent_e) >>> builder.add_edge(agent_a, agent_b) >>> builder.add_edge(agent_b, agent_c1) >>> builder.add_edge(agent_b, agent_c2) >>> builder.add_edge(agent_b, agent_e, condition=\"exit\") >>> builder.add_edge(agent_c1, agent_b, activation_group=\"loop_back_group\", activation_condition=\"any\") >>> builder.add_edge(agent_c2, agent_b, activation_group=\"loop_back_group\", activation_condition=\"any\") add_node(agent: ChatAgent, activation: Literal['all', 'any'] = 'all') → DiGraphBuilder[source]# Add a node to the graph and register its agent. add_edge(source: str | ChatAgent, target: str | ChatAgent, condition: str | Callable[[BaseChatMessage], bool] | None = None, activation_group: str | None = None, activation_condition: Literal['all', 'any'] | None = None) → DiGraphBuilder[source]# Add a directed edge from source to target, optionally with a condition. Parameters: source – Source node (agent name or agent object) target – Target node (agent name or agent object) condition – Optional condition for edge activation. If string, activates when substring is found in message. If callable, activates when function returns True for the message. Returns: Self for method chaining Raises: ValueError – If source or target node doesn’t exist in the builder add_conditional_edges(source: str | ChatAgent, condition_to_target: Dict[str, str | ChatAgent]) → DiGraphBuilder[source]# Add multiple conditional edges from a source node based on keyword checks. Warning This method interface will be changed in the future to support callable conditions. Please use add_edge if you need to specify custom conditions. Parameters: source – Source node (agent name or agent object) condition_to_target – Mapping from condition strings to target nodes Each key is a keyword that will be checked in the message content Each value is the target node to activate when condition is met For each key (keyword), a lambda will be created that checks if the keyword is in the message text. Returns: Self for method chaining set_entry_point(name: str | ChatAgent) → DiGraphBuilder[source]# Set the default start node of the graph. build() → DiGraph[source]# Build and validate the DiGraph. get_participants() → list[ChatAgent][source]# Return the list of agents in the builder, in insertion order. pydantic model DiGraph[source]# Bases: BaseModel Defines a directed graph structure with nodes and edges. GraphFlow uses this to determine execution order and conditions. Warning This is an experimental feature, and the API will change in the future releases. Show JSON schema{ \"title\": \"DiGraph\", \"type\": \"object\", \"properties\": { \"nodes\": { \"default\": null, \"title\": \"Nodes\" }, \"default_start_node\": { \"anyOf\": [ { \"type\": \"string\" }, { \"type\": \"null\" } ], \"default\": null, \"title\": \"Default Start Node\" } } } Fields: default_start_node (str | None) nodes (Dict[str, autogen_agentchat.teams._group_chat._graph._digraph_group_chat.DiGraphNode]) field nodes: Dict[str, DiGraphNode] [Required]# field default_start_node: str | None = None# get_parents() → Dict[str, List[str]][source]# Compute a mapping of each node to its parent nodes. get_start_nodes() → Set[str][source]# Return the nodes that have no incoming edges (entry points). get_leaf_nodes() → Set[str][source]# Return nodes that have no outgoing edges (final output nodes). has_cycles_with_exit() → bool[source]# Check if the graph has any cycles and validate that each cycle has at least one conditional edge. Returns: bool – True if there is at least one cycle and all cycles have an exit condition. False if there are no cycles. Raises: ValueError – If there is a cycle without any conditional edge. get_has_cycles() → bool[source]# Indicates if the graph has at least one cycle (with valid exit conditions). graph_validate() → None[source]# Validate graph structure and execution rules. get_remaining_map() → Dict[str, Dict[str, int]][source]# Get the remaining map that tracks how many edges point to each target node with each activation group. Returns: Dictionary mapping target nodes to their activation groups and remaining counts model_post_init(context: Any, /) → None# This function is meant to behave like a BaseModel method to initialise private attributes. It takes context as an argument since that’s what pydantic-core passes when calling it. Parameters: self – The BaseModel instance. context – The context. pydantic model DiGraphNode[source]# Bases: BaseModel Represents a node (agent) in a DiGraph, with its outgoing edges and activation type. Warning This is an experimental feature, and the API will change in the future releases. Show JSON schema{ \"title\": \"DiGraphNode\", \"type\": \"object\", \"properties\": { \"name\": { \"title\": \"Name\", \"type\": \"string\" }, \"edges\": { \"default\": null, \"title\": \"Edges\" }, \"activation\": { \"default\": \"all\", \"enum\": [ \"all\", \"any\" ], \"title\": \"Activation\", \"type\": \"string\" } }, \"required\": [ \"name\" ] } Fields: activation (Literal['all', 'any']) edges (List[autogen_agentchat.teams._group_chat._graph._digraph_group_chat.DiGraphEdge]) name (str) field name: str [Required]# field edges: List[DiGraphEdge] = []# field activation: Literal['all', 'any'] = 'all'# pydantic model DiGraphEdge[source]# Bases: BaseModel Represents a directed edge in a DiGraph, with an optional execution condition. Warning This is an experimental feature, and the API will change in the future releases. Warning If the condition is a callable, it will not be serialized in the model. Show JSON schema{ \"title\": \"DiGraphEdge\", \"type\": \"object\", \"properties\": { \"target\": { \"title\": \"Target\", \"type\": \"string\" }, \"condition\": { \"anyOf\": [ { \"type\": \"string\" }, { \"type\": \"null\" } ], \"default\": null, \"title\": \"Condition\" }, \"condition_function\": { \"default\": null, \"title\": \"Condition Function\" }, \"activation_group\": { \"default\": \"\", \"title\": \"Activation Group\", \"type\": \"string\" }, \"activation_condition\": { \"default\": \"all\", \"enum\": [ \"all\", \"any\" ], \"title\": \"Activation Condition\", \"type\": \"string\" } }, \"required\": [ \"target\" ] } Fields: activation_condition (Literal['all', 'any']) activation_group (str) condition (str | Callable[[autogen_agentchat.messages.BaseChatMessage], bool] | None) condition_function (Callable[[autogen_agentchat.messages.BaseChatMessage], bool] | None) target (str) Validators: _validate_condition » all fields field target: str [Required]# Validated by: _validate_condition field condition: str | Callable[[BaseChatMessage], bool] | None = None# (Experimental) Condition to execute this edge. If None, the edge is unconditional. If a string, the edge is conditional on the presence of that string in the last agent chat message. If a callable, the edge is conditional on the callable returning True when given the last message. Validated by: _validate_condition field condition_function: Callable[[BaseChatMessage], bool] | None = None# Validated by: _validate_condition field activation_group: str = ''# Group identifier for forward dependencies. When multiple edges point to the same target node, they are grouped by this field. This allows distinguishing between different cycles or dependency patterns. Example: In a graph containing a cycle like A->B->C->B, the two edges pointing to B (A->B and C->B) can be in different activation groups to control how B is activated. Defaults to the target node name if not specified. Validated by: _validate_condition field activation_condition: Literal['all', 'any'] = 'all'# Determines how forward dependencies within the same activation_group are evaluated. “all”: All edges in this activation group must be satisfied before the target node can execute “any”: Any single edge in this activation group being satisfied allows the target node to execute This is used to handle complex dependency patterns in cyclic graphs where multiple paths can lead to the same target node. Validated by: _validate_condition check_condition(message: BaseChatMessage) → bool[source]# Check if the edge condition is satisfied for the given message. Parameters: message – The message to check the condition against. Returns: True if condition is satisfied (None condition always returns True) False otherwise. class GraphFlow(participants: List[ChatAgent], graph: DiGraph, *, name: str | None = None, description: str | None = None, termination_condition: TerminationCondition | None = None, max_turns: int | None = None, runtime: AgentRuntime | None = None, custom_message_types: List[type[BaseAgentEvent | BaseChatMessage]] | None = None)[source]# Bases: BaseGroupChat, Component[GraphFlowConfig] A team that runs a group chat following a Directed Graph execution pattern. Warning This is an experimental feature, and the API will change in the future releases. This group chat executes agents based on a directed graph (DiGraph) structure, allowing complex workflows such as sequential execution, parallel fan-out, conditional branching, join patterns, and loops with explicit exit conditions. The execution order is determined by the edges defined in the DiGraph. Each node in the graph corresponds to an agent, and edges define the flow of messages between agents. Nodes can be configured to activate when: All parent nodes have completed (activation=”all”) → default Any parent node completes (activation=”any”) Conditional branching is supported using edge conditions, where the next agent(s) are selected based on content in the chat history. Loops are permitted as long as there is a condition that eventually exits the loop. Note Use the DiGraphBuilder class to create a DiGraph easily. It provides a fluent API for adding nodes and edges, setting entry points, and validating the graph structure. See the DiGraphBuilder documentation for more details. The GraphFlow class is designed to be used with the DiGraphBuilder for creating complex workflows. Warning When using callable conditions in edges, they will not be serialized when calling dump_component(). This will be addressed in future releases. Parameters: participants (List[ChatAgent]) – The participants in the group chat. termination_condition (TerminationCondition, optional) – Termination condition for the chat. max_turns (int, optional) – Maximum number of turns before forcing termination. graph (DiGraph) – Directed execution graph defining node flow and conditions. Raises: ValueError – If participant names are not unique, or if graph validation fails (e.g., cycles without exit). Examples Sequential Flow: A → B → C import asyncio from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.conditions import MaxMessageTermination from autogen_agentchat.teams import DiGraphBuilder, GraphFlow from autogen_ext.models.openai import OpenAIChatCompletionClient async def main(): # Initialize agents with OpenAI model clients. model_client = OpenAIChatCompletionClient(model=\"gpt-4.1-nano\") agent_a = AssistantAgent(\"A\", model_client=model_client, system_message=\"You are a helpful assistant.\") agent_b = AssistantAgent(\"B\", model_client=model_client, system_message=\"Translate input to Chinese.\") agent_c = AssistantAgent(\"C\", model_client=model_client, system_message=\"Translate input to English.\") # Create a directed graph with sequential flow A -> B -> C. builder = DiGraphBuilder() builder.add_node(agent_a).add_node(agent_b).add_node(agent_c) builder.add_edge(agent_a, agent_b).add_edge(agent_b, agent_c) graph = builder.build() # Create a GraphFlow team with the directed graph. team = GraphFlow( participants=[agent_a, agent_b, agent_c], graph=graph, termination_condition=MaxMessageTermination(5), ) # Run the team and print the events. async for event in team.run_stream(task=\"Write a short story about a cat.\"): print(event) asyncio.run(main()) Parallel Fan-out: A → (B, C) import asyncio from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.conditions import MaxMessageTermination from autogen_agentchat.teams import DiGraphBuilder, GraphFlow from autogen_ext.models.openai import OpenAIChatCompletionClient async def main(): # Initialize agents with OpenAI model clients. model_client = OpenAIChatCompletionClient(model=\"gpt-4.1-nano\") agent_a = AssistantAgent(\"A\", model_client=model_client, system_message=\"You are a helpful assistant.\") agent_b = AssistantAgent(\"B\", model_client=model_client, system_message=\"Translate input to Chinese.\") agent_c = AssistantAgent(\"C\", model_client=model_client, system_message=\"Translate input to Japanese.\") # Create a directed graph with fan-out flow A -> (B, C). builder = DiGraphBuilder() builder.add_node(agent_a).add_node(agent_b).add_node(agent_c) builder.add_edge(agent_a, agent_b).add_edge(agent_a, agent_c) graph = builder.build() # Create a GraphFlow team with the directed graph. team = GraphFlow( participants=[agent_a, agent_b, agent_c], graph=graph, termination_condition=MaxMessageTermination(5), ) # Run the team and print the events. async for event in team.run_stream(task=\"Write a short story about a cat.\"): print(event) asyncio.run(main()) Conditional Branching: A → B (if ‘yes’) or C (otherwise) import asyncio from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.conditions import MaxMessageTermination from autogen_agentchat.teams import DiGraphBuilder, GraphFlow from autogen_ext.models.openai import OpenAIChatCompletionClient async def main(): # Initialize agents with OpenAI model clients. model_client = OpenAIChatCompletionClient(model=\"gpt-4.1-nano\") agent_a = AssistantAgent( \"A\", model_client=model_client, system_message=\"Detect if the input is in Chinese. If it is, say 'yes', else say 'no', and nothing else.\", ) agent_b = AssistantAgent(\"B\", model_client=model_client, system_message=\"Translate input to English.\") agent_c = AssistantAgent(\"C\", model_client=model_client, system_message=\"Translate input to Chinese.\") # Create a directed graph with conditional branching flow A -> B (\"yes\"), A -> C (otherwise). builder = DiGraphBuilder() builder.add_node(agent_a).add_node(agent_b).add_node(agent_c) # Create conditions as callables that check the message content. builder.add_edge(agent_a, agent_b, condition=lambda msg: \"yes\" in msg.to_model_text()) builder.add_edge(agent_a, agent_c, condition=lambda msg: \"yes\" not in msg.to_model_text()) graph = builder.build() # Create a GraphFlow team with the directed graph. team = GraphFlow( participants=[agent_a, agent_b, agent_c], graph=graph, termination_condition=MaxMessageTermination(5), ) # Run the team and print the events. async for event in team.run_stream(task=\"AutoGen is a framework for building AI agents.\"): print(event) asyncio.run(main()) Loop with exit condition: A → B → C (if ‘APPROVE’) or A (otherwise) import asyncio from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.conditions import MaxMessageTermination from autogen_agentchat.teams import DiGraphBuilder, GraphFlow from autogen_ext.models.openai import OpenAIChatCompletionClient async def main(): # Initialize agents with OpenAI model clients. model_client = OpenAIChatCompletionClient(model=\"gpt-4.1\") agent_a = AssistantAgent( \"A\", model_client=model_client, system_message=\"You are a helpful assistant.\", ) agent_b = AssistantAgent( \"B\", model_client=model_client, system_message=\"Provide feedback on the input, if your feedback has been addressed, \" \"say 'APPROVE', otherwise provide a reason for rejection.\", ) agent_c = AssistantAgent( \"C\", model_client=model_client, system_message=\"Translate the final product to Korean.\" ) # Create a loop graph with conditional exit: A -> B -> C (\"APPROVE\"), B -> A (otherwise). builder = DiGraphBuilder() builder.add_node(agent_a).add_node(agent_b).add_node(agent_c) builder.add_edge(agent_a, agent_b) # Create conditional edges using strings builder.add_edge(agent_b, agent_c, condition=lambda msg: \"APPROVE\" in msg.to_model_text()) builder.add_edge(agent_b, agent_a, condition=lambda msg: \"APPROVE\" not in msg.to_model_text()) builder.set_entry_point(agent_a) graph = builder.build() # Create a GraphFlow team with the directed graph. team = GraphFlow( participants=[agent_a, agent_b, agent_c], graph=graph, termination_condition=MaxMessageTermination(20), # Max 20 messages to avoid infinite loop. ) # Run the team and print the events. async for event in team.run_stream(task=\"Write a short poem about AI Agents.\"): print(event) asyncio.run(main()) component_config_schema# alias of GraphFlowConfig component_provider_override: ClassVar[str | None] = 'autogen_agentchat.teams.GraphFlow'# Override the provider string for the component. This should be used to prevent internal module names being a part of the module name. DEFAULT_NAME = 'GraphFlow'# DEFAULT_DESCRIPTION = 'A team of agents'# previous autogen_agentchat.state next autogen_agentchat.tools",
      "code": "Team"
    },
    {
      "description": "Basic example:",
      "code": "import asyncio\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.teams import Swarm\nfrom autogen_agentchat.conditions import MaxMessageTermination\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n    agent1 = AssistantAgent(\n        \"Alice\",\n        model_client=model_client,\n        handoffs=[\"Bob\"],\n        system_message=\"You are Alice and you only answer questions about yourself.\",\n    )\n    agent2 = AssistantAgent(\n        \"Bob\", model_client=model_client, system_message=\"You are Bob and your birthday is on 1st January.\"\n    )\n\n    termination = MaxMessageTermination(3)\n    team = Swarm([agent1, agent2], termination_condition=termination)\n\n    stream = team.run_stream(task=\"What is bob's birthday?\")\n    async for message in stream:\n        print(message)\n\n\nasyncio.run(main())"
    },
    {
      "description": "Example: In a graph containing a cycle like A->B->C->B, the two edges pointing to B (A->B and C->B) can be in different activation groups to control how B is activated. Defaults to the target node name if not specified.",
      "code": "_validate_condition"
    }
  ],
  "links": [
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.teams.html",
    "https://microsoft.github.io/autogen/stable/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/extensions-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/index.html",
    "https://microsoft.github.io/autogen/stable/reference/index.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.agents.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.base.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.conditions.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.messages.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.state.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.tools.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.ui.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.utils.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.code_executor.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.exceptions.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.logging.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.memory.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.model_context.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.models.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.tool_agent.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.tools.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_core.utils.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.cache_store.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.experimental.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.memory.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.teams.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.ui.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.azure.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.file_surfer.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.magentic_one.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.openai.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.video_surfer.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.web_surfer.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.auth.azure.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.cache_store.diskcache.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.cache_store.redis.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.azure.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.docker.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.docker_jupyter.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.jupyter.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.local.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.experimental.task_centric_memory.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.memory.canvas.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.memory.chromadb.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.memory.mem0.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.memory.redis.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.anthropic.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.azure.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.cache.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.llama_cpp.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.ollama.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.openai.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.replay.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.semantic_kernel.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.teams.magentic_one.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.azure.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.code_execution.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.graphrag.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.http.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.langchain.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.mcp.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.semantic_kernel.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.video_surfer.tools.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.web_surfer.playwright_controller.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.experimental.task_centric_memory.utils.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.anthropic.config.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.azure.config.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.ollama.config.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.openai.config.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.protos.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.protos.agent_worker_pb2.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.protos.agent_worker_pb2_grpc.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.protos.cloudevent_pb2.html",
    "https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.protos.cloudevent_pb2_grpc.html"
  ]
}