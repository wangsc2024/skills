{
  "title": "LangSmith Evaluation",
  "content": "Source: https://docs.langchain.com/langsmith/evaluation\n\nLangSmith supports two types of evaluations based on when and where they run:\n\n<CardGroup cols={2}>\n  <Card title=\"Offline Evaluation\" icon=\"flask\">\n    **Test before you ship**\n\nRun evaluations on curated datasets during development to compare versions, benchmark performance, and catch regressions.\n  </Card>\n\n<Card title=\"Online Evaluation\" icon=\"radar\">\n    **Monitor in production**\n\nEvaluate real user interactions in real-time to detect issues and measure quality on live traffic.\n  </Card>\n</CardGroup>\n\n## Evaluation workflow\n\n<Tabs>\n  <Tab title=\"Offline evaluation flow\">\n    <Steps>\n      <Step title=\"Create a dataset\">\n        Create a [dataset](/langsmith/manage-datasets) with <Tooltip tip=\"Individual test cases with inputs and reference outputs\">[examples](/langsmith/evaluation-concepts#examples)</Tooltip> from manually curated test cases, historical production traces, or synthetic data generation.\n      </Step>\n\n<Step title=\"Define evaluators\">\n        Create <Tooltip tip=\"Functions that score how well your application performs\">[evaluators](/langsmith/evaluation-concepts#evaluators)</Tooltip> to score performance:\n\n* [Human](/langsmith/evaluation-concepts#human) review\n        * [Code](/langsmith/evaluation-concepts#code) rules\n        * [LLM-as-judge](/langsmith/llm-as-judge)\n        * [Pairwise](/langsmith/evaluate-pairwise) comparison\n      </Step>\n\n<Step title=\"Run an experiment\">\n        Execute your application on the dataset to create an <Tooltip tip=\"Results of evaluating a specific application version on a dataset\">[experiment](/langsmith/evaluation-concepts#experiment)</Tooltip>. Configure [repetitions, concurrency, and caching](/langsmith/experiment-configuration) to optimize runs.\n      </Step>\n\n<Step title=\"Analyze results\">\n        Compare experiments for [benchmarking](/langsmith/evaluation-types#benchmarking), [unit tests](/langsmith/evaluation-types#unit-tests), [regression tests](/langsmith/evaluation-types#regression-tests), or [backtesting](/langsmith/evaluation-types#backtesting).\n      </Step>\n    </Steps>\n  </Tab>\n\n<Tab title=\"Online evaluation flow\">\n    <Steps>\n      <Step title=\"Deploy your application\">\n        Each interaction creates a <Tooltip tip=\"A single execution trace including inputs, outputs, and intermediate steps\">[run](/langsmith/evaluation-concepts#runs)</Tooltip> without reference outputs.\n      </Step>\n\n<Step title=\"Configure online evaluators\">\n        Set up [evaluators](/langsmith/online-evaluations) to run automatically on production traces: safety checks, format validation, quality heuristics, and reference-free LLM-as-judge. Apply [filters and sampling rates](/langsmith/online-evaluations#4-optional-configure-a-sampling-rate) to control costs.\n      </Step>\n\n<Step title=\"Monitor in real-time\">\n        Evaluators run automatically on [runs](/langsmith/evaluation-concepts#runs) or <Tooltip tip=\"Collections of related runs forming multi-turn conversations\">[threads](/langsmith/online-evaluations#configure-multi-turn-online-evaluators)</Tooltip>, providing real-time monitoring, anomaly detection, and alerting.\n      </Step>\n\n<Step title=\"Establish a feedback loop\">\n        Add failing production traces to your [dataset](/langsmith/manage-datasets), create targeted evaluators, validate fixes with offline experiments, and redeploy.\n      </Step>\n    </Steps>\n  </Tab>\n</Tabs>\n\n<Tip>\n  For more on the differences between offline and online evaluation, refer to the [Evaluation concepts](/langsmith/evaluation-concepts#quick-reference-offline-vs-online-evaluation) page.\n</Tip>\n\n<Columns cols={3}>\n  <Card title=\"Evaluation quickstart\" icon=\"rocket\" href=\"/langsmith/evaluation-quickstart\" arrow=\"true\">\n    Get started with offline evaluation.\n  </Card>\n\n<Card title=\"Manage datasets\" icon=\"database\" href=\"/langsmith/manage-datasets\" arrow=\"true\">\n    Create and manage datasets for evaluation through the UI or SDK.\n  </Card>\n\n<Card title=\"Run offline evaluations\" icon=\"microscope\" href=\"/langsmith/evaluate-llm-application\" arrow=\"true\">\n    Explore evaluation types, techniques, and frameworks for comprehensive testing.\n  </Card>\n\n<Card title=\"Analyze results\" icon=\"chart-bar\" href=\"/langsmith/analyze-an-experiment\" arrow=\"true\">\n    View and analyze evaluation results, compare experiments, filter data, and export findings.\n  </Card>\n\n<Card title=\"Run online evaluations\" icon=\"radar\" href=\"/langsmith/online-evaluations\" arrow=\"true\">\n    Monitor production quality in real-time from the Observability tab.\n  </Card>\n\n<Card title=\"Follow tutorials\" icon=\"book\" href=\"/langsmith/evaluate-chatbot-tutorial\" arrow=\"true\">\n    Learn by following step-by-step tutorials, from simple chatbots to complex agent evaluations.\n  </Card>\n</Columns>\n\n<Note>\n  To set up a LangSmith instance, visit the [Platform setup section](/langsmith/platform-setup) to choose between cloud, hybrid, or self-hosted. All options include observability, evaluation, prompt engineering, and deployment.\n</Note>\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/evaluation.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "code_samples": [],
  "headings": [
    {
      "level": "h2",
      "text": "Evaluation workflow",
      "id": "evaluation-workflow"
    },
    {
      "level": "h2",
      "text": "Get started",
      "id": "get-started"
    }
  ],
  "url": "llms-txt#langsmith-evaluation",
  "links": []
}