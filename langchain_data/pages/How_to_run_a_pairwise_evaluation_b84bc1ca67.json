{
  "title": "How to run a pairwise evaluation",
  "content": "Source: https://docs.langchain.com/langsmith/evaluate-pairwise\n\n<Info>\n  Concept: [Pairwise evaluations](/langsmith/evaluation-concepts#pairwise)\n</Info>\n\nLangSmith supports evaluating **existing** experiments in a comparative manner. Instead of evaluating one output at a time, you can score the output from multiple experiments against each other. In this guide, you'll use [`evaluate()`](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate) with two existing experiments to [define an evaluator](#define-a-pairwise-evaluator) and [run a pairwise evaluation](#run-a-pairwise-evaluation). Finally, you'll use the LangSmith UI to [view the pairwise experiments](#view-pairwise-experiments).\n\n* If you haven't already created experiments to compare, check out the [quick start](/langsmith/evaluation-quickstart) or the [how-to guide](/langsmith/evaluate-llm-application) to get started with evaluations.\n* This guide requires `langsmith` Python version `>=0.2.0` or JS version `>=0.2.9`.\n\n<Info>\n  You can also use [`evaluate_comparative()`](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate_comparative) with more than two existing experiments.\n</Info>\n\n## `evaluate()` comparative args\n\nAt its simplest, `evaluate` / `aevaluate` function takes the following arguments:\n\n| Argument     | Description                                                                                                                        |\n| ------------ | ---------------------------------------------------------------------------------------------------------------------------------- |\n| `target`     | A list of the two **existing experiments** you would like to evaluate against each other. These can be uuids or experiment names.  |\n| `evaluators` | A list of the pairwise evaluators that you would like to attach to this evaluation. See the section below for how to define these. |\n\nAlong with these, you can also pass in the following optional args:\n\n| Argument                                 | Description                                                                                                                                                                                                                                                                                                                                                                    |\n| ---------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| `randomize_order` / `randomizeOrder`     | An optional boolean indicating whether the order of the outputs should be randomized for each evaluation. This is a strategy for minimizing positional bias in your prompt: often, the LLM will be biased towards one of the responses based on the order. This should mainly be addressed via prompt engineering, but this is another optional mitigation. Defaults to False. |\n| `experiment_prefix` / `experimentPrefix` | A prefix to be attached to the beginning of the pairwise experiment name. Defaults to None.                                                                                                                                                                                                                                                                                    |\n| `description`                            | A description of the pairwise experiment. Defaults to None.                                                                                                                                                                                                                                                                                                                    |\n| `max_concurrency` / `maxConcurrency`     | The maximum number of concurrent evaluations to run. Defaults to 5.                                                                                                                                                                                                                                                                                                            |\n| `client`                                 | The LangSmith client to use. Defaults to None.                                                                                                                                                                                                                                                                                                                                 |\n| `metadata`                               | Metadata to attach to your pairwise experiment. Defaults to None.                                                                                                                                                                                                                                                                                                              |\n| `load_nested` / `loadNested`             | Whether to load all child runs for the experiment. When False, only the root trace will be passed to your evaluator. Defaults to False.                                                                                                                                                                                                                                        |\n\n## Define a pairwise evaluator\n\nPairwise evaluators are just functions with an expected signature.\n\nCustom evaluator functions must have specific argument names. They can take any subset of the following arguments:\n\n* `inputs: dict`: A dictionary of the inputs corresponding to a single example in a dataset.\n* `outputs: list[dict]`: A two-item list of the dict outputs produced by each experiment on the given inputs.\n* `reference_outputs` / `referenceOutputs: dict`: A dictionary of the reference outputs associated with the example, if available.\n* `runs: list[Run]`: A two-item list of the full [Run](/langsmith/run-data-format) objects generated by the two experiments on the given example. Use this if you need access to intermediate steps or metadata about each run.\n* `example: Example`: The full dataset [Example](/langsmith/example-data-format), including the example inputs, outputs (if available), and metadata (if available).\n\nFor most use cases you'll only need `inputs`, `outputs`, and `reference_outputs` / `referenceOutputs`. `runs` and `example` are useful only if you need some extra trace or example metadata outside of the actual inputs and outputs of the application.\n\nCustom evaluators are expected to return one of the following types:\n\n* `dict`: dictionary with keys:\n\n* `key`, which represents the feedback key that will be logged\n  * `scores`, which is a mapping from run ID to score for that run.\n  * `comment`, which is a string. Most commonly used for model reasoning.\n\nCurrently Python only\n\n* `list[int | float | bool]`: a two-item list of scores. The list is assumed to have the same order as the `runs` / `outputs` evaluator args. The evaluator function name is used for the feedback key.\n\nNote that you should choose a feedback key that is distinct from standard feedbacks on your run. We recommend prefixing pairwise feedback keys with `pairwise_` or `ranked_`.\n\n## Run a pairwise evaluation\n\nThe following example uses [a prompt](https://smith.langchain.com/hub/langchain-ai/pairwise-evaluation-2) which asks the LLM to decide which is better between two AI assistant responses. It uses structured output to parse the AI's response: 0, 1, or 2.\n\n<Info>\n  In the Python example below, we are pulling [this structured prompt](https://smith.langchain.com/hub/langchain-ai/pairwise-evaluation-2) from the [LangChain Hub](/langsmith/manage-prompts#public-prompt-hub) and using it with a LangChain chat model wrapper.\n\n**Usage of LangChain is totally optional.** To illustrate this point, the TypeScript example uses the OpenAI SDK directly.\n</Info>\n\n* Python: Requires `langsmith>=0.2.0`\n* TypeScript: Requires `langsmith>=0.2.9`\n\n## View pairwise experiments\n\nNavigate to the \"Pairwise Experiments\" tab from the dataset page:\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-from-dataset.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=dddf35fd971055d0d94ae4184c91dea3\" alt=\"Pairwise Experiments Tab\" data-og-width=\"3454\" width=\"3454\" data-og-height=\"1912\" height=\"1912\" data-path=\"langsmith/images/pairwise-from-dataset.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-from-dataset.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=4c1677867b832da9c3b4338a210570f8 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-from-dataset.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=80d4795bc999156850eb8092e8267c9f 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-from-dataset.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=ed2e5fb624828fb649bf33473e7dc797 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-from-dataset.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=05c2248284b4efb2f5a9f38cffef0b9b 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-from-dataset.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=3014131b2f5ae730aa354afaa7312316 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-from-dataset.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=38c4f707158930cf7d1d155db4021362 2500w\" />\n\nClick on a pairwise experiment that you would like to inspect, and you will be brought to the Comparison View:\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-comparison-view.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=8afa7467faf707c0bb5ede23b007beda\" alt=\"Pairwise Comparison View\" data-og-width=\"3430\" width=\"3430\" data-og-height=\"1886\" height=\"1886\" data-path=\"langsmith/images/pairwise-comparison-view.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-comparison-view.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=9a837cee527a1bf5dda5a77b8ce16ba6 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-comparison-view.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=ebb39f8f2fb7a542d2273cfc64c5b4f4 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-comparison-view.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=2f2de8c570a3e6401ba0220da343b3e0 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-comparison-view.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=cf612b4c6938e856b78c7476f8cc6304 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-comparison-view.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=ff05d71cd12f19d0403e6a1e3e64609a 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-comparison-view.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=5674d6403a3070935830983b9e36ac2f 2500w\" />\n\nYou may filter to runs where the first experiment was better or vice versa by clicking the thumbs up/thumbs down buttons in the table header:\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-pairwise.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=677c48099cee9848d2119c154c7b0d88\" alt=\"Pairwise Filtering\" data-og-width=\"3454\" width=\"3454\" data-og-height=\"1914\" height=\"1914\" data-path=\"langsmith/images/filter-pairwise.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-pairwise.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=1ceff9156ccfdb48f246f41c7e0d16ab 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-pairwise.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=745f3dc2bed9e3e2d8333df0ff57a43e 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-pairwise.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=e81f10f544953ee39366866c1f4a5d71 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-pairwise.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=3f4af39e4da50d0ad081d03aaf7b238e 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-pairwise.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=63688564868c7a0989c429c6e740e014 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-pairwise.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=527537f492665790aa8380e0d75e7fb3 2500w\" />\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/evaluate-pairwise.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "code_samples": [
    {
      "code": "",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": "Prerequisites",
      "id": "prerequisites"
    },
    {
      "level": "h2",
      "text": "`evaluate()` comparative args",
      "id": "`evaluate()`-comparative-args"
    },
    {
      "level": "h2",
      "text": "Define a pairwise evaluator",
      "id": "define-a-pairwise-evaluator"
    },
    {
      "level": "h3",
      "text": "Evaluator args",
      "id": "evaluator-args"
    },
    {
      "level": "h3",
      "text": "Evaluator output",
      "id": "evaluator-output"
    },
    {
      "level": "h2",
      "text": "Run a pairwise evaluation",
      "id": "run-a-pairwise-evaluation"
    },
    {
      "level": "h2",
      "text": "View pairwise experiments",
      "id": "view-pairwise-experiments"
    }
  ],
  "url": "llms-txt#how-to-run-a-pairwise-evaluation",
  "links": []
}