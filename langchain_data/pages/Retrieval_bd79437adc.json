{
  "title": "Retrieval",
  "content": "Source: https://docs.langchain.com/oss/python/langchain/retrieval\n\nLarge Language Models (LLMs) are powerful, but they have two key limitations:\n\n* **Finite context** — they can’t ingest entire corpora at once.\n* **Static knowledge** — their training data is frozen at a point in time.\n\nRetrieval addresses these problems by fetching relevant external knowledge at query time. This is the foundation of **Retrieval-Augmented Generation (RAG)**: enhancing an LLM’s answers with context-specific information.\n\n## Building a knowledge base\n\nA **knowledge base** is a repository of documents or structured data used during retrieval.\n\nIf you need a custom knowledge base, you can use LangChain’s document loaders and vector stores to build one from your own data.\n\n<Note>\n  If you already have a knowledge base (e.g., a SQL database, CRM, or internal documentation system), you do **not** need to rebuild it. You can:\n\n* Connect it as a **tool** for an agent in Agentic RAG.\n  * Query it and supply the retrieved content as context to the LLM [(2-Step RAG)](#2-step-rag).\n</Note>\n\nSee the following tutorial to build a searchable knowledge base and minimal RAG workflow:\n\n<Card title=\"Tutorial: Semantic search\" icon=\"database\" href=\"/oss/python/langchain/knowledge-base\" arrow cta=\"Learn more\">\n  Learn how to create a searchable knowledge base from your own data using LangChain’s document loaders, embeddings, and vector stores.\n  In this tutorial, you’ll build a search engine over a PDF, enabling retrieval of passages relevant to a query. You’ll also implement a minimal RAG workflow on top of this engine to see how external knowledge can be integrated into LLM reasoning.\n</Card>\n\n### From retrieval to RAG\n\nRetrieval allows LLMs to access relevant context at runtime. But most real-world applications go one step further: they **integrate retrieval with generation** to produce grounded, context-aware answers.\n\nThis is the core idea behind **Retrieval-Augmented Generation (RAG)**. The retrieval pipeline becomes a foundation for a broader system that combines search with generation.\n\n### Retrieval Pipeline\n\nA typical retrieval workflow looks like this:\n\nEach component is modular: you can swap loaders, splitters, embeddings, or vector stores without rewriting the app’s logic.\n\n<Columns cols={2}>\n  <Card title=\"Document loaders\" icon=\"file-import\" href=\"/oss/python/integrations/document_loaders\" arrow cta=\"Learn more\">\n    Ingest data from external sources (Google Drive, Slack, Notion, etc.), returning standardized [`Document`](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) objects.\n  </Card>\n\n<Card title=\"Text splitters\" icon=\"scissors\" href=\"/oss/python/integrations/splitters\" arrow cta=\"Learn more\">\n    Break large docs into smaller chunks that will be retrievable individually and fit within a model's context window.\n  </Card>\n\n<Card title=\"Embedding models\" icon=\"diagram-project\" href=\"/oss/python/integrations/text_embedding\" arrow cta=\"Learn more\">\n    An embedding model turns text into a vector of numbers so that texts with similar meaning land close together in that vector space.\n  </Card>\n\n<Card title=\"Vector stores\" icon=\"database\" href=\"/oss/python/integrations/vectorstores/\" arrow cta=\"Learn more\">\n    Specialized databases for storing and searching embeddings.\n  </Card>\n\n<Card title=\"Retrievers\" icon=\"binoculars\" href=\"/oss/python/integrations/retrievers/\" arrow cta=\"Learn more\">\n    A retriever is an interface that returns documents given an unstructured query.\n  </Card>\n</Columns>\n\nRAG can be implemented in multiple ways, depending on your system's needs. We outline each type in the sections below.\n\n| Architecture    | Description                                                                | Control   | Flexibility | Latency    | Example Use Case                                  |\n| --------------- | -------------------------------------------------------------------------- | --------- | ----------- | ---------- | ------------------------------------------------- |\n| **2-Step RAG**  | Retrieval always happens before generation. Simple and predictable         | ✅ High    | ❌ Low       | ⚡ Fast     | FAQs, documentation bots                          |\n| **Agentic RAG** | An LLM-powered agent decides *when* and *how* to retrieve during reasoning | ❌ Low     | ✅ High      | ⏳ Variable | Research assistants with access to multiple tools |\n| **Hybrid**      | Combines characteristics of both approaches with validation steps          | ⚖️ Medium | ⚖️ Medium   | ⏳ Variable | Domain-specific Q\\&A with quality validation      |\n\n<Info>\n  **Latency**: Latency is generally more **predictable** in **2-Step RAG**, as the maximum number of LLM calls is known and capped. This predictability assumes that LLM inference time is the dominant factor. However, real-world latency may also be affected by the performance of retrieval steps—such as API response times, network delays, or database queries—which can vary based on the tools and infrastructure in use.\n</Info>\n\nIn **2-Step RAG**, the retrieval step is always executed before the generation step. This architecture is straightforward and predictable, making it suitable for many applications where the retrieval of relevant documents is a clear prerequisite for generating an answer.\n\n<Card title=\"Tutorial: Retrieval-Augmented Generation (RAG)\" icon=\"robot\" href=\"/oss/python/langchain/rag#rag-chains\" arrow cta=\"Learn more\">\n  See how to build a Q\\&A chatbot that can answer questions grounded in your data using Retrieval-Augmented Generation.\n  This tutorial walks through two approaches:\n\n* A **RAG agent** that runs searches with a flexible tool—great for general-purpose use.\n  * A **2-step RAG** chain that requires just one LLM call per query—fast and efficient for simpler tasks.\n</Card>\n\n**Agentic Retrieval-Augmented Generation (RAG)** combines the strengths of Retrieval-Augmented Generation with agent-based reasoning. Instead of retrieving documents before answering, an agent (powered by an LLM) reasons step-by-step and decides **when** and **how** to retrieve information during the interaction.\n\n<Tip>\n  The only thing an agent needs to enable RAG behavior is access to one or more **tools** that can fetch external knowledge — such as documentation loaders, web APIs, or database queries.\n</Tip>\n\n<Expandable title=\"Extended example: Agentic RAG for LangGraph's llms.txt\">\n  This example implements an **Agentic RAG system** to assist users in querying LangGraph documentation. The agent begins by loading [llms.txt](https://llmstxt.org/), which lists available documentation URLs, and can then dynamically use a `fetch_documentation` tool to retrieve and process the relevant content based on the user’s question.\n\n<Card title=\"Tutorial: Retrieval-Augmented Generation (RAG)\" icon=\"robot\" href=\"/oss/python/langchain/rag\" arrow cta=\"Learn more\">\n  See how to build a Q\\&A chatbot that can answer questions grounded in your data using Retrieval-Augmented Generation.\n  This tutorial walks through two approaches:\n\n* A **RAG agent** that runs searches with a flexible tool—great for general-purpose use.\n  * A **2-step RAG** chain that requires just one LLM call per query—fast and efficient for simpler tasks.\n</Card>\n\nHybrid RAG combines characteristics of both 2-Step and Agentic RAG. It introduces intermediate steps such as query preprocessing, retrieval validation, and post-generation checks. These systems offer more flexibility than fixed pipelines while maintaining some control over execution.\n\nTypical components include:\n\n* **Query enhancement**: Modify the input question to improve retrieval quality. This can involve rewriting unclear queries, generating multiple variations, or expanding queries with additional context.\n* **Retrieval validation**: Evaluate whether retrieved documents are relevant and sufficient. If not, the system may refine the query and retrieve again.\n* **Answer validation**: Check the generated answer for accuracy, completeness, and alignment with source content. If needed, the system can regenerate or revise the answer.\n\nThe architecture often supports multiple iterations between these steps:\n\nThis architecture is suitable for:\n\n* Applications with ambiguous or underspecified queries\n* Systems that require validation or quality control steps\n* Workflows involving multiple sources or iterative refinement\n\n<Card title=\"Tutorial: Agentic RAG with Self-Correction\" icon=\"robot\" href=\"/oss/python/langgraph/agentic-rag\" arrow cta=\"Learn more\">\n  An example of **Hybrid RAG** that combines agentic reasoning with retrieval and self-correction.\n</Card>\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/retrieval.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "code_samples": [
    {
      "code": "Each component is modular: you can swap loaders, splitters, embeddings, or vector stores without rewriting the app’s logic.\n\n### Building Blocks\n\n<Columns cols={2}>\n  <Card title=\"Document loaders\" icon=\"file-import\" href=\"/oss/python/integrations/document_loaders\" arrow cta=\"Learn more\">\n    Ingest data from external sources (Google Drive, Slack, Notion, etc.), returning standardized [`Document`](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) objects.\n  </Card>\n\n  <Card title=\"Text splitters\" icon=\"scissors\" href=\"/oss/python/integrations/splitters\" arrow cta=\"Learn more\">\n    Break large docs into smaller chunks that will be retrievable individually and fit within a model's context window.\n  </Card>\n\n  <Card title=\"Embedding models\" icon=\"diagram-project\" href=\"/oss/python/integrations/text_embedding\" arrow cta=\"Learn more\">\n    An embedding model turns text into a vector of numbers so that texts with similar meaning land close together in that vector space.\n  </Card>\n\n  <Card title=\"Vector stores\" icon=\"database\" href=\"/oss/python/integrations/vectorstores/\" arrow cta=\"Learn more\">\n    Specialized databases for storing and searching embeddings.\n  </Card>\n\n  <Card title=\"Retrievers\" icon=\"binoculars\" href=\"/oss/python/integrations/retrievers/\" arrow cta=\"Learn more\">\n    A retriever is an interface that returns documents given an unstructured query.\n  </Card>\n</Columns>\n\n## RAG Architectures\n\nRAG can be implemented in multiple ways, depending on your system's needs. We outline each type in the sections below.\n\n| Architecture    | Description                                                                | Control   | Flexibility | Latency    | Example Use Case                                  |\n| --------------- | -------------------------------------------------------------------------- | --------- | ----------- | ---------- | ------------------------------------------------- |\n| **2-Step RAG**  | Retrieval always happens before generation. Simple and predictable         | ✅ High    | ❌ Low       | ⚡ Fast     | FAQs, documentation bots                          |\n| **Agentic RAG** | An LLM-powered agent decides *when* and *how* to retrieve during reasoning | ❌ Low     | ✅ High      | ⏳ Variable | Research assistants with access to multiple tools |\n| **Hybrid**      | Combines characteristics of both approaches with validation steps          | ⚖️ Medium | ⚖️ Medium   | ⏳ Variable | Domain-specific Q\\&A with quality validation      |\n\n<Info>\n  **Latency**: Latency is generally more **predictable** in **2-Step RAG**, as the maximum number of LLM calls is known and capped. This predictability assumes that LLM inference time is the dominant factor. However, real-world latency may also be affected by the performance of retrieval steps—such as API response times, network delays, or database queries—which can vary based on the tools and infrastructure in use.\n</Info>\n\n### 2-step RAG\n\nIn **2-Step RAG**, the retrieval step is always executed before the generation step. This architecture is straightforward and predictable, making it suitable for many applications where the retrieval of relevant documents is a clear prerequisite for generating an answer.",
      "language": "unknown"
    },
    {
      "code": "<Card title=\"Tutorial: Retrieval-Augmented Generation (RAG)\" icon=\"robot\" href=\"/oss/python/langchain/rag#rag-chains\" arrow cta=\"Learn more\">\n  See how to build a Q\\&A chatbot that can answer questions grounded in your data using Retrieval-Augmented Generation.\n  This tutorial walks through two approaches:\n\n  * A **RAG agent** that runs searches with a flexible tool—great for general-purpose use.\n  * A **2-step RAG** chain that requires just one LLM call per query—fast and efficient for simpler tasks.\n</Card>\n\n### Agentic RAG\n\n**Agentic Retrieval-Augmented Generation (RAG)** combines the strengths of Retrieval-Augmented Generation with agent-based reasoning. Instead of retrieving documents before answering, an agent (powered by an LLM) reasons step-by-step and decides **when** and **how** to retrieve information during the interaction.\n\n<Tip>\n  The only thing an agent needs to enable RAG behavior is access to one or more **tools** that can fetch external knowledge — such as documentation loaders, web APIs, or database queries.\n</Tip>",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "<Expandable title=\"Extended example: Agentic RAG for LangGraph's llms.txt\">\n  This example implements an **Agentic RAG system** to assist users in querying LangGraph documentation. The agent begins by loading [llms.txt](https://llmstxt.org/), which lists available documentation URLs, and can then dynamically use a `fetch_documentation` tool to retrieve and process the relevant content based on the user’s question.",
      "language": "unknown"
    },
    {
      "code": "</Expandable>\n\n<Card title=\"Tutorial: Retrieval-Augmented Generation (RAG)\" icon=\"robot\" href=\"/oss/python/langchain/rag\" arrow cta=\"Learn more\">\n  See how to build a Q\\&A chatbot that can answer questions grounded in your data using Retrieval-Augmented Generation.\n  This tutorial walks through two approaches:\n\n  * A **RAG agent** that runs searches with a flexible tool—great for general-purpose use.\n  * A **2-step RAG** chain that requires just one LLM call per query—fast and efficient for simpler tasks.\n</Card>\n\n### Hybrid RAG\n\nHybrid RAG combines characteristics of both 2-Step and Agentic RAG. It introduces intermediate steps such as query preprocessing, retrieval validation, and post-generation checks. These systems offer more flexibility than fixed pipelines while maintaining some control over execution.\n\nTypical components include:\n\n* **Query enhancement**: Modify the input question to improve retrieval quality. This can involve rewriting unclear queries, generating multiple variations, or expanding queries with additional context.\n* **Retrieval validation**: Evaluate whether retrieved documents are relevant and sufficient. If not, the system may refine the query and retrieve again.\n* **Answer validation**: Check the generated answer for accuracy, completeness, and alignment with source content. If needed, the system can regenerate or revise the answer.\n\nThe architecture often supports multiple iterations between these steps:",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": "Building a knowledge base",
      "id": "building-a-knowledge-base"
    },
    {
      "level": "h3",
      "text": "From retrieval to RAG",
      "id": "from-retrieval-to-rag"
    },
    {
      "level": "h3",
      "text": "Retrieval Pipeline",
      "id": "retrieval-pipeline"
    },
    {
      "level": "h3",
      "text": "Building Blocks",
      "id": "building-blocks"
    },
    {
      "level": "h2",
      "text": "RAG Architectures",
      "id": "rag-architectures"
    },
    {
      "level": "h3",
      "text": "2-step RAG",
      "id": "2-step-rag"
    },
    {
      "level": "h3",
      "text": "Agentic RAG",
      "id": "agentic-rag"
    },
    {
      "level": "h3",
      "text": "Hybrid RAG",
      "id": "hybrid-rag"
    }
  ],
  "url": "llms-txt#retrieval",
  "links": []
}