{
  "title": "Only keep post title, headers, and content from the full HTML.",
  "content": "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\nloader = WebBaseLoader(\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n    bs_kwargs={\"parse_only\": bs4_strainer},\n)\ndocs = loader.load()\n\nassert len(docs) == 1\nprint(f\"Total characters: {len(docs[0].page_content)}\")\ntext  theme={null}\nTotal characters: 43131\npython  theme={null}\nprint(docs[0].page_content[:500])\ntext  theme={null}\n      LLM Powered Autonomous Agents\n\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\nAgent System Overview#\nIn\npython  theme={null}\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,  # chunk size (characters)\n    chunk_overlap=200,  # chunk overlap (characters)\n    add_start_index=True,  # track index in original document\n)\nall_splits = text_splitter.split_documents(docs)\n\nprint(f\"Split blog post into {len(all_splits)} sub-documents.\")\ntext  theme={null}\nSplit blog post into 66 sub-documents.\npython  theme={null}\ndocument_ids = vector_store.add_documents(documents=all_splits)\n\nprint(document_ids[:3])\npython  theme={null}\n['07c18af6-ad58-479a-bfb1-d508033f9c64', '9000bf8e-1993-446f-8d4d-f4e507ba4b8f', 'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6']\npython  theme={null}\nfrom langchain.tools import tool\n\n@tool(response_format=\"content_and_artifact\")\ndef retrieve_context(query: str):\n    \"\"\"Retrieve information to help answer a query.\"\"\"\n    retrieved_docs = vector_store.similarity_search(query, k=2)\n    serialized = \"\\n\\n\".join(\n        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n        for doc in retrieved_docs\n    )\n    return serialized, retrieved_docs\npython  theme={null}\n  from typing import Literal\n\ndef retrieve_context(query: str, section: Literal[\"beginning\", \"middle\", \"end\"]):\n  python  theme={null}\nfrom langchain.agents import create_agent\n\ntools = [retrieve_context]",
  "code_samples": [
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "**Go deeper**\n\n`DocumentLoader`: Object that loads data from a source as list of `Documents`.\n\n* [Integrations](/oss/python/integrations/document_loaders/): 160+ integrations to choose from.\n* [`BaseLoader`](https://reference.langchain.com/python/langchain_core/document_loaders/#langchain_core.document_loaders.BaseLoader): API reference for the base interface.\n\n### Splitting documents\n\nOur loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\n\nTo handle this we'll split the [`Document`](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) into chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\n\nAs in the [semantic search tutorial](/oss/python/langchain/knowledge-base), we use a `RecursiveCharacterTextSplitter`, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "**Go deeper**\n\n`TextSplitter`: Object that splits a list of [`Document`](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) objects into smaller\nchunks for storage and retrieval.\n\n* [Integrations](/oss/python/integrations/splitters/)\n* [Interface](https://python.langchain.com/api_reference/text_splitters/base/langchain_text_splitters.base.TextSplitter.html): API reference for the base interface.\n\n### Storing documents\n\nNow we need to index our 66 text chunks so that we can search over them at runtime. Following the [semantic search tutorial](/oss/python/langchain/knowledge-base), our approach is to [embed](/oss/python/langchain/retrieval#embedding_models/) the contents of each document split and insert these embeddings into a [vector store](/oss/python/langchain/retrieval#vectorstores/). Given an input query, we can then use vector search to retrieve relevant documents.\n\nWe can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the [start of the tutorial](/oss/python/langchain/rag#components).",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "**Go deeper**\n\n`Embeddings`: Wrapper around a text embedding model, used for converting text to embeddings.\n\n* [Integrations](/oss/python/integrations/text_embedding/): 30+ integrations to choose from.\n* [Interface](https://reference.langchain.com/python/langchain_core/embeddings/#langchain_core.embeddings.embeddings.Embeddings): API reference for the base interface.\n\n`VectorStore`: Wrapper around a vector database, used for storing and querying embeddings.\n\n* [Integrations](/oss/python/integrations/vectorstores/): 40+ integrations to choose from.\n* [Interface](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.base.VectorStore.html): API reference for the base interface.\n\nThis completes the **Indexing** portion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.\n\n## 2. Retrieval and Generation\n\nRAG applications commonly work as follows:\n\n1. **Retrieve**: Given a user input, relevant splits are retrieved from storage using a [Retriever](/oss/python/langchain/retrieval#retrievers).\n2. **Generate**: A [model](/oss/python/langchain/models) produces an answer using a prompt that includes both the question with the retrieved data\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_retrieval_generation.png?fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=994c3585cece93c80873d369960afd44\" alt=\"retrieval_diagram\" data-og-width=\"2532\" width=\"2532\" data-og-height=\"1299\" height=\"1299\" data-path=\"images/rag_retrieval_generation.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_retrieval_generation.png?w=280&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=3bd28b3662e08c8364b60b74f510751e 280w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_retrieval_generation.png?w=560&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=43484903ca631a47a54e86191eb5ba22 560w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_retrieval_generation.png?w=840&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=67fe2302e241fc24238a5df1cf56573d 840w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_retrieval_generation.png?w=1100&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=d390a6a758e688ec36352d30b22249b0 1100w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_retrieval_generation.png?w=1650&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=59729377317a0631598b6a4a2a7d8c92 1650w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_retrieval_generation.png?w=2500&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=c07711c71153c3b2dfd5b0104ad3e324 2500w\" />\n\nNow let's write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\n\nWe will demonstrate:\n\n1. A RAG [agent](#rag-agents) that executes searches with a simple tool. This is a good general-purpose implementation.\n2. A two-step RAG [chain](#rag-chains) that uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\n### RAG agents\n\nOne formulation of a RAG application is as a simple [agent](/oss/python/langchain/agents) with a tool that retrieves information. We can assemble a minimal RAG agent by implementing a [tool](/oss/python/langchain/tools) that wraps our vector store:",
      "language": "unknown"
    },
    {
      "code": "<Tip>\n  Here we use the [tool decorator](https://reference.langchain.com/python/langchain/tools/#langchain.tools.tool) to configure the tool to attach raw documents as [artifacts](/oss/python/langchain/messages#param-artifact) to each [ToolMessage](/oss/python/langchain/messages#tool-message). This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.\n</Tip>\n\n<Tip>\n  Retrieval tools are not limited to a single string `query` argument, as in the above example. You can\n  force the LLM to specify additional search parameters by adding argumentsâ€” for example, a category:",
      "language": "unknown"
    },
    {
      "code": "</Tip>\n\nGiven our tool, we can construct the agent:",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h3",
      "text": "Splitting documents",
      "id": "splitting-documents"
    },
    {
      "level": "h3",
      "text": "Storing documents",
      "id": "storing-documents"
    },
    {
      "level": "h2",
      "text": "2. Retrieval and Generation",
      "id": "2.-retrieval-and-generation"
    },
    {
      "level": "h3",
      "text": "RAG agents",
      "id": "rag-agents"
    }
  ],
  "url": "llms-txt#only-keep-post-title,-headers,-and-content-from-the-full-html.",
  "links": []
}