{
  "title": "The metadata contains information about the LLM invocation, including the tags",
  "content": "async for msg, metadata in graph.astream(\n    {\"topic\": \"cats\"},\n    stream_mode=\"messages\",  # [!code highlight]\n):\n    # Filter the streamed tokens by the tags field in the metadata to only include\n    # the tokens from the LLM invocation with the \"joke\" tag\n    if metadata[\"tags\"] == [\"joke\"]:\n        print(msg.content, end=\"|\", flush=True)\npython  theme={null}\n  from typing import TypedDict\n\nfrom langchain.chat_models import init_chat_model\n  from langgraph.graph import START, StateGraph\n\n# The joke_model is tagged with \"joke\"\n  joke_model = init_chat_model(model=\"gpt-4o-mini\", tags=[\"joke\"])\n  # The poem_model is tagged with \"poem\"\n  poem_model = init_chat_model(model=\"gpt-4o-mini\", tags=[\"poem\"])\n\nclass State(TypedDict):\n        topic: str\n        joke: str\n        poem: str\n\nasync def call_model(state, config):\n        topic = state[\"topic\"]\n        print(\"Writing joke...\")\n        # Note: Passing the config through explicitly is required for python < 3.11\n        # Since context var support wasn't added before then: https://docs.python.org/3/library/asyncio-task.html#creating-tasks\n        # The config is passed through explicitly to ensure the context vars are propagated correctly\n        # This is required for Python < 3.11 when using async code. Please see the async section for more details\n        joke_response = await joke_model.ainvoke(\n              [{\"role\": \"user\", \"content\": f\"Write a joke about {topic}\"}],\n              config,\n        )\n        print(\"\\n\\nWriting poem...\")\n        poem_response = await poem_model.ainvoke(\n              [{\"role\": \"user\", \"content\": f\"Write a short poem about {topic}\"}],\n              config,\n        )\n        return {\"joke\": joke_response.content, \"poem\": poem_response.content}\n\ngraph = (\n        StateGraph(State)\n        .add_node(call_model)\n        .add_edge(START, \"call_model\")\n        .compile()\n  )\n\n# The stream_mode is set to \"messages\" to stream LLM tokens\n  # The metadata contains information about the LLM invocation, including the tags\n  async for msg, metadata in graph.astream(\n        {\"topic\": \"cats\"},\n        stream_mode=\"messages\",\n  ):\n      if metadata[\"tags\"] == [\"joke\"]:\n          print(msg.content, end=\"|\", flush=True)\n  python  theme={null}",
  "code_samples": [
    {
      "code": "<Accordion title=\"Extended example: filtering by tags\">",
      "language": "unknown"
    },
    {
      "code": "</Accordion>\n\n#### Filter by node\n\nTo stream tokens only from specific nodes, use `stream_mode=\"messages\"` and filter the outputs by the `langgraph_node` field in the streamed metadata:",
      "language": "unknown"
    }
  ],
  "headings": [],
  "url": "llms-txt#the-metadata-contains-information-about-the-llm-invocation,-including-the-tags",
  "links": []
}