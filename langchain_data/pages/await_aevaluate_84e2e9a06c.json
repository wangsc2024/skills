{
  "title": "await aevaluate(...)",
  "content": "results = await ls_client.aevaluate(\n    researcher_app,\n    data=dataset,\n    evaluators=[concise],\n    # Optional, add concurrency.\n    max_concurrency=2,  # Optional, add concurrency.\n    experiment_prefix=\"gpt-4o-mini-baseline\"  # Optional, random by default.\n)\n```\n\n* [Run an evaluation (synchronously)](/langsmith/evaluate-llm-application)\n* [Handle model rate limits](/langsmith/rate-limiting)\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/evaluation-async.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "code_samples": [],
  "headings": [
    {
      "level": "h2",
      "text": "Related",
      "id": "related"
    }
  ],
  "url": "llms-txt#await-aevaluate(...)",
  "links": []
}