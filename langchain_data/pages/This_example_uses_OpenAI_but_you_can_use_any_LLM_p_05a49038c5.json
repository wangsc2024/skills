{
  "title": "This example uses OpenAI, but you can use any LLM provider of choice",
  "content": "export OPENAI_API_KEY=<your-openai-api-key>\npython Python theme={null}\n  import json\n  import openai\n  import operator\n  from langsmith import traceable\n  from langsmith.wrappers import wrap_openai\n  from typing import Annotated, Literal, TypedDict\n  from langgraph.graph import StateGraph\n\nclass State(TypedDict):\n      messages: Annotated[list, operator.add]\n\ntool_schema = {\n      \"type\": \"function\",\n      \"function\": {\n          \"name\": \"search\",\n          \"description\": \"Call to surf the web.\",\n          \"parameters\": {\n              \"type\": \"object\",\n              \"properties\": {\"query\": {\"type\": \"string\"}},\n              \"required\": [\"query\"],\n          },\n      },\n  }\n\n# Decorating the tool function will automatically trace it with the correct context\n  @traceable(run_type=\"tool\", name=\"Search Tool\")\n  def search(query: str):\n      \"\"\"Call to surf the web.\"\"\"\n      if \"sf\" in query.lower() or \"san francisco\" in query.lower():\n          return \"It's 60 degrees and foggy.\"\n      return \"It's 90 degrees and sunny.\"\n\ndef call_tools(state):\n      function_name_to_function = {\"search\": search}\n      messages = state[\"messages\"]\n      tool_call = messages[-1][\"tool_calls\"][0]\n      function_name = tool_call[\"function\"][\"name\"]\n      function_arguments = tool_call[\"function\"][\"arguments\"]\n      arguments = json.loads(function_arguments)\n      function_response = function_name_to_function[function_name](**arguments)\n      tool_message = {\n          \"tool_call_id\": tool_call[\"id\"],\n          \"role\": \"tool\",\n          \"name\": function_name,\n          \"content\": function_response,\n      }\n      return {\"messages\": [tool_message]}\n\nwrapped_client = wrap_openai(openai.Client())\n\ndef should_continue(state: State) -> Literal[\"tools\", \"__end__\"]:\n      messages = state[\"messages\"]\n      last_message = messages[-1]\n      if last_message[\"tool_calls\"]:\n          return \"tools\"\n      return \"__end__\"\n\ndef call_model(state: State):\n      messages = state[\"messages\"]\n      # Calling the wrapped client will automatically infer the correct tracing context\n      response = wrapped_client.chat.completions.create(\n          messages=messages, model=\"gpt-4o-mini\", tools=[tool_schema]\n      )\n      raw_tool_calls = response.choices[0].message.tool_calls\n      tool_calls = [tool_call.to_dict() for tool_call in raw_tool_calls] if raw_tool_calls else []\n      response_message = {\n          \"role\": \"assistant\",\n          \"content\": response.choices[0].message.content,\n          \"tool_calls\": tool_calls,\n      }\n      return {\"messages\": [response_message]}\n\nworkflow = StateGraph(State)\n  workflow.add_node(\"agent\", call_model)\n  workflow.add_node(\"tools\", call_tools)\n  workflow.add_edge(\"__start__\", \"agent\")\n  workflow.add_conditional_edges(\n      \"agent\",\n      should_continue,\n  )\n  workflow.add_edge(\"tools\", 'agent')\n\napp = workflow.compile()\n\nfinal_state = app.invoke(\n      {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n  )\n\nfinal_state[\"messages\"][-1][\"content\"]\n  typescript TypeScript theme={null}\n  **Note:** The below example requires `langsmith>=0.1.39` and `@langchain/langgraph>=0.0.31`\n\nimport OpenAI from \"openai\";\n  import { StateGraph } from \"@langchain/langgraph\";\n  import { wrapOpenAI } from \"langsmith/wrappers/openai\";\n  import { traceable } from \"langsmith/traceable\";\n\ntype GraphState = {\n    messages: OpenAI.ChatCompletionMessageParam[];\n  };\n\nconst wrappedClient = wrapOpenAI(new OpenAI({}));\n\nconst toolSchema: OpenAI.ChatCompletionTool = {\n    type: \"function\",\n    function: {\n      name: \"search\",\n      description: \"Use this tool to query the web.\",\n      parameters: {\n        type: \"object\",\n        properties: {\n          query: {\n            type: \"string\",\n          },\n        },\n        required: [\"query\"],\n      }\n    }\n  };\n\n// Wrapping the tool function will automatically trace it with the correct context\n  const search = traceable(async ({ query }: { query: string }) => {\n    if (\n      query.toLowerCase().includes(\"sf\") ||\n      query.toLowerCase().includes(\"san francisco\")\n    ) {\n      return \"It's 60 degrees and foggy.\";\n    }\n    return \"It's 90 degrees and sunny.\";\n  }, { run_type: \"tool\", name: \"Search Tool\" });\n\nconst callTools = async ({ messages }: GraphState) => {\n    const mostRecentMessage = messages[messages.length - 1];\n    const toolCalls = (mostRecentMessage as OpenAI.ChatCompletionAssistantMessageParam).tool_calls;\n    if (toolCalls === undefined || toolCalls.length === 0) {\n      throw new Error(\"No tool calls passed to node.\");\n    }\n    const toolNameMap = {\n      search,\n    };\n    const functionName = toolCalls[0].function.name;\n    const functionArguments = JSON.parse(toolCalls[0].function.arguments);\n    const response = await toolNameMap[functionName](functionArguments);\n    const toolMessage = {\n      tool_call_id: toolCalls[0].id,\n      role: \"tool\",\n      name: functionName,\n      content: response,\n    }\n    return { messages: [toolMessage] };\n  };\n\nconst callModel = async ({ messages }: GraphState) => {\n    // Calling the wrapped client will automatically infer the correct tracing context\n    const response = await wrappedClient.chat.completions.create({\n      messages,\n      model: \"gpt-4o-mini\",\n      tools: [toolSchema],\n    });\n    const responseMessage = {\n      role: \"assistant\",\n      content: response.choices[0].message.content,\n      tool_calls: response.choices[0].message.tool_calls ?? [],\n    };\n    return { messages: [responseMessage] };\n  };\n\nconst shouldContinue = ({ messages }: GraphState) => {\n    const lastMessage =\n      messages[messages.length - 1] as OpenAI.ChatCompletionAssistantMessageParam;\n    if (\n      lastMessage?.tool_calls !== undefined &&\n      lastMessage?.tool_calls.length > 0\n    ) {\n      return \"tools\";\n    }\n    return \"__end__\";\n  }\n\nconst workflow = new StateGraph<GraphState>({\n    channels: {\n      messages: {\n        reducer: (a: any, b: any) => a.concat(b),\n      }\n    }\n  });\n\nconst graph = workflow\n    .addNode(\"model\", callModel)\n    .addNode(\"tools\", callTools)\n    .addEdge(\"__start__\", \"model\")\n    .addConditionalEdges(\"model\", shouldContinue, {\n      tools: \"tools\",\n      __end__: \"__end__\",\n    })\n    .addEdge(\"tools\", \"model\")\n    .compile();\n\nawait graph.invoke({\n    messages: [{ role: \"user\", content: \"what is the weather in sf\" }]\n  });\n  ```\n</CodeGroup>\n\nAn example trace from running the above code [looks like this](https://smith.langchain.com/public/353f27da-c221-4b67-b9ec-ede3777f3271/r):\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-without-langchain-trace.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=abe0ae173d182563c343f6596e0ce4e2\" alt=\"Trace tree for a LangGraph run without LangChain\" data-og-width=\"3296\" width=\"3296\" data-og-height=\"1774\" height=\"1774\" data-path=\"langsmith/images/langgraph-without-langchain-trace.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-without-langchain-trace.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=794e9ce04677bbf721880ebb07ada7c6 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-without-langchain-trace.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=d6208cbec91ba187ba8f75f6cc916b3f 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-without-langchain-trace.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=418509190558d6a87363d3ba146b7722 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-without-langchain-trace.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=a3c3e2e11cbdac8c32e80e7a895b1eb0 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-without-langchain-trace.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=ea893862eb3f9bc5910649cf0ccd2abe 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-without-langchain-trace.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=2b244ca0ec1296552991428d1efffde6 2500w\" />\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-with-langgraph.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "code_samples": [
    {
      "code": "<Info>\n  If you are using LangChain.js with LangSmith and are not in a serverless environment, we also recommend setting the following explicitly to reduce latency:\n\n  `export LANGCHAIN_CALLBACKS_BACKGROUND=true`\n\n  If you are in a serverless environment, we recommend setting the reverse to allow tracing to finish before your function ends:\n\n  `export LANGCHAIN_CALLBACKS_BACKGROUND=false`\n\n  See [this LangChain.js guide](https://js.langchain.com/docs/how_to/callbacks_serverless) for more information.\n</Info>\n\n### 3. Log a trace\n\nOnce you've set up your environment, [wrap or decorate the custom functions/SDKs](/langsmith/annotate-code#use-traceable--traceable) you want to trace. LangSmith will then infer the proper tracing config:\n\n<CodeGroup>",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h3",
      "text": "3. Log a trace",
      "id": "3.-log-a-trace"
    }
  ],
  "url": "llms-txt#this-example-uses-openai,-but-you-can-use-any-llm-provider-of-choice",
  "links": []
}