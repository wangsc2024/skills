{
  "title": "How to read experiment results locally",
  "content": "Source: https://docs.langchain.com/langsmith/read-local-experiment-results\n\nWhen running [evaluations](/langsmith/evaluation-concepts), you may want to process results programmatically in your script rather than viewing them in the [LangSmith UI](https://smith.langchain.com). This is useful for scenarios like:\n\n* **CI/CD pipelines**: Implement quality gates that fail builds if evaluation scores drop below a threshold.\n* **Local debugging**: Inspect and analyze results without API calls.\n* **Custom aggregations**: Calculate metrics and statistics using your own logic.\n* **Integration testing**: Use evaluation results to gate merges or deployments.\n\nThis guide shows you how to iterate over and process [experiment](/langsmith/evaluation-concepts#experiment) results from the [`ExperimentResults`](https://reference.langchain.com/python/langsmith/observability/sdk/evaluation/#langsmith.evaluation._runner.ExperimentResults) object returned by [`Client.evaluate()`](https://reference.langchain.com/python/langsmith/observability/sdk/client/#langsmith.client.Client.evaluate).\n\n<Note>\n  This page focuses on processing results programmatically while still uploading them to LangSmith.\n\nIf you want to run evaluations locally **without** recording anything to LangSmith (for quick testing or validation), refer to [Run an evaluation locally](/langsmith/local) which uses `upload_results=False`.\n</Note>\n\n## Iterate over evaluation results\n\nThe [`evaluate()`](https://reference.langchain.com/python/langsmith/observability/sdk/client/#langsmith.client.Client.evaluate) function returns an [`ExperimentResults`](https://reference.langchain.com/python/langsmith/observability/sdk/evaluation/#langsmith.evaluation._runner.ExperimentResults) object that you can iterate over. The `blocking` parameter controls when results become available:\n\n* `blocking=False`: Returns immediately with an iterator that yields results as they're produced. This allows you to process results in real-time as the evaluation runs.\n* `blocking=True` (default): Blocks until all evaluations complete before returning. When you iterate over the results, all data is already available.\n\nBoth modes return the same `ExperimentResults` type; the difference is whether the function waits for completion before returning. Use `blocking=False` for streaming and real-time debugging, or `blocking=True` for batch processing when you need the complete dataset.\n\nThe following example demonstrates `blocking=False`. It iterates over results as they stream in, collects them in a list, then processes them in a separate loop:\n\n```python  theme={null}\nfrom langsmith import Client\nimport random\n\ndef target(inputs):\n    \"\"\"Your application or LLM chain\"\"\"\n    return {\"output\": \"MY OUTPUT\"}\n\ndef evaluator(run, example):\n    \"\"\"Your evaluator function\"\"\"\n    return {\"key\": \"randomness\", \"score\": random.randint(0, 1)}",
  "code_samples": [],
  "headings": [
    {
      "level": "h2",
      "text": "Iterate over evaluation results",
      "id": "iterate-over-evaluation-results"
    }
  ],
  "url": "llms-txt#how-to-read-experiment-results-locally",
  "links": []
}