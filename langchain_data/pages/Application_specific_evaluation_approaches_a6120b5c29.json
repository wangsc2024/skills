{
  "title": "Application-specific evaluation approaches",
  "content": "Source: https://docs.langchain.com/langsmith/evaluation-approaches\n\nBelow, we will discuss evaluation of a few popular types of LLM applications.\n\n[LLM-powered autonomous agents](https://lilianweng.github.io/posts/2023-06-23-agent/) combine three components (1) Tool calling, (2) Memory, and (3) Planning. Agents [use tool calling](https://python.langchain.com/v0.1/docs/modules/agents/agent_types/tool_calling/) with planning (e.g., often via prompting) and memory (e.g., often short-term message history) to generate responses. [Tool calling](https://python.langchain.com/v0.1/docs/modules/model_io/chat/function_calling/) allows a model to respond to a given prompt by generating two things: (1) a tool to invoke and (2) the input arguments required.\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tool-use.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=a1c10f940f40ad89c90de8fae3607c1f\" alt=\"Tool use\" data-og-width=\"1021\" width=\"1021\" data-og-height=\"424\" height=\"424\" data-path=\"langsmith/images/tool-use.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tool-use.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=e80012647614cd82cb468430e62fa9aa 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tool-use.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=67a5febcec316bfe2935ba65506558a2 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tool-use.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=e0b52fb90bdbc41332b09404823fbfca 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tool-use.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=8a4e6b4bda0f788f540ca240012c9e89 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tool-use.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=d403f18524e0904c716d0cba9e928cac 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tool-use.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=69133b8862e6738000f733ff7e34daae 2500w\" />\n\nBelow is a tool-calling agent in [LangGraph](https://langchain-ai.github.io/langgraph/tutorials/introduction/). The `assistant node` is an LLM that determines whether to invoke a tool based upon the input. The `tool condition` sees if a tool was selected by the `assistant node` and, if so, routes to the `tool node`. The `tool node` executes the tool and returns the output as a tool message to the `assistant node`. This loop continues until as long as the `assistant node` selects a tool. If no tool is selected, then the agent directly returns the LLM response.\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-agent.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=37f3c09958c1e2543f633c59cc89df36\" alt=\"Agent\" data-og-width=\"1259\" width=\"1259\" data-og-height=\"492\" height=\"492\" data-path=\"langsmith/images/langgraph-agent.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-agent.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=59cecf5d27098da369bbf60d6a315437 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-agent.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=81426ee5f37211859c572fe51d837c44 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-agent.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=558ca3297a7697a12989f0bdcfd4a4d7 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-agent.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=39a3a80c152f99f133302fe79f4bf63d 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-agent.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=46164054792e7f730cef4897fd9cbf57 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-agent.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=553d05e97a4d052882b6381e1e8b0362 2500w\" />\n\nThis sets up three general types of agent evaluations that users are often interested in:\n\n* `Final Response`: Evaluate the agent's final response.\n* `Single step`: Evaluate any agent step in isolation (e.g., whether it selects the appropriate tool).\n* `Trajectory`: Evaluate whether the agent took the expected path (e.g., of tool calls) to arrive at the final answer.\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/agent-eval.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=5fe3c96402623ed8a61118f22a6426b6\" alt=\"Agent-eval\" data-og-width=\"1825\" width=\"1825\" data-og-height=\"915\" height=\"915\" data-path=\"langsmith/images/agent-eval.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/agent-eval.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=780c3ea6fecdbd41fa62017d3ac6042e 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/agent-eval.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=f43c0cd9d9b49ae1f2a7ead5f5e58bcd 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/agent-eval.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=360b12b491fb97fd46d444e440345737 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/agent-eval.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=88fd055136cd48a538d908e3899daa6e 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/agent-eval.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=c3e40f0ba26f26f2688553596487ab57 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/agent-eval.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=7b238d8ce02b6d1ab3d6ff3fb2c62d4e 2500w\" />\n\nBelow we will cover what these are, the components (inputs, outputs, evaluators) needed for each one, and when you should consider this. Note that you likely will want to do multiple (if not all!) of these types of evaluations - they are not mutually exclusive!\n\n### Evaluating an agent's final response\n\nOne way to evaluate an agent is to assess its overall performance on a task. This basically involves treating the agent as a black box and simply evaluating whether or not it gets the job done.\n\nThe inputs should be the user input and (optionally) a list of tools. In some cases, tool are hardcoded as part of the agent and they don't need to be passed in. In other cases, the agent is more generic, meaning it does not have a fixed set of tools and tools need to be passed in at run time.\n\nThe output should be the agent's final response.\n\nThe evaluator varies depending on the task you are asking the agent to do. Many agents perform a relatively complex set of steps and the output a final text response. Similar to RAG, LLM-as-judge evaluators are often effective for evaluation in these cases because they can assess whether the agent got a job done directly from the text response.\n\nHowever, there are several downsides to this type of evaluation. First, it usually takes a while to run. Second, you are not evaluating anything that happens inside the agent, so it can be hard to debug when failures occur. Third, it can sometimes be hard to define appropriate evaluation metrics.\n\n### Evaluating a single step of an agent\n\nAgents generally perform multiple actions. While it is useful to evaluate them end-to-end, it can also be useful to evaluate these individual actions. This generally involves evaluating a single step of the agent - the LLM call where it decides what to do.\n\nThe inputs should be the input to a single step. Depending on what you are testing, this could just be the raw user input (e.g., a prompt and / or a set of tools) or it can also include previously completed steps.\n\nThe outputs are just the output of that step, which is usually the LLM response. The LLM response often contains tool calls, indicating what action the agent should take next.\n\nThe evaluator for this is usually some binary score for whether the correct tool call was selected, as well as some heuristic for whether the input to the tool was correct. The reference tool can be simply specified as a string.\n\nThere are several benefits to this type of evaluation. It allows you to evaluate individual actions, which lets you hone in where your application may be failing. They are also relatively fast to run (because they only involve a single LLM call) and evaluation often uses simple heuristic evaluation of the selected tool relative to the reference tool. One downside is that they don't capture the full agent - only one particular step. Another downside is that dataset creation can be challenging, particular if you want to include past history in the agent input. It is pretty easy to generate a dataset for steps early on in an agent's trajectory (e.g., this may only include the input prompt), but it can be difficult to generate a dataset for steps later on in the trajectory (e.g., including numerous prior agent actions and responses).\n\n### Evaluating an agent's trajectory\n\nEvaluating an agent's trajectory involves evaluating all the steps an agent took.\n\nThe inputs are again the inputs to the overall agent (the user input, and optionally a list of tools).\n\nThe outputs are a list of tool calls, which can be formulated as an \"exact\" trajectory (e.g., an expected sequence of tool calls) or simply a set of tool calls that are expected (in any order).\n\nThe evaluator here is some function over the steps taken. Assessing the \"exact\" trajectory can use a single binary score that confirms an exact match for each tool name in the sequence. This is simple, but has some flaws. Sometimes there can be multiple correct paths. This evaluation also does not capture the difference between a trajectory being off by a single step versus being completely wrong.\n\nTo address these flaws, evaluation metrics can focused on the number of \"incorrect\" steps taken, which better accounts for trajectories that are close versus ones that deviate significantly. Evaluation metrics can also focus on whether all of the expected tools are called in any order.\n\nHowever, none of these approaches evaluate the input to the tools; they only focus on the tools selected. In order to account for this, another evaluation technique is to pass the full agent's trajectory (along with a reference trajectory) as a set of messages (e.g., all LLM responses and tool calls) an LLM-as-judge. This can evaluate the complete behavior of the agent, but it is the most challenging reference to compile (luckily, using a framework like LangGraph can help with this!). Another downside is that evaluation metrics can be somewhat tricky to come up with.\n\n## Retrieval augmented generation (RAG)\n\nRetrieval Augmented Generation (RAG) is a powerful technique that involves retrieving relevant documents based on a user's input and passing them to a language model for processing. RAG enables AI applications to generate more informed and context-aware responses by leveraging external knowledge.\n\n<Info>\n  For a comprehensive review of RAG concepts, see our [`RAG From Scratch` series](https://github.com/langchain-ai/rag-from-scratch).\n</Info>\n\nWhen evaluating RAG applications, a key consideration is whether you have (or can easily obtain) reference answers for each input question. Reference answers serve as ground truth for assessing the correctness of the generated responses. However, even in the absence of reference answers, various evaluations can still be performed using reference-free RAG evaluation prompts (examples provided below).\n\n`LLM-as-judge` is a commonly used evaluator for RAG because it's an effective way to evaluate factual accuracy or consistency between texts.\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rag-types.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=1252b1369be04ddb4c480af277443ac2\" alt=\"rag-types.png\" data-og-width=\"1696\" width=\"1696\" data-og-height=\"731\" height=\"731\" data-path=\"langsmith/images/rag-types.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rag-types.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=dda9fa7e589b9d37bd31a5ba63d1f0fb 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rag-types.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=e16fca6e49aeb889cc9d6e04baca684a 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rag-types.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=de442b611295218493a5783820794727 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rag-types.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=ad286f13e4a15ec892f5995a539daf5d 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rag-types.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=64c7b545e34ac5d4bf975ee2f44af8df 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rag-types.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=447cb995acde1f42ffeee2883c61a033 2500w\" />\n\nWhen evaluating RAG applications, you can have evaluators that require reference outputs and those that don't:\n\n1. **Require reference output**: Compare the RAG chain's generated answer or retrievals against a reference answer (or retrievals) to assess its correctness.\n2. **Don't require reference output**: Perform self-consistency checks using prompts that don't require a reference answer (represented by orange, green, and red in the above figure).\n\n### Applying RAG Evaluation\n\nWhen applying RAG evaluation, consider the following approaches:\n\n1. `Offline evaluation`: Use offline evaluation for any prompts that rely on a reference answer. This is most commonly used for RAG answer correctness evaluation, where the reference is a ground truth (correct) answer.\n\n2. `Online evaluation`: Employ online evaluation for any reference-free prompts. This allows you to assess the RAG application's performance in real-time scenarios.\n\n3. `Pairwise evaluation`: Utilize pairwise evaluation to compare answers produced by different RAG chains. This evaluation focuses on user-specified criteria (e.g., answer format or style) rather than correctness, which can be evaluated using self-consistency or a ground truth reference.\n\n### RAG evaluation summary\n\n| Evaluator           | Detail                                            | Needs reference output | LLM-as-judge?                                                                         | Pairwise relevant |\n| ------------------- | ------------------------------------------------- | ---------------------- | ------------------------------------------------------------------------------------- | ----------------- |\n| Document relevance  | Are documents relevant to the question?           | No                     | Yes - [prompt](https://smith.langchain.com/hub/langchain-ai/rag-document-relevance)   | No                |\n| Answer faithfulness | Is the answer grounded in the documents?          | No                     | Yes - [prompt](https://smith.langchain.com/hub/langchain-ai/rag-answer-hallucination) | No                |\n| Answer helpfulness  | Does the answer help address the question?        | No                     | Yes - [prompt](https://smith.langchain.com/hub/langchain-ai/rag-answer-helpfulness)   | No                |\n| Answer correctness  | Is the answer consistent with a reference answer? | Yes                    | Yes - [prompt](https://smith.langchain.com/hub/langchain-ai/rag-answer-vs-reference)  | No                |\n| Pairwise comparison | How do multiple answer versions compare?          | No                     | Yes - [prompt](https://smith.langchain.com/hub/langchain-ai/pairwise-evaluation-rag)  | Yes               |\n\nSummarization is one specific type of free-form writing. The evaluation aim is typically to examine the writing (summary) relative to a set of criteria.\n\n`Developer curated examples` of texts to summarize are commonly used for evaluation (see a dataset example [here](https://smith.langchain.com/public/659b07af-1cab-4e18-b21a-91a69a4c3990/d)). However, `user logs` from a production (summarization) app can be used for online evaluation with any of the `Reference-free` evaluation prompts below.\n\n`LLM-as-judge` is typically used for evaluation of summarization (as well as other types of writing) using `Reference-free` prompts that follow provided criteria to grade a summary. It is less common to provide a particular `Reference` summary, because summarization is a creative task and there are many possible correct answers.\n\n`Online` or `Offline` evaluation are feasible because of the `Reference-free` prompt used. `Pairwise` evaluation is also a powerful way to perform comparisons between different summarization chains (e.g., different summarization prompts or LLMs):\n\n| Use Case         | Detail                                                                     | Needs reference output | LLM-as-judge?                                                                                | Pairwise relevant |\n| ---------------- | -------------------------------------------------------------------------- | ---------------------- | -------------------------------------------------------------------------------------------- | ----------------- |\n| Factual accuracy | Is the summary accurate relative to the source documents?                  | No                     | Yes - [prompt](https://smith.langchain.com/hub/langchain-ai/summary-accurancy-evaluator)     | Yes               |\n| Faithfulness     | Is the summary grounded in the source documents (e.g., no hallucinations)? | No                     | Yes - [prompt](https://smith.langchain.com/hub/langchain-ai/summary-hallucination-evaluator) | Yes               |\n| Helpfulness      | Is summary helpful relative to user need?                                  | No                     | Yes - [prompt](https://smith.langchain.com/hub/langchain-ai/summary-helpfulness-evaluator)   | Yes               |\n\n## Classification and tagging\n\nClassification and tagging apply a label to a given input (e.g., for toxicity detection, sentiment analysis, etc). Classification/tagging evaluation typically employs the following components, which we will review in detail below:\n\nA central consideration for classification/tagging evaluation is whether you have a dataset with `reference` labels or not. If not, users frequently want to define an evaluator that uses criteria to apply label (e.g., toxicity, etc) to an input (e.g., text, user-question, etc). However, if ground truth class labels are provided, then the evaluation objective is focused on scoring a classification/tagging chain relative to the ground truth class label (e.g., using metrics such as precision, recall, etc).\n\nIf ground truth reference labels are provided, then it's common to simply define a [custom heuristic evaluator](/langsmith/code-evaluator) to compare ground truth labels to the chain output. However, it is increasingly common given the emergence of LLMs simply use `LLM-as-judge` to perform the classification/tagging of an input based upon specified criteria (without a ground truth reference).\n\n`Online` or `Offline` evaluation is feasible when using `LLM-as-judge` with the `Reference-free` prompt used. In particular, this is well suited to `Online` evaluation when a user wants to tag / classify application input (e.g., for toxicity, etc).\n\n| Use Case  | Detail              | Needs reference output | LLM-as-judge? | Pairwise relevant |\n| --------- | ------------------- | ---------------------- | ------------- | ----------------- |\n| Accuracy  | Standard definition | Yes                    | No            | No                |\n| Precision | Standard definition | Yes                    | No            | No                |\n| Recall    | Standard definition | Yes                    | No            | No                |\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/evaluation-approaches.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "code_samples": [],
  "headings": [
    {
      "level": "h2",
      "text": "Agents",
      "id": "agents"
    },
    {
      "level": "h3",
      "text": "Evaluating an agent's final response",
      "id": "evaluating-an-agent's-final-response"
    },
    {
      "level": "h3",
      "text": "Evaluating a single step of an agent",
      "id": "evaluating-a-single-step-of-an-agent"
    },
    {
      "level": "h3",
      "text": "Evaluating an agent's trajectory",
      "id": "evaluating-an-agent's-trajectory"
    },
    {
      "level": "h2",
      "text": "Retrieval augmented generation (RAG)",
      "id": "retrieval-augmented-generation-(rag)"
    },
    {
      "level": "h3",
      "text": "Dataset",
      "id": "dataset"
    },
    {
      "level": "h3",
      "text": "Evaluator",
      "id": "evaluator"
    },
    {
      "level": "h3",
      "text": "Applying RAG Evaluation",
      "id": "applying-rag-evaluation"
    },
    {
      "level": "h3",
      "text": "RAG evaluation summary",
      "id": "rag-evaluation-summary"
    },
    {
      "level": "h2",
      "text": "Summarization",
      "id": "summarization"
    },
    {
      "level": "h2",
      "text": "Classification and tagging",
      "id": "classification-and-tagging"
    }
  ],
  "url": "llms-txt#application-specific-evaluation-approaches",
  "links": []
}