{
  "title": "To reject",
  "content": "graph.invoke(Command(resume=False), config=config)\npython  theme={null}\n  from typing import Literal, Optional, TypedDict\n\nfrom langgraph.checkpoint.memory import MemorySaver\n  from langgraph.graph import StateGraph, START, END\n  from langgraph.types import Command, interrupt\n\nclass ApprovalState(TypedDict):\n      action_details: str\n      status: Optional[Literal[\"pending\", \"approved\", \"rejected\"]]\n\ndef approval_node(state: ApprovalState) -> Command[Literal[\"proceed\", \"cancel\"]]:\n      # Expose details so the caller can render them in a UI\n      decision = interrupt({\n          \"question\": \"Approve this action?\",\n          \"details\": state[\"action_details\"],\n      })\n\n# Route to the appropriate node after resume\n      return Command(goto=\"proceed\" if decision else \"cancel\")\n\ndef proceed_node(state: ApprovalState):\n      return {\"status\": \"approved\"}\n\ndef cancel_node(state: ApprovalState):\n      return {\"status\": \"rejected\"}\n\nbuilder = StateGraph(ApprovalState)\n  builder.add_node(\"approval\", approval_node)\n  builder.add_node(\"proceed\", proceed_node)\n  builder.add_node(\"cancel\", cancel_node)\n  builder.add_edge(START, \"approval\")\n  builder.add_edge(\"proceed\", END)\n  builder.add_edge(\"cancel\", END)\n\n# Use a more durable checkpointer in production\n  checkpointer = MemorySaver()\n  graph = builder.compile(checkpointer=checkpointer)\n\nconfig = {\"configurable\": {\"thread_id\": \"approval-123\"}}\n  initial = graph.invoke(\n      {\"action_details\": \"Transfer $500\", \"status\": \"pending\"},\n      config=config,\n  )\n  print(initial[\"__interrupt__\"])  # -> [Interrupt(value={'question': ..., 'details': ...})]\n\n# Resume with the decision; True routes to proceed, False to cancel\n  resumed = graph.invoke(Command(resume=True), config=config)\n  print(resumed[\"status\"])  # -> \"approved\"\n  python  theme={null}\nfrom langgraph.types import interrupt\n\ndef review_node(state: State):\n    # Pause and show the current content for review (surfaces in result[\"__interrupt__\"])\n    edited_content = interrupt({\n        \"instruction\": \"Review and edit this content\",\n        \"content\": state[\"generated_text\"]\n    })\n\n# Update the state with the edited version\n    return {\"generated_text\": edited_content}\npython  theme={null}\ngraph.invoke(\n    Command(resume=\"The edited and improved text\"),  # Value becomes the return from interrupt()\n    config=config\n)\npython  theme={null}\n  import sqlite3\n  from typing import TypedDict\n\nfrom langgraph.checkpoint.memory import MemorySaver\n  from langgraph.graph import StateGraph, START, END\n  from langgraph.types import Command, interrupt\n\nclass ReviewState(TypedDict):\n      generated_text: str\n\ndef review_node(state: ReviewState):\n      # Ask a reviewer to edit the generated content\n      updated = interrupt({\n          \"instruction\": \"Review and edit this content\",\n          \"content\": state[\"generated_text\"],\n      })\n      return {\"generated_text\": updated}\n\nbuilder = StateGraph(ReviewState)\n  builder.add_node(\"review\", review_node)\n  builder.add_edge(START, \"review\")\n  builder.add_edge(\"review\", END)\n\ncheckpointer = MemorySaver()\n  graph = builder.compile(checkpointer=checkpointer)\n\nconfig = {\"configurable\": {\"thread_id\": \"review-42\"}}\n  initial = graph.invoke({\"generated_text\": \"Initial draft\"}, config=config)\n  print(initial[\"__interrupt__\"])  # -> [Interrupt(value={'instruction': ..., 'content': ...})]\n\n# Resume with the edited text from the reviewer\n  final_state = graph.invoke(\n      Command(resume=\"Improved draft after review\"),\n      config=config,\n  )\n  print(final_state[\"generated_text\"])  # -> \"Improved draft after review\"\n  python  theme={null}\nfrom langchain.tools import tool\nfrom langgraph.types import interrupt\n\n@tool\ndef send_email(to: str, subject: str, body: str):\n    \"\"\"Send an email to a recipient.\"\"\"\n\n# Pause before sending; payload surfaces in result[\"__interrupt__\"]\n    response = interrupt({\n        \"action\": \"send_email\",\n        \"to\": to,\n        \"subject\": subject,\n        \"body\": body,\n        \"message\": \"Approve sending this email?\"\n    })\n\nif response.get(\"action\") == \"approve\":\n        # Resume value can override inputs before executing\n        final_to = response.get(\"to\", to)\n        final_subject = response.get(\"subject\", subject)\n        final_body = response.get(\"body\", body)\n        return f\"Email sent to {final_to} with subject '{final_subject}'\"\n    return \"Email cancelled by user\"\npython  theme={null}\n  import sqlite3\n  from typing import TypedDict\n\nfrom langchain.tools import tool\n  from langchain_anthropic import ChatAnthropic\n  from langgraph.checkpoint.sqlite import SqliteSaver\n  from langgraph.graph import StateGraph, START, END\n  from langgraph.types import Command, interrupt\n\nclass AgentState(TypedDict):\n      messages: list[dict]\n\n@tool\n  def send_email(to: str, subject: str, body: str):\n      \"\"\"Send an email to a recipient.\"\"\"\n\n# Pause before sending; payload surfaces in result[\"__interrupt__\"]\n      response = interrupt({\n          \"action\": \"send_email\",\n          \"to\": to,\n          \"subject\": subject,\n          \"body\": body,\n          \"message\": \"Approve sending this email?\",\n      })\n\nif response.get(\"action\") == \"approve\":\n          final_to = response.get(\"to\", to)\n          final_subject = response.get(\"subject\", subject)\n          final_body = response.get(\"body\", body)\n\n# Actually send the email (your implementation here)\n          print(f\"[send_email] to={final_to} subject={final_subject} body={final_body}\")\n          return f\"Email sent to {final_to}\"\n\nreturn \"Email cancelled by user\"\n\nmodel = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\").bind_tools([send_email])\n\ndef agent_node(state: AgentState):\n      # LLM may decide to call the tool; interrupt pauses before sending\n      result = model.invoke(state[\"messages\"])\n      return {\"messages\": state[\"messages\"] + [result]}\n\nbuilder = StateGraph(AgentState)\n  builder.add_node(\"agent\", agent_node)\n  builder.add_edge(START, \"agent\")\n  builder.add_edge(\"agent\", END)\n\ncheckpointer = SqliteSaver(sqlite3.connect(\"tool-approval.db\"))\n  graph = builder.compile(checkpointer=checkpointer)\n\nconfig = {\"configurable\": {\"thread_id\": \"email-workflow\"}}\n  initial = graph.invoke(\n      {\n          \"messages\": [\n              {\"role\": \"user\", \"content\": \"Send an email to alice@example.com about the meeting\"}\n          ]\n      },\n      config=config,\n  )\n  print(initial[\"__interrupt__\"])  # -> [Interrupt(value={'action': 'send_email', ...})]\n\n# Resume with approval and optionally edited arguments\n  resumed = graph.invoke(\n      Command(resume={\"action\": \"approve\", \"subject\": \"Updated subject\"}),\n      config=config,\n  )\n  print(resumed[\"messages\"][-1])  # -> Tool result returned by send_email\n  python  theme={null}\nfrom langgraph.types import interrupt\n\ndef get_age_node(state: State):\n    prompt = \"What is your age?\"\n\nwhile True:\n        answer = interrupt(prompt)  # payload surfaces in result[\"__interrupt__\"]\n\n# Validate the input\n        if isinstance(answer, int) and answer > 0:\n            # Valid input - continue\n            break\n        else:\n            # Invalid input - ask again with a more specific prompt\n            prompt = f\"'{answer}' is not a valid age. Please enter a positive number.\"\n\nreturn {\"age\": answer}\npython  theme={null}\n  import sqlite3\n  from typing import TypedDict\n\nfrom langgraph.checkpoint.sqlite import SqliteSaver\n  from langgraph.graph import StateGraph, START, END\n  from langgraph.types import Command, interrupt\n\nclass FormState(TypedDict):\n      age: int | None\n\ndef get_age_node(state: FormState):\n      prompt = \"What is your age?\"\n\nwhile True:\n          answer = interrupt(prompt)  # payload surfaces in result[\"__interrupt__\"]\n\nif isinstance(answer, int) and answer > 0:\n              return {\"age\": answer}\n\nprompt = f\"'{answer}' is not a valid age. Please enter a positive number.\"\n\nbuilder = StateGraph(FormState)\n  builder.add_node(\"collect_age\", get_age_node)\n  builder.add_edge(START, \"collect_age\")\n  builder.add_edge(\"collect_age\", END)\n\ncheckpointer = SqliteSaver(sqlite3.connect(\"forms.db\"))\n  graph = builder.compile(checkpointer=checkpointer)\n\nconfig = {\"configurable\": {\"thread_id\": \"form-1\"}}\n  first = graph.invoke({\"age\": None}, config=config)\n  print(first[\"__interrupt__\"])  # -> [Interrupt(value='What is your age?', ...)]\n\n# Provide invalid data; the node re-prompts\n  retry = graph.invoke(Command(resume=\"thirty\"), config=config)\n  print(retry[\"__interrupt__\"])  # -> [Interrupt(value=\"'thirty' is not a valid age...\", ...)]\n\n# Provide valid data; loop exits and state updates\n  final = graph.invoke(Command(resume=30), config=config)\n  print(final[\"age\"])  # -> 30\n  python Separating logic theme={null}\n  def node_a(state: State):\n      # ‚úÖ Good: interrupting first, then handling\n      # error conditions separately\n      interrupt(\"What's your name?\")\n      try:\n          fetch_data()  # This can fail\n      except Exception as e:\n          print(e)\n      return state\n  python Explicit exception handling theme={null}\n  def node_a(state: State):\n      # ‚úÖ Good: catching specific exception types\n      # will not catch the interrupt exception\n      try:\n          name = interrupt(\"What's your name?\")\n          fetch_data()  # This can fail\n      except NetworkException as e:\n          print(e)\n      return state\n  python  theme={null}\ndef node_a(state: State):\n    # ‚ùå Bad: wrapping interrupt in bare try/except\n    # will catch the interrupt exception\n    try:\n        interrupt(\"What's your name?\")\n    except Exception as e:\n        print(e)\n    return state\npython  theme={null}\ndef node_a(state: State):\n    # ‚úÖ Good: interrupt calls happen in the same order every time\n    name = interrupt(\"What's your name?\")\n    age = interrupt(\"What's your age?\")\n    city = interrupt(\"What's your city?\")\n\nreturn {\n        \"name\": name,\n        \"age\": age,\n        \"city\": city\n    }\npython Skipping interrupts theme={null}\n  def node_a(state: State):\n      # ‚ùå Bad: conditionally skipping interrupts changes the order\n      name = interrupt(\"What's your name?\")\n\n# On first run, this might skip the interrupt\n      # On resume, it might not skip it - causing index mismatch\n      if state.get(\"needs_age\"):\n          age = interrupt(\"What's your age?\")\n\ncity = interrupt(\"What's your city?\")\n\nreturn {\"name\": name, \"city\": city}\n  python Looping interrupts theme={null}\n  def node_a(state: State):\n      # ‚ùå Bad: looping based on non-deterministic data\n      # The number of interrupts changes between executions\n      results = []\n      for item in state.get(\"dynamic_list\", []):  # List might change between runs\n          result = interrupt(f\"Approve {item}?\")\n          results.append(result)\n\nreturn {\"results\": results}\n  python Simple values theme={null}\n  def node_a(state: State):\n      # ‚úÖ Good: passing simple types that are serializable\n      name = interrupt(\"What's your name?\")\n      count = interrupt(42)\n      approved = interrupt(True)\n\nreturn {\"name\": name, \"count\": count, \"approved\": approved}\n  python Structured data theme={null}\n  def node_a(state: State):\n      # ‚úÖ Good: passing dictionaries with simple values\n      response = interrupt({\n          \"question\": \"Enter user details\",\n          \"fields\": [\"name\", \"email\", \"age\"],\n          \"current_values\": state.get(\"user\", {})\n      })\n\nreturn {\"user\": response}\n  python Functions theme={null}\n  def validate_input(value):\n      return len(value) > 0\n\ndef node_a(state: State):\n      # ‚ùå Bad: passing a function to interrupt\n      # The function cannot be serialized\n      response = interrupt({\n          \"question\": \"What's your name?\",\n          \"validator\": validate_input  # This will fail\n      })\n      return {\"name\": response}\n  python Class instances theme={null}\n  class DataProcessor:\n      def __init__(self, config):\n          self.config = config\n\ndef node_a(state: State):\n      processor = DataProcessor({\"mode\": \"strict\"})\n\n# ‚ùå Bad: passing a class instance to interrupt\n      # The instance cannot be serialized\n      response = interrupt({\n          \"question\": \"Enter data to process\",\n          \"processor\": processor  # This will fail\n      })\n      return {\"result\": response}\n  python Idempotent operations theme={null}\n  def node_a(state: State):\n      # ‚úÖ Good: using upsert operation which is idempotent\n      # Running this multiple times will have the same result\n      db.upsert_user(\n          user_id=state[\"user_id\"],\n          status=\"pending_approval\"\n      )\n\napproved = interrupt(\"Approve this change?\")\n\nreturn {\"approved\": approved}\n  python Side effects after interrupt theme={null}\n  def node_a(state: State):\n      # ‚úÖ Good: placing side effect after the interrupt\n      # This ensures it only runs once after approval is received\n      approved = interrupt(\"Approve this change?\")\n\nif approved:\n          db.create_audit_log(\n              user_id=state[\"user_id\"],\n              action=\"approved\"\n          )\n\nreturn {\"approved\": approved}\n  python Separating into different nodes theme={null}\n  def approval_node(state: State):\n      # ‚úÖ Good: only handling the interrupt in this node\n      approved = interrupt(\"Approve this change?\")\n\nreturn {\"approved\": approved}\n\ndef notification_node(state: State):\n      # ‚úÖ Good: side effect happens in a separate node\n      # This runs after approval, so it only executes once\n      if (state.approved):\n          send_notification(\n              user_id=state[\"user_id\"],\n              status=\"approved\"\n          )\n\nreturn state\n  python Creating records theme={null}\n  def node_a(state: State):\n      # ‚ùå Bad: creating a new record before interrupt\n      # This will create duplicate records on each resume\n      audit_id = db.create_audit_log({\n          \"user_id\": state[\"user_id\"],\n          \"action\": \"pending_approval\",\n          \"timestamp\": datetime.now()\n      })\n\napproved = interrupt(\"Approve this change?\")\n\nreturn {\"approved\": approved, \"audit_id\": audit_id}\n  python Appending to lists theme={null}\n  def node_a(state: State):\n      # ‚ùå Bad: appending to a list before interrupt\n      # This will add duplicate entries on each resume\n      db.append_to_history(state[\"user_id\"], \"approval_requested\")\n\napproved = interrupt(\"Approve this change?\")\n\nreturn {\"approved\": approved}\n  python  theme={null}\ndef node_in_parent_graph(state: State):\n    some_code()  # <-- This will re-execute when resumed\n    # Invoke a subgraph as a function.\n    # The subgraph contains an `interrupt` call.\n    subgraph_result = subgraph.invoke(some_input)\n    # ...\n\ndef node_in_subgraph(state: State):\n    some_other_code()  # <-- This will also re-execute when resumed\n    result = interrupt(\"What's your name?\")\n    # ...\npython  theme={null}\n    graph = builder.compile(\n        interrupt_before=[\"node_a\"],  # [!code highlight]\n        interrupt_after=[\"node_b\", \"node_c\"],  # [!code highlight]\n        checkpointer=checkpointer,\n    )\n\n# Pass a thread ID to the graph\n    config = {\n        \"configurable\": {\n            \"thread_id\": \"some_thread\"\n        }\n    }\n\n# Run the graph until the breakpoint\n    graph.invoke(inputs, config=config)  # [!code highlight]\n\n# Resume the graph\n    graph.invoke(None, config=config)  # [!code highlight]\n    python  theme={null}\n    config = {\n        \"configurable\": {\n            \"thread_id\": \"some_thread\"\n        }\n    }\n\n# Run the graph until the breakpoint\n    graph.invoke(\n        inputs,\n        interrupt_before=[\"node_a\"],  # [!code highlight]\n        interrupt_after=[\"node_b\", \"node_c\"],  # [!code highlight]\n        config=config,\n    )\n\n# Resume the graph\n    graph.invoke(None, config=config)  # [!code highlight]\n    ```\n\n1. `graph.invoke` is called with the `interrupt_before` and `interrupt_after` parameters. This is a run-time configuration and can be changed for every invocation.\n    2. `interrupt_before` specifies the nodes where execution should pause before the node is executed.\n    3. `interrupt_after` specifies the nodes where execution should pause after the node is executed.\n    4. The graph is run until the first breakpoint is hit.\n    5. The graph is resumed by passing in `None` for the input. This will run the graph until the next breakpoint is hit.\n  </Tab>\n</Tabs>\n\n### Using LangGraph Studio\n\nYou can use [LangGraph Studio](/langsmith/studio) to set static interrupts in your graph in the UI before running the graph. You can also use the UI to inspect the graph state at any point in the execution.\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/static-interrupt.png?fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=5aa4e7cea2ab147cef5b4e210dd6c4a1\" alt=\"image\" data-og-width=\"1252\" width=\"1252\" data-og-height=\"1040\" height=\"1040\" data-path=\"oss/images/static-interrupt.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/static-interrupt.png?w=280&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=52d02b507d0a6a879f7fb88d9c6767d0 280w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/static-interrupt.png?w=560&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=e363cd4980edff9bab422f4f1c0ee3c8 560w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/static-interrupt.png?w=840&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=49d26a3641953c23ef3fbc51e828c305 840w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/static-interrupt.png?w=1100&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=2dba15683b3baa1a61bc3bcada35ae1e 1100w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/static-interrupt.png?w=1650&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=9f9a2c0f2631c0e69cd248f6319933fe 1650w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/static-interrupt.png?w=2500&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=5a46b765b436ab5d0dc2f41c01ffad80 2500w\" />\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/interrupts.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "code_samples": [
    {
      "code": "<Accordion title=\"Full example\">",
      "language": "unknown"
    },
    {
      "code": "</Accordion>\n\n### Review and edit state\n\nSometimes you want to let a human review and edit part of the graph state before continuing. This is useful for correcting LLMs, adding missing information, or making adjustments.",
      "language": "unknown"
    },
    {
      "code": "When resuming, provide the edited content:",
      "language": "unknown"
    },
    {
      "code": "<Accordion title=\"Full example\">",
      "language": "unknown"
    },
    {
      "code": "</Accordion>\n\n### Interrupts in tools\n\nYou can also place interrupts directly inside tool functions. This makes the tool itself pause for approval whenever it's called, and allows for human review and editing of the tool call before it is executed.\n\nFirst, define a tool that uses [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt):",
      "language": "unknown"
    },
    {
      "code": "This approach is useful when you want the approval logic to live with the tool itself, making it reusable across different parts of your graph. The LLM can call the tool naturally, and the interrupt will pause execution whenever the tool is invoked, allowing you to approve, edit, or cancel the action.\n\n<Accordion title=\"Full example\">",
      "language": "unknown"
    },
    {
      "code": "</Accordion>\n\n### Validating human input\n\nSometimes you need to validate input from humans and ask again if it's invalid. You can do this using multiple [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) calls in a loop.",
      "language": "unknown"
    },
    {
      "code": "Each time you resume the graph with invalid input, it will ask again with a clearer message. Once valid input is provided, the node completes and the graph continues.\n\n<Accordion title=\"Full example\">",
      "language": "unknown"
    },
    {
      "code": "</Accordion>\n\n## Rules of interrupts\n\nWhen you call [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) within a node, LangGraph suspends execution by raising an exception that signals the runtime to pause. This exception propagates up through the call stack and is caught by the runtime, which notifies the graph to save the current state and wait for external input.\n\nWhen execution resumes (after you provide the requested input), the runtime restarts the entire node from the beginning‚Äîit does not resume from the exact line where [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) was called. This means any code that ran before the [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) will execute again. Because of this, there's a few important rules to follow when working with interrupts to ensure they behave as expected.\n\n### Do not wrap `interrupt` calls in try/except\n\nThe way that [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) pauses execution at the point of the call is by throwing a special exception. If you wrap the [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) call in a try/except block, you will catch this exception and the interrupt will not be passed back to the graph.\n\n* ‚úÖ Separate [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) calls from error-prone code\n* ‚úÖ Use specific exception types in try/except blocks\n\n<CodeGroup>",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</CodeGroup>\n\n* üî¥ Do not wrap [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) calls in bare try/except blocks",
      "language": "unknown"
    },
    {
      "code": "### Do not reorder `interrupt` calls within a node\n\nIt's common to use multiple interrupts in a single node, however this can lead to unexpected behavior if not handled carefully.\n\nWhen a node contains multiple interrupt calls, LangGraph keeps a list of resume values specific to the task executing the node. Whenever execution resumes, it starts at the beginning of the node. For each interrupt encountered, LangGraph checks if a matching value exists in the task's resume list. Matching is **strictly index-based**, so the order of interrupt calls within the node is important.\n\n* ‚úÖ Keep [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) calls consistent across node executions",
      "language": "unknown"
    },
    {
      "code": "* üî¥ Do not conditionally skip [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) calls within a node\n* üî¥ Do not loop [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) calls using logic that isn't deterministic across executions\n\n<CodeGroup>",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</CodeGroup>\n\n### Do not return complex values in `interrupt` calls\n\nDepending on which checkpointer is used, complex values may not be serializable (e.g. you can't serialize a function). To make your graphs adaptable to any deployment, it's best practice to only use values that can be reasonably serialized.\n\n* ‚úÖ Pass simple, JSON-serializable types to [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt)\n* ‚úÖ Pass dictionaries/objects with simple values\n\n<CodeGroup>",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</CodeGroup>\n\n* üî¥ Do not pass functions, class instances, or other complex objects to [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt)\n\n<CodeGroup>",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</CodeGroup>\n\n### Side effects called before `interrupt` must be idempotent\n\nBecause interrupts work by re-running the nodes they were called from, side effects called before [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) should (ideally) be idempotent. For context, idempotency means that the same operation can be applied multiple times without changing the result beyond the initial execution.\n\nAs an example, you might have an API call to update a record inside of a node. If [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) is called after that call is made, it will be re-run multiple times when the node is resumed, potentially overwriting the initial update or creating duplicate records.\n\n* ‚úÖ Use idempotent operations before [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt)\n* ‚úÖ Place side effects after [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) calls\n* ‚úÖ Separate side effects into separate nodes when possible\n\n<CodeGroup>",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</CodeGroup>\n\n* üî¥ Do not perform non-idempotent operations before [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt)\n* üî¥ Do not create new records without checking if they exist\n\n<CodeGroup>",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</CodeGroup>\n\n## Using with subgraphs called as functions\n\nWhen invoking a subgraph within a node, the parent graph will resume execution from the **beginning of the node** where the subgraph was invoked and the [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) was triggered. Similarly, the **subgraph** will also resume from the beginning of the node where [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) was called.",
      "language": "unknown"
    },
    {
      "code": "## Debugging with interrupts\n\nTo debug and test a graph, you can use static interrupts as breakpoints to step through the graph execution one node at a time. Static interrupts are triggered at defined points either before or after a node executes. You can set these by specifying `interrupt_before` and `interrupt_after` when compiling the graph.\n\n<Note>\n  Static interrupts are **not** recommended for human-in-the-loop workflows. Use the [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) function instead.\n</Note>\n\n<Tabs>\n  <Tab title=\"At compile time\">",
      "language": "unknown"
    },
    {
      "code": "1. The breakpoints are set during `compile` time.\n    2. `interrupt_before` specifies the nodes where execution should pause before the node is executed.\n    3. `interrupt_after` specifies the nodes where execution should pause after the node is executed.\n    4. A checkpointer is required to enable breakpoints.\n    5. The graph is run until the first breakpoint is hit.\n    6. The graph is resumed by passing in `None` for the input. This will run the graph until the next breakpoint is hit.\n  </Tab>\n\n  <Tab title=\"At run time\">",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h3",
      "text": "Review and edit state",
      "id": "review-and-edit-state"
    },
    {
      "level": "h3",
      "text": "Interrupts in tools",
      "id": "interrupts-in-tools"
    },
    {
      "level": "h3",
      "text": "Validating human input",
      "id": "validating-human-input"
    },
    {
      "level": "h2",
      "text": "Rules of interrupts",
      "id": "rules-of-interrupts"
    },
    {
      "level": "h3",
      "text": "Do not wrap `interrupt` calls in try/except",
      "id": "do-not-wrap-`interrupt`-calls-in-try/except"
    },
    {
      "level": "h3",
      "text": "Do not reorder `interrupt` calls within a node",
      "id": "do-not-reorder-`interrupt`-calls-within-a-node"
    },
    {
      "level": "h3",
      "text": "Do not return complex values in `interrupt` calls",
      "id": "do-not-return-complex-values-in-`interrupt`-calls"
    },
    {
      "level": "h3",
      "text": "Side effects called before `interrupt` must be idempotent",
      "id": "side-effects-called-before-`interrupt`-must-be-idempotent"
    },
    {
      "level": "h2",
      "text": "Using with subgraphs called as functions",
      "id": "using-with-subgraphs-called-as-functions"
    },
    {
      "level": "h2",
      "text": "Debugging with interrupts",
      "id": "debugging-with-interrupts"
    },
    {
      "level": "h3",
      "text": "Using LangGraph Studio",
      "id": "using-langgraph-studio"
    }
  ],
  "url": "llms-txt#to-reject",
  "links": []
}