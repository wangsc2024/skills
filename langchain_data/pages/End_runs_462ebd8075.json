{
  "title": "End runs",
  "content": "patch_run(child_run_id, chat_completion.dict())\npatch_run(parent_run_id, {\"answer\": chat_completion.choices[0].message.content})\npython  theme={null}\nimport json\nimport os\nimport uuid\nfrom datetime import datetime, timezone\nfrom typing import Dict, List\nimport requests\nfrom requests_toolbelt import MultipartEncoder\nfrom uuid_utils.compat import uuid7\n\ndef create_dotted_order(\n    start_time: datetime | None = None,\n    run_id: uuid.UUID | None = None\n) -> str:\n    \"\"\"Create a dotted order string for run ordering and hierarchy.\n\nThe dotted order is used to establish the sequence and relationships between runs.\n    It combines a timestamp with a unique identifier to ensure proper ordering and tracing.\n    \"\"\"\n    st = start_time or datetime.now(timezone.utc)\n    id_ = run_id or uuid7()\n    return f\"{st.strftime('%Y%m%dT%H%M%S%fZ')}{id_}\"\n\ndef create_run_base(\n    name: str,\n    run_type: str,\n    inputs: dict,\n    start_time: datetime\n) -> dict:\n    \"\"\"Create the base structure for a run.\"\"\"\n    run_id = uuid7()\n    return {\n        \"id\": str(run_id),\n        \"trace_id\": str(run_id),\n        \"name\": name,\n        \"start_time\": start_time.isoformat(),\n        \"inputs\": inputs,\n        \"run_type\": run_type,\n    }\n\ndef construct_run(\n    name: str,\n    run_type: str,\n    inputs: dict,\n    parent_dotted_order: str | None = None,\n) -> dict:\n    \"\"\"Construct a run dictionary with the given parameters.\n\nThis function creates a run with a unique ID and dotted order, establishing its place\n    in the trace hierarchy if it's a child run.\n    \"\"\"\n    start_time = datetime.now(timezone.utc)\n    run = create_run_base(name, run_type, inputs, start_time)\n    current_dotted_order = create_dotted_order(start_time, uuid.UUID(run[\"id\"]))\n\nif parent_dotted_order:\n        current_dotted_order = f\"{parent_dotted_order}.{current_dotted_order}\"\n        run[\"trace_id\"] = parent_dotted_order.split(\".\")[0].split(\"Z\")[1]\n        run[\"parent_run_id\"] = parent_dotted_order.split(\".\")[-1].split(\"Z\")[1]\n\nrun[\"dotted_order\"] = current_dotted_order\n    return run\n\ndef serialize_run(operation: str, run_data: dict) -> List[tuple]:\n    \"\"\"Serialize a run for the multipart request.\n\nThis function separates the run data into parts for efficient transmission and storage.\n    The main run data and optional fields (inputs, outputs, events) are serialized separately.\n    \"\"\"\n    run_id = run_data.get(\"id\", str(uuid7()))\n\n# Separate optional fields\n    inputs = run_data.pop(\"inputs\", None)\n    outputs = run_data.pop(\"outputs\", None)\n    events = run_data.pop(\"events\", None)\n\n# Serialize main run data\n    run_data_json = json.dumps(run_data).encode(\"utf-8\")\n    parts.append(\n        (\n            f\"{operation}.{run_id}\",\n            (\n                None,\n                run_data_json,\n                \"application/json\",\n                {\"Content-Length\": str(len(run_data_json))},\n            ),\n        )\n    )\n\n# Serialize optional fields\n    for key, value in [(\"inputs\", inputs), (\"outputs\", outputs), (\"events\", events)]:\n        if value:\n            serialized_value = json.dumps(value).encode(\"utf-8\")\n            parts.append(\n                (\n                    f\"{operation}.{run_id}.{key}\",\n                    (\n                        None,\n                        serialized_value,\n                        \"application/json\",\n                        {\"Content-Length\": str(len(serialized_value))},\n                    ),\n                )\n            )\n\ndef batch_ingest_runs(\n    api_url: str,\n    api_key: str,\n    posts: list[dict] | None = None,\n    patches: list[dict] | None = None,\n) -> None:\n    \"\"\"Ingest multiple runs in a single batch request.\n\nThis function handles both creating new runs (posts) and updating existing runs (patches).\n    It's more efficient for ingesting multiple runs compared to individual API calls.\n    \"\"\"\n    boundary = uuid.uuid4().hex\n    all_parts = []\n\nfor operation, runs in zip((\"post\", \"patch\"), (posts, patches)):\n        if runs:\n            all_parts.extend(\n                [part for run in runs for part in serialize_run(operation, run)]\n            )\n\nencoder = MultipartEncoder(fields=all_parts, boundary=boundary)\n    headers = {\"Content-Type\": encoder.content_type, \"x-api-key\": api_key}\n\ntry:\n        response = requests.post(\n            f\"{api_url}/runs/multipart\",\n            data=encoder,\n            headers=headers\n        )\n        response.raise_for_status()\n        print(\"Successfully ingested runs.\")\n    except requests.RequestException as e:\n        print(f\"Error ingesting runs: {e}\")\n        # In a production environment, you might want to log this error or handle it more robustly",
  "code_samples": [
    {
      "code": "See the doc on the [Run (span) data format](/langsmith/run-data-format) for more information.\n\n## Batch Ingestion\n\nFor faster ingestion of runs and higher rate limits, you can use the POST `/runs/multipart` [link](https://api.smith.langchain.com/redoc#tag/run/operation/multipart_ingest_runs_api_v1_runs_multipart_post) endpoint. Below is an example. It requires `orjson` (for fast json ), [`requests-toolbelt`](https://pypi.org/project/requests-toolbelt/) and [`uuid-utils`](https://pypi.org/project/uuid-utils/) to run",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": "Batch Ingestion",
      "id": "batch-ingestion"
    }
  ],
  "url": "llms-txt#end-runs",
  "links": []
}