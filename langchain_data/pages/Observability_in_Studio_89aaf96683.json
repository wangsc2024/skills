{
  "title": "Observability in Studio",
  "content": "Source: https://docs.langchain.com/langsmith/observability-studio\n\nLangSmith [Studio](/langsmith/studio) provides tools to inspect, debug, and improve your app beyond execution. By working with traces, datasets, and prompts, you can see how your application behaves in detail, measure its performance, and refine its outputs:\n\n* [Iterate on prompts](#iterate-on-prompts): Modify prompts inside graph nodes directly or with the LangSmith playground.\n* [Run experiments over a dataset](#run-experiments-over-a-dataset): Execute your assistant over a LangSmith dataset to score and compare results.\n* [Debug LangSmith traces](#debug-langsmith-traces): Import traced runs into Studio and optionally clone them into your local agent.\n* [Add a node to a dataset](#add-node-to-dataset): Turn parts of thread history into dataset examples for evaluation or further analysis.\n\n## Iterate on prompts\n\nStudio supports the following methods for modifying prompts in your graph:\n\n* [Direct node editing](#direct-node-editing)\n* [Playground interface](#playground)\n\n### Direct node editing\n\nStudio allows you to edit prompts used inside individual nodes, directly from the graph interface.\n\n### Graph configuration\n\nDefine your [configuration](/oss/python/langgraph/use-graph-api#add-runtime-configuration) to specify prompt fields and their associated nodes using `langgraph_nodes` and `langgraph_type` keys.\n\n#### `langgraph_nodes`\n\n* **Description**: Specifies which nodes of the graph a configuration field is associated with.\n* **Value Type**: Array of strings, where each string is the name of a node in your graph.\n* **Usage Context**: Include in the `json_schema_extra` dictionary for Pydantic models or the `metadata[\"json_schema_extra\"]` dictionary for dataclasses.\n* **Example**:\n\n#### `langgraph_type`\n\n* **Description**: Specifies the type of configuration field, which determines how it's handled in the UI.\n* **Value Type**: String\n* **Supported Values**:\n  * `\"prompt\"`: Indicates the field contains prompt text that should be treated specially in the UI.\n* **Usage Context**: Include in the `json_schema_extra` dictionary for Pydantic models or the `metadata[\"json_schema_extra\"]` dictionary for dataclasses.\n* **Example**:\n\n<Accordion title=\"Full example configuration\">\n  \n</Accordion>\n\n#### Editing prompts in the UI\n\n1. Locate the gear icon on nodes with associated configuration fields.\n2. Click to open the configuration modal.\n3. Edit the values.\n4. Save to update the current assistant version or create a new one.\n\nThe [playground](/langsmith/create-a-prompt) interface allows testing individual LLM calls without running the full graph:\n\n1. Select a thread.\n2. Click **View LLM Runs** on a node. This lists all the LLM calls (if any) made inside the node.\n3. Select an LLM run to open in the playground.\n4. Modify prompts and test different model and tool settings.\n5. Copy updated prompts back to your graph.\n\n## Run experiments over a dataset\n\nStudio lets you run [evaluations](/langsmith/evaluation-concepts) by executing your assistant against a predefined LangSmith [dataset](/langsmith/evaluation-concepts#datasets). This allows you to test performance across a variety of inputs, compare outputs to reference answers, and score results with configured [evaluators](/langsmith/evaluation-concepts#evaluators).\n\nThis guide shows you how to run a full end-to-end experiment directly from Studio.\n\nBefore running an experiment, ensure you have the following:\n\n* **A LangSmith dataset**: Your dataset should contain the inputs you want to test and optionally, reference outputs for comparison. The schema for the inputs must match the required input schema for the assistant. For more information on schemas, see [here](/oss/python/langgraph/use-graph-api#schema). For more on creating datasets, refer to [How to Manage Datasets](/langsmith/manage-datasets-in-application#set-up-your-dataset).\n* **(Optional) Evaluators**: You can attach evaluators (e.g., LLM-as-a-Judge, heuristics, or custom functions) to your dataset in LangSmith. These will run automatically after the graph has processed all inputs.\n* **A running application**: The experiment can be run against:\n  * An application deployed on [LangSmith](/langsmith/deployments).\n  * A locally running application started via the [langgraph-cli](/langsmith/local-server).\n\n<Note>\n  Studio experiments follow the same [data retention](/langsmith/administration-overview#data-retention) rules as other experiments. By default, traces have base tier retention (14 days). However, traces will automatically upgrade to extended tier retention (400 days) if feedback is added to them. Feedback can be added in one of two ways:\n\n* The [dataset has evaluators configured](/langsmith/bind-evaluator-to-dataset).\n  * [Feedback](/langsmith/observability-concepts#feedback) is manually added to a trace.\n\nThis auto-upgrade increases both the retention period and the cost of the trace. For more details, refer to [Data retention auto-upgrades](/langsmith/administration-overview#how-it-works).\n</Note>\n\n1. Launch the experiment. Click the **Run experiment** button in the top right corner of the Studio page.\n2. Select your dataset. In the modal that appears, select the dataset (or a specific dataset split) to use for the experiment and click **Start**.\n3. Monitor the progress. All of the inputs in the dataset will now be run against the active assistant. Monitor the experiment's progress via the badge in the top right corner.\n4. You can continue to work in Studio while the experiment runs in the background. Click the arrow icon button at any time to navigate to LangSmith and view the detailed experiment results.\n\n## Debug LangSmith traces\n\nThis guide explains how to open LangSmith traces in Studio for interactive investigation and debugging.\n\n### Open deployed threads\n\n1. Open the LangSmith trace, selecting the root run.\n2. Click **Run in Studio**.\n\nThis will open Studio connected to the associated deployment with the trace's parent thread selected.\n\n### Testing local agents with remote traces\n\nThis section explains how to test a local agent against remote traces from LangSmith. This enables you to use production traces as input for local testing, allowing you to debug and verify agent modifications in your development environment.\n\n* A LangSmith traced thread\n* A [locally running agent](/langsmith/local-server#local-development-server).\n\n<Info>\n  **Local agent requirements**\n\n* langgraph>=0.3.18\n  * langgraph-api>=0.0.32\n  * Contains the same set of nodes present in the remote trace\n</Info>\n\n1. Open the LangSmith trace, selecting the root run.\n2. Click the dropdown next to **Run in Studio**.\n3. Enter your local agent's URL.\n4. Select **Clone thread locally**.\n5. If multiple graphs exist, select the target graph.\n\nA new thread will be created in your local agent with the thread history inferred and copied from the remote thread, and you will be navigated to Studio for your locally running application.\n\n## Add node to dataset\n\nAdd [examples](/langsmith/evaluation-concepts#examples) to [LangSmith datasets](/langsmith/manage-datasets) from nodes in the thread log. This is useful to evaluate individual steps of the agent.\n\n1. Select a thread.\n2. Click **Add to Dataset**.\n3. Select nodes whose input/output you want to add to a dataset.\n4. For each selected node, select the target dataset to create the example in. By default a dataset for the specific assistant and node will be selected. If this dataset does not yet exist, it will be created.\n5. Edit the example's input/output as needed before adding it to the dataset.\n6. Select **Add to dataset** at the bottom of the page to add all selected nodes to their respective datasets.\n\nFor more details, refer to [How to evaluate an application's intermediate steps](/langsmith/evaluate-on-intermediate-steps).\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/observability-studio.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "code_samples": [
    {
      "code": "#### `langgraph_type`\n\n* **Description**: Specifies the type of configuration field, which determines how it's handled in the UI.\n* **Value Type**: String\n* **Supported Values**:\n  * `\"prompt\"`: Indicates the field contains prompt text that should be treated specially in the UI.\n* **Usage Context**: Include in the `json_schema_extra` dictionary for Pydantic models or the `metadata[\"json_schema_extra\"]` dictionary for dataclasses.\n* **Example**:",
      "language": "unknown"
    },
    {
      "code": "<Accordion title=\"Full example configuration\">",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": "Iterate on prompts",
      "id": "iterate-on-prompts"
    },
    {
      "level": "h3",
      "text": "Direct node editing",
      "id": "direct-node-editing"
    },
    {
      "level": "h3",
      "text": "Graph configuration",
      "id": "graph-configuration"
    },
    {
      "level": "h3",
      "text": "Playground",
      "id": "playground"
    },
    {
      "level": "h2",
      "text": "Run experiments over a dataset",
      "id": "run-experiments-over-a-dataset"
    },
    {
      "level": "h3",
      "text": "Prerequisites",
      "id": "prerequisites"
    },
    {
      "level": "h3",
      "text": "Experiment setup",
      "id": "experiment-setup"
    },
    {
      "level": "h2",
      "text": "Debug LangSmith traces",
      "id": "debug-langsmith-traces"
    },
    {
      "level": "h3",
      "text": "Open deployed threads",
      "id": "open-deployed-threads"
    },
    {
      "level": "h3",
      "text": "Testing local agents with remote traces",
      "id": "testing-local-agents-with-remote-traces"
    },
    {
      "level": "h2",
      "text": "Add node to dataset",
      "id": "add-node-to-dataset"
    }
  ],
  "url": "llms-txt#observability-in-studio",
  "links": []
}