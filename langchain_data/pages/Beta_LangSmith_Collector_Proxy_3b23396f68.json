{
  "title": "Beta LangSmith Collector-Proxy",
  "content": "Source: https://docs.langchain.com/langsmith/collector-proxy\n\n<Note>\n  This is a beta feature. The API may change in future releases.\n</Note>\n\nThe LangSmith Collector-Proxy is a lightweight, high-performance proxy server that sits between your application and the LangSmith backend. It batches and compresses trace data before sending it to LangSmith, reducing network overhead and improving performance.\n\n## When to Use the Collector-Proxy\n\nThe Collector-Proxy is particularly valuable when:\n\n* You're running multiple instances of your application in parallel and need to efficiently aggregate traces\n* You want more efficient tracing than direct OTEL API calls to LangSmith (the collector optimizes batching and compression)\n* You're using a language that doesn't have a native LangSmith SDK\n\n* **Efficient Data Transfer** Batches multiple spans into fewer, larger uploads.\n* **Compression** Uses zstd to minimize payload size.\n* **OTLP Support** Accepts OTLP JSON and Protobuf over HTTP POST.\n* **Semantic Translation** Maps GenAI/OpenInference conventions to the LangSmith Run model.\n* **Flexible Batching** Flush by span count or time interval.\n\nConfigure via environment variables:\n\n| Variable             | Description                       | Default                           |\n| -------------------- | --------------------------------- | --------------------------------- |\n| `HTTP_PORT`          | Port to run the proxy server      | `4318`                            |\n| `LANGSMITH_ENDPOINT` | LangSmith backend URL             | `https://api.smith.langchain.com` |\n| `LANGSMITH_API_KEY`  | API key for LangSmith             | **Required** (env var or header)  |\n| `LANGSMITH_PROJECT`  | Default tracing project           | Default project if not specified  |\n| `BATCH_SIZE`         | Spans per upload batch            | `100`                             |\n| `FLUSH_INTERVAL_MS`  | Flush interval in milliseconds    | `1000`                            |\n| `MAX_BUFFER_BYTES`   | Max uncompressed buffer size      | `10485760` (10 MB)                |\n| `MAX_BODY_BYTES`     | Max incoming request body size    | `209715200` (200 MB)              |\n| `MAX_RETRIES`        | Retry attempts for failed uploads | `3`                               |\n| `RETRY_BACKOFF_MS`   | Initial backoff in milliseconds   | `100`                             |\n\n### Project Configuration\n\nThe Collector-Proxy supports LangSmith project configuration with the following priority:\n\n1. If a project is specified in the request headers (`Langsmith-Project`), that project will be used\n2. If no project is specified in headers, it will use the project set in the `LANGSMITH_PROJECT` environment variable\n3. If neither is set, it will trace to the `default` project.\n\nThe API key can be provided either:\n\n* As an environment variable (`LANGSMITH_API_KEY`)\n* In the request headers (`X-API-Key`)\n\n## Deployment (Docker)\n\nYou can deploy the Collector-Proxy with Docker:\n\n1. **Build the image**\n\n2. **Run the container**\n\nPoint any OTLP-compatible client or the OpenTelemetry Collector exporter at:\n\n* **Liveness**: `GET /live` → 200\n* **Readiness**: `GET /ready` → 200\n\n## Horizontal Scaling\n\nTo ensure full traces are batched correctly, route spans with the same trace ID to the same instance (e.g., via consistent hashing).\n\nFork the [Collector-Proxy repo on GitHub](https://github.com/langchain-ai/langsmith-collector-proxy) and implement your own converter:\n\n* Create a custom `GenAiConverter` or modify the existing one in `internal/translator/otel_converter.go`\n* Register the custom converter in `internal/translator/translator.go`\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/collector-proxy.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "code_samples": [
    {
      "code": "2. **Run the container**",
      "language": "unknown"
    },
    {
      "code": "## Usage\n\nPoint any OTLP-compatible client or the OpenTelemetry Collector exporter at:",
      "language": "unknown"
    },
    {
      "code": "Send a test trace:",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": "When to Use the Collector-Proxy",
      "id": "when-to-use-the-collector-proxy"
    },
    {
      "level": "h2",
      "text": "Key Features",
      "id": "key-features"
    },
    {
      "level": "h2",
      "text": "Configuration",
      "id": "configuration"
    },
    {
      "level": "h3",
      "text": "Project Configuration",
      "id": "project-configuration"
    },
    {
      "level": "h3",
      "text": "Authentication",
      "id": "authentication"
    },
    {
      "level": "h2",
      "text": "Deployment (Docker)",
      "id": "deployment-(docker)"
    },
    {
      "level": "h2",
      "text": "Usage",
      "id": "usage"
    },
    {
      "level": "h2",
      "text": "Health & Scaling",
      "id": "health-&-scaling"
    },
    {
      "level": "h2",
      "text": "Horizontal Scaling",
      "id": "horizontal-scaling"
    },
    {
      "level": "h2",
      "text": "Fork & Extend",
      "id": "fork-&-extend"
    }
  ],
  "url": "llms-txt#beta-langsmith-collector-proxy",
  "links": []
}