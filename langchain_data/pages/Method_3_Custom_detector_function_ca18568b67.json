{
  "title": "Method 3: Custom detector function",
  "content": "def detect_ssn(content: str) -> list[dict[str, str | int]]:\n    \"\"\"Detect SSN with validation.\n\nReturns a list of dictionaries with 'text', 'start', and 'end' keys.\n    \"\"\"\n    import re\n    matches = []\n    pattern = r\"\\d{3}-\\d{2}-\\d{4}\"\n    for match in re.finditer(pattern, content):\n        ssn = match.group(0)\n        # Validate: first 3 digits shouldn't be 000, 666, or 900-999\n        first_three = int(ssn[:3])\n        if first_three not in [0, 666] and not (900 <= first_three <= 999):\n            matches.append({\n                \"text\": ssn,\n                \"start\": match.start(),\n                \"end\": match.end(),\n            })\n    return matches\n\nagent3 = create_agent(\n    model=\"gpt-4o\",\n    tools=[],\n    middleware=[\n        PIIMiddleware(\n            \"ssn\",\n            detector=detect_ssn,\n            strategy=\"hash\",\n        ),\n    ],\n)\npython  theme={null}\ndef detector(content: str) -> list[dict[str, str | int]]:\n    return [\n        {\"text\": \"matched_text\", \"start\": 0, \"end\": 12},\n        # ... more matches\n    ]\npython  theme={null}\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import TodoListMiddleware\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[read_file, write_file, run_tests],\n    middleware=[TodoListMiddleware()],\n)\npython  theme={null}\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import LLMToolSelectorMiddleware\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[tool1, tool2, tool3, tool4, tool5, ...],\n    middleware=[\n        LLMToolSelectorMiddleware(\n            model=\"gpt-4o-mini\",\n            max_tools=3,\n            always_include=[\"search\"],\n        ),\n    ],\n)\npython  theme={null}\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import ToolRetryMiddleware\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[search_tool, database_tool],\n    middleware=[\n        ToolRetryMiddleware(\n            max_retries=3,\n            backoff_factor=2.0,\n            initial_delay=1.0,\n        ),\n    ],\n)\npython  theme={null}\n  from langchain.agents import create_agent\n  from langchain.agents.middleware import ToolRetryMiddleware\n\nagent = create_agent(\n      model=\"gpt-4o\",\n      tools=[search_tool, database_tool, api_tool],\n      middleware=[\n          ToolRetryMiddleware(\n              max_retries=3,\n              backoff_factor=2.0,\n              initial_delay=1.0,\n              max_delay=60.0,\n              jitter=True,\n              tools=[\"api_tool\"],\n              retry_on=(ConnectionError, TimeoutError),\n              on_failure=\"continue\",\n          ),\n      ],\n  )\n  python  theme={null}\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import ModelRetryMiddleware\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[search_tool, database_tool],\n    middleware=[\n        ModelRetryMiddleware(\n            max_retries=3,\n            backoff_factor=2.0,\n            initial_delay=1.0,\n        ),\n    ],\n)\npython  theme={null}\n  from langchain.agents import create_agent\n  from langchain.agents.middleware import ModelRetryMiddleware\n\n# Basic usage with default settings (2 retries, exponential backoff)\n  agent = create_agent(\n      model=\"gpt-4o\",\n      tools=[search_tool],\n      middleware=[ModelRetryMiddleware()],\n  )\n\n# Custom exception filtering\n  class TimeoutError(Exception):\n      \"\"\"Custom exception for timeout errors.\"\"\"\n      pass\n\nclass ConnectionError(Exception):\n      \"\"\"Custom exception for connection errors.\"\"\"\n      pass\n\n# Retry specific exceptions only\n  retry = ModelRetryMiddleware(\n      max_retries=4,\n      retry_on=(TimeoutError, ConnectionError),\n      backoff_factor=1.5,\n  )\n\ndef should_retry(error: Exception) -> bool:\n      # Only retry on rate limit errors\n      if isinstance(error, TimeoutError):\n          return True\n      # Or check for specific HTTP status codes\n      if hasattr(error, \"status_code\"):\n          return error.status_code in (429, 503)\n      return False\n\nretry_with_filter = ModelRetryMiddleware(\n      max_retries=3,\n      retry_on=should_retry,\n  )\n\n# Return error message instead of raising\n  retry_continue = ModelRetryMiddleware(\n      max_retries=4,\n      on_failure=\"continue\",  # Return AIMessage with error instead of raising\n  )\n\n# Custom error message formatting\n  def format_error(error: Exception) -> str:\n      return f\"Model call failed: {error}. Please try again later.\"\n\nretry_with_formatter = ModelRetryMiddleware(\n      max_retries=4,\n      on_failure=format_error,\n  )\n\n# Constant backoff (no exponential growth)\n  constant_backoff = ModelRetryMiddleware(\n      max_retries=5,\n      backoff_factor=0.0,  # No exponential growth\n      initial_delay=2.0,  # Always wait 2 seconds\n  )\n\n# Raise exception on failure\n  strict_retry = ModelRetryMiddleware(\n      max_retries=2,\n      on_failure=\"error\",  # Re-raise exception instead of returning message\n  )\n  python  theme={null}\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import LLMToolEmulator\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[get_weather, search_database, send_email],\n    middleware=[\n        LLMToolEmulator(),  # Emulate all tools\n    ],\n)\npython  theme={null}\n  from langchain.agents import create_agent\n  from langchain.agents.middleware import LLMToolEmulator\n  from langchain.tools import tool\n\n@tool\n  def get_weather(location: str) -> str:\n      \"\"\"Get the current weather for a location.\"\"\"\n      return f\"Weather in {location}\"\n\n@tool\n  def send_email(to: str, subject: str, body: str) -> str:\n      \"\"\"Send an email.\"\"\"\n      return \"Email sent\"\n\n# Emulate all tools (default behavior)\n  agent = create_agent(\n      model=\"gpt-4o\",\n      tools=[get_weather, send_email],\n      middleware=[LLMToolEmulator()],\n  )\n\n# Emulate specific tools only\n  agent2 = create_agent(\n      model=\"gpt-4o\",\n      tools=[get_weather, send_email],\n      middleware=[LLMToolEmulator(tools=[\"get_weather\"])],\n  )\n\n# Use custom model for emulation\n  agent4 = create_agent(\n      model=\"gpt-4o\",\n      tools=[get_weather, send_email],\n      middleware=[LLMToolEmulator(model=\"claude-sonnet-4-5-20250929\")],\n  )\n  python  theme={null}\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import ContextEditingMiddleware, ClearToolUsesEdit\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[],\n    middleware=[\n        ContextEditingMiddleware(\n            edits=[\n                ClearToolUsesEdit(\n                    trigger=100000,\n                    keep=3,\n                ),\n            ],\n        ),\n    ],\n)\npython  theme={null}\n  from langchain.agents import create_agent\n  from langchain.agents.middleware import ContextEditingMiddleware, ClearToolUsesEdit\n\nagent = create_agent(\n      model=\"gpt-4o\",\n      tools=[search_tool, your_calculator_tool, database_tool],\n      middleware=[\n          ContextEditingMiddleware(\n              edits=[\n                  ClearToolUsesEdit(\n                      trigger=2000,\n                      keep=3,\n                      clear_tool_inputs=False,\n                      exclude_tools=[],\n                      placeholder=\"[cleared]\",\n                  ),\n              ],\n          ),\n      ],\n  )\n  python  theme={null}\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import (\n    ShellToolMiddleware,\n    HostExecutionPolicy,\n)\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[search_tool],\n    middleware=[\n        ShellToolMiddleware(\n            workspace_root=\"/workspace\",\n            execution_policy=HostExecutionPolicy(),\n        ),\n    ],\n)\npython  theme={null}\n  from langchain.agents import create_agent\n  from langchain.agents.middleware import (\n      ShellToolMiddleware,\n      HostExecutionPolicy,\n      DockerExecutionPolicy,\n      RedactionRule,\n  )\n\n# Basic shell tool with host execution\n  agent = create_agent(\n      model=\"gpt-4o\",\n      tools=[search_tool],\n      middleware=[\n          ShellToolMiddleware(\n              workspace_root=\"/workspace\",\n              execution_policy=HostExecutionPolicy(),\n          ),\n      ],\n  )\n\n# Docker isolation with startup commands\n  agent_docker = create_agent(\n      model=\"gpt-4o\",\n      tools=[],\n      middleware=[\n          ShellToolMiddleware(\n              workspace_root=\"/workspace\",\n              startup_commands=[\"pip install requests\", \"export PYTHONPATH=/workspace\"],\n              execution_policy=DockerExecutionPolicy(\n                  image=\"python:3.11-slim\",\n                  command_timeout=60.0,\n              ),\n          ),\n      ],\n  )\n\n# With output redaction\n  agent_redacted = create_agent(\n      model=\"gpt-4o\",\n      tools=[],\n      middleware=[\n          ShellToolMiddleware(\n              workspace_root=\"/workspace\",\n              redaction_rules=[\n                  RedactionRule(pii_type=\"api_key\", detector=r\"sk-[a-zA-Z0-9]{32}\"),\n              ],\n          ),\n      ],\n  )\n  python  theme={null}\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import FilesystemFileSearchMiddleware\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[],\n    middleware=[\n        FilesystemFileSearchMiddleware(\n            root_path=\"/workspace\",\n            use_ripgrep=True,\n        ),\n    ],\n)\npython  theme={null}\n  from langchain.agents import create_agent\n  from langchain.agents.middleware import FilesystemFileSearchMiddleware\n  from langchain.messages import HumanMessage\n\nagent = create_agent(\n      model=\"gpt-4o\",\n      tools=[],\n      middleware=[\n          FilesystemFileSearchMiddleware(\n              root_path=\"/workspace\",\n              use_ripgrep=True,\n              max_file_size_mb=10,\n          ),\n      ],\n  )\n\n# Agent can now use glob_search and grep_search tools\n  result = agent.invoke({\n      \"messages\": [HumanMessage(\"Find all Python files containing 'async def'\")]\n  })\n\n# The agent will use:\n  # 1. glob_search(pattern=\"**/*.py\") to find Python files\n  # 2. grep_search(pattern=\"async def\", include=\"*.py\") to find async functions\n  ```\n</Accordion>\n\n## Provider-specific middleware\n\nThese middleware are optimized for specific LLM providers. See each provider's documentation for full details and examples.\n\n<Columns cols={2}>\n  <Card title=\"Anthropic\" href=\"/oss/python/integrations/middleware/anthropic\" icon=\"anthropic\" arrow>\n    Prompt caching, bash tool, text editor, memory, and file search middleware for Claude models.\n  </Card>\n\n<Card title=\"OpenAI\" href=\"/oss/python/integrations/middleware/openai\" icon=\"openai\" arrow>\n    Content moderation middleware for OpenAI models.\n  </Card>\n</Columns>\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/middleware/built-in.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "code_samples": [
    {
      "code": "**Custom detector function signature:**\n\nThe detector function must accept a string (content) and return matches:\n\nReturns a list of dictionaries with `text`, `start`, and `end` keys:",
      "language": "unknown"
    },
    {
      "code": "<Tip>\n  For custom detectors:\n\n  * Use regex strings for simple patterns\n  * Use RegExp objects when you need flags (e.g., case-insensitive matching)\n  * Use custom functions when you need validation logic beyond pattern matching\n  * Custom functions give you full control over detection logic and can implement complex validation rules\n</Tip>\n\n<Accordion title=\"Configuration options\">\n  <ParamField body=\"pii_type\" type=\"string\" required>\n    Type of PII to detect. Can be a built-in type (`email`, `credit_card`, `ip`, `mac_address`, `url`) or a custom type name.\n  </ParamField>\n\n  <ParamField body=\"strategy\" type=\"string\" default=\"redact\">\n    How to handle detected PII. Options:\n\n    * `'block'` - Raise exception when detected\n    * `'redact'` - Replace with `[REDACTED_{PII_TYPE}]`\n    * `'mask'` - Partially mask (e.g., `****-****-****-1234`)\n    * `'hash'` - Replace with deterministic hash\n  </ParamField>\n\n  <ParamField body=\"detector\" type=\"function | regex\">\n    Custom detector function or regex pattern. If not provided, uses built-in detector for the PII type.\n  </ParamField>\n\n  <ParamField body=\"apply_to_input\" type=\"boolean\" default=\"True\">\n    Check user messages before model call\n  </ParamField>\n\n  <ParamField body=\"apply_to_output\" type=\"boolean\" default=\"False\">\n    Check AI messages after model call\n  </ParamField>\n\n  <ParamField body=\"apply_to_tool_results\" type=\"boolean\" default=\"False\">\n    Check tool result messages after execution\n  </ParamField>\n</Accordion>\n\n### To-do list\n\nEquip agents with task planning and tracking capabilities for complex multi-step tasks. To-do lists are useful for the following:\n\n* Complex multi-step tasks requiring coordination across multiple tools.\n* Long-running operations where progress visibility is important.\n\n<Note>\n  This middleware automatically provides agents with a `write_todos` tool and system prompts to guide effective task planning.\n</Note>\n\n**API reference:** [`TodoListMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.TodoListMiddleware)",
      "language": "unknown"
    },
    {
      "code": "<Callout icon=\"circle-play\" iconType=\"solid\">\n  Watch this [video guide](https://www.youtube.com/watch?v=yTWocbVKQxw) demonstrating To-do List middleware behavior.\n</Callout>\n\n<Accordion title=\"Configuration options\">\n  <ParamField body=\"system_prompt\" type=\"string\">\n    Custom system prompt for guiding todo usage. Uses built-in prompt if not specified.\n  </ParamField>\n\n  <ParamField body=\"tool_description\" type=\"string\">\n    Custom description for the `write_todos` tool. Uses built-in description if not specified.\n  </ParamField>\n</Accordion>\n\n### LLM tool selector\n\nUse an LLM to intelligently select relevant tools before calling the main model. LLM tool selectors are useful for the following:\n\n* Agents with many tools (10+) where most aren't relevant per query.\n* Reducing token usage by filtering irrelevant tools.\n* Improving model focus and accuracy.\n\nThis middleware uses structured output to ask an LLM which tools are most relevant for the current query. The structured output schema defines the available tool names and descriptions. Model providers often add this structured output information to the system prompt behind the scenes.\n\n**API reference:** [`LLMToolSelectorMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.LLMToolSelectorMiddleware)",
      "language": "unknown"
    },
    {
      "code": "<Accordion title=\"Configuration options\">\n  <ParamField body=\"model\" type=\"string | BaseChatModel\">\n    Model for tool selection. Can be a model identifier string (e.g., `'openai:gpt-4o-mini'`) or a `BaseChatModel` instance. See [`init_chat_model`](https://reference.langchain.com/python/langchain/models/#langchain.chat_models.init_chat_model\\(model\\)) for more information.\n\n    Defaults to the agent's main model.\n  </ParamField>\n\n  <ParamField body=\"system_prompt\" type=\"string\">\n    Instructions for the selection model. Uses built-in prompt if not specified.\n  </ParamField>\n\n  <ParamField body=\"max_tools\" type=\"number\">\n    Maximum number of tools to select. If the model selects more, only the first max\\_tools will be used. No limit if not specified.\n  </ParamField>\n\n  <ParamField body=\"always_include\" type=\"list[string]\">\n    Tool names to always include regardless of selection. These do not count against the max\\_tools limit.\n  </ParamField>\n</Accordion>\n\n### Tool retry\n\nAutomatically retry failed tool calls with configurable exponential backoff. Tool retry is useful for the following:\n\n* Handling transient failures in external API calls.\n* Improving reliability of network-dependent tools.\n* Building resilient agents that gracefully handle temporary errors.\n\n**API reference:** [`ToolRetryMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.ToolRetryMiddleware)",
      "language": "unknown"
    },
    {
      "code": "<Accordion title=\"Configuration options\">\n  <ParamField body=\"max_retries\" type=\"number\" default=\"2\">\n    Maximum number of retry attempts after the initial call (3 total attempts with default)\n  </ParamField>\n\n  <ParamField body=\"tools\" type=\"list[BaseTool | str]\">\n    Optional list of tools or tool names to apply retry logic to. If `None`, applies to all tools.\n  </ParamField>\n\n  <ParamField body=\"retry_on\" type=\"tuple[type[Exception], ...] | callable\" default=\"(Exception,)\">\n    Either a tuple of exception types to retry on, or a callable that takes an exception and returns `True` if it should be retried.\n  </ParamField>\n\n  <ParamField body=\"on_failure\" type=\"string | callable\" default=\"return_message\">\n    Behavior when all retries are exhausted. Options:\n\n    * `'return_message'` - Return a `ToolMessage` with error details (allows LLM to handle failure)\n    * `'raise'` - Re-raise the exception (stops agent execution)\n    * Custom callable - Function that takes the exception and returns a string for the `ToolMessage` content\n  </ParamField>\n\n  <ParamField body=\"backoff_factor\" type=\"number\" default=\"2.0\">\n    Multiplier for exponential backoff. Each retry waits `initial_delay * (backoff_factor ** retry_number)` seconds. Set to `0.0` for constant delay.\n  </ParamField>\n\n  <ParamField body=\"initial_delay\" type=\"number\" default=\"1.0\">\n    Initial delay in seconds before first retry\n  </ParamField>\n\n  <ParamField body=\"max_delay\" type=\"number\" default=\"60.0\">\n    Maximum delay in seconds between retries (caps exponential backoff growth)\n  </ParamField>\n\n  <ParamField body=\"jitter\" type=\"boolean\" default=\"true\">\n    Whether to add random jitter (`±25%`) to delay to avoid thundering herd\n  </ParamField>\n</Accordion>\n\n<Accordion title=\"Full example\">\n  The middleware automatically retries failed tool calls with exponential backoff.\n\n  **Key configuration:**\n\n  * `max_retries` - Number of retry attempts (default: 2)\n  * `backoff_factor` - Multiplier for exponential backoff (default: 2.0)\n  * `initial_delay` - Starting delay in seconds (default: 1.0)\n  * `max_delay` - Cap on delay growth (default: 60.0)\n  * `jitter` - Add random variation (default: True)\n\n  **Failure handling:**\n\n  * `on_failure='return_message'` - Return error message\n  * `on_failure='raise'` - Re-raise exception\n  * Custom function - Function returning error message",
      "language": "unknown"
    },
    {
      "code": "</Accordion>\n\n### Model retry\n\nAutomatically retry failed model calls with configurable exponential backoff. Model retry is useful for the following:\n\n* Handling transient failures in model API calls.\n* Improving reliability of network-dependent model requests.\n* Building resilient agents that gracefully handle temporary model errors.\n\n**API reference:** [`ModelRetryMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.ModelRetryMiddleware)",
      "language": "unknown"
    },
    {
      "code": "<Accordion title=\"Configuration options\">\n  <ParamField body=\"max_retries\" type=\"number\" default=\"2\">\n    Maximum number of retry attempts after the initial call (3 total attempts with default)\n  </ParamField>\n\n  <ParamField body=\"retry_on\" type=\"tuple[type[Exception], ...] | callable\" default=\"(Exception,)\">\n    Either a tuple of exception types to retry on, or a callable that takes an exception and returns `True` if it should be retried.\n  </ParamField>\n\n  <ParamField body=\"on_failure\" type=\"string | callable\" default=\"continue\">\n    Behavior when all retries are exhausted. Options:\n\n    * `'continue'` (default) - Return an `AIMessage` with error details, allowing the agent to potentially handle the failure gracefully\n    * `'error'` - Re-raise the exception (stops agent execution)\n    * Custom callable - Function that takes the exception and returns a string for the `AIMessage` content\n  </ParamField>\n\n  <ParamField body=\"backoff_factor\" type=\"number\" default=\"2.0\">\n    Multiplier for exponential backoff. Each retry waits `initial_delay * (backoff_factor ** retry_number)` seconds. Set to `0.0` for constant delay.\n  </ParamField>\n\n  <ParamField body=\"initial_delay\" type=\"number\" default=\"1.0\">\n    Initial delay in seconds before first retry\n  </ParamField>\n\n  <ParamField body=\"max_delay\" type=\"number\" default=\"60.0\">\n    Maximum delay in seconds between retries (caps exponential backoff growth)\n  </ParamField>\n\n  <ParamField body=\"jitter\" type=\"boolean\" default=\"true\">\n    Whether to add random jitter (`±25%`) to delay to avoid thundering herd\n  </ParamField>\n</Accordion>\n\n<Accordion title=\"Full example\">\n  The middleware automatically retries failed model calls with exponential backoff.",
      "language": "unknown"
    },
    {
      "code": "</Accordion>\n\n### LLM tool emulator\n\nEmulate tool execution using an LLM for testing purposes, replacing actual tool calls with AI-generated responses. LLM tool emulators are useful for the following:\n\n* Testing agent behavior without executing real tools.\n* Developing agents when external tools are unavailable or expensive.\n* Prototyping agent workflows before implementing actual tools.\n\n**API reference:** [`LLMToolEmulator`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.LLMToolEmulator)",
      "language": "unknown"
    },
    {
      "code": "<Accordion title=\"Configuration options\">\n  <ParamField body=\"tools\" type=\"list[str | BaseTool]\">\n    List of tool names (str) or BaseTool instances to emulate. If `None` (default), ALL tools will be emulated. If empty list `[]`, no tools will be emulated. If array with tool names/instances, only those tools will be emulated.\n  </ParamField>\n\n  <ParamField body=\"model\" type=\"string | BaseChatModel\">\n    Model to use for generating emulated tool responses. Can be a model identifier string (e.g., `'anthropic:claude-sonnet-4-5-20250929'`) or a `BaseChatModel` instance. Defaults to the agent's model if not specified. See [`init_chat_model`](https://reference.langchain.com/python/langchain/models/#langchain.chat_models.init_chat_model\\(model\\)) for more information.\n  </ParamField>\n</Accordion>\n\n<Accordion title=\"Full example\">\n  The middleware uses an LLM to generate plausible responses for tool calls instead of executing the actual tools.",
      "language": "unknown"
    },
    {
      "code": "</Accordion>\n\n### Context editing\n\nManage conversation context by clearing older tool call outputs when token limits are reached, while preserving recent results. This helps keep context windows manageable in long conversations with many tool calls. Context editing is useful for the following:\n\n* Long conversations with many tool calls that exceed token limits\n* Reducing token costs by removing older tool outputs that are no longer relevant\n* Maintaining only the most recent N tool results in context\n\n**API reference:** [`ContextEditingMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.ContextEditingMiddleware), [`ClearToolUsesEdit`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.ClearToolUsesEdit)",
      "language": "unknown"
    },
    {
      "code": "<Accordion title=\"Configuration options\">\n  <ParamField body=\"edits\" type=\"list[ContextEdit]\" default=\"[ClearToolUsesEdit()]\">\n    List of [`ContextEdit`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.ContextEdit) strategies to apply\n  </ParamField>\n\n  <ParamField body=\"token_count_method\" type=\"string\" default=\"approximate\">\n    Token counting method. Options: `'approximate'` or `'model'`\n  </ParamField>\n\n  **[`ClearToolUsesEdit`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.ClearToolUsesEdit) options:**\n\n  <ParamField body=\"trigger\" type=\"number\" default=\"100000\">\n    Token count that triggers the edit. When the conversation exceeds this token count, older tool outputs will be cleared.\n  </ParamField>\n\n  <ParamField body=\"clear_at_least\" type=\"number\" default=\"0\">\n    Minimum number of tokens to reclaim when the edit runs. If set to 0, clears as much as needed.\n  </ParamField>\n\n  <ParamField body=\"keep\" type=\"number\" default=\"3\">\n    Number of most recent tool results that must be preserved. These will never be cleared.\n  </ParamField>\n\n  <ParamField body=\"clear_tool_inputs\" type=\"boolean\" default=\"False\">\n    Whether to clear the originating tool call parameters on the AI message. When `True`, tool call arguments are replaced with empty objects.\n  </ParamField>\n\n  <ParamField body=\"exclude_tools\" type=\"list[string]\" default=\"()\">\n    List of tool names to exclude from clearing. These tools will never have their outputs cleared.\n  </ParamField>\n\n  <ParamField body=\"placeholder\" type=\"string\" default=\"[cleared]\">\n    Placeholder text inserted for cleared tool outputs. This replaces the original tool message content.\n  </ParamField>\n</Accordion>\n\n<Accordion title=\"Full example\">\n  The middleware applies context editing strategies when token limits are reached. The most common strategy is `ClearToolUsesEdit`, which clears older tool results while preserving recent ones.\n\n  **How it works:**\n\n  1. Monitor token count in conversation\n  2. When threshold is reached, clear older tool outputs\n  3. Keep most recent N tool results\n  4. Optionally preserve tool call arguments for context",
      "language": "unknown"
    },
    {
      "code": "</Accordion>\n\n### Shell tool\n\nExpose a persistent shell session to agents for command execution. Shell tool middleware is useful for the following:\n\n* Agents that need to execute system commands\n* Development and deployment automation tasks\n* Testing and validation workflows\n* File system operations and script execution\n\n<Warning>\n  **Security consideration**: Use appropriate execution policies (`HostExecutionPolicy`, `DockerExecutionPolicy`, or `CodexSandboxExecutionPolicy`) to match your deployment's security requirements.\n</Warning>\n\n<Note>\n  **Limitation**: Persistent shell sessions do not currently work with interrupts (human-in-the-loop). We anticipate adding support for this in the future.\n</Note>\n\n**API reference:** [`ShellToolMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.ShellToolMiddleware)",
      "language": "unknown"
    },
    {
      "code": "<Accordion title=\"Configuration options\">\n  <ParamField body=\"workspace_root\" type=\"str | Path | None\">\n    Base directory for the shell session. If omitted, a temporary directory is created when the agent starts and removed when it ends.\n  </ParamField>\n\n  <ParamField body=\"startup_commands\" type=\"tuple[str, ...] | list[str] | str | None\">\n    Optional commands executed sequentially after the session starts\n  </ParamField>\n\n  <ParamField body=\"shutdown_commands\" type=\"tuple[str, ...] | list[str] | str | None\">\n    Optional commands executed before the session shuts down\n  </ParamField>\n\n  <ParamField body=\"execution_policy\" type=\"BaseExecutionPolicy | None\">\n    Execution policy controlling timeouts, output limits, and resource configuration. Options:\n\n    * `HostExecutionPolicy` - Full host access (default); best for trusted environments where the agent already runs inside a container or VM\n    * `DockerExecutionPolicy` - Launches a separate Docker container for each agent run, providing harder isolation\n    * `CodexSandboxExecutionPolicy` - Reuses the Codex CLI sandbox for additional syscall/filesystem restrictions\n  </ParamField>\n\n  <ParamField body=\"redaction_rules\" type=\"tuple[RedactionRule, ...] | list[RedactionRule] | None\">\n    Optional redaction rules to sanitize command output before returning it to the model\n  </ParamField>\n\n  <ParamField body=\"tool_description\" type=\"str | None\">\n    Optional override for the registered shell tool description\n  </ParamField>\n\n  <ParamField body=\"shell_command\" type=\"Sequence[str] | str | None\">\n    Optional shell executable (string) or argument sequence used to launch the persistent session. Defaults to `/bin/bash`.\n  </ParamField>\n\n  <ParamField body=\"env\" type=\"Mapping[str, Any] | None\">\n    Optional environment variables to supply to the shell session. Values are coerced to strings before command execution.\n  </ParamField>\n</Accordion>\n\n<Accordion title=\"Full example\">\n  The middleware provides a single persistent shell session that agents can use to execute commands sequentially.\n\n  **Execution policies:**\n\n  * `HostExecutionPolicy` (default) - Native execution with full host access\n  * `DockerExecutionPolicy` - Isolated Docker container execution\n  * `CodexSandboxExecutionPolicy` - Sandboxed execution via Codex CLI",
      "language": "unknown"
    },
    {
      "code": "</Accordion>\n\n### File search\n\nProvide Glob and Grep search tools over a filesystem. File search middleware is useful for the following:\n\n* Code exploration and analysis\n* Finding files by name patterns\n* Searching code content with regex\n* Large codebases where file discovery is needed\n\n**API reference:** [`FilesystemFileSearchMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.FilesystemFileSearchMiddleware)",
      "language": "unknown"
    },
    {
      "code": "<Accordion title=\"Configuration options\">\n  <ParamField body=\"root_path\" type=\"str\" required>\n    Root directory to search. All file operations are relative to this path.\n  </ParamField>\n\n  <ParamField body=\"use_ripgrep\" type=\"bool\" default=\"True\">\n    Whether to use ripgrep for search. Falls back to Python regex if ripgrep is unavailable.\n  </ParamField>\n\n  <ParamField body=\"max_file_size_mb\" type=\"int\" default=\"10\">\n    Maximum file size to search in MB. Files larger than this are skipped.\n  </ParamField>\n</Accordion>\n\n<Accordion title=\"Full example\">\n  The middleware adds two search tools to agents:\n\n  **Glob tool** - Fast file pattern matching:\n\n  * Supports patterns like `**/*.py`, `src/**/*.ts`\n  * Returns matching file paths sorted by modification time\n\n  **Grep tool** - Content search with regex:\n\n  * Full regex syntax support\n  * Filter by file patterns with `include` parameter\n  * Three output modes: `files_with_matches`, `content`, `count`",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h3",
      "text": "To-do list",
      "id": "to-do-list"
    },
    {
      "level": "h3",
      "text": "LLM tool selector",
      "id": "llm-tool-selector"
    },
    {
      "level": "h3",
      "text": "Tool retry",
      "id": "tool-retry"
    },
    {
      "level": "h3",
      "text": "Model retry",
      "id": "model-retry"
    },
    {
      "level": "h3",
      "text": "LLM tool emulator",
      "id": "llm-tool-emulator"
    },
    {
      "level": "h3",
      "text": "Context editing",
      "id": "context-editing"
    },
    {
      "level": "h3",
      "text": "Shell tool",
      "id": "shell-tool"
    },
    {
      "level": "h3",
      "text": "File search",
      "id": "file-search"
    },
    {
      "level": "h2",
      "text": "Provider-specific middleware",
      "id": "provider-specific-middleware"
    }
  ],
  "url": "llms-txt#method-3:-custom-detector-function",
  "links": []
}