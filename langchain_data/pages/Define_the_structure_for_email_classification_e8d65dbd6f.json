{
  "title": "Define the structure for email classification",
  "content": "class EmailClassification(TypedDict):\n    intent: Literal[\"question\", \"bug\", \"billing\", \"feature\", \"complex\"]\n    urgency: Literal[\"low\", \"medium\", \"high\", \"critical\"]\n    topic: str\n    summary: str\n\nclass EmailAgentState(TypedDict):\n    # Raw email data\n    email_content: str\n    sender_email: str\n    email_id: str\n\n# Classification result\n    classification: EmailClassification | None\n\n# Raw search/API results\n    search_results: list[str] | None  # List of raw document chunks\n    customer_history: dict | None  # Raw customer data from CRM\n\n# Generated content\n    draft_response: str | None\n    messages: list[str] | None\npython  theme={null}\n    from langgraph.types import RetryPolicy\n\nworkflow.add_node(\n        \"search_documentation\",\n        search_documentation,\n        retry_policy=RetryPolicy(max_attempts=3, initial_interval=1.0)\n    )\n    python  theme={null}\n    from langgraph.types import Command\n\ndef execute_tool(state: State) -> Command[Literal[\"agent\", \"execute_tool\"]]:\n        try:\n            result = run_tool(state['tool_call'])\n            return Command(update={\"tool_result\": result}, goto=\"agent\")\n        except ToolError as e:\n            # Let the LLM see what went wrong and try again\n            return Command(\n                update={\"tool_result\": f\"Tool error: {str(e)}\"},\n                goto=\"agent\"\n            )\n    python  theme={null}\n    from langgraph.types import Command\n\ndef lookup_customer_history(state: State) -> Command[Literal[\"draft_response\"]]:\n        if not state.get('customer_id'):\n            user_input = interrupt({\n                \"message\": \"Customer ID needed\",\n                \"request\": \"Please provide the customer's account ID to look up their subscription history\"\n            })\n            return Command(\n                update={\"customer_id\": user_input['customer_id']},\n                goto=\"lookup_customer_history\"\n            )\n        # Now proceed with the lookup\n        customer_data = fetch_customer_history(state['customer_id'])\n        return Command(update={\"customer_history\": customer_data}, goto=\"draft_response\")\n    python  theme={null}\n    def send_reply(state: EmailAgentState):\n        try:\n            email_service.send(state[\"draft_response\"])\n        except Exception:\n            raise  # Surface unexpected errors\n    python  theme={null}\n    from typing import Literal\n    from langgraph.graph import StateGraph, START, END\n    from langgraph.types import interrupt, Command, RetryPolicy\n    from langchain_openai import ChatOpenAI\n    from langchain.messages import HumanMessage\n\nllm = ChatOpenAI(model=\"gpt-5-nano\")\n\ndef read_email(state: EmailAgentState) -> dict:\n        \"\"\"Extract and parse email content\"\"\"\n        # In production, this would connect to your email service\n        return {\n            \"messages\": [HumanMessage(content=f\"Processing email: {state['email_content']}\")]\n        }\n\ndef classify_intent(state: EmailAgentState) -> Command[Literal[\"search_documentation\", \"human_review\", \"draft_response\", \"bug_tracking\"]]:\n        \"\"\"Use LLM to classify email intent and urgency, then route accordingly\"\"\"\n\n# Create structured LLM that returns EmailClassification dict\n        structured_llm = llm.with_structured_output(EmailClassification)\n\n# Format the prompt on-demand, not stored in state\n        classification_prompt = f\"\"\"\n        Analyze this customer email and classify it:\n\nEmail: {state['email_content']}\n        From: {state['sender_email']}\n\nProvide classification including intent, urgency, topic, and summary.\n        \"\"\"\n\n# Get structured response directly as dict\n        classification = structured_llm.invoke(classification_prompt)\n\n# Determine next node based on classification\n        if classification['intent'] == 'billing' or classification['urgency'] == 'critical':\n            goto = \"human_review\"\n        elif classification['intent'] in ['question', 'feature']:\n            goto = \"search_documentation\"\n        elif classification['intent'] == 'bug':\n            goto = \"bug_tracking\"\n        else:\n            goto = \"draft_response\"\n\n# Store classification as a single dict in state\n        return Command(\n            update={\"classification\": classification},\n            goto=goto\n        )\n    python  theme={null}\n    def search_documentation(state: EmailAgentState) -> Command[Literal[\"draft_response\"]]:\n        \"\"\"Search knowledge base for relevant information\"\"\"\n\n# Build search query from classification\n        classification = state.get('classification', {})\n        query = f\"{classification.get('intent', '')} {classification.get('topic', '')}\"\n\ntry:\n            # Implement your search logic here\n            # Store raw search results, not formatted text\n            search_results = [\n                \"Reset password via Settings > Security > Change Password\",\n                \"Password must be at least 12 characters\",\n                \"Include uppercase, lowercase, numbers, and symbols\"\n            ]\n        except SearchAPIError as e:\n            # For recoverable search errors, store error and continue\n            search_results = [f\"Search temporarily unavailable: {str(e)}\"]\n\nreturn Command(\n            update={\"search_results\": search_results},  # Store raw results or error\n            goto=\"draft_response\"\n        )\n\ndef bug_tracking(state: EmailAgentState) -> Command[Literal[\"draft_response\"]]:\n        \"\"\"Create or update bug tracking ticket\"\"\"\n\n# Create ticket in your bug tracking system\n        ticket_id = \"BUG-12345\"  # Would be created via API\n\nreturn Command(\n            update={\n                \"search_results\": [f\"Bug ticket {ticket_id} created\"],\n                \"current_step\": \"bug_tracked\"\n            },\n            goto=\"draft_response\"\n        )\n    python  theme={null}\n    def draft_response(state: EmailAgentState) -> Command[Literal[\"human_review\", \"send_reply\"]]:\n        \"\"\"Generate response using context and route based on quality\"\"\"\n\nclassification = state.get('classification', {})\n\n# Format context from raw state data on-demand\n        context_sections = []\n\nif state.get('search_results'):\n            # Format search results for the prompt\n            formatted_docs = \"\\n\".join([f\"- {doc}\" for doc in state['search_results']])\n            context_sections.append(f\"Relevant documentation:\\n{formatted_docs}\")\n\nif state.get('customer_history'):\n            # Format customer data for the prompt\n            context_sections.append(f\"Customer tier: {state['customer_history'].get('tier', 'standard')}\")\n\n# Build the prompt with formatted context\n        draft_prompt = f\"\"\"\n        Draft a response to this customer email:\n        {state['email_content']}\n\nEmail intent: {classification.get('intent', 'unknown')}\n        Urgency level: {classification.get('urgency', 'medium')}\n\n{chr(10).join(context_sections)}\n\nGuidelines:\n        - Be professional and helpful\n        - Address their specific concern\n        - Use the provided documentation when relevant\n        \"\"\"\n\nresponse = llm.invoke(draft_prompt)\n\n# Determine if human review needed based on urgency and intent\n        needs_review = (\n            classification.get('urgency') in ['high', 'critical'] or\n            classification.get('intent') == 'complex'\n        )\n\n# Route to appropriate next node\n        goto = \"human_review\" if needs_review else \"send_reply\"\n\nreturn Command(\n            update={\"draft_response\": response.content},  # Store only the raw response\n            goto=goto\n        )\n\ndef human_review(state: EmailAgentState) -> Command[Literal[\"send_reply\", END]]:\n        \"\"\"Pause for human review using interrupt and route based on decision\"\"\"\n\nclassification = state.get('classification', {})\n\n# interrupt() must come first - any code before it will re-run on resume\n        human_decision = interrupt({\n            \"email_id\": state.get('email_id',''),\n            \"original_email\": state.get('email_content',''),\n            \"draft_response\": state.get('draft_response',''),\n            \"urgency\": classification.get('urgency'),\n            \"intent\": classification.get('intent'),\n            \"action\": \"Please review and approve/edit this response\"\n        })\n\n# Now process the human's decision\n        if human_decision.get(\"approved\"):\n            return Command(\n                update={\"draft_response\": human_decision.get(\"edited_response\", state.get('draft_response',''))},\n                goto=\"send_reply\"\n            )\n        else:\n            # Rejection means human will handle directly\n            return Command(update={}, goto=END)\n\ndef send_reply(state: EmailAgentState) -> dict:\n        \"\"\"Send the email response\"\"\"\n        # Integrate with email service\n        print(f\"Sending reply: {state['draft_response'][:100]}...\")\n        return {}\n    python  theme={null}\n  from langgraph.checkpoint.memory import MemorySaver\n  from langgraph.types import RetryPolicy\n\n# Create the graph\n  workflow = StateGraph(EmailAgentState)\n\n# Add nodes with appropriate error handling\n  workflow.add_node(\"read_email\", read_email)\n  workflow.add_node(\"classify_intent\", classify_intent)\n\n# Add retry policy for nodes that might have transient failures\n  workflow.add_node(\n      \"search_documentation\",\n      search_documentation,\n      retry_policy=RetryPolicy(max_attempts=3)\n  )\n  workflow.add_node(\"bug_tracking\", bug_tracking)\n  workflow.add_node(\"draft_response\", draft_response)\n  workflow.add_node(\"human_review\", human_review)\n  workflow.add_node(\"send_reply\", send_reply)\n\n# Add only the essential edges\n  workflow.add_edge(START, \"read_email\")\n  workflow.add_edge(\"read_email\", \"classify_intent\")\n  workflow.add_edge(\"send_reply\", END)\n\n# Compile with checkpointer for persistence, in case run graph with Local_Server --> Please compile without checkpointer\n  memory = MemorySaver()\n  app = workflow.compile(checkpointer=memory)\n  python  theme={null}\n  # Test with an urgent billing issue\n  initial_state = {\n      \"email_content\": \"I was charged twice for my subscription! This is urgent!\",\n      \"sender_email\": \"customer@example.com\",\n      \"email_id\": \"email_123\",\n      \"messages\": []\n  }\n\n# Run with a thread_id for persistence\n  config = {\"configurable\": {\"thread_id\": \"customer_123\"}}\n  result = app.invoke(initial_state, config)\n  # The graph will pause at human_review\n  print(f\"human review interrupt:{result['__interrupt__']}\")\n\n# When ready, provide human input to resume\n  from langgraph.types import Command\n\nhuman_response = Command(\n      resume={\n          \"approved\": True,\n          \"edited_response\": \"We sincerely apologize for the double charge. I've initiated an immediate refund...\"\n      }\n  )\n\n# Resume execution\n  final_result = app.invoke(human_response, config)\n  print(f\"Email sent successfully!\")\n  ```\n</Accordion>\n\nThe graph pauses when it hits `interrupt()`, saves everything to the checkpointer, and waits. It can resume days later, picking up exactly where it left off. The `thread_id` ensures all state for this conversation is preserved together.\n\n## Summary and next steps\n\nBuilding this email agent has shown us the LangGraph way of thinking:\n\n<CardGroup cols={2}>\n  <Card title=\"Break into discrete steps\" icon=\"sitemap\" href=\"#step-1-map-out-your-workflow-as-discrete-steps\">\n    Each node does one thing well. This decomposition enables streaming progress updates, durable execution that can pause and resume, and clear debugging since you can inspect state between steps.\n  </Card>\n\n<Card title=\"State is shared memory\" icon=\"database\" href=\"#step-3-design-your-state\">\n    Store raw data, not formatted text. This lets different nodes use the same information in different ways.\n  </Card>\n\n<Card title=\"Nodes are functions\" icon=\"code\" href=\"#step-4-build-your-nodes\">\n    They take state, do work, and return updates. When they need to make routing decisions, they specify both the state updates and the next destination.\n  </Card>\n\n<Card title=\"Errors are part of the flow\" icon=\"triangle-exclamation\" href=\"#handle-errors-appropriately\">\n    Transient failures get retries, LLM-recoverable errors loop back with context, user-fixable problems pause for input, and unexpected errors bubble up for debugging.\n  </Card>\n\n<Card title=\"Human input is first-class\" icon=\"user\" href=\"/oss/python/langgraph/interrupts\">\n    The `interrupt()` function pauses execution indefinitely, saves all state, and resumes exactly where it left off when you provide input. When combined with other operations in a node, it must come first.\n  </Card>\n\n<Card title=\"Graph structure emerges naturally\" icon=\"diagram-project\" href=\"#step-5-wire-it-together\">\n    You define the essential connections, and your nodes handle their own routing logic. This keeps control flow explicit and traceable - you can always understand what your agent will do next by looking at the current node.\n  </Card>\n</CardGroup>\n\n### Advanced considerations\n\n<Accordion title=\"Node granularity trade-offs\" icon=\"sliders\">\n  <Info>\n    This section explores the trade-offs in node granularity design. Most applications can skip this and use the patterns shown above.\n  </Info>\n\nYou might wonder: why not combine `Read Email` and `Classify Intent` into one node?\n\nOr why separate Doc Search from Draft Reply?\n\nThe answer involves trade-offs between resilience and observability.\n\n**The resilience consideration:** LangGraph's [durable execution](/oss/python/langgraph/durable-execution) creates checkpoints at node boundaries. When a workflow resumes after an interruption or failure, it starts from the beginning of the node where execution stopped. Smaller nodes mean more frequent checkpoints, which means less work to repeat if something goes wrong. If you combine multiple operations into one large node, a failure near the end means re-executing everything from the start of that node.\n\nWhy we chose this breakdown for the email agent:\n\n* **Isolation of external services:** Doc Search and Bug Track are separate nodes because they call external APIs. If the search service is slow or fails, we want to isolate that from the LLM calls. We can add retry policies to these specific nodes without affecting others.\n\n* **Intermediate visibility:** Having `Classify Intent` as its own node lets us inspect what the LLM decided before taking action. This is valuable for debugging and monitoring—you can see exactly when and why the agent routes to human review.\n\n* **Different failure modes:** LLM calls, database lookups, and email sending have different retry strategies. Separate nodes let you configure these independently.\n\n* **Reusability and testing:** Smaller nodes are easier to test in isolation and reuse in other workflows.\n\nA different valid approach: You could combine `Read Email` and `Classify Intent` into a single node. You'd lose the ability to inspect the raw email before classification and would repeat both operations on any failure in that node. For most applications, the observability and debugging benefits of separate nodes are worth the trade-off.\n\nApplication-level concerns: The caching discussion in Step 2 (whether to cache search results) is an application-level decision, not a LangGraph framework feature. You implement caching within your node functions based on your specific requirements—LangGraph doesn't prescribe this.\n\nPerformance considerations: More nodes doesn't mean slower execution. LangGraph writes checkpoints in the background by default ([async durability mode](/oss/python/langgraph/durable-execution#durability-modes)), so your graph continues running without waiting for checkpoints to complete. This means you get frequent checkpoints with minimal performance impact. You can adjust this behavior if needed—use `\"exit\"` mode to checkpoint only at completion, or `\"sync\"` mode to block execution until each checkpoint is written.\n</Accordion>\n\n### Where to go from here\n\nThis was an introduction to thinking about building agents with LangGraph. You can extend this foundation with:\n\n<CardGroup cols={2}>\n  <Card title=\"Human-in-the-loop patterns\" icon=\"user-check\" href=\"/oss/python/langgraph/interrupts\">\n    Learn how to add tool approval before execution, batch approval, and other patterns\n  </Card>\n\n<Card title=\"Subgraphs\" icon=\"diagram-nested\" href=\"/oss/python/langgraph/use-subgraphs\">\n    Create subgraphs for complex multi-step operations\n  </Card>\n\n<Card title=\"Streaming\" icon=\"tower-broadcast\" href=\"/oss/python/langgraph/streaming\">\n    Add streaming to show real-time progress to users\n  </Card>\n\n<Card title=\"Observability\" icon=\"chart-line\" href=\"/oss/python/langgraph/observability\">\n    Add observability with LangSmith for debugging and monitoring\n  </Card>\n\n<Card title=\"Tool Integration\" icon=\"wrench\" href=\"/oss/python/langchain/tools\">\n    Integrate more tools for web search, database queries, and API calls\n  </Card>\n\n<Card title=\"Retry Logic\" icon=\"rotate\" href=\"/oss/python/langgraph/use-graph-api#add-retry-policies\">\n    Implement retry logic with exponential backoff for failed operations\n  </Card>\n</CardGroup>\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/thinking-in-langgraph.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "code_samples": [
    {
      "code": "Notice that the state contains only raw data – no prompt templates, no formatted strings, no instructions. The classification output is stored as a single dictionary, straight from the LLM.\n\n## Step 4: Build your nodes\n\nNow we implement each step as a function. A node in LangGraph is just a Python function that takes the current state and returns updates to it.\n\n### Handle errors appropriately\n\nDifferent errors need different handling strategies:\n\n| Error Type                                                      | Who Fixes It       | Strategy                           | When to Use                                      |\n| --------------------------------------------------------------- | ------------------ | ---------------------------------- | ------------------------------------------------ |\n| Transient errors (network issues, rate limits)                  | System (automatic) | Retry policy                       | Temporary failures that usually resolve on retry |\n| LLM-recoverable errors (tool failures, parsing issues)          | LLM                | Store error in state and loop back | LLM can see the error and adjust its approach    |\n| User-fixable errors (missing information, unclear instructions) | Human              | Pause with `interrupt()`           | Need user input to proceed                       |\n| Unexpected errors                                               | Developer          | Let them bubble up                 | Unknown issues that need debugging               |\n\n<Tabs>\n  <Tab title=\"Transient errors\" icon=\"rotate\">\n    Add a retry policy to automatically retry network issues and rate limits:",
      "language": "unknown"
    },
    {
      "code": "</Tab>\n\n  <Tab title=\"LLM-recoverable\" icon=\"brain\">\n    Store the error in state and loop back so the LLM can see what went wrong and try again:",
      "language": "unknown"
    },
    {
      "code": "</Tab>\n\n  <Tab title=\"User-fixable\" icon=\"user\">\n    Pause and collect information from the user when needed (like account IDs, order numbers, or clarifications):",
      "language": "unknown"
    },
    {
      "code": "</Tab>\n\n  <Tab title=\"Unexpected\" icon=\"triangle-exclamation\">\n    Let them bubble up for debugging. Don't catch what you can't handle:",
      "language": "unknown"
    },
    {
      "code": "</Tab>\n</Tabs>\n\n### Implementing our email agent nodes\n\nWe'll implement each node as a simple function. Remember: nodes take state, do work, and return updates.\n\n<AccordionGroup>\n  <Accordion title=\"Read and classify nodes\" icon=\"brain\">",
      "language": "unknown"
    },
    {
      "code": "</Accordion>\n\n  <Accordion title=\"Search and tracking nodes\" icon=\"database\">",
      "language": "unknown"
    },
    {
      "code": "</Accordion>\n\n  <Accordion title=\"Response nodes\" icon=\"pen-to-square\">",
      "language": "unknown"
    },
    {
      "code": "</Accordion>\n</AccordionGroup>\n\n## Step 5: Wire it together\n\nNow we connect our nodes into a working graph. Since our nodes handle their own routing decisions, we only need a few essential edges.\n\nTo enable [human-in-the-loop](/oss/python/langgraph/interrupts) with `interrupt()`, we need to compile with a [checkpointer](/oss/python/langgraph/persistence) to save state between runs:\n\n<Accordion title=\"Graph compilation code\" icon=\"diagram-project\" defaultOpen={true}>",
      "language": "unknown"
    },
    {
      "code": "</Accordion>\n\nThe graph structure is minimal because routing happens inside nodes through [`Command`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.Command) objects. Each node declares where it can go using type hints like `Command[Literal[\"node1\", \"node2\"]]`, making the flow explicit and traceable.\n\n### Try out your agent\n\nLet's run our agent with an urgent billing issue that needs human review:\n\n<Accordion title=\"Testing the agent\" icon=\"flask\">",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": "Step 4: Build your nodes",
      "id": "step-4:-build-your-nodes"
    },
    {
      "level": "h3",
      "text": "Handle errors appropriately",
      "id": "handle-errors-appropriately"
    },
    {
      "level": "h3",
      "text": "Implementing our email agent nodes",
      "id": "implementing-our-email-agent-nodes"
    },
    {
      "level": "h2",
      "text": "Step 5: Wire it together",
      "id": "step-5:-wire-it-together"
    },
    {
      "level": "h3",
      "text": "Try out your agent",
      "id": "try-out-your-agent"
    },
    {
      "level": "h2",
      "text": "Summary and next steps",
      "id": "summary-and-next-steps"
    },
    {
      "level": "h3",
      "text": "Key Insights",
      "id": "key-insights"
    },
    {
      "level": "h3",
      "text": "Advanced considerations",
      "id": "advanced-considerations"
    },
    {
      "level": "h3",
      "text": "Where to go from here",
      "id": "where-to-go-from-here"
    }
  ],
  "url": "llms-txt#define-the-structure-for-email-classification",
  "links": []
}