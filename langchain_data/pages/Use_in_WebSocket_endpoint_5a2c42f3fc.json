{
  "title": "Use in WebSocket endpoint",
  "content": "@app.websocket(\"/ws\")\nasync def websocket_endpoint(websocket: WebSocket):\n    await websocket.accept()\n\nasync def websocket_audio_stream():\n        \"\"\"Yield audio bytes from WebSocket.\"\"\"\n        while True:\n            data = await websocket.receive_bytes()\n            yield data\n\n# Transform audio through pipeline\n    output_stream = pipeline.atransform(websocket_audio_stream())\n\n# Send TTS audio back to client\n    async for event in output_stream:\n        if event.type == \"tts_chunk\":\n            await websocket.send_bytes(event.audio)\n```\n\nWe use [RunnableGenerators](https://reference.langchain.com/python/langchain_core/runnables/#langchain_core.runnables.base.RunnableGenerator) to compose each step of the pipeline. This is an abstraction LangChain uses internally to manage [streaming across components](https://reference.langchain.com/python/langchain_core/runnables/).\n\nEach stage processes events independently and concurrently: audio transcription begins as soon as audio arrives, the agent starts reasoning as soon as a transcript is available, and speech synthesis begins as soon as agent text is generated. This architecture can achieve sub-700ms latency to support natural conversation.\n\nFor more on building agents with LangChain, see the [Agents guide](/oss/python/langchain/agents).\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/voice-agent.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "code_samples": [],
  "headings": [],
  "url": "llms-txt#use-in-websocket-endpoint",
  "links": []
}