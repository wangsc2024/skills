{
  "title": "Both agents execute simultaneously, each receiving only the query it needs",
  "content": "python  theme={null}\n{\"results\": [{\"source\": \"github\", \"result\": \"...\"}]}\npython  theme={null}\n  \"\"\"\n  Multi-Source Knowledge Router Example\n\nThis example demonstrates the router pattern for multi-agent systems.\n  A router classifies queries, routes them to specialized agents in parallel,\n  and synthesizes results into a combined response.\n  \"\"\"\n\nimport operator\n  from typing import Annotated, Literal, TypedDict\n\nfrom langchain.agents import create_agent\n  from langchain.chat_models import init_chat_model\n  from langchain.tools import tool\n  from langgraph.graph import StateGraph, START, END\n  from langgraph.types import Send\n  from pydantic import BaseModel, Field\n\n# State definitions\n  class AgentInput(TypedDict):\n      \"\"\"Simple input state for each subagent.\"\"\"\n      query: str\n\nclass AgentOutput(TypedDict):\n      \"\"\"Output from each subagent.\"\"\"\n      source: str\n      result: str\n\nclass Classification(TypedDict):\n      \"\"\"A single routing decision: which agent to call with what query.\"\"\"\n      source: Literal[\"github\", \"notion\", \"slack\"]\n      query: str\n\nclass RouterState(TypedDict):\n      query: str\n      classifications: list[Classification]\n      results: Annotated[list[AgentOutput], operator.add]\n      final_answer: str\n\n# Structured output schema for classifier\n  class ClassificationResult(BaseModel):\n      \"\"\"Result of classifying a user query into agent-specific sub-questions.\"\"\"\n      classifications: list[Classification] = Field(\n          description=\"List of agents to invoke with their targeted sub-questions\"\n      )\n\n# Tools\n  @tool\n  def search_code(query: str, repo: str = \"main\") -> str:\n      \"\"\"Search code in GitHub repositories.\"\"\"\n      return f\"Found code matching '{query}' in {repo}: authentication middleware in src/auth.py\"\n\n@tool\n  def search_issues(query: str) -> str:\n      \"\"\"Search GitHub issues and pull requests.\"\"\"\n      return f\"Found 3 issues matching '{query}': #142 (API auth docs), #89 (OAuth flow), #203 (token refresh)\"\n\n@tool\n  def search_prs(query: str) -> str:\n      \"\"\"Search pull requests for implementation details.\"\"\"\n      return f\"PR #156 added JWT authentication, PR #178 updated OAuth scopes\"\n\n@tool\n  def search_notion(query: str) -> str:\n      \"\"\"Search Notion workspace for documentation.\"\"\"\n      return f\"Found documentation: 'API Authentication Guide' - covers OAuth2 flow, API keys, and JWT tokens\"\n\n@tool\n  def get_page(page_id: str) -> str:\n      \"\"\"Get a specific Notion page by ID.\"\"\"\n      return f\"Page content: Step-by-step authentication setup instructions\"\n\n@tool\n  def search_slack(query: str) -> str:\n      \"\"\"Search Slack messages and threads.\"\"\"\n      return f\"Found discussion in #engineering: 'Use Bearer tokens for API auth, see docs for refresh flow'\"\n\n@tool\n  def get_thread(thread_id: str) -> str:\n      \"\"\"Get a specific Slack thread.\"\"\"\n      return f\"Thread discusses best practices for API key rotation\"\n\n# Models and agents\n  model = init_chat_model(\"openai:gpt-4o\")\n  router_llm = init_chat_model(\"openai:gpt-4o-mini\")\n\ngithub_agent = create_agent(\n      model,\n      tools=[search_code, search_issues, search_prs],\n      system_prompt=(\n          \"You are a GitHub expert. Answer questions about code, \"\n          \"API references, and implementation details by searching \"\n          \"repositories, issues, and pull requests.\"\n      ),\n  )\n\nnotion_agent = create_agent(\n      model,\n      tools=[search_notion, get_page],\n      system_prompt=(\n          \"You are a Notion expert. Answer questions about internal \"\n          \"processes, policies, and team documentation by searching \"\n          \"the organization's Notion workspace.\"\n      ),\n  )\n\nslack_agent = create_agent(\n      model,\n      tools=[search_slack, get_thread],\n      system_prompt=(\n          \"You are a Slack expert. Answer questions by searching \"\n          \"relevant threads and discussions where team members have \"\n          \"shared knowledge and solutions.\"\n      ),\n  )\n\n# Workflow nodes\n  def classify_query(state: RouterState) -> dict:\n      \"\"\"Classify query and determine which agents to invoke.\"\"\"\n      structured_llm = router_llm.with_structured_output(ClassificationResult)\n\nresult = structured_llm.invoke([\n          {\n              \"role\": \"system\",\n              \"content\": \"\"\"Analyze this query and determine which knowledge bases to consult.\n  For each relevant source, generate a targeted sub-question optimized for that source.\n\nAvailable sources:\n  - github: Code, API references, implementation details, issues, pull requests\n  - notion: Internal documentation, processes, policies, team wikis\n  - slack: Team discussions, informal knowledge sharing, recent conversations\n\nReturn ONLY the sources that are relevant to the query.\"\"\"\n          },\n          {\"role\": \"user\", \"content\": state[\"query\"]}\n      ])\n\nreturn {\"classifications\": result.classifications}\n\ndef route_to_agents(state: RouterState) -> list[Send]:\n      \"\"\"Fan out to agents based on classifications.\"\"\"\n      return [\n          Send(c[\"source\"], {\"query\": c[\"query\"]})\n          for c in state[\"classifications\"]\n      ]\n\ndef query_github(state: AgentInput) -> dict:\n      \"\"\"Query the GitHub agent.\"\"\"\n      result = github_agent.invoke({\n          \"messages\": [{\"role\": \"user\", \"content\": state[\"query\"]}]\n      })\n      return {\"results\": [{\"source\": \"github\", \"result\": result[\"messages\"][-1].content}]}\n\ndef query_notion(state: AgentInput) -> dict:\n      \"\"\"Query the Notion agent.\"\"\"\n      result = notion_agent.invoke({\n          \"messages\": [{\"role\": \"user\", \"content\": state[\"query\"]}]\n      })\n      return {\"results\": [{\"source\": \"notion\", \"result\": result[\"messages\"][-1].content}]}\n\ndef query_slack(state: AgentInput) -> dict:\n      \"\"\"Query the Slack agent.\"\"\"\n      result = slack_agent.invoke({\n          \"messages\": [{\"role\": \"user\", \"content\": state[\"query\"]}]\n      })\n      return {\"results\": [{\"source\": \"slack\", \"result\": result[\"messages\"][-1].content}]}\n\ndef synthesize_results(state: RouterState) -> dict:\n      \"\"\"Combine results from all agents into a coherent answer.\"\"\"\n      if not state[\"results\"]:\n          return {\"final_answer\": \"No results found from any knowledge source.\"}\n\nformatted = [\n          f\"**From {r['source'].title()}:**\\n{r['result']}\"\n          for r in state[\"results\"]\n      ]\n\nsynthesis_response = router_llm.invoke([\n          {\n              \"role\": \"system\",\n              \"content\": f\"\"\"Synthesize these search results to answer the original question: \"{state['query']}\"\n\n- Combine information from multiple sources without redundancy\n  - Highlight the most relevant and actionable information\n  - Note any discrepancies between sources\n  - Keep the response concise and well-organized\"\"\"\n          },\n          {\"role\": \"user\", \"content\": \"\\n\\n\".join(formatted)}\n      ])\n\nreturn {\"final_answer\": synthesis_response.content}\n\n# Build workflow\n  workflow = (\n      StateGraph(RouterState)\n      .add_node(\"classify\", classify_query)\n      .add_node(\"github\", query_github)\n      .add_node(\"notion\", query_notion)\n      .add_node(\"slack\", query_slack)\n      .add_node(\"synthesize\", synthesize_results)\n      .add_edge(START, \"classify\")\n      .add_conditional_edges(\"classify\", route_to_agents, [\"github\", \"notion\", \"slack\"])\n      .add_edge(\"github\", \"synthesize\")\n      .add_edge(\"notion\", \"synthesize\")\n      .add_edge(\"slack\", \"synthesize\")\n      .add_edge(\"synthesize\", END)\n      .compile()\n  )\n\nif __name__ == \"__main__\":\n      result = workflow.invoke({\n          \"query\": \"How do I authenticate API requests?\"\n      })\n\nprint(\"Original query:\", result[\"query\"])\n      print(\"\\nClassifications:\")\n      for c in result[\"classifications\"]:\n          print(f\"  {c['source']}: {c['query']}\")\n      print(\"\\n\" + \"=\" * 60 + \"\\n\")\n      print(\"Final Answer:\")\n      print(result[\"final_answer\"])\n  python  theme={null}\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n@tool\ndef search_knowledge_base(query: str) -> str:\n    \"\"\"Search across multiple knowledge sources (GitHub, Notion, Slack).\n\nUse this to find information about code, documentation, or team discussions.\n    \"\"\"\n    result = workflow.invoke({\"query\": query})\n    return result[\"final_answer\"]\n\nconversational_agent = create_agent(\n    model,\n    tools=[search_knowledge_base],\n    system_prompt=(\n        \"You are a helpful assistant that answers questions about our organization. \"\n        \"Use the search_knowledge_base tool to find information across our code, \"\n        \"documentation, and team discussions.\"\n    ),\n    checkpointer=InMemorySaver(),\n)\npython  theme={null}\nconfig = {\"configurable\": {\"thread_id\": \"user-123\"}}\n\nresult = conversational_agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"How do I authenticate API requests?\"}]},\n    config\n)\nprint(result[\"messages\"][-1].content)\n\nresult = conversational_agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What about rate limiting for those endpoints?\"}]},\n    config\n)\nprint(result[\"messages\"][-1].content)\n```\n\n<Tip>\n  The tool wrapper approach is recommended for most use cases. It provides clean separation: the router handles multi-source querying, while the conversational agent handles context and memory.\n</Tip>\n\n### Full persistence approach\n\nIf you need the router itself to maintain state—for example, to use previous search results in routing decisions—use [persistence](/oss/python/langchain/short-term-memory) to store message history at the router level.\n\n<Warning>\n  **Stateful routers add complexity.** When routing to different agents across turns, conversations may feel inconsistent if agents have different tones or prompts. Consider the [handoffs pattern](/oss/python/langchain/multi-agent/handoffs) or [subagents pattern](/oss/python/langchain/multi-agent/subagents) instead—both provide clearer semantics for multi-turn conversations with different agents.\n</Warning>\n\nThe router pattern excels when you have:\n\n* **Distinct verticals**: Separate knowledge domains that each require specialized tools and prompts\n* **Parallel query needs**: Questions that benefit from querying multiple sources simultaneously\n* **Synthesis requirements**: Results from multiple sources need to be combined into a coherent response\n\nThe pattern has three phases: **decompose** (analyze the query and generate targeted sub-questions), **route** (execute queries in parallel), and **synthesize** (combine results).\n\n<Tip>\n  **When to use the router pattern**\n\nUse the router pattern when you have multiple independent knowledge sources, need low-latency parallel queries, and want explicit control over routing logic.\n\nFor simpler cases with dynamic tool selection, consider the [subagents pattern](/oss/python/langchain/multi-agent/subagents). For workflows where agents need to converse with users sequentially, consider [handoffs](/oss/python/langchain/multi-agent/handoffs).\n</Tip>\n\n* Learn about [handoffs](/oss/python/langchain/multi-agent/handoffs) for agent-to-agent conversations\n* Explore the [subagents pattern](/oss/python/langchain/multi-agent/subagents-personal-assistant) for centralized orchestration\n* Read the [multi-agent overview](/oss/python/langchain/multi-agent) to compare different patterns\n* Use [LangSmith](https://smith.langchain.com) to debug and monitor your router\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/multi-agent/router-knowledge-base.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "code_samples": [
    {
      "code": "Each agent node receives a simple `AgentInput` with just a `query` field—not the full router state. This keeps the interface clean and explicit.\n\n### Result collection with reducers\n\nAgent results flow back to the main state via a **reducer**. Each agent returns:",
      "language": "unknown"
    },
    {
      "code": "The reducer (`operator.add` in Python) concatenates these lists, collecting all parallel results into `state[\"results\"]`.\n\n### Synthesis phase\n\nAfter all agents complete, the `synthesize_results` function iterates over the collected results:\n\n* Waits for all parallel branches to complete (LangGraph handles this automatically)\n* References the original query to ensure the answer addresses what the user asked\n* Combines information from all sources without redundancy\n\n<Note>\n  **Partial results**: In this tutorial, all selected agents must complete before synthesis. For more advanced patterns where you want to handle partial results or timeouts, see the [map-reduce guide](/oss/python/langchain/map-reduce).\n</Note>\n\n## 8. Complete working example\n\nHere's everything together in a runnable script:\n\n<Expandable title=\"View complete code\" defaultOpen={false}>",
      "language": "unknown"
    },
    {
      "code": "</Expandable>\n\n## 9. Advanced: Stateful routers\n\nThe router we've built so far is **stateless**—each request is handled independently with no memory between calls. For multi-turn conversations, you need a **stateful** approach.\n\n### Tool wrapper approach\n\nThe simplest way to add conversation memory is to wrap the stateless router as a tool that a conversational agent can call:",
      "language": "unknown"
    },
    {
      "code": "This approach keeps the router stateless while the conversational agent handles memory and context. The user can have a multi-turn conversation, and the agent will call the router tool as needed.",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h3",
      "text": "Result collection with reducers",
      "id": "result-collection-with-reducers"
    },
    {
      "level": "h3",
      "text": "Synthesis phase",
      "id": "synthesis-phase"
    },
    {
      "level": "h2",
      "text": "8. Complete working example",
      "id": "8.-complete-working-example"
    },
    {
      "level": "h2",
      "text": "9. Advanced: Stateful routers",
      "id": "9.-advanced:-stateful-routers"
    },
    {
      "level": "h3",
      "text": "Tool wrapper approach",
      "id": "tool-wrapper-approach"
    },
    {
      "level": "h3",
      "text": "Full persistence approach",
      "id": "full-persistence-approach"
    },
    {
      "level": "h2",
      "text": "10. Key takeaways",
      "id": "10.-key-takeaways"
    },
    {
      "level": "h2",
      "text": "Next steps",
      "id": "next-steps"
    }
  ],
  "url": "llms-txt#both-agents-execute-simultaneously,-each-receiving-only-the-query-it-needs",
  "links": []
}