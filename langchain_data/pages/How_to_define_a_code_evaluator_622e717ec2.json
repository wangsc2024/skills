{
  "title": "How to define a code evaluator",
  "content": "Source: https://docs.langchain.com/langsmith/code-evaluator\n\n<Info>\n  * [Evaluators](/langsmith/evaluation-concepts#evaluators)\n</Info>\n\nCode evaluators are just functions that take a dataset example and the resulting application output, and return one or more metrics. These functions can be passed directly into [evaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate) / [aevaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._arunner.aevaluate).\n\ncode evaluator functions must have specific argument names. They can take any subset of the following arguments:\n\n* `run: Run`: The full [Run](/langsmith/run-data-format) object generated by the application on the given example.\n* `example: Example`: The full dataset [Example](/langsmith/example-data-format), including the example inputs, outputs (if available), and metdata (if available).\n* `inputs: dict`: A dictionary of the inputs corresponding to a single example in a dataset.\n* `outputs: dict`: A dictionary of the outputs generated by the application on the given `inputs`.\n* `reference_outputs/referenceOutputs: dict`: A dictionary of the reference outputs associated with the example, if available.\n\nFor most use cases you'll only need `inputs`, `outputs`, and `reference_outputs`. `run` and `example` are useful only if you need some extra trace or example metadata outside of the actual inputs and outputs of the application.\n\nWhen using JS/TS these should all be passed in as part of a single object argument.\n\nCode evaluators are expected to return one of the following types:\n\n* `dict`: dicts of the form `{\"score\" | \"value\": ..., \"key\": ...}` allow you to customize the metric type (\"score\" for numerical and \"value\" for categorical) and metric name. This if useful if, for example, you want to log an integer as a categorical metric.\n\n* `int | float | bool`: this is interepreted as an continuous metric that can be averaged, sorted, etc. The function name is used as the name of the metric.\n* `str`: this is intepreted as a categorical metric. The function name is used as the name of the metric.\n* `list[dict]`: return multiple metrics using a single function.\n\n## Additional examples\n\nRequires `langsmith>=0.2.0`\n\n* [Evaluate aggregate experiment results](/langsmith/summary): Define summary evaluators, which compute metrics for an entire experiment.\n* [Run an evaluation comparing two experiments](/langsmith/evaluate-pairwise): Define pairwise evaluators, which compute metrics by comparing two (or more) experiments against each other.\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/code-evaluator.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "code_samples": [
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</CodeGroup>\n\n## Evaluator args\n\ncode evaluator functions must have specific argument names. They can take any subset of the following arguments:\n\n* `run: Run`: The full [Run](/langsmith/run-data-format) object generated by the application on the given example.\n* `example: Example`: The full dataset [Example](/langsmith/example-data-format), including the example inputs, outputs (if available), and metdata (if available).\n* `inputs: dict`: A dictionary of the inputs corresponding to a single example in a dataset.\n* `outputs: dict`: A dictionary of the outputs generated by the application on the given `inputs`.\n* `reference_outputs/referenceOutputs: dict`: A dictionary of the reference outputs associated with the example, if available.\n\nFor most use cases you'll only need `inputs`, `outputs`, and `reference_outputs`. `run` and `example` are useful only if you need some extra trace or example metadata outside of the actual inputs and outputs of the application.\n\nWhen using JS/TS these should all be passed in as part of a single object argument.\n\n## Evaluator output\n\nCode evaluators are expected to return one of the following types:\n\nPython and JS/TS\n\n* `dict`: dicts of the form `{\"score\" | \"value\": ..., \"key\": ...}` allow you to customize the metric type (\"score\" for numerical and \"value\" for categorical) and metric name. This if useful if, for example, you want to log an integer as a categorical metric.\n\nPython only\n\n* `int | float | bool`: this is interepreted as an continuous metric that can be averaged, sorted, etc. The function name is used as the name of the metric.\n* `str`: this is intepreted as a categorical metric. The function name is used as the name of the metric.\n* `list[dict]`: return multiple metrics using a single function.\n\n## Additional examples\n\nRequires `langsmith>=0.2.0`\n\n<CodeGroup>",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": "Basic example",
      "id": "basic-example"
    },
    {
      "level": "h2",
      "text": "Evaluator args",
      "id": "evaluator-args"
    },
    {
      "level": "h2",
      "text": "Evaluator output",
      "id": "evaluator-output"
    },
    {
      "level": "h2",
      "text": "Additional examples",
      "id": "additional-examples"
    },
    {
      "level": "h2",
      "text": "Related",
      "id": "related"
    }
  ],
  "url": "llms-txt#how-to-define-a-code-evaluator",
  "links": []
}