{
  "title": "Add to conversation history",
  "content": "messages = [\n    SystemMessage(\"You are a helpful assistant\"),\n    HumanMessage(\"Can you help me?\"),\n    ai_msg,  # Insert as if it came from the model\n    HumanMessage(\"Great! What's 2+2?\")\n]\n\nresponse = model.invoke(messages)\npython  theme={null}\nfrom langchain.chat_models import init_chat_model\n\nmodel = init_chat_model(\"gpt-5-nano\")\n\ndef get_weather(location: str) -> str:\n    \"\"\"Get the weather at a location.\"\"\"\n    ...\n\nmodel_with_tools = model.bind_tools([get_weather])\nresponse = model_with_tools.invoke(\"What's the weather in Paris?\")\n\nfor tool_call in response.tool_calls:\n    print(f\"Tool: {tool_call['name']}\")\n    print(f\"Args: {tool_call['args']}\")\n    print(f\"ID: {tool_call['id']}\")\npython  theme={null}\nfrom langchain.chat_models import init_chat_model\n\nmodel = init_chat_model(\"gpt-5-nano\")\n\nresponse = model.invoke(\"Hello!\")\nresponse.usage_metadata\n\n{'input_tokens': 8,\n 'output_tokens': 304,\n 'total_tokens': 312,\n 'input_token_details': {'audio': 0, 'cache_read': 0},\n 'output_token_details': {'audio': 0, 'reasoning': 256}}\npython  theme={null}\nchunks = []\nfull_message = None\nfor chunk in model.stream(\"Hi\"):\n    chunks.append(chunk)\n    print(chunk.text)\n    full_message = chunk if full_message is None else full_message + chunk\npython  theme={null}\nfrom langchain.messages import AIMessage\nfrom langchain.messages import ToolMessage",
  "code_samples": [
    {
      "code": "<Accordion title=\"Attributes\">\n  <ParamField path=\"text\" type=\"string\">\n    The text content of the message.\n  </ParamField>\n\n  <ParamField path=\"content\" type=\"string | dict[]\">\n    The raw content of the message.\n  </ParamField>\n\n  <ParamField path=\"content_blocks\" type=\"ContentBlock[]\">\n    The standardized [content blocks](#message-content) of the message.\n  </ParamField>\n\n  <ParamField path=\"tool_calls\" type=\"dict[] | None\">\n    The tool calls made by the model.\n\n    Empty if no tools are called.\n  </ParamField>\n\n  <ParamField path=\"id\" type=\"string\">\n    A unique identifier for the message (either automatically generated by LangChain or returned in the provider response)\n  </ParamField>\n\n  <ParamField path=\"usage_metadata\" type=\"dict | None\">\n    The usage metadata of the message, which can contain token counts when available.\n  </ParamField>\n\n  <ParamField path=\"response_metadata\" type=\"ResponseMetadata | None\">\n    The response metadata of the message.\n  </ParamField>\n</Accordion>\n\n#### Tool calls\n\nWhen models make [tool calls](/oss/python/langchain/models#tool-calling), they're included in the [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage):",
      "language": "unknown"
    },
    {
      "code": "Other structured data, such as reasoning or citations, can also appear in message [content](/oss/python/langchain/messages#message-content).\n\n#### Token usage\n\nAn [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) can hold token counts and other usage metadata in its [`usage_metadata`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.UsageMetadata) field:",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "See [`UsageMetadata`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.UsageMetadata) for details.\n\n#### Streaming and chunks\n\nDuring streaming, you'll receive [`AIMessageChunk`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessageChunk) objects that can be combined into a full message object:",
      "language": "unknown"
    },
    {
      "code": "<Note>\n  Learn more:\n\n  * [Streaming tokens from chat models](/oss/python/langchain/models#stream)\n  * [Streaming tokens and/or steps from agents](/oss/python/langchain/streaming)\n</Note>\n\n***\n\n### Tool Message\n\nFor models that support [tool calling](/oss/python/langchain/models#tool-calling), AI messages can contain tool calls. Tool messages are used to pass the results of a single tool execution back to the model.\n\n[Tools](/oss/python/langchain/tools) can generate [`ToolMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.ToolMessage) objects directly. Below, we show a simple example. Read more in the [tools guide](/oss/python/langchain/tools).",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h3",
      "text": "Tool Message",
      "id": "tool-message"
    }
  ],
  "url": "llms-txt#add-to-conversation-history",
  "links": []
}