{
  "title": "Set up online evaluators",
  "content": "Source: https://docs.langchain.com/langsmith/online-evaluations\n\n<Tip>\n  **Recommended Reading**\n\nBefore diving into this content, it might be helpful to read the following:\n\n* Running [online evaluations](/langsmith/evaluation-concepts#online-evaluation)\n</Tip>\n\nOnline evaluations provide real-time feedback on your production traces. This is useful to continuously monitor the performance of your application—to identify issues, measure improvements, and ensure consistent quality over time.\n\nThere are two types of online evaluations supported in LangSmith:\n\n* **[LLM-as-a-judge](/langsmith/evaluation-concepts#llm-as-judge)**: Use an LLM to evaluate traces as a scalable substitute for human-like judgment (e.g., toxicity, hallucinations, correctness). Supports two different levels of granularity:\n  * **Run level**: Evaluate a single run.\n  * [**Thread level**](/langsmith/online-evaluations#configure-multi-turn-online-evaluators): Evaluate all traces in a thread.\n* **Custom Code**: Write an evaluator in Python directly in LangSmith. Often used for validating structure or statistical properties of your data.\n\n<Note>When an online evaluator runs on any run within a trace, the trace will be auto-upgraded to [extended data retention](/langsmith/administration-overview#data-retention-auto-upgrades). This upgrade will impact trace pricing, but ensures that traces meeting your evaluation criteria (typically those most valuable for analysis) are preserved for investigation. </Note>\n\n## View online evaluators\n\nHead to the **Tracing Projects** tab and select a tracing project. To view existing online evaluators for that project, click on the **Evaluators** tab.\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-evaluators.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=471b55b0d23b6c54ea5044406f0c55f7\" alt=\"View online evaluators\" data-og-width=\"1350\" width=\"1350\" data-og-height=\"639\" height=\"639\" data-path=\"langsmith/images/view-evaluators.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-evaluators.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=141082993aba37d45550bfff9da502df 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-evaluators.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=211cc6c5359e00ab23f0cf55bd67fd93 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-evaluators.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=fcdae1f3bce28bfcdd91059e43f9e1be 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-evaluators.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=93b239efbd10f6ab5013e91b08384df6 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-evaluators.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=b6e496bee86cfb221cccde72366f83bb 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-evaluators.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=79817ff090124e1ec7b5f25eb2ddd978 2500w\" />\n\n## Configure online evaluators\n\n#### 1. Navigate to online evaluators\n\nHead to the **Tracing Projects** tab and select a tracing project. Click on **+ New** in the top right corner of the tracing project page, then click on **New Evaluator**. Select the evaluator you want to configure.\n\n#### 2. Name your evaluator\n\n#### 3. Create a filter\n\nFor example, you may want to apply specific evaluators based on:\n\n* Runs where a [user left feedback](/langsmith/attach-user-feedback) indicating the response was unsatisfactory.\n* Runs that invoke a specific tool call. See [filtering for tool calls](/langsmith/filter-traces-in-application#example-filtering-for-tool-calls) for more information.\n* Runs that match a particular piece of metadata (e.g. if you log traces with a `plan_type` and only want to run evaluations on traces from your enterprise customers). See [adding metadata to your traces](/langsmith/add-metadata-tags) for more information.\n\nFilters on evaluators work the same way as when you're filtering traces in a project. For more information on filters, you can refer to [this guide](./filter-traces-in-application).\n\n<Tip>\n  It's often helpful to inspect runs as you're creating a filter for your evaluator. With the evaluator configuration panel open, you can inspect runs and apply filters to them. Any filters you apply to the runs table will automatically be reflected in filters on your evaluator.\n</Tip>\n\n#### 4. (Optional) Configure a sampling rate\n\nConfigure a sampling rate to control the percentage of filtered runs that trigger the automation action. For example, to control costs, you may want to set a filter to only apply the evaluator to 10% of traces. In order to do this, you would set the sampling rate to 0.1.\n\n#### 5. (Optional) Apply rule to past runs\n\nApply rule to past runs by toggling the **Apply to past runs** and entering a \"Backfill from\" date. This is only possible upon rule creation. Note: the backfill is processed as a background job, so you will not see the results immediately.\n\nIn order to track progress of the backfill, you can view logs for your evaluator by heading to the **Evaluators** tab within a tracing project and clicking the Logs button for the evaluator you created. Online evaluator logs are similar to [automation rule logs](./rules#view-logs-for-your-automations).\n\n* Add an evaluator name\n* Optionally filter runs that you would like to apply your evaluator on or configure a sampling rate.\n* Select **Apply Evaluator**\n\n#### 6. Select evaluator type\n\n* Configuring [LLM-as-a-judge evaluators](/langsmith/online-evaluations#configure-a-llm-as-a-judge-online-evaluator)\n* Configuring [custom code evaluators](/langsmith/online-evaluations#configure-a-custom-code-evaluator)\n\n### Configure a LLM-as-a-judge online evaluator\n\nView this guide to configure an [LLM-as-a-judge evaluator](/langsmith/llm-as-judge?mode=ui#pre-built-evaluators-1).\n\n### Configure a custom code evaluator\n\nSelect **custom code** evaluator.\n\n#### Write your evaluation function\n\n<Note>\n  **Custom code evaluators restrictions.**\n\n**Allowed Libraries**: You can import all standard library functions, as well as the following public packages:\n\n**Network Access**: You cannot access the internet from a custom code evaluator.\n</Note>\n\nCustom code evaluators must be written inline. We recommend testing locally before setting up your custom code evaluator in LangSmith.\n\nIn the UI, you will see a panel that lets you write your code inline, with some starter code:\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/online-eval-custom-code.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=cf7b75691edb3afaa10652a79813e581\" alt=\"Online eval custom code\" data-og-width=\"2910\" width=\"2910\" data-og-height=\"902\" height=\"902\" data-path=\"langsmith/images/online-eval-custom-code.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/online-eval-custom-code.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=acf6e6f3be5751c93a7287971fb18907 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/online-eval-custom-code.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=5df847b9d1f8171120853834d5d12f38 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/online-eval-custom-code.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=17d028d3e0709087a8be7e31343f0ab0 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/online-eval-custom-code.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=668fd653028ebe056fed1e3963d3dc8e 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/online-eval-custom-code.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=62465285a785c577d0b0537c5ad307ca 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/online-eval-custom-code.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=7c342ba749987066e8ae05129e6a4fb7 2500w\" />\n\nCustom code evaluators take in one argument:\n\n* A `Run` ([reference](/langsmith/run-data-format)). This represents the sampled run to evaluate.\n\nThey return a single value:\n\n* Feedback(s) Dictionary: A dictionary whose keys are the type of feedback you want to return, and values are the score you will give for that feedback key. For example, `{\"correctness\": 1, \"silliness\": 0}` would create two types of feedback on the run, one saying it is correct, and the other saying it is not silly.\n\nIn the below screenshot, you can see an example of a simple function that validates that each run in the experiment has a known json field:\n\n#### Test and save your evaluation function\n\nBefore saving, you can test your evaluator function on a recent run by clicking **Test Code** to make sure that your code executes properly.\n\nOnce you **Save**, your online evaluator will run over newly sampled runs (or backfilled ones too if you chose the backfill option).\n\nIf you prefer a video tutorial, check out the [Online Evaluations video](https://academy.langchain.com/pages/intro-to-langsmith-preview) from the Introduction to LangSmith Course.\n\n<iframe className=\"w-full aspect-video rounded-xl\" src=\"https://www.youtube.com/embed/z69cBXTJFZ0?si=GBKQ9_muHR1zllLl\" title=\"YouTube video player\" frameBorder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowFullScreen />\n\n## Configure multi-turn online evaluators\n\nMulti-turn online evaluators allow you to evaluate entire conversations between a human and an agent — not just individual exchanges. They measure end-to-end interaction quality across all turns in a thread.\n\nYou can use multi-turn evaluations to measure:\n\n1. Semantic Intent: What the user was trying to do.\n2. Semantic Outcome: What actually happened, did the task succeed.\n3. Trajectory: How the conversation unfolded, including trajectory of tool calls.\n\n<Note> Running multi-turn online evals will auto-upgrade each trace within a thread to [extended data retention](/langsmith/administration-overview#data-retention-auto-upgrades). This upgrade will impact trace pricing, but ensures that traces meeting your evaluation criteria (typically those most valuable for analysis) are preserved for investigation. </Note>\n\n* Your tracing project must be using [threads](/langsmith/threads).\n* The top-level inputs and outputs of each trace in a thread must have a `messages` key that contains a list of messages. We support messages in [LangChain](/langsmith/log-llm-trace#messages-format), [OpenAI Chat Completions](https://platform.openai.com/docs/api-reference/chat/create), and [Anthropic Messages](https://platform.claude.com/docs/en/api/messages) formats.\n  * If the top-level inputs and outputs of each trace only contain the latest message in the conversation, LangSmith will automatically combine messages across turns into a thread.\n  * If the top-level inputs and outputs of each trace contain the full conversation history, LangSmith will use that directly.\n\n<Note>\n  If your traces don't follow the format above, thread level evaluators won't work. You’ll need to update how you trace to LangSmith to ensure each trace’s top-level inputs and outputs contain a list of `messages`.\n\nPlease refer to the [troubleshooting](#troubleshooting) section for more information.\n</Note>\n\n1. Navigate to the **Tracing Projects** tab and select a tracing project.\n2. Click **+ New** in the top right corner of the tracing project page >  **New Evaluator** > **Evaluate a multi-turn thread**.\n3. **Name your evaluator**.\n4. **Apply filters or a sampling rate**. <br />\n   Use filters or sampling to control evaluator cost. For example, evaluate only threads under *N* turns or sample 10% of all threads.\n5. **Configure an idle time**. <br />\n   The first time you configure a thread level evaluator, you’ll define the idle time — the amount of time after the last trace in a thread before it’s considered complete and ready for evaluation. This value should reflect the expected length of user interactions in your app. It applies across all evaluators in the project.\n\n<Tip>\n  When first testing your evaluator, use a short idle time so you can see results quickly. Once validated, increase it to match the expected length of user interactions.\n</Tip>\n\n6. **Configure your model.**<br />\n   Select the provider and model you want to use for your evaluator. Threads tend to get long, so you should use a model with a higher context window in order to avoid running into limits. For example, OpenAI's GPT-4.1 mini or Gemini 2.5 Flash are good options as they both have 1M+ token context windows.\n\n7. **Configure your LLM-as-a-judge prompt.**<br />\n   Define what you want to evaluate. This prompt will be used to evaluate the thread. You can also configure which parts of the `messages` list are passed to the evaluator to control the content it receives:\n   * All messages: Send the full message list.\n   * Human and AI pairs: Send only user and assistant messages (excluding system messages, tool calls, etc.).\n   * First human and last AI: Send only the first user message and the last assistant reply.\n\n8. **Set up your feedback configuration**.<br />\n   Configure a name for the feedback key, the format for the feedback you want to collect and optionally enable reasoning on the feedback.\n\n<Warning>\n  We don't recommend using the same feedback key for a thread-level evaluator and a run-level evaluator as it can be hard to distinguish between the two.\n</Warning>\n\n8. **Save your evaluator.**\n\nAfter saving, your evaluator will appear in the **Evaluators** tab. You can test it once the idle time has passed for any new threads created after saving.\n\nThese are the current limits for multi-turn online evaluators (subject to change). Please reach out if you are running into any of these limits.\n\n* **Runs must be less than one week old**: When a thread becomes idle, only runs within the past 7 days are eligible for evaluation.\n* **Maximum of 500 threads evaluated at once**: If you have more than 500 threads marked as idle in a five minute period, we will automatically sample beyond 500.\n* **Maximum of 10 multi-turn online evaluators per workspace**\n\n**Checking the status of your evaluator** <br />\nYou can check when your evaluator was last run by heading to the **Evaluators** tab within a tracing project and clicking the **Logs** button for the evaluator you created to view its run history.\n\n**Inspect the data sent to the evaluator** <br />\nInspect the data sent to the evaluator by heading to the **Evaluators** tab within a tracing project, clicking on the evaluator you created and clicking the **Evaluator traces** tab.\n\nIn this tab, you can see the inputs passed into the LLM-as-a-judge evaluator. If your messages are not being passed in correctly, you will see blank values in the inputs. This can happen if your messages are not formatted in one of [the expected formats](/langsmith/online-evaluations#prerequisites).\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/online-evaluations.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "code_samples": [
    {
      "code": "numpy (v2.2.2): \"numpy\"\n  pandas (v1.5.2): \"pandas\"\n  jsonschema (v4.21.1): \"jsonschema\"\n  scipy (v1.14.1): \"scipy\"\n  sklearn (v1.26.4): \"scikit-learn\"",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": "View online evaluators",
      "id": "view-online-evaluators"
    },
    {
      "level": "h2",
      "text": "Configure online evaluators",
      "id": "configure-online-evaluators"
    },
    {
      "level": "h3",
      "text": "Configure a LLM-as-a-judge online evaluator",
      "id": "configure-a-llm-as-a-judge-online-evaluator"
    },
    {
      "level": "h3",
      "text": "Configure a custom code evaluator",
      "id": "configure-a-custom-code-evaluator"
    },
    {
      "level": "h3",
      "text": "Video guide",
      "id": "video-guide"
    },
    {
      "level": "h2",
      "text": "Configure multi-turn online evaluators",
      "id": "configure-multi-turn-online-evaluators"
    },
    {
      "level": "h3",
      "text": "Prerequisites",
      "id": "prerequisites"
    },
    {
      "level": "h3",
      "text": "Configuration",
      "id": "configuration"
    },
    {
      "level": "h3",
      "text": "Limits",
      "id": "limits"
    },
    {
      "level": "h3",
      "text": "Troubleshooting",
      "id": "troubleshooting"
    }
  ],
  "url": "llms-txt#set-up-online-evaluators",
  "links": []
}