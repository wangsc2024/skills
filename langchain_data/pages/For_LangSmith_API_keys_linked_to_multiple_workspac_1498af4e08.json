{
  "title": "For LangSmith API keys linked to multiple workspaces, set the LANGSMITH_WORKSPACE_ID environment variable to specify which workspace to use.",
  "content": "export LANGSMITH_WORKSPACE_ID=<your-workspace-id>\npython Python theme={null}\n  from typing import Literal\n  from langchain.messages import HumanMessage\n  from langchain_openai import ChatOpenAI\n  from langchain.tools import tool\n  from langgraph.prebuilt import ToolNode\n  from langgraph.graph import StateGraph, MessagesState\n\n@tool\n  def search(query: str):\n      \"\"\"Call to surf the web.\"\"\"\n      if \"sf\" in query.lower() or \"san francisco\" in query.lower():\n          return \"It's 60 degrees and foggy.\"\n      return \"It's 90 degrees and sunny.\"\n\ntools = [search]\n  tool_node = ToolNode(tools)\n\nmodel = ChatOpenAI(model=\"gpt-4o\", temperature=0).bind_tools(tools)\n\ndef should_continue(state: MessagesState) -> Literal[\"tools\", \"__end__\"]:\n      messages = state['messages']\n      last_message = messages[-1]\n      if last_message.tool_calls:\n          return \"tools\"\n      return \"__end__\"\n\ndef call_model(state: MessagesState):\n      messages = state['messages']\n      # Invoking `model` will automatically infer the correct tracing context\n      response = model.invoke(messages)\n      return {\"messages\": [response]}\n\nworkflow = StateGraph(MessagesState)\n  workflow.add_node(\"agent\", call_model)\n  workflow.add_node(\"tools\", tool_node)\n  workflow.add_edge(\"__start__\", \"agent\")\n  workflow.add_conditional_edges(\n      \"agent\",\n      should_continue,\n  )\n  workflow.add_edge(\"tools\", 'agent')\n\napp = workflow.compile()\n\nfinal_state = app.invoke(\n      {\"messages\": [HumanMessage(content=\"what is the weather in sf\")]},\n      config={\"configurable\": {\"thread_id\": 42}}\n  )\n\nfinal_state[\"messages\"][-1].content\n  typescript TypeScript theme={null}\n  import { HumanMessage, AIMessage } from \"@langchain/core/messages\";\n  import { tool } from \"@langchain/core/tools\";\n  import { z } from \"zod\";\n  import { ChatOpenAI } from \"@langchain/openai\";\n  import { StateGraph, StateGraphArgs } from \"@langchain/langgraph\";\n  import { ToolNode } from \"@langchain/langgraph/prebuilt\";\n\ninterface AgentState {\n    messages: HumanMessage[];\n  }\n\nconst graphState: StateGraphArgs<AgentState>[\"channels\"] = {\n    messages: {\n      reducer: (x: HumanMessage[], y: HumanMessage[]) => x.concat(y),\n    },\n  };\n\nconst searchTool = tool(async ({ query }: { query: string }) => {\n    if (query.toLowerCase().includes(\"sf\") || query.toLowerCase().includes(\"san francisco\")) {\n      return \"It's 60 degrees and foggy.\"\n    }\n    return \"It's 90 degrees and sunny.\"\n  }, {\n    name: \"search\",\n    description:\n      \"Call to surf the web.\",\n    schema: z.object({\n      query: z.string().describe(\"The query to use in your search.\"),\n    }),\n  });\n\nconst tools = [searchTool];\n  const toolNode = new ToolNode<AgentState>(tools);\n\nconst model = new ChatOpenAI({\n    model: \"gpt-4o\",\n    temperature: 0,\n  }).bindTools(tools);\n\nfunction shouldContinue(state: AgentState) {\n    const messages = state.messages;\n    const lastMessage = messages[messages.length - 1] as AIMessage;\n    if (lastMessage.tool_calls?.length) {\n      return \"tools\";\n    }\n    return \"__end__\";\n  }\n\nasync function callModel(state: AgentState) {\n    const messages = state.messages;\n    // Invoking `model` will automatically infer the correct tracing context\n    const response = await model.invoke(messages);\n    return { messages: [response] };\n  }\n\nconst workflow = new StateGraph<AgentState>({ channels: graphState })\n    .addNode(\"agent\", callModel)\n    .addNode(\"tools\", toolNode)\n    .addEdge(\"__start__\", \"agent\")\n    .addConditionalEdges(\"agent\", shouldContinue)\n    .addEdge(\"tools\", \"agent\");\n\nconst app = workflow.compile();\n\nconst finalState = await app.invoke(\n    { messages: [new HumanMessage(\"what is the weather in sf\")] },\n    { configurable: { thread_id: \"42\" } }\n  );\n\nfinalState.messages[finalState.messages.length - 1].content;\n  bash pip theme={null}\n  pip install openai langsmith langgraph\n  bash yarn theme={null}\n  yarn add openai langsmith @langchain/langgraph\n  bash npm theme={null}\n  npm install openai langsmith @langchain/langgraph\n  bash pnpm theme={null}\n  pnpm add openai langsmith @langchain/langgraph\n  bash wrap theme={null}\nexport LANGSMITH_TRACING=true\nexport LANGSMITH_API_KEY=<your-api-key>",
  "code_samples": [
    {
      "code": "<Info>\n  If you are using LangChain.js with LangSmith and are not in a serverless environment, we also recommend setting the following explicitly to reduce latency:\n\n  `export LANGCHAIN_CALLBACKS_BACKGROUND=true`\n\n  If you are in a serverless environment, we recommend setting the reverse to allow tracing to finish before your function ends:\n\n  `export LANGCHAIN_CALLBACKS_BACKGROUND=false`\n\n  See [this LangChain.js guide](https://js.langchain.com/docs/how_to/callbacks_serverless) for more information.\n</Info>\n\n### 3. Log a trace\n\nOnce you've set up your environment, you can call LangChain runnables as normal. LangSmith will infer the proper tracing config:\n\n<CodeGroup>",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</CodeGroup>\n\nAn example trace from running the above code [looks like this](https://smith.langchain.com/public/10863294-ee79-484a-927f-0558230f1547/r):\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-with-langchain-trace.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=a589f14351fb48e721205d1e363753ea\" alt=\"Trace tree for a LangGraph run with LangChain\" data-og-width=\"3314\" width=\"3314\" data-og-height=\"1766\" height=\"1766\" data-path=\"langsmith/images/langgraph-with-langchain-trace.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-with-langchain-trace.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=01650d2547ba6e440a66ceb0bdeb566a 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-with-langchain-trace.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=c0d7d3f04d58edd25e9b99d2a61890ce 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-with-langchain-trace.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=cf074976fbc12b0baad0e8320fd7fa47 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-with-langchain-trace.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=627f66d7b9e2b8a054e66e8dbc7afa73 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-with-langchain-trace.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=7946baa3ad54fa851cd8a34eb31b47b0 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-with-langchain-trace.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=d6b8e6a216b312f5ae3c74503fabba63 2500w\" />\n\n## Without LangChain\n\nIf you are using other SDKs or custom functions within LangGraph, you will need to [wrap or decorate them appropriately](/langsmith/annotate-code#use-traceable--traceable) (with the `@traceable` decorator in Python or the `traceable` function in JS, or something like e.g. `wrap_openai` for SDKs). If you do so, LangSmith will automatically nest traces from those wrapped methods.\n\nHere's an example. You can also see this page for more information.\n\n### 1. Installation\n\nInstall the LangGraph library and the OpenAI SDK for Python and JS (we use the OpenAI integration for the code snippets below).\n\n<CodeGroup>",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</CodeGroup>\n\n### 2. Configure your environment",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h3",
      "text": "3. Log a trace",
      "id": "3.-log-a-trace"
    },
    {
      "level": "h2",
      "text": "Without LangChain",
      "id": "without-langchain"
    },
    {
      "level": "h3",
      "text": "1. Installation",
      "id": "1.-installation"
    },
    {
      "level": "h3",
      "text": "2. Configure your environment",
      "id": "2.-configure-your-environment"
    }
  ],
  "url": "llms-txt#for-langsmith-api-keys-linked-to-multiple-workspaces,-set-the-langsmith_workspace_id-environment-variable-to-specify-which-workspace-to-use.",
  "links": []
}