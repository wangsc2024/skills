{
  "title": "Enable TTL and data retention",
  "content": "Source: https://docs.langchain.com/langsmith/self-host-ttl\n\nLangSmith Self-Hosted allows enablement of automatic TTL and Data Retention of traces. This can be useful if you're complying with data privacy regulations, or if you want to have more efficient space usage and auto cleanup of your traces. Traces will also have their data retention period automatically extended based on certain actions or run rule applications.\n\nYou can configure retention through helm or environment variable settings. There are a few options that are configurable:\n\n* *Enabled:* Whether data retention is enabled or disabled. If enabled, via the UI you can your default organization and project TTL tiers to apply to traces (see [data retention guide](/langsmith/administration-overview#data-retention) for details).\n* *Retention Periods:* You can configure system-wide retention periods for shortlived and longlived traces. Once configured, you can manage the retention level at each project as well as set an organization-wide default for new projects.\n\n## ClickHouse TTL Cleanup Job\n\nAs of version **0.11**, a cron job runs on weekends to assist in deleting expired data that may not have been cleaned up by ClickHouse's built-in TTL mechanism.\n\n<Warning>\n  This job uses potentially long running **mutations** (`ALTER TABLE DELETE`), which are expensive operations that can impact ClickHouse's performance. We recommend running these operations only during off-peak hours (nights and weekends). During testing with **1 concurrent active** mutation (default), we did not observe significant CPU, memory, or latency increases.\n</Warning>\n\nBy default, the cleanup job runs:\n\n* **Saturday**: 8pm and 10pm UTC\n* **Sunday**: 12am, 2am, and 4am UTC\n\n### Disabling the Job\n\nTo disable the cleanup job entirely:\n\n### Configuring the Schedule\n\nYou can customize when the cleanup job runs by modifying the cron expressions:\n\n<Tip>\n  To run the job on a single cron schedule, set both `CLICKHOUSE_TTL_CLEANUP_CRON_WEEKEND_EVENING` and `CLICKHOUSE_TTL_CLEANUP_CRON_WEEKEND_MORNING` to the same value. Job locking prevents overlapping executions.\n</Tip>\n\n### Configuring Minimum Expired Rows Per Part\n\nThe job goes table by table, scanning parts and deleting data from parts containing a minimum number of expired rows. This threshold balances efficiency and thoroughness:\n\n* **Too low**: Job scans entire parts to clear minimal data (inefficient)\n* **Too high**: Job misses parts with significant expired data\n\n#### Checking Expired Rows\n\nUse this query to analyze expired rows in your tables, and tweak your minimum value accordingly:\n\n### Configuring Maximum Active Mutations\n\nDelete operations can be time-consuming (\\~50 minutes for a 100GB part). You can increase concurrent mutations to speed up the process:\n\n<Warning>\n  Increasing concurrent DELETE operations can severely impact system performance. Monitor your system carefully and only increase this value if you can tolerate potentially slower insert and read latencies.\n</Warning>\n\n### Emergency: Stopping Running Mutations\n\nIf you experience latency spikes and need to terminate a running mutation:\n\n1. **Find active mutations**:\n\nLook for the `mutation_id` where the `command` column contains a `DELETE` statement.\n\n2. **Kill the mutation**:\n\n### Backups and Data Retention\n\nIf disk space does not decrease after running this job, or if it continues to increase, backups may be causing the issue by creating file system hard links. These links prevent ClickHouse from cleaning up the data.\n\nTo verify, check the following directories inside your ClickHouse pod:\n\n* `/var/lib/clickhouse/backup`\n* `/var/lib/clickhouse/shadow`\n\nIf backups are present, copy them to an external filesystem or blob storage (e.g., S3), then clear the directories. Within a few minutes, you will notice disk space releasing.\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-host-ttl.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "code_samples": [
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</CodeGroup>\n\n## ClickHouse TTL Cleanup Job\n\nAs of version **0.11**, a cron job runs on weekends to assist in deleting expired data that may not have been cleaned up by ClickHouse's built-in TTL mechanism.\n\n<Warning>\n  This job uses potentially long running **mutations** (`ALTER TABLE DELETE`), which are expensive operations that can impact ClickHouse's performance. We recommend running these operations only during off-peak hours (nights and weekends). During testing with **1 concurrent active** mutation (default), we did not observe significant CPU, memory, or latency increases.\n</Warning>\n\n### Default Schedule\n\nBy default, the cleanup job runs:\n\n* **Saturday**: 8pm and 10pm UTC\n* **Sunday**: 12am, 2am, and 4am UTC\n\n### Disabling the Job\n\nTo disable the cleanup job entirely:",
      "language": "unknown"
    },
    {
      "code": "### Configuring the Schedule\n\nYou can customize when the cleanup job runs by modifying the cron expressions:",
      "language": "unknown"
    },
    {
      "code": "<Tip>\n  To run the job on a single cron schedule, set both `CLICKHOUSE_TTL_CLEANUP_CRON_WEEKEND_EVENING` and `CLICKHOUSE_TTL_CLEANUP_CRON_WEEKEND_MORNING` to the same value. Job locking prevents overlapping executions.\n</Tip>\n\n### Configuring Minimum Expired Rows Per Part\n\nThe job goes table by table, scanning parts and deleting data from parts containing a minimum number of expired rows. This threshold balances efficiency and thoroughness:\n\n* **Too low**: Job scans entire parts to clear minimal data (inefficient)\n* **Too high**: Job misses parts with significant expired data",
      "language": "unknown"
    },
    {
      "code": "#### Checking Expired Rows\n\nUse this query to analyze expired rows in your tables, and tweak your minimum value accordingly:",
      "language": "unknown"
    },
    {
      "code": "### Configuring Maximum Active Mutations\n\nDelete operations can be time-consuming (\\~50 minutes for a 100GB part). You can increase concurrent mutations to speed up the process:",
      "language": "unknown"
    },
    {
      "code": "<Warning>\n  Increasing concurrent DELETE operations can severely impact system performance. Monitor your system carefully and only increase this value if you can tolerate potentially slower insert and read latencies.\n</Warning>\n\n### Emergency: Stopping Running Mutations\n\nIf you experience latency spikes and need to terminate a running mutation:\n\n1. **Find active mutations**:",
      "language": "unknown"
    },
    {
      "code": "Look for the `mutation_id` where the `command` column contains a `DELETE` statement.\n\n2. **Kill the mutation**:",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": "Requirements",
      "id": "requirements"
    },
    {
      "level": "h2",
      "text": "ClickHouse TTL Cleanup Job",
      "id": "clickhouse-ttl-cleanup-job"
    },
    {
      "level": "h3",
      "text": "Default Schedule",
      "id": "default-schedule"
    },
    {
      "level": "h3",
      "text": "Disabling the Job",
      "id": "disabling-the-job"
    },
    {
      "level": "h3",
      "text": "Configuring the Schedule",
      "id": "configuring-the-schedule"
    },
    {
      "level": "h3",
      "text": "Configuring Minimum Expired Rows Per Part",
      "id": "configuring-minimum-expired-rows-per-part"
    },
    {
      "level": "h3",
      "text": "Configuring Maximum Active Mutations",
      "id": "configuring-maximum-active-mutations"
    },
    {
      "level": "h3",
      "text": "Emergency: Stopping Running Mutations",
      "id": "emergency:-stopping-running-mutations"
    },
    {
      "level": "h3",
      "text": "Backups and Data Retention",
      "id": "backups-and-data-retention"
    }
  ],
  "url": "llms-txt#enable-ttl-and-data-retention",
  "links": []
}