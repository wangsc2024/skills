{
  "title": "async_client = wrap_anthropic(anthropic.AsyncAnthropic())",
  "content": "@traceable(run_type=\"tool\", name=\"Retrieve Context\")\ndef my_tool(question: str) -> str:\n    return \"During this morning's meeting, we solved all world conflict.\"\n\n@traceable(name=\"Chat Pipeline\")\ndef chat_pipeline(question: str):\n    context = my_tool(question)\n    messages = [\n        { \"role\": \"user\", \"content\": f\"Question: {question}\\nContext: {context}\"}\n    ]\n    messages = client.messages.create(\n      model=\"claude-sonnet-4-5-20250929\",\n      messages=messages,\n      max_tokens=1024,\n      system=\"You are a helpful assistant. Please respond to the user's request only based on the given context.\"\n    )\n    return messages\n\nchat_pipeline(\"Can you summarize this morning's meetings?\")\n```\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-anthropic.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "code_samples": [],
  "headings": [],
  "url": "llms-txt#async_client-=-wrap_anthropic(anthropic.asyncanthropic())",
  "links": []
}