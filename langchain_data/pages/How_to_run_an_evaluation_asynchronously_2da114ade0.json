{
  "title": "How to run an evaluation asynchronously",
  "content": "Source: https://docs.langchain.com/langsmith/evaluation-async\n\n<Info>\n  [Evaluations](/langsmith/evaluation-concepts#applying-evaluations) | [Evaluators](/langsmith/evaluation-concepts#evaluators) | [Datasets](/langsmith/evaluation-concepts#datasets) | [Experiments](/langsmith/evaluation-concepts#experiments)\n</Info>\n\nWe can run evaluations asynchronously via the SDK using [aevaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._arunner.aevaluate), which accepts all of the same arguments as [evaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate) but expects the application function to be asynchronous. You can learn more about how to use the `evaluate()` function [here](/langsmith/evaluate-llm-application).\n\n<Info>\n  This guide is only relevant when using the Python SDK. In JS/TS the `evaluate()` function is already async. You can see how to use it [here](/langsmith/evaluate-llm-application).\n</Info>\n\nRequires `langsmith>=0.3.13`\n\n```python  theme={null}\nfrom langsmith import wrappers, Client\nfrom openai import AsyncOpenAI",
  "code_samples": [],
  "headings": [
    {
      "level": "h2",
      "text": "Use `aevaluate()`",
      "id": "use-`aevaluate()`"
    }
  ],
  "url": "llms-txt#how-to-run-an-evaluation-asynchronously",
  "links": []
}