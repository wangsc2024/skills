{
  "title": "Augment the LLM with tools",
  "content": "tools = [add, multiply, divide]\ntools_by_name = {tool.name: tool for tool in tools}\nllm_with_tools = llm.bind_tools(tools)\npython Graph API theme={null}\n  from langgraph.graph import MessagesState\n  from langchain.messages import SystemMessage, HumanMessage, ToolMessage\n\n# Nodes\n  def llm_call(state: MessagesState):\n      \"\"\"LLM decides whether to call a tool or not\"\"\"\n\nreturn {\n          \"messages\": [\n              llm_with_tools.invoke(\n                  [\n                      SystemMessage(\n                          content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n                      )\n                  ]\n                  + state[\"messages\"]\n              )\n          ]\n      }\n\ndef tool_node(state: dict):\n      \"\"\"Performs the tool call\"\"\"\n\nresult = []\n      for tool_call in state[\"messages\"][-1].tool_calls:\n          tool = tools_by_name[tool_call[\"name\"]]\n          observation = tool.invoke(tool_call[\"args\"])\n          result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\n      return {\"messages\": result}\n\n# Conditional edge function to route to the tool node or end based upon whether the LLM made a tool call\n  def should_continue(state: MessagesState) -> Literal[\"tool_node\", END]:\n      \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\n\nmessages = state[\"messages\"]\n      last_message = messages[-1]\n\n# If the LLM makes a tool call, then perform an action\n      if last_message.tool_calls:\n          return \"tool_node\"\n\n# Otherwise, we stop (reply to the user)\n      return END\n\n# Build workflow\n  agent_builder = StateGraph(MessagesState)\n\n# Add nodes\n  agent_builder.add_node(\"llm_call\", llm_call)\n  agent_builder.add_node(\"tool_node\", tool_node)\n\n# Add edges to connect nodes\n  agent_builder.add_edge(START, \"llm_call\")\n  agent_builder.add_conditional_edges(\n      \"llm_call\",\n      should_continue,\n      [\"tool_node\", END]\n  )\n  agent_builder.add_edge(\"tool_node\", \"llm_call\")\n\n# Compile the agent\n  agent = agent_builder.compile()\n\n# Show the agent\n  display(Image(agent.get_graph(xray=True).draw_mermaid_png()))\n\n# Invoke\n  messages = [HumanMessage(content=\"Add 3 and 4.\")]\n  messages = agent.invoke({\"messages\": messages})\n  for m in messages[\"messages\"]:\n      m.pretty_print()\n  python Functional API theme={null}\n  from langgraph.graph import add_messages\n  from langchain.messages import (\n      SystemMessage,\n      HumanMessage,\n      ToolCall,\n  )\n  from langchain_core.messages import BaseMessage\n\n@task\n  def call_llm(messages: list[BaseMessage]):\n      \"\"\"LLM decides whether to call a tool or not\"\"\"\n      return llm_with_tools.invoke(\n          [\n              SystemMessage(\n                  content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n              )\n          ]\n          + messages\n      )\n\n@task\n  def call_tool(tool_call: ToolCall):\n      \"\"\"Performs the tool call\"\"\"\n      tool = tools_by_name[tool_call[\"name\"]]\n      return tool.invoke(tool_call)\n\n@entrypoint()\n  def agent(messages: list[BaseMessage]):\n      llm_response = call_llm(messages).result()\n\nwhile True:\n          if not llm_response.tool_calls:\n              break\n\n# Execute tools\n          tool_result_futures = [\n              call_tool(tool_call) for tool_call in llm_response.tool_calls\n          ]\n          tool_results = [fut.result() for fut in tool_result_futures]\n          messages = add_messages(messages, [llm_response, *tool_results])\n          llm_response = call_llm(messages).result()\n\nmessages = add_messages(messages, llm_response)\n      return messages\n\n# Invoke\n  messages = [HumanMessage(content=\"Add 3 and 4.\")]\n  for chunk in agent.stream(messages, stream_mode=\"updates\"):\n      print(chunk)\n      print(\"\\n\")\n  ```\n</CodeGroup>\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/workflows-agents.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "code_samples": [
    {
      "code": "<CodeGroup>",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    }
  ],
  "headings": [],
  "url": "llms-txt#augment-the-llm-with-tools",
  "links": []
}