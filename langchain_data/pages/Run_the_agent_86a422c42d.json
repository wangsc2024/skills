{
  "title": "Run the agent",
  "content": "agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n)\npython wrap theme={null}\n    SYSTEM_PROMPT = \"\"\"You are an expert weather forecaster, who speaks in puns.\n\nYou have access to two tools:\n\n- get_weather_for_location: use this to get the weather for a specific location\n    - get_user_location: use this to get the user's location\n\nIf a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean wherever they are, use the get_user_location tool to find their location.\"\"\"\n    python  theme={null}\n    from dataclasses import dataclass\n    from langchain.tools import tool, ToolRuntime\n\n@tool\n    def get_weather_for_location(city: str) -> str:\n        \"\"\"Get weather for a given city.\"\"\"\n        return f\"It's always sunny in {city}!\"\n\n@dataclass\n    class Context:\n        \"\"\"Custom runtime context schema.\"\"\"\n        user_id: str\n\n@tool\n    def get_user_location(runtime: ToolRuntime[Context]) -> str:\n        \"\"\"Retrieve user information based on user ID.\"\"\"\n        user_id = runtime.context.user_id\n        return \"Florida\" if user_id == \"1\" else \"SF\"\n    python  theme={null}\n    from langchain.chat_models import init_chat_model\n\nmodel = init_chat_model(\n        \"claude-sonnet-4-5-20250929\",\n        temperature=0.5,\n        timeout=10,\n        max_tokens=1000\n    )\n    python  theme={null}\n    from dataclasses import dataclass\n\n# We use a dataclass here, but Pydantic models are also supported.\n    @dataclass\n    class ResponseFormat:\n        \"\"\"Response schema for the agent.\"\"\"\n        # A punny response (always required)\n        punny_response: str\n        # Any interesting information about the weather if available\n        weather_conditions: str | None = None\n    python  theme={null}\n    from langgraph.checkpoint.memory import InMemorySaver\n\ncheckpointer = InMemorySaver()\n    python  theme={null}\n    from langchain.agents.structured_output import ToolStrategy\n\nagent = create_agent(\n        model=model,\n        system_prompt=SYSTEM_PROMPT,\n        tools=[get_user_location, get_weather_for_location],\n        context_schema=Context,\n        response_format=ToolStrategy(ResponseFormat),\n        checkpointer=checkpointer\n    )\n\n# `thread_id` is a unique identifier for a given conversation.\n    config = {\"configurable\": {\"thread_id\": \"1\"}}\n\nresponse = agent.invoke(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather outside?\"}]},\n        config=config,\n        context=Context(user_id=\"1\")\n    )\n\nprint(response['structured_response'])\n    # ResponseFormat(\n    #     punny_response=\"Florida is still having a 'sun-derful' day! The sunshine is playing 'ray-dio' hits all day long! I'd say it's the perfect weather for some 'solar-bration'! If you were hoping for rain, I'm afraid that idea is all 'washed up' - the forecast remains 'clear-ly' brilliant!\",\n    #     weather_conditions=\"It's always sunny in Florida!\"\n    # )\n\n# Note that we can continue the conversation using the same `thread_id`.\n    response = agent.invoke(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"thank you!\"}]},\n        config=config,\n        context=Context(user_id=\"1\")\n    )\n\nprint(response['structured_response'])\n    # ResponseFormat(\n    #     punny_response=\"You're 'thund-erfully' welcome! It's always a 'breeze' to help you stay 'current' with the weather. I'm just 'cloud'-ing around waiting to 'shower' you with more forecasts whenever you need them. Have a 'sun-sational' day in the Florida sunshine!\",\n    #     weather_conditions=None\n    # )\n    python  theme={null}\n  from dataclasses import dataclass\n\nfrom langchain.agents import create_agent\n  from langchain.chat_models import init_chat_model\n  from langchain.tools import tool, ToolRuntime\n  from langgraph.checkpoint.memory import InMemorySaver\n  from langchain.agents.structured_output import ToolStrategy\n\n# Define system prompt\n  SYSTEM_PROMPT = \"\"\"You are an expert weather forecaster, who speaks in puns.\n\nYou have access to two tools:\n\n- get_weather_for_location: use this to get the weather for a specific location\n  - get_user_location: use this to get the user's location\n\nIf a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean wherever they are, use the get_user_location tool to find their location.\"\"\"\n\n# Define context schema\n  @dataclass\n  class Context:\n      \"\"\"Custom runtime context schema.\"\"\"\n      user_id: str\n\n# Define tools\n  @tool\n  def get_weather_for_location(city: str) -> str:\n      \"\"\"Get weather for a given city.\"\"\"\n      return f\"It's always sunny in {city}!\"\n\n@tool\n  def get_user_location(runtime: ToolRuntime[Context]) -> str:\n      \"\"\"Retrieve user information based on user ID.\"\"\"\n      user_id = runtime.context.user_id\n      return \"Florida\" if user_id == \"1\" else \"SF\"\n\n# Configure model\n  model = init_chat_model(\n      \"claude-sonnet-4-5-20250929\",\n      temperature=0\n  )\n\n# Define response format\n  @dataclass\n  class ResponseFormat:\n      \"\"\"Response schema for the agent.\"\"\"\n      # A punny response (always required)\n      punny_response: str\n      # Any interesting information about the weather if available\n      weather_conditions: str | None = None\n\n# Set up memory\n  checkpointer = InMemorySaver()\n\n# Create agent\n  agent = create_agent(\n      model=model,\n      system_prompt=SYSTEM_PROMPT,\n      tools=[get_user_location, get_weather_for_location],\n      context_schema=Context,\n      response_format=ToolStrategy(ResponseFormat),\n      checkpointer=checkpointer\n  )\n\n# Run agent\n  # `thread_id` is a unique identifier for a given conversation.\n  config = {\"configurable\": {\"thread_id\": \"1\"}}\n\nresponse = agent.invoke(\n      {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather outside?\"}]},\n      config=config,\n      context=Context(user_id=\"1\")\n  )\n\nprint(response['structured_response'])\n  # ResponseFormat(\n  #     punny_response=\"Florida is still having a 'sun-derful' day! The sunshine is playing 'ray-dio' hits all day long! I'd say it's the perfect weather for some 'solar-bration'! If you were hoping for rain, I'm afraid that idea is all 'washed up' - the forecast remains 'clear-ly' brilliant!\",\n  #     weather_conditions=\"It's always sunny in Florida!\"\n  # )\n\n# Note that we can continue the conversation using the same `thread_id`.\n  response = agent.invoke(\n      {\"messages\": [{\"role\": \"user\", \"content\": \"thank you!\"}]},\n      config=config,\n      context=Context(user_id=\"1\")\n  )\n\nprint(response['structured_response'])\n  # ResponseFormat(\n  #     punny_response=\"You're 'thund-erfully' welcome! It's always a 'breeze' to help you stay 'current' with the weather. I'm just 'cloud'-ing around waiting to 'shower' you with more forecasts whenever you need them. Have a 'sun-sational' day in the Florida sunshine!\",\n  #     weather_conditions=None\n  # )\n  ```\n</Expandable>\n\n<Tip>\n  To learn how to trace your agent with LangSmith, see the [LangSmith documentation](/langsmith/trace-with-langchain).\n</Tip>\n\nCongratulations! You now have an AI agent that can:\n\n* **Understand context** and remember conversations\n* **Use multiple tools** intelligently\n* **Provide structured responses** in a consistent format\n* **Handle user-specific information** through context\n* **Maintain conversation state** across interactions\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/quickstart.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "code_samples": [
    {
      "code": "<Tip>\n  To learn how to trace your agent with LangSmith, see the [LangSmith documentation](/langsmith/trace-with-langchain).\n</Tip>\n\n## Build a real-world agent\n\nNext, build a practical weather forecasting agent that demonstrates key production concepts:\n\n1. **Detailed system prompts** for better agent behavior\n2. **Create tools** that integrate with external data\n3. **Model configuration** for consistent responses\n4. **Structured output** for predictable results\n5. **Conversational memory** for chat-like interactions\n6. **Create and run the agent** create a fully functional agent\n\nLet's walk through each step:\n\n<Steps>\n  <Step title=\"Define the system prompt\">\n    The system prompt defines your agentâ€™s role and behavior. Keep it specific and actionable:",
      "language": "unknown"
    },
    {
      "code": "</Step>\n\n  <Step title=\"Create tools\">\n    [Tools](/oss/python/langchain/tools) let a model interact with external systems by calling functions you define.\n    Tools can depend on [runtime context](/oss/python/langchain/runtime) and also interact with [agent memory](/oss/python/langchain/short-term-memory).\n\n    Notice below how the `get_user_location` tool uses runtime context:",
      "language": "unknown"
    },
    {
      "code": "<Tip>\n      Tools should be well-documented: their name, description, and argument names become part of the model's prompt.\n      LangChain's [`@tool` decorator](https://reference.langchain.com/python/langchain/tools/#langchain.tools.tool) adds metadata and enables runtime injection via the `ToolRuntime` parameter.\n    </Tip>\n  </Step>\n\n  <Step title=\"Configure your model\">\n    Set up your [language model](/oss/python/langchain/models) with the right parameters for your use case:",
      "language": "unknown"
    },
    {
      "code": "Depending on the model and provider chosen, initialization parameters may vary; refer to their reference pages for details.\n  </Step>\n\n  <Step title=\"Define response format\">\n    Optionally, define a structured response format if you need the agent responses to match\n    a specific schema.",
      "language": "unknown"
    },
    {
      "code": "</Step>\n\n  <Step title=\"Add memory\">\n    Add [memory](/oss/python/langchain/short-term-memory) to your agent to maintain state across interactions. This allows\n    the agent to remember previous conversations and context.",
      "language": "unknown"
    },
    {
      "code": "<Info>\n      In production, use a persistent checkpointer that saves to a database.\n      See [Add and manage memory](/oss/python/langgraph/add-memory#manage-short-term-memory) for more details.\n    </Info>\n  </Step>\n\n  <Step title=\"Create and run the agent\">\n    Now assemble your agent with all the components and run it!",
      "language": "unknown"
    },
    {
      "code": "</Step>\n</Steps>\n\n<Expandable title=\"Full example code\">",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": "Build a real-world agent",
      "id": "build-a-real-world-agent"
    }
  ],
  "url": "llms-txt#run-the-agent",
  "links": []
}