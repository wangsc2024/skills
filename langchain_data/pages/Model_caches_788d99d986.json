{
  "title": "Model caches",
  "content": "Source: https://docs.langchain.com/oss/javascript/integrations/llm_caching/index\n\n[Caching LLM calls](/oss/javascript/langchain/models#caching) can be useful for testing, cost savings, and speed.\n\nBelow are some integrations that allow you to cache results of individual LLM calls using different caches with different strategies.\n\n<Columns cols={3}>\n  <Card title=\"Azure Cosmos DB NoSQL Semantic Cache\" icon=\"link\" href=\"/oss/javascript/integrations/llm_caching/azure_cosmosdb_nosql\" arrow=\"true\" cta=\"View guide\" />\n</Columns>\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/integrations/llm_caching/index.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "code_samples": [],
  "headings": [],
  "url": "llms-txt#model-caches",
  "links": []
}