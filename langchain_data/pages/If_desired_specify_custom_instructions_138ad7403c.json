{
  "title": "If desired, specify custom instructions",
  "content": "prompt = (\n    \"You have access to a tool that retrieves context from a blog post. \"\n    \"Use the tool to help answer user queries.\"\n)\nagent = create_agent(model, tools, system_prompt=prompt)\npython  theme={null}\nquery = (\n    \"What is the standard method for Task Decomposition?\\n\\n\"\n    \"Once you get the answer, look up common extensions of that method.\"\n)\n\nfor event in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n    stream_mode=\"values\",\n):\n    event[\"messages\"][-1].pretty_print()\n\n================================ Human Message =================================\n\nWhat is the standard method for Task Decomposition?\n\nOnce you get the answer, look up common extensions of that method.\n================================== Ai Message ==================================\nTool Calls:\n  retrieve_context (call_d6AVxICMPQYwAKj9lgH4E337)\n Call ID: call_d6AVxICMPQYwAKj9lgH4E337\n  Args:\n    query: standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\n\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\n\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nTool Calls:\n  retrieve_context (call_0dbMOw7266jvETbXWn4JqWpR)\n Call ID: call_0dbMOw7266jvETbXWn4JqWpR\n  Args:\n    query: common extensions of the standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\n\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\n\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\n\nThe standard method for Task Decomposition often used is the Chain of Thought (CoT)...\npython  theme={null}\nfrom langchain.agents.middleware import dynamic_prompt, ModelRequest\n\n@dynamic_prompt\ndef prompt_with_context(request: ModelRequest) -> str:\n    \"\"\"Inject context into state messages.\"\"\"\n    last_query = request.state[\"messages\"][-1].text\n    retrieved_docs = vector_store.similarity_search(last_query)\n\ndocs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n\nsystem_message = (\n        \"You are a helpful assistant. Use the following context in your response:\"\n        f\"\\n\\n{docs_content}\"\n    )\n\nreturn system_message\n\nagent = create_agent(model, tools=[], middleware=[prompt_with_context])\npython  theme={null}\nquery = \"What is task decomposition?\"\nfor step in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n    stream_mode=\"values\",\n):\n    step[\"messages\"][-1].pretty_print()\n\n================================ Human Message =================================\n\nWhat is task decomposition?\n================================== Ai Message ==================================\n\nTask decomposition is...\npython  theme={null}\n  from typing import Any\n  from langchain_core.documents import Document\n  from langchain.agents.middleware import AgentMiddleware, AgentState\n\nclass State(AgentState):\n      context: list[Document]\n\nclass RetrieveDocumentsMiddleware(AgentMiddleware[State]):\n      state_schema = State\n\ndef before_model(self, state: AgentState) -> dict[str, Any] | None:\n          last_message = state[\"messages\"][-1]\n          retrieved_docs = vector_store.similarity_search(last_message.text)\n\ndocs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n\naugmented_message_content = (\n              f\"{last_message.text}\\n\\n\"\n              \"Use the following context to answer the query:\\n\"\n              f\"{docs_content}\"\n          )\n          return {\n              \"messages\": [last_message.model_copy(update={\"content\": augmented_message_content})],\n              \"context\": retrieved_docs,\n          }\n\nagent = create_agent(\n      model,\n      tools=[],\n      middleware=[RetrieveDocumentsMiddleware()],\n  )\n  ```\n</Accordion>\n\nNow that we've implemented a simple RAG application via [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent), we can easily incorporate new features and go deeper:\n\n* [Stream](/oss/python/langchain/streaming) tokens and other information for responsive user experiences\n* Add [conversational memory](/oss/python/langchain/short-term-memory) to support multi-turn interactions\n* Add [long-term memory](/oss/python/langchain/long-term-memory) to support memory across conversational threads\n* Add [structured responses](/oss/python/langchain/structured-output)\n* Deploy your application with [LangSmith Deployment](/langsmith/deployments)\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/rag.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "code_samples": [
    {
      "code": "Let's test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "Note that the agent:\n\n1. Generates a query to search for a standard method for task decomposition;\n2. Receiving the answer, generates a second query to search for common extensions of it;\n3. Having received all necessary context, answers the question.\n\nWe can see the full sequence of steps, along with latency and other metadata, in the [LangSmith trace](https://smith.langchain.com/public/7b42d478-33d2-4631-90a4-7cb731681e88/r).\n\n<Tip>\n  You can add a deeper level of control and customization using the [LangGraph](/oss/python/langgraph/overview) framework directly— for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraph's [Agentic RAG tutorial](/oss/python/langgraph/agentic-rag) for more advanced formulations.\n</Tip>\n\n### RAG chains\n\nIn the above [agentic RAG](#rag-agents) formulation we allow the LLM to use its discretion in generating a [tool call](/oss/python/langchain/models#tool-calling) to help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:\n\n| ✅ Benefits                                                                                                                                                 | ⚠️ Drawbacks                                                                                                                                |\n| ---------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- |\n| **Search only when needed** – The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches.                        | **Two inference calls** – When a search is performed, it requires one call to generate the query and another to produce the final response. |\n| **Contextual search queries** – By treating search as a tool with a `query` input, the LLM crafts its own queries that incorporate conversational context. | **Reduced control** – The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary.                    |\n| **Multiple searches allowed** – The LLM can execute several searches in support of a single user query.                                                    |                                                                                                                                             |\n\nAnother common approach is a two-step chain, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query. This results in a single inference call per query, buying reduced latency at the expense of flexibility.\n\nIn this approach we no longer call the model in a loop, but instead make a single pass.\n\nWe can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:",
      "language": "unknown"
    },
    {
      "code": "Let's try this out:",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "In the [LangSmith trace](https://smith.langchain.com/public/0322904b-bc4c-4433-a568-54c6b31bbef4/r/9ef1c23e-380e-46bf-94b3-d8bb33df440c) we can see the retrieved context incorporated into the model prompt.\n\nThis is a fast and effective method for simple queries in constrained settings, when we typically do want to run user queries through semantic search to pull additional context.\n\n<Accordion title=\"Returning source documents\">\n  The above [RAG chain](#rag-chains) incorporates retrieved context into a single system message for that run.\n\n  As in the [agentic RAG](#rag-agents) formulation, we sometimes want to include raw source documents in the application state to have access to document metadata. We can do this for the two-step chain case by:\n\n  1. Adding a key to the state to store the retrieved documents\n  2. Adding a new node via a [pre-model hook](/oss/python/langchain/agents#pre-model-hook) to populate that key (as well as inject the context).",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h3",
      "text": "RAG chains",
      "id": "rag-chains"
    },
    {
      "level": "h2",
      "text": "Next steps",
      "id": "next-steps"
    }
  ],
  "url": "llms-txt#if-desired,-specify-custom-instructions",
  "links": []
}