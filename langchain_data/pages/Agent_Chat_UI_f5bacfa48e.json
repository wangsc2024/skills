{
  "title": "Agent Chat UI",
  "content": "Source: https://docs.langchain.com/oss/python/langgraph/ui\n\n[Agent Chat UI](https://github.com/langchain-ai/agent-chat-ui) is a Next.js application that provides a conversational interface for interacting with any LangChain agent. It supports real-time chat, tool visualization, and advanced features like time-travel debugging and state forking. Agent Chat UI works seamlessly with agents created using [`create_agent`](../langchain/agents) and provides interactive experiences for your agents with minimal setup, whether you're running locally or in a deployed context (such as [LangSmith](/langsmith/home)).\n\nAgent Chat UI is open source and can be adapted to your application needs.\n\n<Frame>\n  <iframe className=\"w-full aspect-video rounded-xl\" src=\"https://www.youtube.com/embed/lInrwVnZ83o?si=Uw66mPtCERJm0EjU\" title=\"Agent Chat UI\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowFullScreen />\n</Frame>\n\n<Tip>\n  You can use generative UI in the Agent Chat UI. For more information, see [Implement generative user interfaces with LangGraph](/langsmith/generative-ui-react).\n</Tip>\n\nThe fastest way to get started is using the hosted version:\n\n1. **Visit [Agent Chat UI](https://agentchat.vercel.app)**\n2. **Connect your agent** by entering your deployment URL or local server address\n3. **Start chatting** - the UI will automatically detect and render tool calls and interrupts\n\n### Local development\n\nFor customization or local development, you can run Agent Chat UI locally:\n\n### Connect to your agent\n\nAgent Chat UI can connect to both [local](/oss/python/langgraph/studio#setup-local-agent-server) and [deployed agents](/oss/python/langgraph/deploy).\n\nAfter starting Agent Chat UI, you'll need to configure it to connect to your agent:\n\n1. **Graph ID**: Enter your graph name (find this under `graphs` in your `langgraph.json` file)\n2. **Deployment URL**: Your Agent server's endpoint (e.g., `http://localhost:2024` for local development, or your deployed agent's URL)\n3. **LangSmith API key (optional)**: Add your LangSmith API key (not required if you're using a local Agent server)\n\nOnce configured, Agent Chat UI will automatically fetch and display any interrupted threads from your agent.\n\n<Tip>\n  Agent Chat UI has out-of-the-box support for rendering tool calls and tool result messages. To customize what messages are shown, see [Hiding Messages in the Chat](https://github.com/langchain-ai/agent-chat-ui?tab=readme-ov-file#hiding-messages-in-the-chat).\n</Tip>\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/ui.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "code_samples": [
    {
      "code": "",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h3",
      "text": "Quick start",
      "id": "quick-start"
    },
    {
      "level": "h3",
      "text": "Local development",
      "id": "local-development"
    },
    {
      "level": "h3",
      "text": "Connect to your agent",
      "id": "connect-to-your-agent"
    }
  ],
  "url": "llms-txt#agent-chat-ui",
  "links": []
}