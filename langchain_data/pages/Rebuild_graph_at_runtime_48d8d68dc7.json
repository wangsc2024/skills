{
  "title": "Rebuild graph at runtime",
  "content": "Source: https://docs.langchain.com/langsmith/graph-rebuild\n\nYou might need to rebuild your graph with a different configuration for a new run. For example, you might need to use a different graph state or graph structure depending on the config. This guide shows how you can do this.\n\n<Note>\n  **Note**\n  In most cases, customizing behavior based on the config should be handled by a single graph where each node can read a config and change its behavior based on it\n</Note>\n\nMake sure to check out [this how-to guide](/langsmith/setup-app-requirements-txt) on setting up your app for deployment first.\n\nLet's say you have an app with a simple graph that calls an LLM and returns the response to the user. The app file directory looks like the following:\n\nwhere the graph is defined in `openai_agent.py`.\n\nIn the standard LangGraph API configuration, the server uses the compiled graph instance that's defined at the top level of `openai_agent.py`, which looks like the following:\n\nTo make the server aware of your graph, you need to specify a path to the variable that contains the [`CompiledStateGraph`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph) instance in your LangGraph API configuration (`langgraph.json`), e.g.:\n\nTo make your graph rebuild on each new run with custom configuration, you need to rewrite `openai_agent.py` to instead provide a *function* that takes a config and returns a graph (or compiled graph) instance. Let's say we want to return our existing graph for user ID '1', and a tool-calling agent for other users. We can modify `openai_agent.py` as follows:\n\n```python  theme={null}\nfrom typing import Annotated\nfrom typing_extensions import TypedDict\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.graph import END, START, MessageGraph\nfrom langgraph.graph.state import StateGraph\nfrom langgraph.graph.message import add_messages\nfrom langchain.tools import tool\nfrom langgraph.prebuilt import ToolNode\nfrom langchain_core.messages import BaseMessage\nfrom langchain_core.runnables import RunnableConfig\n\nclass State(TypedDict):\n    messages: Annotated[list[BaseMessage], add_messages]\n\nmodel = ChatOpenAI(temperature=0)\n\ndef make_default_graph():\n    \"\"\"Make a simple LLM agent\"\"\"\n    graph_workflow = StateGraph(State)\n    def call_model(state):\n        return {\"messages\": [model.invoke(state[\"messages\"])]}\n\ngraph_workflow.add_node(\"agent\", call_model)\n    graph_workflow.add_edge(\"agent\", END)\n    graph_workflow.add_edge(START, \"agent\")\n\nagent = graph_workflow.compile()\n    return agent\n\ndef make_alternative_graph():\n    \"\"\"Make a tool-calling agent\"\"\"\n\n@tool\n    def add(a: float, b: float):\n        \"\"\"Adds two numbers.\"\"\"\n        return a + b\n\ntool_node = ToolNode([add])\n    model_with_tools = model.bind_tools([add])\n    def call_model(state):\n        return {\"messages\": [model_with_tools.invoke(state[\"messages\"])]}\n\ndef should_continue(state: State):\n        if state[\"messages\"][-1].tool_calls:\n            return \"tools\"\n        else:\n            return END\n\ngraph_workflow = StateGraph(State)\n\ngraph_workflow.add_node(\"agent\", call_model)\n    graph_workflow.add_node(\"tools\", tool_node)\n    graph_workflow.add_edge(\"tools\", \"agent\")\n    graph_workflow.add_edge(START, \"agent\")\n    graph_workflow.add_conditional_edges(\"agent\", should_continue)\n\nagent = graph_workflow.compile()\n    return agent",
  "code_samples": [
    {
      "code": "my-app/\n|-- requirements.txt\n|-- .env\n|-- openai_agent.py     # code for your graph",
      "language": "unknown"
    },
    {
      "code": "To make the server aware of your graph, you need to specify a path to the variable that contains the [`CompiledStateGraph`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph) instance in your LangGraph API configuration (`langgraph.json`), e.g.:",
      "language": "unknown"
    },
    {
      "code": "### Rebuild\n\nTo make your graph rebuild on each new run with custom configuration, you need to rewrite `openai_agent.py` to instead provide a *function* that takes a config and returns a graph (or compiled graph) instance. Let's say we want to return our existing graph for user ID '1', and a tool-calling agent for other users. We can modify `openai_agent.py` as follows:",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": "Prerequisites",
      "id": "prerequisites"
    },
    {
      "level": "h2",
      "text": "Define graphs",
      "id": "define-graphs"
    },
    {
      "level": "h3",
      "text": "No rebuild",
      "id": "no-rebuild"
    },
    {
      "level": "h3",
      "text": "Rebuild",
      "id": "rebuild"
    }
  ],
  "url": "llms-txt#rebuild-graph-at-runtime",
  "links": []
}