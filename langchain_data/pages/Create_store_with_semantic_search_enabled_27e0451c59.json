{
  "title": "Create store with semantic search enabled",
  "content": "embeddings = init_embeddings(\"openai:text-embedding-3-small\")\nstore = InMemoryStore(\n    index={\n        \"embed\": embeddings,\n        \"dims\": 1536,\n    }\n)\n\nstore.put((\"user_123\", \"memories\"), \"1\", {\"text\": \"I love pizza\"})\nstore.put((\"user_123\", \"memories\"), \"2\", {\"text\": \"I am a plumber\"})\n\nitems = store.search(\n    (\"user_123\", \"memories\"), query=\"I'm hungry\", limit=1\n)\npython  theme={null}\n\nfrom langchain.embeddings import init_embeddings\n  from langchain.chat_models import init_chat_model\n  from langgraph.store.base import BaseStore\n  from langgraph.store.memory import InMemoryStore\n  from langgraph.graph import START, MessagesState, StateGraph\n\nmodel = init_chat_model(\"gpt-4o-mini\")\n\n# Create store with semantic search enabled\n  embeddings = init_embeddings(\"openai:text-embedding-3-small\")\n  store = InMemoryStore(\n      index={\n          \"embed\": embeddings,\n          \"dims\": 1536,\n      }\n  )\n\nstore.put((\"user_123\", \"memories\"), \"1\", {\"text\": \"I love pizza\"})\n  store.put((\"user_123\", \"memories\"), \"2\", {\"text\": \"I am a plumber\"})\n\ndef chat(state, *, store: BaseStore):\n      # Search based on user's last message\n      items = store.search(\n          (\"user_123\", \"memories\"), query=state[\"messages\"][-1].content, limit=2\n      )\n      memories = \"\\n\".join(item.value[\"text\"] for item in items)\n      memories = f\"## Memories of user\\n{memories}\" if memories else \"\"\n      response = model.invoke(\n          [\n              {\"role\": \"system\", \"content\": f\"You are a helpful assistant.\\n{memories}\"},\n              *state[\"messages\"],\n          ]\n      )\n      return {\"messages\": [response]}\n\nbuilder = StateGraph(MessagesState)\n  builder.add_node(chat)\n  builder.add_edge(START, \"chat\")\n  graph = builder.compile(store=store)\n\nfor message, metadata in graph.stream(\n      input={\"messages\": [{\"role\": \"user\", \"content\": \"I'm hungry\"}]},\n      stream_mode=\"messages\",\n  ):\n      print(message.content, end=\"\")\n  python  theme={null}\nfrom langchain_core.messages.utils import (  # [!code highlight]\n    trim_messages,  # [!code highlight]\n    count_tokens_approximately  # [!code highlight]\n)  # [!code highlight]\n\ndef call_model(state: MessagesState):\n    messages = trim_messages(  # [!code highlight]\n        state[\"messages\"],\n        strategy=\"last\",\n        token_counter=count_tokens_approximately,\n        max_tokens=128,\n        start_on=\"human\",\n        end_on=(\"human\", \"tool\"),\n    )\n    response = model.invoke(messages)\n    return {\"messages\": [response]}\n\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(call_model)\n...\npython  theme={null}\n  from langchain_core.messages.utils import (\n      trim_messages,  # [!code highlight]\n      count_tokens_approximately  # [!code highlight]\n  )\n  from langchain.chat_models import init_chat_model\n  from langgraph.graph import StateGraph, START, MessagesState\n\nmodel = init_chat_model(\"claude-sonnet-4-5-20250929\")\n  summarization_model = model.bind(max_tokens=128)\n\ndef call_model(state: MessagesState):\n      messages = trim_messages(  # [!code highlight]\n          state[\"messages\"],\n          strategy=\"last\",\n          token_counter=count_tokens_approximately,\n          max_tokens=128,\n          start_on=\"human\",\n          end_on=(\"human\", \"tool\"),\n      )\n      response = model.invoke(messages)\n      return {\"messages\": [response]}\n\ncheckpointer = InMemorySaver()\n  builder = StateGraph(MessagesState)\n  builder.add_node(call_model)\n  builder.add_edge(START, \"call_model\")\n  graph = builder.compile(checkpointer=checkpointer)\n\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\n  graph.invoke({\"messages\": \"hi, my name is bob\"}, config)\n  graph.invoke({\"messages\": \"write a short poem about cats\"}, config)\n  graph.invoke({\"messages\": \"now do the same but for dogs\"}, config)\n  final_response = graph.invoke({\"messages\": \"what's my name?\"}, config)\n\nfinal_response[\"messages\"][-1].pretty_print()\n  \n  ================================== Ai Message ==================================\n\nYour name is Bob, as you mentioned when you first introduced yourself.\n  python  theme={null}\nfrom langchain.messages import RemoveMessage  # [!code highlight]\n\ndef delete_messages(state):\n    messages = state[\"messages\"]\n    if len(messages) > 2:\n        # remove the earliest two messages\n        return {\"messages\": [RemoveMessage(id=m.id) for m in messages[:2]]}  # [!code highlight]\npython  theme={null}\nfrom langgraph.graph.message import REMOVE_ALL_MESSAGES  # [!code highlight]\n\ndef delete_messages(state):\n    return {\"messages\": [RemoveMessage(id=REMOVE_ALL_MESSAGES)]}  # [!code highlight]\npython  theme={null}\n  from langchain.messages import RemoveMessage  # [!code highlight]\n\ndef delete_messages(state):\n      messages = state[\"messages\"]\n      if len(messages) > 2:\n          # remove the earliest two messages\n          return {\"messages\": [RemoveMessage(id=m.id) for m in messages[:2]]}  # [!code highlight]\n\ndef call_model(state: MessagesState):\n      response = model.invoke(state[\"messages\"])\n      return {\"messages\": response}\n\nbuilder = StateGraph(MessagesState)\n  builder.add_sequence([call_model, delete_messages])\n  builder.add_edge(START, \"call_model\")\n\ncheckpointer = InMemorySaver()\n  app = builder.compile(checkpointer=checkpointer)\n\nfor event in app.stream(\n      {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n      config,\n      stream_mode=\"values\"\n  ):\n      print([(message.type, message.content) for message in event[\"messages\"]])\n\nfor event in app.stream(\n      {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n      config,\n      stream_mode=\"values\"\n  ):\n      print([(message.type, message.content) for message in event[\"messages\"]])\n  \n  [('human', \"hi! I'm bob\")]\n  [('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?')]\n  [('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?'), ('human', \"what's my name?\")]\n  [('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?'), ('human', \"what's my name?\"), ('ai', 'Your name is Bob.')]\n  [('human', \"what's my name?\"), ('ai', 'Your name is Bob.')]\n  python  theme={null}\nfrom langgraph.graph import MessagesState\nclass State(MessagesState):\n    summary: str\npython  theme={null}\ndef summarize_conversation(state: State):\n\n# First, we get any existing summary\n    summary = state.get(\"summary\", \"\")\n\n# Create our summarization prompt\n    if summary:\n\n# A summary already exists\n        summary_message = (\n            f\"This is a summary of the conversation to date: {summary}\\n\\n\"\n            \"Extend the summary by taking into account the new messages above:\"\n        )\n\nelse:\n        summary_message = \"Create a summary of the conversation above:\"\n\n# Add prompt to our history\n    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n    response = model.invoke(messages)\n\n# Delete all but the 2 most recent messages\n    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n    return {\"summary\": response.content, \"messages\": delete_messages}\npython  theme={null}\n  from typing import Any, TypedDict\n\nfrom langchain.chat_models import init_chat_model\n  from langchain.messages import AnyMessage\n  from langchain_core.messages.utils import count_tokens_approximately\n  from langgraph.graph import StateGraph, START, MessagesState\n  from langgraph.checkpoint.memory import InMemorySaver\n  from langmem.short_term import SummarizationNode, RunningSummary  # [!code highlight]\n\nmodel = init_chat_model(\"claude-sonnet-4-5-20250929\")\n  summarization_model = model.bind(max_tokens=128)\n\nclass State(MessagesState):\n      context: dict[str, RunningSummary]  # [!code highlight]\n\nclass LLMInputState(TypedDict):  # [!code highlight]\n      summarized_messages: list[AnyMessage]\n      context: dict[str, RunningSummary]\n\nsummarization_node = SummarizationNode(  # [!code highlight]\n      token_counter=count_tokens_approximately,\n      model=summarization_model,\n      max_tokens=256,\n      max_tokens_before_summary=256,\n      max_summary_tokens=128,\n  )\n\ndef call_model(state: LLMInputState):  # [!code highlight]\n      response = model.invoke(state[\"summarized_messages\"])\n      return {\"messages\": [response]}\n\ncheckpointer = InMemorySaver()\n  builder = StateGraph(State)\n  builder.add_node(call_model)\n  builder.add_node(\"summarize\", summarization_node)  # [!code highlight]\n  builder.add_edge(START, \"summarize\")\n  builder.add_edge(\"summarize\", \"call_model\")\n  graph = builder.compile(checkpointer=checkpointer)\n\n# Invoke the graph\n  config = {\"configurable\": {\"thread_id\": \"1\"}}\n  graph.invoke({\"messages\": \"hi, my name is bob\"}, config)\n  graph.invoke({\"messages\": \"write a short poem about cats\"}, config)\n  graph.invoke({\"messages\": \"now do the same but for dogs\"}, config)\n  final_response = graph.invoke({\"messages\": \"what's my name?\"}, config)\n\nfinal_response[\"messages\"][-1].pretty_print()\n  print(\"\\nSummary:\", final_response[\"context\"][\"running_summary\"].summary)\n  \n  ================================== Ai Message ==================================\n\nFrom our conversation, I can see that you introduced yourself as Bob. That's the name you shared with me when we began talking.\n\nSummary: In this conversation, I was introduced to Bob, who then asked me to write a poem about cats. I composed a poem titled \"The Mystery of Cats\" that captured cats' graceful movements, independent nature, and their special relationship with humans. Bob then requested a similar poem about dogs, so I wrote \"The Joy of Dogs,\" which highlighted dogs' loyalty, enthusiasm, and loving companionship. Both poems were written in a similar style but emphasized the distinct characteristics that make each pet special.\n  python  theme={null}\n    config = {\n        \"configurable\": {\n            \"thread_id\": \"1\",  # [!code highlight]\n            # optionally provide an ID for a specific checkpoint,\n            # otherwise the latest checkpoint is shown\n            # \"checkpoint_id\": \"1f029ca3-1f5b-6704-8004-820c16b69a5a\"  # [!code highlight]\n\n}\n    }\n    graph.get_state(config)  # [!code highlight]\n    \n    StateSnapshot(\n        values={'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today?), HumanMessage(content=\"what's my name?\"), AIMessage(content='Your name is Bob.')]}, next=(),\n        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}},\n        metadata={\n            'source': 'loop',\n            'writes': {'call_model': {'messages': AIMessage(content='Your name is Bob.')}},\n            'step': 4,\n            'parents': {},\n            'thread_id': '1'\n        },\n        created_at='2025-05-05T16:01:24.680462+00:00',\n        parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},\n        tasks=(),\n        interrupts=()\n    )\n    python  theme={null}\n    config = {\n        \"configurable\": {\n            \"thread_id\": \"1\",  # [!code highlight]\n            # optionally provide an ID for a specific checkpoint,\n            # otherwise the latest checkpoint is shown\n            # \"checkpoint_id\": \"1f029ca3-1f5b-6704-8004-820c16b69a5a\"  # [!code highlight]\n\n}\n    }\n    checkpointer.get_tuple(config)  # [!code highlight]\n    \n    CheckpointTuple(\n        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}},\n        checkpoint={\n            'v': 3,\n            'ts': '2025-05-05T16:01:24.680462+00:00',\n            'id': '1f029ca3-1f5b-6704-8004-820c16b69a5a',\n            'channel_versions': {'__start__': '00000000000000000000000000000005.0.5290678567601859', 'messages': '00000000000000000000000000000006.0.3205149138784782', 'branch:to:call_model': '00000000000000000000000000000006.0.14611156755133758'}, 'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000004.0.5736472536395331'}, 'call_model': {'branch:to:call_model': '00000000000000000000000000000005.0.1410174088651449'}},\n            'channel_values': {'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today?), HumanMessage(content=\"what's my name?\"), AIMessage(content='Your name is Bob.')]},\n        },\n        metadata={\n            'source': 'loop',\n            'writes': {'call_model': {'messages': AIMessage(content='Your name is Bob.')}},\n            'step': 4,\n            'parents': {},\n            'thread_id': '1'\n        },\n        parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},\n        pending_writes=[]\n    )\n    python  theme={null}\n    config = {\n        \"configurable\": {\n            \"thread_id\": \"1\"  # [!code highlight]\n        }\n    }\n    list(graph.get_state_history(config))  # [!code highlight]\n    \n    [\n        StateSnapshot(\n            values={'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'), HumanMessage(content=\"what's my name?\"), AIMessage(content='Your name is Bob.')]},\n            next=(),\n            config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}},\n            metadata={'source': 'loop', 'writes': {'call_model': {'messages': AIMessage(content='Your name is Bob.')}}, 'step': 4, 'parents': {}, 'thread_id': '1'},\n            created_at='2025-05-05T16:01:24.680462+00:00',\n            parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},\n            tasks=(),\n            interrupts=()\n        ),\n        StateSnapshot(\n            values={'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'), HumanMessage(content=\"what's my name?\")]},\n            next=('call_model',),\n            config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},\n            metadata={'source': 'loop', 'writes': None, 'step': 3, 'parents': {}, 'thread_id': '1'},\n            created_at='2025-05-05T16:01:23.863421+00:00',\n            parent_config={...}\n            tasks=(PregelTask(id='8ab4155e-6b15-b885-9ce5-bed69a2c305c', name='call_model', path=('__pregel_pull', 'call_model'), error=None, interrupts=(), state=None, result={'messages': AIMessage(content='Your name is Bob.')}),),\n            interrupts=()\n        ),\n        StateSnapshot(\n            values={'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')]},\n            next=('__start__',),\n            config={...},\n            metadata={'source': 'input', 'writes': {'__start__': {'messages': [{'role': 'user', 'content': \"what's my name?\"}]}}, 'step': 2, 'parents': {}, 'thread_id': '1'},\n            created_at='2025-05-05T16:01:23.863173+00:00',\n            parent_config={...}\n            tasks=(PregelTask(id='24ba39d6-6db1-4c9b-f4c5-682aeaf38dcd', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'messages': [{'role': 'user', 'content': \"what's my name?\"}]}),),\n            interrupts=()\n        ),\n        StateSnapshot(\n            values={'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')]},\n            next=(),\n            config={...},\n            metadata={'source': 'loop', 'writes': {'call_model': {'messages': AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')}}, 'step': 1, 'parents': {}, 'thread_id': '1'},\n            created_at='2025-05-05T16:01:23.862295+00:00',\n            parent_config={...}\n            tasks=(),\n            interrupts=()\n        ),\n        StateSnapshot(\n            values={'messages': [HumanMessage(content=\"hi! I'm bob\")]},\n            next=('call_model',),\n            config={...},\n            metadata={'source': 'loop', 'writes': None, 'step': 0, 'parents': {}, 'thread_id': '1'},\n            created_at='2025-05-05T16:01:22.278960+00:00',\n            parent_config={...}\n            tasks=(PregelTask(id='8cbd75e0-3720-b056-04f7-71ac805140a0', name='call_model', path=('__pregel_pull', 'call_model'), error=None, interrupts=(), state=None, result={'messages': AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')}),),\n            interrupts=()\n        ),\n        StateSnapshot(\n            values={'messages': []},\n            next=('__start__',),\n            config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-0870-6ce2-bfff-1f3f14c3e565'}},\n            metadata={'source': 'input', 'writes': {'__start__': {'messages': [{'role': 'user', 'content': \"hi! I'm bob\"}]}}, 'step': -1, 'parents': {}, 'thread_id': '1'},\n            created_at='2025-05-05T16:01:22.277497+00:00',\n            parent_config=None,\n            tasks=(PregelTask(id='d458367b-8265-812c-18e2-33001d199ce6', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'messages': [{'role': 'user', 'content': \"hi! I'm bob\"}]}),),\n            interrupts=()\n        )\n    ]\n    python  theme={null}\n    config = {\n        \"configurable\": {\n            \"thread_id\": \"1\"  # [!code highlight]\n        }\n    }\n    list(checkpointer.list(config))  # [!code highlight]\n    \n    [\n        CheckpointTuple(\n            config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}},\n            checkpoint={\n                'v': 3,\n                'ts': '2025-05-05T16:01:24.680462+00:00',\n                'id': '1f029ca3-1f5b-6704-8004-820c16b69a5a',\n                'channel_versions': {'__start__': '00000000000000000000000000000005.0.5290678567601859', 'messages': '00000000000000000000000000000006.0.3205149138784782', 'branch:to:call_model': '00000000000000000000000000000006.0.14611156755133758'},\n                'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000004.0.5736472536395331'}, 'call_model': {'branch:to:call_model': '00000000000000000000000000000005.0.1410174088651449'}},\n                'channel_values': {'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'), HumanMessage(content=\"what's my name?\"), AIMessage(content='Your name is Bob.')]},\n            },\n            metadata={'source': 'loop', 'writes': {'call_model': {'messages': AIMessage(content='Your name is Bob.')}}, 'step': 4, 'parents': {}, 'thread_id': '1'},\n            parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},\n            pending_writes=[]\n        ),\n        CheckpointTuple(\n            config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},\n            checkpoint={\n                'v': 3,\n                'ts': '2025-05-05T16:01:23.863421+00:00',\n                'id': '1f029ca3-1790-6b0a-8003-baf965b6a38f',\n                'channel_versions': {'__start__': '00000000000000000000000000000005.0.5290678567601859', 'messages': '00000000000000000000000000000006.0.3205149138784782', 'branch:to:call_model': '00000000000000000000000000000006.0.14611156755133758'},\n                'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000004.0.5736472536395331'}, 'call_model': {'branch:to:call_model': '00000000000000000000000000000005.0.1410174088651449'}},\n                'channel_values': {'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'), HumanMessage(content=\"what's my name?\")], 'branch:to:call_model': None}\n            },\n            metadata={'source': 'loop', 'writes': None, 'step': 3, 'parents': {}, 'thread_id': '1'},\n            parent_config={...},\n            pending_writes=[('8ab4155e-6b15-b885-9ce5-bed69a2c305c', 'messages', AIMessage(content='Your name is Bob.'))]\n        ),\n        CheckpointTuple(\n            config={...},\n            checkpoint={\n                'v': 3,\n                'ts': '2025-05-05T16:01:23.863173+00:00',\n                'id': '1f029ca3-1790-616e-8002-9e021694a0cd',\n                'channel_versions': {'__start__': '00000000000000000000000000000004.0.5736472536395331', 'messages': '00000000000000000000000000000003.0.7056767754077798', 'branch:to:call_model': '00000000000000000000000000000003.0.22059023329132854'},\n                'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000001.0.7040775356287469'}, 'call_model': {'branch:to:call_model': '00000000000000000000000000000002.0.9300422176788571'}},\n                'channel_values': {'__start__': {'messages': [{'role': 'user', 'content': \"what's my name?\"}]}, 'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')]}\n            },\n            metadata={'source': 'input', 'writes': {'__start__': {'messages': [{'role': 'user', 'content': \"what's my name?\"}]}}, 'step': 2, 'parents': {}, 'thread_id': '1'},\n            parent_config={...},\n            pending_writes=[('24ba39d6-6db1-4c9b-f4c5-682aeaf38dcd', 'messages', [{'role': 'user', 'content': \"what's my name?\"}]), ('24ba39d6-6db1-4c9b-f4c5-682aeaf38dcd', 'branch:to:call_model', None)]\n        ),\n        CheckpointTuple(\n            config={...},\n            checkpoint={\n                'v': 3,\n                'ts': '2025-05-05T16:01:23.862295+00:00',\n                'id': '1f029ca3-178d-6f54-8001-d7b180db0c89',\n                'channel_versions': {'__start__': '00000000000000000000000000000002.0.18673090920108737', 'messages': '00000000000000000000000000000003.0.7056767754077798', 'branch:to:call_model': '00000000000000000000000000000003.0.22059023329132854'},\n                'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000001.0.7040775356287469'}, 'call_model': {'branch:to:call_model': '00000000000000000000000000000002.0.9300422176788571'}},\n                'channel_values': {'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')]}\n            },\n            metadata={'source': 'loop', 'writes': {'call_model': {'messages': AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')}}, 'step': 1, 'parents': {}, 'thread_id': '1'},\n            parent_config={...},\n            pending_writes=[]\n        ),\n        CheckpointTuple(\n            config={...},\n            checkpoint={\n                'v': 3,\n                'ts': '2025-05-05T16:01:22.278960+00:00',\n                'id': '1f029ca3-0874-6612-8000-339f2abc83b1',\n                'channel_versions': {'__start__': '00000000000000000000000000000002.0.18673090920108737', 'messages': '00000000000000000000000000000002.0.30296526818059655', 'branch:to:call_model': '00000000000000000000000000000002.0.9300422176788571'},\n                'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000001.0.7040775356287469'}},\n                'channel_values': {'messages': [HumanMessage(content=\"hi! I'm bob\")], 'branch:to:call_model': None}\n            },\n            metadata={'source': 'loop', 'writes': None, 'step': 0, 'parents': {}, 'thread_id': '1'},\n            parent_config={...},\n            pending_writes=[('8cbd75e0-3720-b056-04f7-71ac805140a0', 'messages', AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'))]\n        ),\n        CheckpointTuple(\n            config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-0870-6ce2-bfff-1f3f14c3e565'}},\n            checkpoint={\n                'v': 3,\n                'ts': '2025-05-05T16:01:22.277497+00:00',\n                'id': '1f029ca3-0870-6ce2-bfff-1f3f14c3e565',\n                'channel_versions': {'__start__': '00000000000000000000000000000001.0.7040775356287469'},\n                'versions_seen': {'__input__': {}},\n                'channel_values': {'__start__': {'messages': [{'role': 'user', 'content': \"hi! I'm bob\"}]}}\n            },\n            metadata={'source': 'input', 'writes': {'__start__': {'messages': [{'role': 'user', 'content': \"hi! I'm bob\"}]}}, 'step': -1, 'parents': {}, 'thread_id': '1'},\n            parent_config=None,\n            pending_writes=[('d458367b-8265-812c-18e2-33001d199ce6', 'messages', [{'role': 'user', 'content': \"hi! I'm bob\"}]), ('d458367b-8265-812c-18e2-33001d199ce6', 'branch:to:call_model', None)]\n        )\n    ]\n    python  theme={null}\nthread_id = \"1\"\ncheckpointer.delete_thread(thread_id)\n```\n\n## Prebuilt memory tools\n\n**LangMem** is a LangChain-maintained library that offers tools for managing long-term memories in your agent. See the [LangMem documentation](https://langchain-ai.github.io/langmem/) for usage examples.\n\n## Database management\n\nIf you are using any database-backed persistence implementation (such as Postgres or Redis) to store short and/or long-term memory, you will need to run migrations to set up the required schema before you can use it with your database.\n\nBy convention, most database-specific libraries define a `setup()` method on the checkpointer or store instance that runs the required migrations. However, you should check with your specific implementation of [`BaseCheckpointSaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.base.BaseCheckpointSaver) or [`BaseStore`](https://reference.langchain.com/python/langgraph/store/#langgraph.store.base.BaseStore) to confirm the exact method name and usage.\n\nWe recommend running migrations as a dedicated deployment step, or you can ensure they're run as part of server startup.\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/add-memory.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "code_samples": [
    {
      "code": "<Accordion title=\"Long-term memory with semantic search\">",
      "language": "unknown"
    },
    {
      "code": "</Accordion>\n\n## Manage short-term memory\n\nWith [short-term memory](#add-short-term-memory) enabled, long conversations can exceed the LLM's context window. Common solutions are:\n\n* [Trim messages](#trim-messages): Remove first or last N messages (before calling LLM)\n* [Delete messages](#delete-messages) from LangGraph state permanently\n* [Summarize messages](#summarize-messages): Summarize earlier messages in the history and replace them with a summary\n* [Manage checkpoints](#manage-checkpoints) to store and retrieve message history\n* Custom strategies (e.g., message filtering, etc.)\n\nThis allows the agent to keep track of the conversation without exceeding the LLM's context window.\n\n### Trim messages\n\nMost LLMs have a maximum supported context window (denominated in tokens). One way to decide when to truncate messages is to count the tokens in the message history and truncate whenever it approaches that limit. If you're using LangChain, you can use the trim messages utility and specify the number of tokens to keep from the list, as well as the `strategy` (e.g., keep the last `max_tokens`) to use for handling the boundary.\n\nTo trim message history, use the [`trim_messages`](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.trim_messages.html) function:",
      "language": "unknown"
    },
    {
      "code": "<Accordion title=\"Full example: trim messages\">",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</Accordion>\n\n### Delete messages\n\nYou can delete messages from the graph state to manage the message history. This is useful when you want to remove specific messages or clear the entire message history.\n\nTo delete messages from the graph state, you can use the `RemoveMessage`. For `RemoveMessage` to work, you need to use a state key with [`add_messages`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.message.add_messages) [reducer](/oss/python/langgraph/graph-api#reducers), like [`MessagesState`](/oss/python/langgraph/graph-api#messagesstate).\n\nTo remove specific messages:",
      "language": "unknown"
    },
    {
      "code": "To remove **all** messages:",
      "language": "unknown"
    },
    {
      "code": "<Warning>\n  When deleting messages, **make sure** that the resulting message history is valid. Check the limitations of the LLM provider you're using. For example:\n\n  * Some providers expect message history to start with a `user` message\n  * Most providers require `assistant` messages with tool calls to be followed by corresponding `tool` result messages.\n</Warning>\n\n<Accordion title=\"Full example: delete messages\">",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</Accordion>\n\n### Summarize messages\n\nThe problem with trimming or removing messages, as shown above, is that you may lose information from culling of the message queue. Because of this, some applications benefit from a more sophisticated approach of summarizing the message history using a chat model.\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=c8ed3facdccd4ef5c7e52902c72ba938\" alt=\"Summary\" data-og-width=\"609\" width=\"609\" data-og-height=\"242\" height=\"242\" data-path=\"oss/images/summary.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?w=280&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=4208b9b0cc9f459f3dc4e5219918471b 280w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?w=560&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=7acb77c081545f57042368f4e9d0c8cb 560w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?w=840&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=2fcfdb0c481d2e1d361e76db763a41e5 840w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?w=1100&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=4abdac693a562788aa0db8681bef8ea7 1100w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?w=1650&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=40acfefa91dcb11b247a6e4a7705f22b 1650w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?w=2500&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=8d765aaf7551e8b0fc2720de7d2ac2a8 2500w\" />\n\nPrompting and orchestration logic can be used to summarize the message history. For example, in LangGraph you can extend the [`MessagesState`](/oss/python/langgraph/graph-api#working-with-messages-in-graph-state) to include a `summary` key:",
      "language": "unknown"
    },
    {
      "code": "Then, you can generate a summary of the chat history, using any existing summary as context for the next summary. This `summarize_conversation` node can be called after some number of messages have accumulated in the `messages` state key.",
      "language": "unknown"
    },
    {
      "code": "<Accordion title=\"Full example: summarize messages\">",
      "language": "unknown"
    },
    {
      "code": "1. We will keep track of our running summary in the `context` field\n\n  (expected by the `SummarizationNode`).\n\n  1. Define private state that will be used only for filtering\n\n  the inputs to `call_model` node.\n\n  1. We're passing a private input state here to isolate the messages returned by the summarization node",
      "language": "unknown"
    },
    {
      "code": "</Accordion>\n\n### Manage checkpoints\n\nYou can view and delete the information stored by the checkpointer.\n\n<a id=\"checkpoint\" />\n\n#### View thread state\n\n<Tabs>\n  <Tab title=\"Graph/Functional API\">",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</Tab>\n\n  <Tab title=\"Checkpointer API\">",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</Tab>\n</Tabs>\n\n<a id=\"checkpoints\" />\n\n#### View the history of the thread\n\n<Tabs>\n  <Tab title=\"Graph/Functional API\">",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</Tab>\n\n  <Tab title=\"Checkpointer API\">",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</Tab>\n</Tabs>\n\n#### Delete all checkpoints for a thread",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": "Manage short-term memory",
      "id": "manage-short-term-memory"
    },
    {
      "level": "h3",
      "text": "Trim messages",
      "id": "trim-messages"
    },
    {
      "level": "h3",
      "text": "Delete messages",
      "id": "delete-messages"
    },
    {
      "level": "h3",
      "text": "Summarize messages",
      "id": "summarize-messages"
    },
    {
      "level": "h3",
      "text": "Manage checkpoints",
      "id": "manage-checkpoints"
    },
    {
      "level": "h2",
      "text": "Prebuilt memory tools",
      "id": "prebuilt-memory-tools"
    },
    {
      "level": "h2",
      "text": "Database management",
      "id": "database-management"
    }
  ],
  "url": "llms-txt#create-store-with-semantic-search-enabled",
  "links": []
}