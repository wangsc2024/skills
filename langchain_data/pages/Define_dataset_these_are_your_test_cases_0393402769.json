{
  "title": "Define dataset: these are your test cases",
  "content": "dataset_name = \"QA Example Dataset\"\ndataset = client.create_dataset(dataset_name)\n\nclient.create_examples(\n    dataset_id=dataset.id,\n    examples=[\n        {\n            \"inputs\": {\"question\": \"What is LangChain?\"},\n            \"outputs\": {\"answer\": \"A framework for building LLM applications\"},\n        },\n        {\n            \"inputs\": {\"question\": \"What is LangSmith?\"},\n            \"outputs\": {\"answer\": \"A platform for observing and evaluating LLM applications\"},\n        },\n        {\n            \"inputs\": {\"question\": \"What is OpenAI?\"},\n            \"outputs\": {\"answer\": \"A company that creates Large Language Models\"},\n        },\n        {\n            \"inputs\": {\"question\": \"What is Google?\"},\n            \"outputs\": {\"answer\": \"A technology company known for search\"},\n        },\n        {\n            \"inputs\": {\"question\": \"What is Mistral?\"},\n            \"outputs\": {\"answer\": \"A company that creates Large Language Models\"},\n        }\n    ]\n)\npython  theme={null}\nimport openai\nfrom langsmith import wrappers\n\nopenai_client = wrappers.wrap_openai(openai.OpenAI())\n\neval_instructions = \"You are an expert professor specialized in grading students' answers to questions.\"\n\ndef correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\n    user_content = f\"\"\"You are grading the following question:\n{inputs['question']}\nHere is the real answer:\n{reference_outputs['answer']}\nYou are grading the following predicted answer:\n{outputs['response']}\nRespond with CORRECT or INCORRECT:\nGrade:\"\"\"\n    response = openai_client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        temperature=0,\n        messages=[\n            {\"role\": \"system\", \"content\": eval_instructions},\n            {\"role\": \"user\", \"content\": user_content},\n        ],\n    ).choices[0].message.content\n    return response == \"CORRECT\"\npython  theme={null}\ndef concision(outputs: dict, reference_outputs: dict) -> bool:\n    return int(len(outputs[\"response\"]) < 2 * len(reference_outputs[\"answer\"]))\npython  theme={null}\ndefault_instructions = \"Respond to the users question in a short, concise manner (one short sentence).\"\n\ndef my_app(question: str, model: str = \"gpt-4o-mini\", instructions: str = default_instructions) -> str:\n    return openai_client.chat.completions.create(\n        model=model,\n        temperature=0,\n        messages=[\n            {\"role\": \"system\", \"content\": instructions},\n            {\"role\": \"user\", \"content\": question},\n        ],\n    ).choices[0].message.content\npython  theme={null}\ndef ls_target(inputs: str) -> dict:\n    return {\"response\": my_app(inputs[\"question\"])}\npython  theme={null}\nexperiment_results = client.evaluate(\n    ls_target, # Your AI system\n    data=dataset_name, # The data to predict and grade over\n    evaluators=[concision, correctness], # The evaluators to score the results\n    experiment_prefix=\"openai-4o-mini\", # A prefix for your experiment names to easily identify them\n)\npython  theme={null}\ndef ls_target_v2(inputs: str) -> dict:\n    return {\"response\": my_app(inputs[\"question\"], model=\"gpt-4-turbo\")}\n\nexperiment_results = client.evaluate(\n    ls_target_v2,\n    data=dataset_name,\n    evaluators=[concision, correctness],\n    experiment_prefix=\"openai-4-turbo\",\n)\npython  theme={null}\ninstructions_v3 = \"Respond to the users question in a short, concise manner (one short sentence). Do NOT use more than ten words.\"\n\ndef ls_target_v3(inputs: str) -> dict:\n    response = my_app(\n        inputs[\"question\"],\n        model=\"gpt-4-turbo\",\n        instructions=instructions_v3\n    )\n    return {\"response\": response}\n\nexperiment_results = client.evaluate(\n    ls_target_v3,\n    data=dataset_name,\n    evaluators=[concision, correctness],\n    experiment_prefix=\"strict-openai-4-turbo\",\n)\npython  theme={null}\ndef test_length_score() -> None:\n    \"\"\"Test that the length score is at least 80%.\"\"\"\n    experiment_results = evaluate(\n        ls_target, # Your AI system\n        data=dataset_name, # The data to predict and grade over\n        evaluators=[concision, correctness], # The evaluators to score the results\n    )\n    # This will be cleaned up in the next release:\n    feedback = client.list_feedback(\n        run_ids=[r.id for r in client.list_runs(project_name=experiment_results.experiment_name)],\n        feedback_key=\"concision\"\n    )\n    scores = [f.score for f in feedback]\n    assert sum(scores) / len(scores) >= 0.8, \"Aggregate score should be at least .8\"\npython  theme={null}\n  import openai\n  from langsmith import Client, wrappers\n\n# Application code\n  openai_client = wrappers.wrap_openai(openai.OpenAI())\n\ndefault_instructions = \"Respond to the users question in a short, concise manner (one short sentence).\"\n\ndef my_app(question: str, model: str = \"gpt-4o-mini\", instructions: str = default_instructions) -> str:\n      return openai_client.chat.completions.create(\n          model=model,\n          temperature=0,\n          messages=[\n              {\"role\": \"system\", \"content\": instructions},\n              {\"role\": \"user\", \"content\": question},\n          ],\n      ).choices[0].message.content\n\n# Define dataset: these are your test cases\n  dataset_name = \"QA Example Dataset\"\n  dataset = client.create_dataset(dataset_name)\n\nclient.create_examples(\n      dataset_id=dataset.id,\n      examples=[\n          {\n              \"inputs\": {\"question\": \"What is LangChain?\"},\n              \"outputs\": {\"answer\": \"A framework for building LLM applications\"},\n          },\n          {\n              \"inputs\": {\"question\": \"What is LangSmith?\"},\n              \"outputs\": {\"answer\": \"A platform for observing and evaluating LLM applications\"},\n          },\n          {\n              \"inputs\": {\"question\": \"What is OpenAI?\"},\n              \"outputs\": {\"answer\": \"A company that creates Large Language Models\"},\n          },\n          {\n              \"inputs\": {\"question\": \"What is Google?\"},\n              \"outputs\": {\"answer\": \"A technology company known for search\"},\n          },\n          {\n              \"inputs\": {\"question\": \"What is Mistral?\"},\n              \"outputs\": {\"answer\": \"A company that creates Large Language Models\"},\n          }\n      ]\n  )\n\n# Define evaluators\n  eval_instructions = \"You are an expert professor specialized in grading students' answers to questions.\"\n\ndef correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\n      user_content = f\"\"\"You are grading the following question:\n  {inputs['question']}\n  Here is the real answer:\n  {reference_outputs['answer']}\n  You are grading the following predicted answer:\n  {outputs['response']}\n  Respond with CORRECT or INCORRECT:\n  Grade:\"\"\"\n      response = openai_client.chat.completions.create(\n          model=\"gpt-4o-mini\",\n          temperature=0,\n          messages=[\n              {\"role\": \"system\", \"content\": eval_instructions},\n              {\"role\": \"user\", \"content\": user_content},\n          ],\n      ).choices[0].message.content\n      return response == \"CORRECT\"\n\ndef concision(outputs: dict, reference_outputs: dict) -> bool:\n      return int(len(outputs[\"response\"]) < 2 * len(reference_outputs[\"answer\"]))\n\n# Run evaluations\n  def ls_target(inputs: str) -> dict:\n      return {\"response\": my_app(inputs[\"question\"])}\n\nexperiment_results_v1 = client.evaluate(\n      ls_target, # Your AI system\n      data=dataset_name, # The data to predict and grade over\n      evaluators=[concision, correctness], # The evaluators to score the results\n      experiment_prefix=\"openai-4o-mini\", # A prefix for your experiment names to easily identify them\n  )\n\ndef ls_target_v2(inputs: str) -> dict:\n      return {\"response\": my_app(inputs[\"question\"], model=\"gpt-4-turbo\")}\n\nexperiment_results_v2 = client.evaluate(\n      ls_target_v2,\n      data=dataset_name,\n      evaluators=[concision, correctness],\n      experiment_prefix=\"openai-4-turbo\",\n  )\n\ninstructions_v3 = \"Respond to the users question in a short, concise manner (one short sentence). Do NOT use more than ten words.\"\n\ndef ls_target_v3(inputs: str) -> dict:\n      response = my_app(\n          inputs[\"question\"],\n          model=\"gpt-4-turbo\",\n          instructions=instructions_v3\n      )\n      return {\"response\": response}\n\nexperiment_results_v3 = client.evaluate(\n      ls_target_v3,\n      data=dataset_name,\n      evaluators=[concision, correctness],\n      experiment_prefix=\"strict-openai-4-turbo\",\n  )\n  ```\n</Accordion>\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/evaluate-chatbot-tutorial.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "code_samples": [
    {
      "code": "Now, if we go the LangSmith UI and look for `QA Example Dataset` in the `Datasets & Testing` page, when we click into it we should see that we have five new examples.\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-dataset.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=9ab5110714d009d5865ba0e2d8ee0ffa\" alt=\"Testing tutorial dataset\" data-og-width=\"1251\" width=\"1251\" data-og-height=\"560\" height=\"560\" data-path=\"langsmith/images/testing-tutorial-dataset.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-dataset.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=e4b38ded6968e649ed8ab507f63f1f3e 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-dataset.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=f7aee5327f8058dd99684cd43e44c791 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-dataset.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=9e853ed05b0a2ad40f9e4d0403e7004c 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-dataset.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=331654a31885b89a93924eaac4fa95da 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-dataset.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=833bf2a60b392323bba47fbe42655537 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-dataset.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=3410e4bc7ac5c28f8838fc5fb88026bd 2500w\" />\n\n## Define metrics\n\nAfter creating our dataset, we can now define some metrics to evaluate our responses on. Since we have an expected answer, we can compare to that as part of our evaluation. However, we do not expect our application to output those **exact** answers, but rather something that is similar. This makes our evaluation a little trickier.\n\nIn addition to evaluating correctness, let's also make sure our answers are short and concise. This will be a little easier - we can define a simple Python function to measure the length of the response.\n\nLet's go ahead and define these two metrics.\n\nFor the first, we will use an LLM to **judge** whether the output is correct (with respect to the expected output). This **LLM-as-a-judge** is relatively common for cases that are too complex to measure with a simple function. We can define our own prompt and LLM to use for evaluation here:",
      "language": "unknown"
    },
    {
      "code": "For evaluating the length of the response, this is a lot easier! We can just define a simple function that checks whether the actual output is less than 2x the length of the expected result.",
      "language": "unknown"
    },
    {
      "code": "## Run Evaluations\n\nGreat! So now how do we run evaluations? Now that we have a dataset and evaluators, all that we need is our application! We will build a simple application that just has a system message with instructions on how to respond and then passes it to the LLM. We will build this using the OpenAI SDK directly:",
      "language": "unknown"
    },
    {
      "code": "Before running this through LangSmith evaluations, we need to define a simple wrapper that maps the input keys from our dataset to the function we want to call, and then also maps the output of the function to the output key we expect.",
      "language": "unknown"
    },
    {
      "code": "Great! Now we're ready to run an evaluation. Let's do it!",
      "language": "unknown"
    },
    {
      "code": "This will output a URL. If we click on it, we should see results of our evaluation!\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-run.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=9517dd9f9fc23062fcba7b061fe5cdda\" alt=\"Testing tutorial run\" data-og-width=\"3022\" width=\"3022\" data-og-height=\"1128\" height=\"1128\" data-path=\"langsmith/images/testing-tutorial-run.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-run.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=06b6a2a70ea3f85929dca6f03653be68 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-run.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=3ddae16867d89b93f0155dc654dede93 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-run.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=5ba93e0b10423be56081602f65ec41bc 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-run.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=2088e69be2ee5efdf949296ea3c74652 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-run.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=b6fa551b7edab188da50ccc63dbd9769 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-run.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=0b79a1b6f5a351ed986810f53d14d9d9 2500w\" />\n\nIf we go back to the dataset page and select the `Experiments` tab, we can now see a summary of our one run!\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-one-run.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=4c30f7474727d2f537c75e5f80ae1298\" alt=\"Testing tutorial one run\" data-og-width=\"3022\" width=\"3022\" data-og-height=\"1532\" height=\"1532\" data-path=\"langsmith/images/testing-tutorial-one-run.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-one-run.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=4886dc0021394767078872237779a8f3 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-one-run.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=cd111a19a463ed45fabe53caa2fc08be 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-one-run.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=25363336afaecae0198f7c55f5bfe739 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-one-run.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=42473dfcfad36f26ad7cfea316dbb458 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-one-run.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=4317ea45ce5de4f3917a902d84b3e2e3 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-one-run.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=0b0371997a45b936b37c8f44539fd1aa 2500w\" />\n\nLet's now try it out with a different model! Let's try `gpt-4-turbo`",
      "language": "unknown"
    },
    {
      "code": "And now let's use GPT-4 but also update the prompt to be a bit more strict in requiring the answer to be short.",
      "language": "unknown"
    },
    {
      "code": "If we go back to the `Experiments` tab on the datasets page, we should see that all three runs now show up!\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-three-runs.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=9d74c18991c33d4fbd5180dbb12a4f91\" alt=\"Testing tutorial three runs\" data-og-width=\"3020\" width=\"3020\" data-og-height=\"1540\" height=\"1540\" data-path=\"langsmith/images/testing-tutorial-three-runs.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-three-runs.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=11a8a19c576760e949137952786ad325 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-three-runs.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=dedd52ea55db2e7c886bdbddab19bcf4 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-three-runs.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=d10e4b9efd6dbea2c50859fde066afb7 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-three-runs.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=bc78ab5c6cad42ee2aa0aaf5350329ac 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-three-runs.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=84edcc8ea892ac534009c9a05212bf26 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-three-runs.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=aefc9019026a0c38b5a92c2aa5ac3462 2500w\" />\n\n## Comparing results\n\nAwesome, we've evaluated three different runs. But how can we compare results? The first way we can do this is just by looking at the runs in the `Experiments` tab. If we do that, we can see a high level view of the metrics for each run:\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-compare-metrics.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=224acfbea78b8b1d0e08ce59d06b5088\" alt=\"Testing tutorial compare metrics\" data-og-width=\"3020\" width=\"3020\" data-og-height=\"1540\" height=\"1540\" data-path=\"langsmith/images/testing-tutorial-compare-metrics.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-compare-metrics.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=abf04bbd16675f00a4a3941c27e21ac7 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-compare-metrics.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=40893fc5274fb96ea5a1bd26919604a7 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-compare-metrics.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=3ebe594101ccb3c8efcf3d9cf0e8d906 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-compare-metrics.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=6a934bbb56ebf268636db8f3c775d73d 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-compare-metrics.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=178412fa63f8011046cd44d5e4411f68 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-compare-metrics.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=8f893b4d545e6723dc2efe8d76c4da9f 2500w\" />\n\nGreat! So we can tell that GPT-4 is better than GPT-3.5 at knowing who companies are, and we can see that the strict prompt helped a lot with the length. But what if we want to explore in more detail?\n\nIn order to do that, we can select all the runs we want to compare (in this case all three) and open them up in a comparison view. We immediately see all three tests side by side. Some of the cells are color coded - this is showing a regression of *a certain metric* compared to *a certain baseline*. We automatically choose defaults for the baseline and metric, but you can change those yourself. You can also choose which columns and which metrics you see by using the `Display` control. You can also automatically filter to only see the runs that have improvements/regressions by clicking on the icons at the top.\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-compare-runs.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=de5575b837cdf97d479e5c91aff9dc78\" alt=\"Testing tutorial compare runs\" data-og-width=\"3022\" width=\"3022\" data-og-height=\"1548\" height=\"1548\" data-path=\"langsmith/images/testing-tutorial-compare-runs.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-compare-runs.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=09d73536620b794bab530f1c154ca1cd 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-compare-runs.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=3cc976e3066b5a1777bd76ef367e5cd9 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-compare-runs.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=8fa9010ec69b095f97094ea3f1322b7c 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-compare-runs.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=03f984b970ffeceafa30bc6b242dd468 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-compare-runs.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=9d7ba0782fdf722b27ff1382a8156ea9 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-compare-runs.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=64ff5b28a56fcffedb5d0daaa2c87e33 2500w\" />\n\nIf we want to see more information, we can also select the `Expand` button that appears when hovering over a row to open up a side panel with more detailed information:\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-side-panel.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=a72c4924a0ad9bebceae2da9518c56cc\" alt=\"Testing tutorial side panel\" data-og-width=\"2824\" width=\"2824\" data-og-height=\"1546\" height=\"1546\" data-path=\"langsmith/images/testing-tutorial-side-panel.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-side-panel.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=f10bab81b285ece30e110570962caeaf 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-side-panel.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=ced32c31ada87af7f81ca78ee4d9b1a5 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-side-panel.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=282ab40f9b90b46589a92fb9cd1da680 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-side-panel.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=b584cee17dd93e0e121b7a30ccdc1d3f 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-side-panel.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=b2b51c2aaf8e582eedae0586b0a08031 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-side-panel.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=fc077628c53f5684d16bb309df8fc84a 2500w\" />\n\n## Set up automated testing to run in CI/CD\n\nNow that we've run this in a one-off manner, we can set it to run in an automated fashion. We can do this pretty easily by just including it as a pytest file that we run in CI/CD. As part of this, we can either just log the results OR set up some criteria to determine if it passes or not. For example, if I wanted to ensure that we always got at least 80% of generated responses passing the `length` check, we could set that up with a test like:",
      "language": "unknown"
    },
    {
      "code": "## Track results over time\n\nNow that we've got these experiments running in an automated fashion, we want to track these results over time. We can do this from the overall `Experiments` tab in the datasets page. By default, we show evaluation metrics over time (highlighted in red). We also automatically track git metrics, to easily associate it with the branch of your code (highlighted in yellow).\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-over-time.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=a5961747b6ea92bb2f838d025ca5e3d5\" alt=\"Testing tutorial over time\" data-og-width=\"3020\" width=\"3020\" data-og-height=\"1544\" height=\"1544\" data-path=\"langsmith/images/testing-tutorial-over-time.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-over-time.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=e0dc96840d39b210e1ae957e8dfb5fb2 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-over-time.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=8f56098392ba5b2c0670ceb16e29ac0c 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-over-time.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=df47e336e15b6d23f36b20915db7192d 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-over-time.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=e6e38b84ee80b1df85ee926a16e3b654 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-over-time.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=8ef1345d6b299b41399a1a60fbef2219 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-over-time.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=679f95e97b7ac941e3380f658b158ef5 2500w\" />\n\n## Conclusion\n\nThat's it for this tutorial!\n\nWe've gone over how to create an initial test set, define some evaluation metrics, run experiments, compare them manually, set up CI/CD, and track results over time. Hopefully this can help you iterate with confidence.\n\nThis is just the start. As mentioned earlier, evaluation is an ongoing process. For example - the datapoints you will want to evaluate on will likely continue to change over time. There are many types of evaluators you may wish to explore. For information on this, check out the [how-to guides](/langsmith/evaluation).\n\nAdditionally, there are other ways to evaluate data besides in this \"offline\" manner (e.g. you can evaluate production data). For more information on online evaluation, check out [this guide](/langsmith/online-evaluations).\n\n## Reference code\n\n<Accordion title=\"Click to see a consolidated code snippet\">",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": "Define metrics",
      "id": "define-metrics"
    },
    {
      "level": "h2",
      "text": "Run Evaluations",
      "id": "run-evaluations"
    },
    {
      "level": "h2",
      "text": "Comparing results",
      "id": "comparing-results"
    },
    {
      "level": "h2",
      "text": "Set up automated testing to run in CI/CD",
      "id": "set-up-automated-testing-to-run-in-ci/cd"
    },
    {
      "level": "h2",
      "text": "Track results over time",
      "id": "track-results-over-time"
    },
    {
      "level": "h2",
      "text": "Conclusion",
      "id": "conclusion"
    },
    {
      "level": "h2",
      "text": "Reference code",
      "id": "reference-code"
    }
  ],
  "url": "llms-txt#define-dataset:-these-are-your-test-cases",
  "links": []
}