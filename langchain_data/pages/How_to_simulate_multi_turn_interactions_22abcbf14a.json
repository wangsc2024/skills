{
  "title": "How to simulate multi-turn interactions",
  "content": "Source: https://docs.langchain.com/langsmith/multi-turn-simulation\n\n<Info>\n  * [Multi-turn interactions](/langsmith/evaluation-concepts#multi-turn-interactions)\n  * [Evaluators](/langsmith/evaluation-concepts#evaluators)\n  * [LLM-as-judge](/langsmith/evaluation-concepts#llm-as-judge)\n  * [OpenEvals](https://github.com/langchain-ai/openevals)\n</Info>\n\nAI applications with conversational interfaces, like chatbots, operate over multiple interactions with a user, also called conversation *turns*. When evaluating the performance of such applications, core concepts such as [building a dataset](/langsmith/evaluation-concepts#datasets) and defining [evaluators](/langsmith/evaluation-concepts#evaluators) and metrics to judge your app outputs remain useful. However, you may also find it useful to run a *simulation* between your app and a user, then evaluate this dynamically created trajectory.\n\nSome advantages of doing this are:\n\n* Ease of getting started vs. an evaluation over a full dataset of pre-existing trajectories\n* End-to-end coverage from an initial query until a successful or unsuccessful resolution\n* The ability to detect repetitive behavior or context loss over several iterations of your app\n\nThe downside is that because you are broadening your evaluation surface area to contain multiple turns, there is less consistency than evaluating a single output from your app given a static input from a dataset.\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=c903f388600ab575e70edb92209c6b2e\" alt=\"Multi turn trace\" data-og-width=\"2952\" width=\"2952\" data-og-height=\"1790\" height=\"1790\" data-path=\"langsmith/images/multi-turn-trace.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=a5113736dc83834150a2b414619626b2 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=f0bfc1f1764c80efcdebfcc07149ef8a 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=53e53c0f05f5638b5e17576c0f37d195 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=dc05ed0dcfe9d6da5b8872df189f6ed9 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=7fbd598ec4a113bb143d0a0a1ca68a91 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=a9dcabf49b84ba2ad8aac4159b5b0657 2500w\" />\n\nThis guide will show you how to simulate multi-turn interactions and evaluate them using the open-source [`openevals`](https://github.com/langchain-ai/openevals) package, which contains prebuilt evaluators and other convenient resources for evaluating your AI apps. It will also use OpenAI models, though you can use other providers as well.\n\nFirst, ensure you have the required dependencies installed:\n\n<Info>\n  If you are using `yarn` as your package manager, you will also need to manually install `@langchain/core` as a peer dependency of `openevals`. This is not required for LangSmith evals in general.\n</Info>\n\nAnd set up your environment variables:\n\n## Running a simulation\n\nThere are two primary components you'll need to get started:\n\n* `app`: Your application, or a function wrapping it. Must accept a single chat message (dict with \"role\" and \"content\" keys) as an input arg and a `thread_id` as a kwarg. Should accept other kwargs as more may be added in future releases. Returns a chat message as output with at least role and content keys.\n* `user`: The simulated user. In this guide, we will use an imported prebuilt function named `create_llm_simulated_user` which uses an LLM to generate user responses, though you can [create your own too](https://github.com/langchain-ai/openevals?tab=readme-ov-file#custom-simulated-users).\n\nThe simulator in `openevals` passes a single chat message to your `app` from the `user` for each turn. Therefore you should statefully track the current history internally based on `thread_id` if needed.\n\nHere's an example that simulates a multi-turn customer support interaction. This guide uses a simple chat app that wraps a single call to the OpenAI chat completions API, however this is where you would call your application or agent. In this example, our simulated user is playing the role of a particularly aggressive customer:\n\nThe response looks like this:\n\nThe simulation first generates an initial query from the simulated `user`, then passes response chat messages back and forth until it reaches `max_turns` (you can alternatively pass a `stopping_condition` that takes the current trajectory and returns `True` or `False` - [see the OpenEvals README for more information](https://github.com/langchain-ai/openevals?tab=readme-ov-file#multiturn-simulation)). The return value is the final list of chat messages that make up the converation's **trajectory**.\n\n<Info>\n  There are several ways to configure the simulated user, such as having it return fixed responses for the first turns of your simulation, as well as the simulation as a whole. For full details, check out [the OpenEvals README](https://github.com/langchain-ai/openevals?tab=readme-ov-file#multiturn-simulation).\n</Info>\n\nThe final trace will look something [like this](https://smith.langchain.com/public/648ca37d-1c4d-4f7b-9b6a-89e35dc5d4f0/r) with responses from your `app` and `user` interleaved:\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=c903f388600ab575e70edb92209c6b2e\" alt=\"Multi turn trace\" data-og-width=\"2952\" width=\"2952\" data-og-height=\"1790\" height=\"1790\" data-path=\"langsmith/images/multi-turn-trace.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=a5113736dc83834150a2b414619626b2 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=f0bfc1f1764c80efcdebfcc07149ef8a 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=53e53c0f05f5638b5e17576c0f37d195 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=dc05ed0dcfe9d6da5b8872df189f6ed9 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=7fbd598ec4a113bb143d0a0a1ca68a91 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=a9dcabf49b84ba2ad8aac4159b5b0657 2500w\" />\n\nCongrats! You just ran your first multi-turn simulation. Next, we'll cover how to run it in a LangSmith experiment.\n\n## Running in LangSmith experiments\n\nYou can use the results of multi-turn simulations as part of a LangSmith experiment to track performance and progress over time. For these sections, it helps to be familiar with at least one of LangSmith's [`pytest`](/langsmith/pytest) (Python-only), [`Vitest`/`Jest`](/langsmith/vitest-jest) (JS only), or [`evaluate`](/langsmith/evaluate-llm-application) runners.\n\n### Using `pytest` or `Vitest/Jest`\n\n<Check>\n  See the following guides to learn how to set up evals using LangSmith's integrations with test frameworks:\n\n* [`pytest`](https://docs.smith.langchain.com/langsmith/pytest)\n  * [`Vitest` or `Jest`](https://docs.smith.langchain.com/langsmith/vitest-jest)\n</Check>\n\nIf you are using one of the [LangSmith test framework integrations](/langsmith/pytest), you can pass in an array of OpenEvals evaluators as a `trajectory_evaluators` param when running the simulation. These evaluators will run at the end of the simulation, taking the final list of chat messages as an `outputs` kwarg. Your passed `trajectory_evaluator` must therefore accept this kwarg.\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-vitest.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=04c56e67e7bb9f01cb905d8a184d62d5\" alt=\"Multi turn vitest\" data-og-width=\"3448\" width=\"3448\" data-og-height=\"1128\" height=\"1128\" data-path=\"langsmith/images/multi-turn-vitest.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-vitest.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=41ac969c6ceb99ac0976ab3027b00e89 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-vitest.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=c10eece031225173dc0ded446e3e2e3c 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-vitest.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=90bda951b2cfa02bde0c8ad204a7dac7 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-vitest.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=2ca95c911f68412eb09e2f8a0a6b42e4 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-vitest.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=12be96d7f216cd8ce664c01f61f45288 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-vitest.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=1faa3a34b3daf67705e2afeca748e353 2500w\" />\n\nLangSmith will automatically detect and log the feedback returned from the passed `trajectory_evaluators`, adding it to the experiment. Note also that the test case uses the `fixed_responses` param on the simulated user to start the conversation with a specific input, which you can log and make part of your stored dataset.\n\nYou may also find it convenient to have the simulated user's system prompt to be part of your logged dataset as well.\n\nYou can also use the [`evaluate`](/langsmith/evaluate-llm-application) runner to evaluate simulated multi-turn interactions. This will be a little bit different from the `pytest`/`Vitest`/`Jest` example in the following ways:\n\n* The simulation should be part of your `target` function, and your target function should return the final trajectory.\n  * This will make the trajectory the `outputs` that LangSmith will pass to your evaluators.\n* Instead of using the `trajectory_evaluators` param, you should pass your evaluators as a param into the `evaluate()` method.\n* You will need an existing dataset of inputs and (optionally) reference trajectories.\n\n## Modifying the simulated user persona\n\nThe above examples run using the same simulated user persona for all input examples, defined by the `system` parameter passed into `create_llm_simulated_user`. If you would like to use a different persona for specific items in your dataset, you can update your dataset examples to also contain an extra field with the desired `system` prompt, then pass that field in when creating your simulated user like this:\n\nYou've just seen some techniques for simulating multi-turn interactions and running them in LangSmith evals.\n\nHere are some topics you might want to explore next:\n\n* [Trace multiturn conversations across different traces](/langsmith/threads)\n* [Use multiple messages in the playground UI](/langsmith/multiple-messages)\n* [Return multiple metrics in one evaluator](/langsmith/multiple-scores)\n\nYou can also explore the [OpenEvals readme](https://github.com/langchain-ai/openevals) for more on prebuilt evaluators.\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/multi-turn-simulation.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "code_samples": [
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</CodeGroup>\n\n<Info>\n  If you are using `yarn` as your package manager, you will also need to manually install `@langchain/core` as a peer dependency of `openevals`. This is not required for LangSmith evals in general.\n</Info>\n\nAnd set up your environment variables:",
      "language": "unknown"
    },
    {
      "code": "## Running a simulation\n\nThere are two primary components you'll need to get started:\n\n* `app`: Your application, or a function wrapping it. Must accept a single chat message (dict with \"role\" and \"content\" keys) as an input arg and a `thread_id` as a kwarg. Should accept other kwargs as more may be added in future releases. Returns a chat message as output with at least role and content keys.\n* `user`: The simulated user. In this guide, we will use an imported prebuilt function named `create_llm_simulated_user` which uses an LLM to generate user responses, though you can [create your own too](https://github.com/langchain-ai/openevals?tab=readme-ov-file#custom-simulated-users).\n\nThe simulator in `openevals` passes a single chat message to your `app` from the `user` for each turn. Therefore you should statefully track the current history internally based on `thread_id` if needed.\n\nHere's an example that simulates a multi-turn customer support interaction. This guide uses a simple chat app that wraps a single call to the OpenAI chat completions API, however this is where you would call your application or agent. In this example, our simulated user is playing the role of a particularly aggressive customer:\n\n<CodeGroup>",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</CodeGroup>\n\nThe response looks like this:",
      "language": "unknown"
    },
    {
      "code": "The simulation first generates an initial query from the simulated `user`, then passes response chat messages back and forth until it reaches `max_turns` (you can alternatively pass a `stopping_condition` that takes the current trajectory and returns `True` or `False` - [see the OpenEvals README for more information](https://github.com/langchain-ai/openevals?tab=readme-ov-file#multiturn-simulation)). The return value is the final list of chat messages that make up the converation's **trajectory**.\n\n<Info>\n  There are several ways to configure the simulated user, such as having it return fixed responses for the first turns of your simulation, as well as the simulation as a whole. For full details, check out [the OpenEvals README](https://github.com/langchain-ai/openevals?tab=readme-ov-file#multiturn-simulation).\n</Info>\n\nThe final trace will look something [like this](https://smith.langchain.com/public/648ca37d-1c4d-4f7b-9b6a-89e35dc5d4f0/r) with responses from your `app` and `user` interleaved:\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=c903f388600ab575e70edb92209c6b2e\" alt=\"Multi turn trace\" data-og-width=\"2952\" width=\"2952\" data-og-height=\"1790\" height=\"1790\" data-path=\"langsmith/images/multi-turn-trace.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=a5113736dc83834150a2b414619626b2 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=f0bfc1f1764c80efcdebfcc07149ef8a 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=53e53c0f05f5638b5e17576c0f37d195 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=dc05ed0dcfe9d6da5b8872df189f6ed9 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=7fbd598ec4a113bb143d0a0a1ca68a91 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=a9dcabf49b84ba2ad8aac4159b5b0657 2500w\" />\n\nCongrats! You just ran your first multi-turn simulation. Next, we'll cover how to run it in a LangSmith experiment.\n\n## Running in LangSmith experiments\n\nYou can use the results of multi-turn simulations as part of a LangSmith experiment to track performance and progress over time. For these sections, it helps to be familiar with at least one of LangSmith's [`pytest`](/langsmith/pytest) (Python-only), [`Vitest`/`Jest`](/langsmith/vitest-jest) (JS only), or [`evaluate`](/langsmith/evaluate-llm-application) runners.\n\n### Using `pytest` or `Vitest/Jest`\n\n<Check>\n  See the following guides to learn how to set up evals using LangSmith's integrations with test frameworks:\n\n  * [`pytest`](https://docs.smith.langchain.com/langsmith/pytest)\n  * [`Vitest` or `Jest`](https://docs.smith.langchain.com/langsmith/vitest-jest)\n</Check>\n\nIf you are using one of the [LangSmith test framework integrations](/langsmith/pytest), you can pass in an array of OpenEvals evaluators as a `trajectory_evaluators` param when running the simulation. These evaluators will run at the end of the simulation, taking the final list of chat messages as an `outputs` kwarg. Your passed `trajectory_evaluator` must therefore accept this kwarg.\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-vitest.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=04c56e67e7bb9f01cb905d8a184d62d5\" alt=\"Multi turn vitest\" data-og-width=\"3448\" width=\"3448\" data-og-height=\"1128\" height=\"1128\" data-path=\"langsmith/images/multi-turn-vitest.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-vitest.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=41ac969c6ceb99ac0976ab3027b00e89 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-vitest.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=c10eece031225173dc0ded446e3e2e3c 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-vitest.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=90bda951b2cfa02bde0c8ad204a7dac7 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-vitest.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=2ca95c911f68412eb09e2f8a0a6b42e4 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-vitest.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=12be96d7f216cd8ce664c01f61f45288 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-vitest.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=1faa3a34b3daf67705e2afeca748e353 2500w\" />\n\nHere's an example:\n\n<CodeGroup>",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</CodeGroup>\n\nLangSmith will automatically detect and log the feedback returned from the passed `trajectory_evaluators`, adding it to the experiment. Note also that the test case uses the `fixed_responses` param on the simulated user to start the conversation with a specific input, which you can log and make part of your stored dataset.\n\nYou may also find it convenient to have the simulated user's system prompt to be part of your logged dataset as well.\n\n### Using `evaluate`\n\nYou can also use the [`evaluate`](/langsmith/evaluate-llm-application) runner to evaluate simulated multi-turn interactions. This will be a little bit different from the `pytest`/`Vitest`/`Jest` example in the following ways:\n\n* The simulation should be part of your `target` function, and your target function should return the final trajectory.\n  * This will make the trajectory the `outputs` that LangSmith will pass to your evaluators.\n* Instead of using the `trajectory_evaluators` param, you should pass your evaluators as a param into the `evaluate()` method.\n* You will need an existing dataset of inputs and (optionally) reference trajectories.\n\nHere's an example:\n\n<CodeGroup>",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</CodeGroup>\n\n## Modifying the simulated user persona\n\nThe above examples run using the same simulated user persona for all input examples, defined by the `system` parameter passed into `create_llm_simulated_user`. If you would like to use a different persona for specific items in your dataset, you can update your dataset examples to also contain an extra field with the desired `system` prompt, then pass that field in when creating your simulated user like this:\n\n<CodeGroup>",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": "Setup",
      "id": "setup"
    },
    {
      "level": "h2",
      "text": "Running a simulation",
      "id": "running-a-simulation"
    },
    {
      "level": "h2",
      "text": "Running in LangSmith experiments",
      "id": "running-in-langsmith-experiments"
    },
    {
      "level": "h3",
      "text": "Using `pytest` or `Vitest/Jest`",
      "id": "using-`pytest`-or-`vitest/jest`"
    },
    {
      "level": "h3",
      "text": "Using `evaluate`",
      "id": "using-`evaluate`"
    },
    {
      "level": "h2",
      "text": "Modifying the simulated user persona",
      "id": "modifying-the-simulated-user-persona"
    },
    {
      "level": "h2",
      "text": "Next steps",
      "id": "next-steps"
    }
  ],
  "url": "llms-txt#how-to-simulate-multi-turn-interactions",
  "links": []
}