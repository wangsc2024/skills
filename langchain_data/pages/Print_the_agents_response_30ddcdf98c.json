{
  "title": "Print the agent's response",
  "content": "print(result[\"messages\"][-1].content)\n```\n\nYour deep agent automatically:\n\n1. **Planned its approach**: Used the built-in `write_todos` tool to break down the research task\n2. **Conducted research**: Called the `internet_search` tool to gather information\n3. **Managed context**: Used file system tools (`write_file`, `read_file`) to offload large search results\n4. **Spawned subagents** (if needed): Delegated complex subtasks to specialized subagents\n5. **Synthesized a report**: Compiled findings into a coherent response\n\nNow that you've built your first deep agent:\n\n* **Customize your agent**: Learn about [customization options](/oss/python/deepagents/customization), including custom system prompts, tools, and subagents.\n* **Understand middleware**: Dive into the [middleware architecture](/oss/python/deepagents/middleware) that powers deep agents.\n* **Add long-term memory**: Enable [persistent memory](/oss/python/deepagents/long-term-memory) across conversations.\n* **Deploy to production**: Learn about [deployment options](/oss/python/langgraph/deploy) for LangGraph applications.\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/deepagents/quickstart.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "code_samples": [],
  "headings": [
    {
      "level": "h2",
      "text": "What happened?",
      "id": "what-happened?"
    },
    {
      "level": "h2",
      "text": "Next steps",
      "id": "next-steps"
    }
  ],
  "url": "llms-txt#print-the-agent's-response",
  "links": []
}