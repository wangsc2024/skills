{
  "title": "[{\"type\": \"text\", \"text\": \"The sky is typically blue...\"}]",
  "content": "python  theme={null}\n    async for event in model.astream_events(\"Hello\"):\n\nif event[\"event\"] == \"on_chat_model_start\":\n            print(f\"Input: {event['data']['input']}\")\n\nelif event[\"event\"] == \"on_chat_model_stream\":\n            print(f\"Token: {event['data']['chunk'].text}\")\n\nelif event[\"event\"] == \"on_chat_model_end\":\n            print(f\"Full message: {event['data']['output'].text}\")\n\nelse:\n            pass\n    txt  theme={null}\n    Input: Hello\n    Token: Hi\n    Token:  there\n    Token: !\n    Token:  How\n    Token:  can\n    Token:  I\n    ...\n    Full message: Hi there! How can I help today?\n    python Batch theme={null}\nresponses = model.batch([\n    \"Why do parrots have colorful feathers?\",\n    \"How do airplanes fly?\",\n    \"What is quantum computing?\"\n])\nfor response in responses:\n    print(response)\npython Yield batch responses upon completion theme={null}\nfor response in model.batch_as_completed([\n    \"Why do parrots have colorful feathers?\",\n    \"How do airplanes fly?\",\n    \"What is quantum computing?\"\n]):\n    print(response)\npython Batch with max concurrency theme={null}\n  model.batch(\n      list_of_inputs,\n      config={\n          'max_concurrency': 5,  # Limit to 5 parallel calls\n      }\n  )\n  mermaid  theme={null}\nsequenceDiagram\n    participant U as User\n    participant M as Model\n    participant T as Tools\n\nU->>M: \"What's the weather in SF and NYC?\"\n    M->>M: Analyze request & decide tools needed\n\npar Parallel Tool Calls\n        M->>T: get_weather(\"San Francisco\")\n        M->>T: get_weather(\"New York\")\n    end\n\npar Tool Execution\n        T-->>M: SF weather data\n        T-->>M: NYC weather data\n    end\n\nM->>M: Process results & generate response\n    M->>U: \"SF: 72°F sunny, NYC: 68°F cloudy\"\npython Binding user tools theme={null}\nfrom langchain.tools import tool\n\n@tool\ndef get_weather(location: str) -> str:\n    \"\"\"Get the weather at a location.\"\"\"\n    return f\"It's sunny in {location}.\"\n\nmodel_with_tools = model.bind_tools([get_weather])  # [!code highlight]\n\nresponse = model_with_tools.invoke(\"What's the weather like in Boston?\")\nfor tool_call in response.tool_calls:\n    # View tool calls made by the model\n    print(f\"Tool: {tool_call['name']}\")\n    print(f\"Args: {tool_call['args']}\")\npython Tool execution loop theme={null}\n    # Bind (potentially multiple) tools to the model\n    model_with_tools = model.bind_tools([get_weather])\n\n# Step 1: Model generates tool calls\n    messages = [{\"role\": \"user\", \"content\": \"What's the weather in Boston?\"}]\n    ai_msg = model_with_tools.invoke(messages)\n    messages.append(ai_msg)\n\n# Step 2: Execute tools and collect results\n    for tool_call in ai_msg.tool_calls:\n        # Execute the tool with the generated arguments\n        tool_result = get_weather.invoke(tool_call)\n        messages.append(tool_result)\n\n# Step 3: Pass results back to model for final response\n    final_response = model_with_tools.invoke(messages)\n    print(final_response.text)\n    # \"The current weather in Boston is 72°F and sunny.\"\n    python Force use of any tool theme={null}\n      model_with_tools = model.bind_tools([tool_1], tool_choice=\"any\")\n      python Force use of specific tools theme={null}\n      model_with_tools = model.bind_tools([tool_1], tool_choice=\"tool_1\")\n      python Parallel tool calls theme={null}\n    model_with_tools = model.bind_tools([get_weather])\n\nresponse = model_with_tools.invoke(\n        \"What's the weather in Boston and Tokyo?\"\n    )\n\n# The model may generate multiple tool calls\n    print(response.tool_calls)\n    # [\n    #   {'name': 'get_weather', 'args': {'location': 'Boston'}, 'id': 'call_1'},\n    #   {'name': 'get_weather', 'args': {'location': 'Tokyo'}, 'id': 'call_2'},\n    # ]\n\n# Execute all tools (can be done in parallel with async)\n    results = []\n    for tool_call in response.tool_calls:\n        if tool_call['name'] == 'get_weather':\n            result = get_weather.invoke(tool_call)\n        ...\n        results.append(result)\n    python  theme={null}\n      model.bind_tools([get_weather], parallel_tool_calls=False)\n      python Streaming tool calls theme={null}\n    for chunk in model_with_tools.stream(\n        \"What's the weather in Boston and Tokyo?\"\n    ):\n        # Tool call chunks arrive progressively\n        for tool_chunk in chunk.tool_call_chunks:\n            if name := tool_chunk.get(\"name\"):\n                print(f\"Tool: {name}\")\n            if id_ := tool_chunk.get(\"id\"):\n                print(f\"ID: {id_}\")\n            if args := tool_chunk.get(\"args\"):\n                print(f\"Args: {args}\")\n\n# Output:\n    # Tool: get_weather\n    # ID: call_SvMlU1TVIZugrFLckFE2ceRE\n    # Args: {\"lo\n    # Args: catio\n    # Args: n\": \"B\n    # Args: osto\n    # Args: n\"}\n    # Tool: get_weather\n    # ID: call_QMZdy6qInx13oWKE7KhuhOLR\n    # Args: {\"lo\n    # Args: catio\n    # Args: n\": \"T\n    # Args: okyo\n    # Args: \"}\n    python Accumulate tool calls theme={null}\n    gathered = None\n    for chunk in model_with_tools.stream(\"What's the weather in Boston?\"):\n        gathered = chunk if gathered is None else gathered + chunk\n        print(gathered.tool_calls)\n    python  theme={null}\n    from pydantic import BaseModel, Field\n\nclass Movie(BaseModel):\n        \"\"\"A movie with details.\"\"\"\n        title: str = Field(..., description=\"The title of the movie\")\n        year: int = Field(..., description=\"The year the movie was released\")\n        director: str = Field(..., description=\"The director of the movie\")\n        rating: float = Field(..., description=\"The movie's rating out of 10\")\n\nmodel_with_structure = model.with_structured_output(Movie)\n    response = model_with_structure.invoke(\"Provide details about the movie Inception\")\n    print(response)  # Movie(title=\"Inception\", year=2010, director=\"Christopher Nolan\", rating=8.8)\n    python  theme={null}\n    from typing_extensions import TypedDict, Annotated\n\nclass MovieDict(TypedDict):\n        \"\"\"A movie with details.\"\"\"\n        title: Annotated[str, ..., \"The title of the movie\"]\n        year: Annotated[int, ..., \"The year the movie was released\"]\n        director: Annotated[str, ..., \"The director of the movie\"]\n        rating: Annotated[float, ..., \"The movie's rating out of 10\"]\n\nmodel_with_structure = model.with_structured_output(MovieDict)\n    response = model_with_structure.invoke(\"Provide details about the movie Inception\")\n    print(response)  # {'title': 'Inception', 'year': 2010, 'director': 'Christopher Nolan', 'rating': 8.8}\n    python  theme={null}\n    import json\n\njson_schema = {\n        \"title\": \"Movie\",\n        \"description\": \"A movie with details\",\n        \"type\": \"object\",\n        \"properties\": {\n            \"title\": {\n                \"type\": \"string\",\n                \"description\": \"The title of the movie\"\n            },\n            \"year\": {\n                \"type\": \"integer\",\n                \"description\": \"The year the movie was released\"\n            },\n            \"director\": {\n                \"type\": \"string\",\n                \"description\": \"The director of the movie\"\n            },\n            \"rating\": {\n                \"type\": \"number\",\n                \"description\": \"The movie's rating out of 10\"\n            }\n        },\n        \"required\": [\"title\", \"year\", \"director\", \"rating\"]\n    }\n\nmodel_with_structure = model.with_structured_output(\n        json_schema,\n        method=\"json_schema\",\n    )\n    response = model_with_structure.invoke(\"Provide details about the movie Inception\")\n    print(response)  # {'title': 'Inception', 'year': 2010, ...}\n    python  theme={null}\n  from pydantic import BaseModel, Field\n\nclass Movie(BaseModel):\n      \"\"\"A movie with details.\"\"\"\n      title: str = Field(..., description=\"The title of the movie\")\n      year: int = Field(..., description=\"The year the movie was released\")\n      director: str = Field(..., description=\"The director of the movie\")\n      rating: float = Field(..., description=\"The movie's rating out of 10\")\n\nmodel_with_structure = model.with_structured_output(Movie, include_raw=True)  # [!code highlight]\n  response = model_with_structure.invoke(\"Provide details about the movie Inception\")\n  response\n  # {\n  #     \"raw\": AIMessage(...),\n  #     \"parsed\": Movie(title=..., year=..., ...),\n  #     \"parsing_error\": None,\n  # }\n  python Pydantic BaseModel theme={null}\n    from pydantic import BaseModel, Field\n\nclass Actor(BaseModel):\n        name: str\n        role: str\n\nclass MovieDetails(BaseModel):\n        title: str\n        year: int\n        cast: list[Actor]\n        genres: list[str]\n        budget: float | None = Field(None, description=\"Budget in millions USD\")\n\nmodel_with_structure = model.with_structured_output(MovieDetails)\n    python TypedDict theme={null}\n    from typing_extensions import Annotated, TypedDict\n\nclass Actor(TypedDict):\n        name: str\n        role: str\n\nclass MovieDetails(TypedDict):\n        title: str\n        year: int\n        cast: list[Actor]\n        genres: list[str]\n        budget: Annotated[float | None, ..., \"Budget in millions USD\"]\n\nmodel_with_structure = model.with_structured_output(MovieDetails)\n    python  theme={null}\nmodel.profile",
  "code_samples": [
    {
      "code": "The resulting message can be treated the same as a message that was generated with [`invoke()`](#invoke) – for example, it can be aggregated into a message history and passed back to the model as conversational context.\n\n<Warning>\n  Streaming only works if all steps in the program know how to process a stream of chunks. For instance, an application that isn't streaming-capable would be one that needs to store the entire output in memory before it can be processed.\n</Warning>\n\n<Accordion title=\"Advanced streaming topics\">\n  <Accordion title=\"Streaming events\">\n    LangChain chat models can also stream semantic events using `astream_events()`.\n\n    This simplifies filtering based on event types and other metadata, and will aggregate the full message in the background. See below for an example.",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "<Tip>\n      See the [`astream_events()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.astream_events) reference for event types and other details.\n    </Tip>\n  </Accordion>\n\n  <Accordion title=\"&#x22;Auto-streaming&#x22; chat models\">\n    LangChain simplifies streaming from chat models by automatically enabling streaming mode in certain cases, even when you're not explicitly calling the streaming methods. This is particularly useful when you use the non-streaming invoke method but still want to stream the entire application, including intermediate results from the chat model.\n\n    In [LangGraph agents](/oss/python/langchain/agents), for example, you can call `model.invoke()` within nodes, but LangChain will automatically delegate to streaming if running in a streaming mode.\n\n    #### How it works\n\n    When you `invoke()` a chat model, LangChain will automatically switch to an internal streaming mode if it detects that you are trying to stream the overall application. The result of the invocation will be the same as far as the code that was using invoke is concerned; however, while the chat model is being streamed, LangChain will take care of invoking [`on_llm_new_token`](https://reference.langchain.com/python/langchain_core/callbacks/#langchain_core.callbacks.base.AsyncCallbackHandler.on_llm_new_token) events in LangChain's callback system.\n\n    Callback events allow LangGraph `stream()` and `astream_events()` to surface the chat model's output in real-time.\n  </Accordion>\n</Accordion>\n\n### Batch\n\nBatching a collection of independent requests to a model can significantly improve performance and reduce costs, as the processing can be done in parallel:",
      "language": "unknown"
    },
    {
      "code": "<Note>\n  This section describes a chat model method [`batch()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.batch), which parallelizes model calls client-side.\n\n  It is **distinct** from batch APIs supported by inference providers, such as [OpenAI](https://platform.openai.com/docs/guides/batch) or [Anthropic](https://platform.claude.com/docs/en/build-with-claude/batch-processing#message-batches-api).\n</Note>\n\nBy default, [`batch()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.batch) will only return the final output for the entire batch. If you want to receive the output for each individual input as it finishes generating, you can stream results with [`batch_as_completed()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.batch_as_completed):",
      "language": "unknown"
    },
    {
      "code": "<Note>\n  When using [`batch_as_completed()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.batch_as_completed), results may arrive out of order. Each includes the input index for matching to reconstruct the original order as needed.\n</Note>\n\n<Tip>\n  When processing a large number of inputs using [`batch()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.batch) or [`batch_as_completed()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.batch_as_completed), you may want to control the maximum number of parallel calls. This can be done by setting the [`max_concurrency`](https://reference.langchain.com/python/langchain_core/runnables/#langchain_core.runnables.RunnableConfig.max_concurrency) attribute in the [`RunnableConfig`](https://reference.langchain.com/python/langchain_core/runnables/#langchain_core.runnables.RunnableConfig) dictionary.",
      "language": "unknown"
    },
    {
      "code": "See the [`RunnableConfig`](https://reference.langchain.com/python/langchain_core/runnables/#langchain_core.runnables.RunnableConfig) reference for a full list of supported attributes.\n</Tip>\n\nFor more details on batching, see the [reference](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.batch).\n\n***\n\n## Tool calling\n\nModels can request to call tools that perform tasks such as fetching data from a database, searching the web, or running code. Tools are pairings of:\n\n1. A schema, including the name of the tool, a description, and/or argument definitions (often a JSON schema)\n2. A function or <Tooltip tip=\"A method that can suspend execution and resume at a later time\">coroutine</Tooltip> to execute.\n\n<Note>\n  You may hear the term \"function calling\". We use this interchangeably with \"tool calling\".\n</Note>\n\nHere's the basic tool calling flow between a user and a model:",
      "language": "unknown"
    },
    {
      "code": "To make tools that you have defined available for use by a model, you must bind them using [`bind_tools`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.bind_tools). In subsequent invocations, the model can choose to call any of the bound tools as needed.\n\nSome model providers offer <Tooltip tip=\"Tools that are executed server-side, such as web search and code interpreters\">built-in tools</Tooltip> that can be enabled via model or invocation parameters (e.g. [`ChatOpenAI`](/oss/python/integrations/chat/openai), [`ChatAnthropic`](/oss/python/integrations/chat/anthropic)). Check the respective [provider reference](/oss/python/integrations/providers/overview) for details.\n\n<Tip>\n  See the [tools guide](/oss/python/langchain/tools) for details and other options for creating tools.\n</Tip>",
      "language": "unknown"
    },
    {
      "code": "When binding user-defined tools, the model's response includes a **request** to execute a tool. When using a model separately from an [agent](/oss/python/langchain/agents), it is up to you to execute the requested tool and return the result back to the model for use in subsequent reasoning. When using an [agent](/oss/python/langchain/agents), the agent loop will handle the tool execution loop for you.\n\nBelow, we show some common ways you can use tool calling.\n\n<AccordionGroup>\n  <Accordion title=\"Tool execution loop\" icon=\"arrow-rotate-right\">\n    When a model returns tool calls, you need to execute the tools and pass the results back to the model. This creates a conversation loop where the model can use tool results to generate its final response. LangChain includes [agent](/oss/python/langchain/agents) abstractions that handle this orchestration for you.\n\n    Here's a simple example of how to do this:",
      "language": "unknown"
    },
    {
      "code": "Each [`ToolMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.ToolMessage) returned by the tool includes a `tool_call_id` that matches the original tool call, helping the model correlate results with requests.\n  </Accordion>\n\n  <Accordion title=\"Forcing tool calls\" icon=\"asterisk\">\n    By default, the model has the freedom to choose which bound tool to use based on the user's input. However, you might want to force choosing a tool, ensuring the model uses either a particular tool or **any** tool from a given list:\n\n    <CodeGroup>",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</CodeGroup>\n  </Accordion>\n\n  <Accordion title=\"Parallel tool calls\" icon=\"layer-group\">\n    Many models support calling multiple tools in parallel when appropriate. This allows the model to gather information from different sources simultaneously.",
      "language": "unknown"
    },
    {
      "code": "The model intelligently determines when parallel execution is appropriate based on the independence of the requested operations.\n\n    <Tip>\n      Most models supporting tool calling enable parallel tool calls by default. Some (including [OpenAI](/oss/python/integrations/chat/openai) and [Anthropic](/oss/python/integrations/chat/anthropic)) allow you to disable this feature. To do this, set `parallel_tool_calls=False`:",
      "language": "unknown"
    },
    {
      "code": "</Tip>\n  </Accordion>\n\n  <Accordion title=\"Streaming tool calls\" icon=\"rss\">\n    When streaming responses, tool calls are progressively built through [`ToolCallChunk`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.ToolCallChunk). This allows you to see tool calls as they're being generated rather than waiting for the complete response.",
      "language": "unknown"
    },
    {
      "code": "You can accumulate chunks to build complete tool calls:",
      "language": "unknown"
    },
    {
      "code": "</Accordion>\n</AccordionGroup>\n\n***\n\n## Structured output\n\nModels can be requested to provide their response in a format matching a given schema. This is useful for ensuring the output can be easily parsed and used in subsequent processing. LangChain supports multiple schema types and methods for enforcing structured output.\n\n<Tabs>\n  <Tab title=\"Pydantic\">\n    [Pydantic models](https://docs.pydantic.dev/latest/concepts/models/#basic-model-usage) provide the richest feature set with field validation, descriptions, and nested structures.",
      "language": "unknown"
    },
    {
      "code": "</Tab>\n\n  <Tab title=\"TypedDict\">\n    `TypedDict` provides a simpler alternative using Python's built-in typing, ideal when you don't need runtime validation.",
      "language": "unknown"
    },
    {
      "code": "</Tab>\n\n  <Tab title=\"JSON Schema\">\n    For maximum control or interoperability, you can provide a raw JSON Schema.",
      "language": "unknown"
    },
    {
      "code": "</Tab>\n</Tabs>\n\n<Note>\n  **Key considerations for structured output:**\n\n  * **Method parameter**: Some providers support different methods (`'json_schema'`, `'function_calling'`, `'json_mode'`)\n    * `'json_schema'` typically refers to dedicated structured output features offered by a provider\n    * `'function_calling'` derives structured output by forcing a [tool call](#tool-calling) following the given schema\n    * `'json_mode'` is a precursor to `'json_schema'` offered by some providers - it generates valid json, but the schema must be described in the prompt\n  * **Include raw**: Use `include_raw=True` to get both the parsed output and the raw AI message\n  * **Validation**: Pydantic models provide automatic validation, while `TypedDict` and JSON Schema require manual validation\n</Note>\n\n<Accordion title=\"Example: Message output alongside parsed structure\">\n  It can be useful to return the raw [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) object alongside the parsed representation to access response metadata such as [token counts](#token-usage). To do this, set [`include_raw=True`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.with_structured_output\\(include_raw\\)) when calling [`with_structured_output`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.with_structured_output):",
      "language": "unknown"
    },
    {
      "code": "</Accordion>\n\n<Accordion title=\"Example: Nested structures\">\n  Schemas can be nested:\n\n  <CodeGroup>",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</CodeGroup>\n</Accordion>\n\n***\n\n## Supported models\n\nLangChain supports all major model providers, including OpenAI, Anthropic, Google, Azure, AWS Bedrock, and more. Each provider offers a variety of models with different capabilities. For a full list of supported models in LangChain, see the [integrations page](/oss/python/integrations/providers/overview).\n\n***\n\n## Advanced topics\n\n### Model profiles\n\n<Warning> This is a beta feature. The format of model profiles is subject to change. </Warning>\n\n<Info> Model profiles require `langchain>=1.1`. </Info>\n\nLangChain chat models can expose a dictionary of supported features and capabilities through a `.profile` attribute:",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h3",
      "text": "Batch",
      "id": "batch"
    },
    {
      "level": "h2",
      "text": "Tool calling",
      "id": "tool-calling"
    },
    {
      "level": "h2",
      "text": "Structured output",
      "id": "structured-output"
    },
    {
      "level": "h2",
      "text": "Supported models",
      "id": "supported-models"
    },
    {
      "level": "h2",
      "text": "Advanced topics",
      "id": "advanced-topics"
    },
    {
      "level": "h3",
      "text": "Model profiles",
      "id": "model-profiles"
    }
  ],
  "url": "llms-txt#[{\"type\":-\"text\",-\"text\":-\"the-sky-is-typically-blue...\"}]",
  "links": []
}