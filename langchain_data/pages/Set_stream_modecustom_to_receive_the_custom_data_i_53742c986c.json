{
  "title": "Set stream_mode=\"custom\" to receive the custom data in the stream",
  "content": "for chunk in graph.stream(\n    {\"topic\": \"cats\"},\n    stream_mode=\"custom\",  # [!code highlight]\n\n):\n    # The chunk will contain the custom data streamed from the llm\n    print(chunk)\npython  theme={null}\n  import operator\n  import json\n\nfrom typing import TypedDict\n  from typing_extensions import Annotated\n  from langgraph.graph import StateGraph, START\n\nfrom openai import AsyncOpenAI\n\nopenai_client = AsyncOpenAI()\n  model_name = \"gpt-4o-mini\"\n\nasync def stream_tokens(model_name: str, messages: list[dict]):\n      response = await openai_client.chat.completions.create(\n          messages=messages, model=model_name, stream=True\n      )\n      role = None\n      async for chunk in response:\n          delta = chunk.choices[0].delta\n\nif delta.role is not None:\n              role = delta.role\n\nif delta.content:\n              yield {\"role\": role, \"content\": delta.content}\n\n# this is our tool\n  async def get_items(place: str) -> str:\n      \"\"\"Use this tool to list items one might find in a place you're asked about.\"\"\"\n      writer = get_stream_writer()\n      response = \"\"\n      async for msg_chunk in stream_tokens(\n          model_name,\n          [\n              {\n                  \"role\": \"user\",\n                  \"content\": (\n                      \"Can you tell me what kind of items \"\n                      f\"i might find in the following place: '{place}'. \"\n                      \"List at least 3 such items separating them by a comma. \"\n                      \"And include a brief description of each item.\"\n                  ),\n              }\n          ],\n      ):\n          response += msg_chunk[\"content\"]\n          writer(msg_chunk)\n\nclass State(TypedDict):\n      messages: Annotated[list[dict], operator.add]\n\n# this is the tool-calling graph node\n  async def call_tool(state: State):\n      ai_message = state[\"messages\"][-1]\n      tool_call = ai_message[\"tool_calls\"][-1]\n\nfunction_name = tool_call[\"function\"][\"name\"]\n      if function_name != \"get_items\":\n          raise ValueError(f\"Tool {function_name} not supported\")\n\nfunction_arguments = tool_call[\"function\"][\"arguments\"]\n      arguments = json.loads(function_arguments)\n\nfunction_response = await get_items(**arguments)\n      tool_message = {\n          \"tool_call_id\": tool_call[\"id\"],\n          \"role\": \"tool\",\n          \"name\": function_name,\n          \"content\": function_response,\n      }\n      return {\"messages\": [tool_message]}\n\ngraph = (\n      StateGraph(State)\n      .add_node(call_tool)\n      .add_edge(START, \"call_tool\")\n      .compile()\n  )\n  python  theme={null}\n  inputs = {\n      \"messages\": [\n          {\n              \"content\": None,\n              \"role\": \"assistant\",\n              \"tool_calls\": [\n                  {\n                      \"id\": \"1\",\n                      \"function\": {\n                          \"arguments\": '{\"place\":\"bedroom\"}',\n                          \"name\": \"get_items\",\n                      },\n                      \"type\": \"function\",\n                  }\n              ],\n          }\n      ]\n  }\n\nasync for chunk in graph.astream(\n      inputs,\n      stream_mode=\"custom\",\n  ):\n      print(chunk[\"content\"], end=\"|\", flush=True)\n  python  theme={null}\n    from langchain.chat_models import init_chat_model\n\nmodel = init_chat_model(\n        \"claude-sonnet-4-5-20250929\",\n        # Set streaming=False to disable streaming for the chat model\n        streaming=False  # [!code highlight]\n    )\n    python  theme={null}\n    from langchain_openai import ChatOpenAI\n\n# Set streaming=False to disable streaming for the chat model\n    model = ChatOpenAI(model=\"o1-preview\", streaming=False)\n    python  theme={null}\n  from typing import TypedDict\n  from langgraph.graph import START, StateGraph\n  from langchain.chat_models import init_chat_model\n\nmodel = init_chat_model(model=\"gpt-4o-mini\")\n\nclass State(TypedDict):\n      topic: str\n      joke: str\n\n# Accept config as an argument in the async node function\n  async def call_model(state, config):\n      topic = state[\"topic\"]\n      print(\"Generating joke...\")\n      # Pass config to model.ainvoke() to ensure proper context propagation\n      joke_response = await model.ainvoke(  # [!code highlight]\n          [{\"role\": \"user\", \"content\": f\"Write a joke about {topic}\"}],\n          config,\n      )\n      return {\"joke\": joke_response.content}\n\ngraph = (\n      StateGraph(State)\n      .add_node(call_model)\n      .add_edge(START, \"call_model\")\n      .compile()\n  )\n\n# Set stream_mode=\"messages\" to stream LLM tokens\n  async for chunk, metadata in graph.astream(\n      {\"topic\": \"ice cream\"},\n      stream_mode=\"messages\",  # [!code highlight]\n  ):\n      if chunk.content:\n          print(chunk.content, end=\"|\", flush=True)\n  python  theme={null}\n  from typing import TypedDict\n  from langgraph.types import StreamWriter\n\nclass State(TypedDict):\n        topic: str\n        joke: str\n\n# Add writer as an argument in the function signature of the async node or tool\n  # LangGraph will automatically pass the stream writer to the function\n  async def generate_joke(state: State, writer: StreamWriter):  # [!code highlight]\n        writer({\"custom_key\": \"Streaming custom data while generating a joke\"})\n        return {\"joke\": f\"This is a joke about {state['topic']}\"}\n\ngraph = (\n        StateGraph(State)\n        .add_node(generate_joke)\n        .add_edge(START, \"generate_joke\")\n        .compile()\n  )\n\n# Set stream_mode=\"custom\" to receive the custom data in the stream  # [!code highlight]\n  async for chunk in graph.astream(\n        {\"topic\": \"ice cream\"},\n        stream_mode=\"custom\",\n  ):\n        print(chunk)\n  ```\n</Accordion>\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/streaming.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "code_samples": [
    {
      "code": "<Accordion title=\"Extended example: streaming arbitrary chat model\">",
      "language": "unknown"
    },
    {
      "code": "Let's invoke the graph with an [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) that includes a tool call:",
      "language": "unknown"
    },
    {
      "code": "</Accordion>\n\n## Disable streaming for specific chat models\n\nIf your application mixes models that support streaming with those that do not, you may need to explicitly disable streaming for\nmodels that do not support it.\n\nSet `streaming=False` when initializing the model.\n\n<Tabs>\n  <Tab title=\"init_chat_model\">",
      "language": "unknown"
    },
    {
      "code": "</Tab>\n\n  <Tab title=\"Chat model interface\">",
      "language": "unknown"
    },
    {
      "code": "</Tab>\n</Tabs>\n\n<Note>\n  Not all chat model integrations support the `streaming` parameter. If your model doesn't support it, use `disable_streaming=True` instead. This parameter is available on all chat models via the base class.\n</Note>\n\n<a id=\"async\" />\n\n### Async with Python \\< 3.11\n\nIn Python versions \\< 3.11, [asyncio tasks](https://docs.python.org/3/library/asyncio-task.html#asyncio.create_task) do not support the `context` parameter.\nThis limits LangGraph ability to automatically propagate context, and affects LangGraph's streaming mechanisms in two key ways:\n\n1. You **must** explicitly pass [`RunnableConfig`](https://python.langchain.com/docs/concepts/runnables/#runnableconfig) into async LLM calls (e.g., `ainvoke()`), as callbacks are not automatically propagated.\n2. You **cannot** use [`get_stream_writer`](https://reference.langchain.com/python/langgraph/config/#langgraph.config.get_stream_writer) in async nodes or tools â€” you must pass a `writer` argument directly.\n\n<Accordion title=\"Extended example: async LLM call with manual config\">",
      "language": "unknown"
    },
    {
      "code": "</Accordion>\n\n<Accordion title=\"Extended example: async custom streaming with stream writer\">",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": "Disable streaming for specific chat models",
      "id": "disable-streaming-for-specific-chat-models"
    },
    {
      "level": "h3",
      "text": "Async with Python \\< 3.11",
      "id": "async-with-python-\\<-3.11"
    }
  ],
  "url": "llms-txt#set-stream_mode=\"custom\"-to-receive-the-custom-data-in-the-stream",
  "links": []
}