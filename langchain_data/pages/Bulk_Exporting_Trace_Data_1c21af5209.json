{
  "title": "Bulk Exporting Trace Data",
  "content": "Source: https://docs.langchain.com/langsmith/data-export\n\n<Info>\n  **Plan restrictions apply**\n\nPlease note that the Data Export functionality is only supported for [LangSmith Plus or Enterprise tiers](https://www.langchain.com/pricing-langsmith).\n</Info>\n\nLangSmith's bulk data export functionality allows you to export your traces into an external destination. This can be useful if you want to analyze the\ndata offline in a tool such as BigQuery, Snowflake, RedShift, Jupyter Notebooks, etc.\n\nAn export can be launched to target a specific LangSmith project and date range. Once a batch export is launched, our system will handle the orchestration and resilience of the export process.\nPlease note that exporting your data may take some time depending on the size of your data. We also have a limit on how many of your exports can run at the same time.\nBulk exports also have a runtime timeout of 24 hours.\n\nCurrently we support exporting to an S3 bucket or S3 API compatible bucket that you provide. The data will be exported in\n[Parquet](https://parquet.apache.org/docs/overview/) columnar format. This format will allow you to easily import the data into\nother systems. The data export will contain equivalent data fields as the [Run data format](/langsmith/run-data-format).\n\n### Destinations - Providing a S3 bucket\n\nTo export LangSmith data, you will need to provide an S3 bucket where the data will be exported to.\n\nThe following information is needed for the export:\n\n* **Bucket Name**: The name of the S3 bucket where the data will be exported to.\n* **Prefix**: The root prefix within the bucket where the data will be exported to.\n* **S3 Region**: The region of the bucket - this is needed for AWS S3 buckets.\n* **Endpoint URL**: The endpoint URL for the S3 bucket - this is needed for S3 API compatible buckets.\n* **Access Key**: The access key for the S3 bucket.\n* **Secret Key**: The secret key for the S3 bucket.\n* **Include Bucket in Prefix** (optional): Whether to include the bucket name as part of the path prefix. Defaults to `false` for new destinations or when the bucket name is already present in the path. Set to `true` for legacy compatibility or when using storage systems that require the bucket name in the path.\n\nWe support any S3 compatible bucket, for non AWS buckets such as GCS or MinIO, you will need to provide the endpoint URL.\n\n### Preparing the Destination\n\n<Note>\n  **For self-hosted and EU region deployments**\n\nUpdate the LangSmith URL appropriately for self-hosted installations or organizations in the EU region in the requests below.\n  For the EU region, use `eu.api.smith.langchain.com`.\n</Note>\n\n<Note>\n  **Permissions required**\n\nBoth the `backend` and `queue` services require write access to the destination bucket:\n\n* The `backend` service attempts to write a test file to the destination bucket when the export destination is created.\n    It will delete the test file if it has permission to do so (delete access is optional).\n  * The `queue` service is responsible for bulk export execution and uploading the files to the bucket.\n</Note>\n\nThe following example demonstrates how to create a destination using cURL. Replace the placeholder values with your actual configuration details.\nNote that credentials will be stored securely in an encrypted form in our system.\n\nUse the returned `id` to reference this destination in subsequent bulk export operations.\n\n**If you receive an error while creating a destination, see [debug destination errors](#debugging-destination-errors) for details on how to debug this.**\n\n#### Credentials configuration\n\n<Note>**Requires LangSmith Helm version >= `0.10.34` (application version >= `0.10.91`)**</Note>\n\nWe support the following additional credentials formats besides static `access_key_id` and `secret_access_key`:\n\n* To use [temporary credentials](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp_use-resources.html) that include an AWS session token,\n  additionally provide the `credentials.session_token` key when creating the bulk export destination.\n* (Self-hosted only): To use environment-based credentials such as with [AWS IAM Roles for Service Accounts](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html) (IRSA),\n  omit the `credentials` key from the request when creating the bulk export destination.\n  In this case, the [standard Boto3 credentials locations](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html#credentials) will be checked in the order defined by the library.\n\nFor AWS S3, you can leave off the `endpoint_url` and supply the region that matches the region of your bucket.\n\n#### Google GCS XML S3 compatible bucket\n\nWhen using Google's GCS bucket, you need to use the XML S3 compatible API, and supply the `endpoint_url`\nwhich is typically `https://storage.googleapis.com`.\nHere is an example of the API request when using the GCS XML API which is compatible with S3:\n\nSee [Google documentation](https://cloud.google.com/storage/docs/interoperability#xml_api) for more info\n\n### Create an export job\n\nTo export data, you will need to create an export job. This job will specify the destination, the project, the date range, and filter expression of the data to export. The filter expression is used to narrow down the set of runs exported and is optional. Not setting the filter field will export all runs. Refer to our [filter query language](/langsmith/trace-query-syntax#filter-query-language) and [examples](/langsmith/export-traces#use-filter-query-language) to determine the correct filter expression for your export.\n\nYou can use the following cURL command to create the job:\n\n<Note>\n  The `session_id` is also known as the Tracing Project ID, which can be copied from the individual project view by clicking into the project in the Tracing Projects list.\n</Note>\n\nUse the returned `id` to reference this export in subsequent bulk export operations.\n\n#### Limiting exported fields\n\n<Note>\n  Requires LangSmith Helm version >= `0.12.11` (application version >= `0.12.42`). This feature **is supported** in [scheduled bulk exports](#scheduled-exports) and [standard bulk exports](#create-an-export-job).\n</Note>\n\nYou can improve bulk export speed and reduce row size by limiting which fields are included in the exported Parquet files using the `export_fields` parameter. When `export_fields` is provided, only the specified fields are exported as columns in the Parquet files. When `export_fields` is not provided, all exportable fields are included.\n\nThis is particularly useful when you want to exclude larger fields like `inputs` and `outputs`.\n\nThe following example creates an export job that only includes specific fields:\n\nThe `export_fields` parameter accepts an array of field names. Available fields include the [Run data format](/langsmith/run-data-format) fields as well as additional export-only fields:\n\n* `tenant_id`\n* `is_root`\n\n<Tip>\n  **Performance tip**: Excluding `inputs` and `outputs` from your export can significantly improve export performance and reduce file sizes, especially for large runs. Only include these fields if you need them for your analysis.\n</Tip>\n\n### Scheduled exports\n\n<Note>\n  Requires LangSmith Helm version >= `0.10.42` (application version >= `0.10.109`)\n</Note>\n\nScheduled exports collect runs periodically and export to the configured destination.\nTo create a scheduled export, include `interval_hours` and remove `end_time`:\n\nYou can also use `export_fields` with scheduled exports to limit which fields are exported:\n\n* `interval_hours` must be between 1 hour and 168 hours (1 week) inclusive.\n* For spawned exports, the first time range exported is `start_time=(scheduled_export_start_time), end_time=(start_time + interval_hours)`.\n  Then `start_time=(previous_export_end_time), end_time=(this_export_start_time + interval_hours)`, and so on.\n* `end_time` must be omitted for scheduled exports. `end_time` is still required for non-scheduled exports.\n* Scheduled exports can be stopped by [cancelling the export](#stop-an-export).\n  * Exports that have been spawned by a scheduled export have the `source_bulk_export_id` attribute filled.\n  * If desired, these spawned bulk exports must be canceled separately from the source scheduled bulk export -\n    canceling the source bulk export **does not** cancel the spawned bulk exports.\n* Spawned exports run at `end_time + 10 minutes` to account for any runs that are submitted with `end_time` in the recent past.\n* `format_version` (optional): The format version to use for the parquet files. `\"v2_beta\"` has (1) enhanced datatypes for the columns and (2) a Hive-compliant folder structure.\n\nIf a scheduled bulk export is created with `start_time=2025-07-16T00:00:00Z` and `interval_hours=6`:\n\n| Export | Start Time           | End Time             | Runs At              |\n| ------ | -------------------- | -------------------- | -------------------- |\n| 1      | 2025-07-16T00:00:00Z | 2025-07-16T06:00:00Z | 2025-07-16T06:10:00Z |\n| 2      | 2025-07-16T06:00:00Z | 2025-07-16T12:00:00Z | 2025-07-16T12:10:00Z |\n| 3      | 2025-07-16T12:00:00Z | 2025-07-16T18:00:00Z | 2025-07-16T18:10:00Z |\n\n## Monitoring the Export Job\n\n### Monitor Export Status\n\nTo monitor the status of an export job, use the following cURL command:\n\nReplace `{export_id}` with the ID of the export you want to monitor. This command retrieves the current status of the specified export job.\n\n### List Runs for an Export\n\nAn export is typically broken up into multiple runs which correspond to a specific date partition to export.\nTo list all runs associated with a specific export, use the following cURL command:\n\nThis command fetches all runs related to the specified export, providing details such as run ID, status, creation time, rows exported, etc.\n\nTo retrieve a list of all export jobs, use the following cURL command:\n\nThis command returns a list of all export jobs along with their current statuses and creation timestamps.\n\nTo stop an existing export, use the following cURL command:\n\nReplace `{export_id}` with the ID of the export you wish to cancel. Note that a job cannot be restarted once it has been cancelled,\nyou will need to create a new export job instead.\n\n## Partitioning Scheme\n\nData will be exported into your bucket into the follow Hive partitioned format:\n\n## Importing Data into other systems\n\nImporting data from S3 and Parquet format is commonly supported by the majority of analytical systems. See below for documentation links:\n\nTo import your data into BigQuery, see [Loading Data from Parquet](https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-parquet) and also\n[Hive Partitioned loads](https://cloud.google.com/bigquery/docs/hive-partitioned-loads-gcs).\n\nYou can load data into Snowflake from S3 by following the [Load from Cloud Document](https://docs.snowflake.com/en/user-guide/tutorials/load-from-cloud-tutorial).\n\nYou can COPY data from S3 or Parquet into Amazon Redshift by following the [AWS COPY command documentation](https://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html).\n\nYou can directly query data in S3 / Parquet format in Clickhouse. As an example, if using GCS, you can query the data as follows:\n\nSee [Clickhouse S3 Integration Documentation](https://clickhouse.com/docs/en/engines/table-engines/integrations/s3) for more information.\n\nYou can query the data from S3 in-memory with SQL using DuckDB. See [S3 import Documentation](https://duckdb.org/docs/guides/network_cloud_storage/s3_import.html).\n\n### Debugging Destination Errors\n\nThe destinations API endpoint will validate that the destination and credentials are valid and that write access is\nis present for the bucket.\n\nIf you receive an error, and would like to debug this error, you can use the [AWS CLI](https://aws.amazon.com/cli/)\nto test the connectivity to the bucket. You should be able to write a file with the CLI using the same\ndata that you supplied to the destinations API above.\n\n```bash  theme={null}\naws configure",
  "code_samples": [
    {
      "code": "Use the returned `id` to reference this destination in subsequent bulk export operations.\n\n**If you receive an error while creating a destination, see [debug destination errors](#debugging-destination-errors) for details on how to debug this.**\n\n#### Credentials configuration\n\n<Note>**Requires LangSmith Helm version >= `0.10.34` (application version >= `0.10.91`)**</Note>\n\nWe support the following additional credentials formats besides static `access_key_id` and `secret_access_key`:\n\n* To use [temporary credentials](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp_use-resources.html) that include an AWS session token,\n  additionally provide the `credentials.session_token` key when creating the bulk export destination.\n* (Self-hosted only): To use environment-based credentials such as with [AWS IAM Roles for Service Accounts](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html) (IRSA),\n  omit the `credentials` key from the request when creating the bulk export destination.\n  In this case, the [standard Boto3 credentials locations](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html#credentials) will be checked in the order defined by the library.\n\n#### AWS S3 bucket\n\nFor AWS S3, you can leave off the `endpoint_url` and supply the region that matches the region of your bucket.",
      "language": "unknown"
    },
    {
      "code": "#### Google GCS XML S3 compatible bucket\n\nWhen using Google's GCS bucket, you need to use the XML S3 compatible API, and supply the `endpoint_url`\nwhich is typically `https://storage.googleapis.com`.\nHere is an example of the API request when using the GCS XML API which is compatible with S3:",
      "language": "unknown"
    },
    {
      "code": "See [Google documentation](https://cloud.google.com/storage/docs/interoperability#xml_api) for more info\n\n### Create an export job\n\nTo export data, you will need to create an export job. This job will specify the destination, the project, the date range, and filter expression of the data to export. The filter expression is used to narrow down the set of runs exported and is optional. Not setting the filter field will export all runs. Refer to our [filter query language](/langsmith/trace-query-syntax#filter-query-language) and [examples](/langsmith/export-traces#use-filter-query-language) to determine the correct filter expression for your export.\n\nYou can use the following cURL command to create the job:",
      "language": "unknown"
    },
    {
      "code": "<Note>\n  The `session_id` is also known as the Tracing Project ID, which can be copied from the individual project view by clicking into the project in the Tracing Projects list.\n</Note>\n\nUse the returned `id` to reference this export in subsequent bulk export operations.\n\n#### Limiting exported fields\n\n<Note>\n  Requires LangSmith Helm version >= `0.12.11` (application version >= `0.12.42`). This feature **is supported** in [scheduled bulk exports](#scheduled-exports) and [standard bulk exports](#create-an-export-job).\n</Note>\n\nYou can improve bulk export speed and reduce row size by limiting which fields are included in the exported Parquet files using the `export_fields` parameter. When `export_fields` is provided, only the specified fields are exported as columns in the Parquet files. When `export_fields` is not provided, all exportable fields are included.\n\nThis is particularly useful when you want to exclude larger fields like `inputs` and `outputs`.\n\nThe following example creates an export job that only includes specific fields:",
      "language": "unknown"
    },
    {
      "code": "The `export_fields` parameter accepts an array of field names. Available fields include the [Run data format](/langsmith/run-data-format) fields as well as additional export-only fields:\n\n* `tenant_id`\n* `is_root`\n\n<Tip>\n  **Performance tip**: Excluding `inputs` and `outputs` from your export can significantly improve export performance and reduce file sizes, especially for large runs. Only include these fields if you need them for your analysis.\n</Tip>\n\n### Scheduled exports\n\n<Note>\n  Requires LangSmith Helm version >= `0.10.42` (application version >= `0.10.109`)\n</Note>\n\nScheduled exports collect runs periodically and export to the configured destination.\nTo create a scheduled export, include `interval_hours` and remove `end_time`:",
      "language": "unknown"
    },
    {
      "code": "You can also use `export_fields` with scheduled exports to limit which fields are exported:",
      "language": "unknown"
    },
    {
      "code": "**Details**\n\n* `interval_hours` must be between 1 hour and 168 hours (1 week) inclusive.\n* For spawned exports, the first time range exported is `start_time=(scheduled_export_start_time), end_time=(start_time + interval_hours)`.\n  Then `start_time=(previous_export_end_time), end_time=(this_export_start_time + interval_hours)`, and so on.\n* `end_time` must be omitted for scheduled exports. `end_time` is still required for non-scheduled exports.\n* Scheduled exports can be stopped by [cancelling the export](#stop-an-export).\n  * Exports that have been spawned by a scheduled export have the `source_bulk_export_id` attribute filled.\n  * If desired, these spawned bulk exports must be canceled separately from the source scheduled bulk export -\n    canceling the source bulk export **does not** cancel the spawned bulk exports.\n* Spawned exports run at `end_time + 10 minutes` to account for any runs that are submitted with `end_time` in the recent past.\n* `format_version` (optional): The format version to use for the parquet files. `\"v2_beta\"` has (1) enhanced datatypes for the columns and (2) a Hive-compliant folder structure.\n\n**Example**\n\nIf a scheduled bulk export is created with `start_time=2025-07-16T00:00:00Z` and `interval_hours=6`:\n\n| Export | Start Time           | End Time             | Runs At              |\n| ------ | -------------------- | -------------------- | -------------------- |\n| 1      | 2025-07-16T00:00:00Z | 2025-07-16T06:00:00Z | 2025-07-16T06:10:00Z |\n| 2      | 2025-07-16T06:00:00Z | 2025-07-16T12:00:00Z | 2025-07-16T12:10:00Z |\n| 3      | 2025-07-16T12:00:00Z | 2025-07-16T18:00:00Z | 2025-07-16T18:10:00Z |\n\n## Monitoring the Export Job\n\n### Monitor Export Status\n\nTo monitor the status of an export job, use the following cURL command:",
      "language": "unknown"
    },
    {
      "code": "Replace `{export_id}` with the ID of the export you want to monitor. This command retrieves the current status of the specified export job.\n\n### List Runs for an Export\n\nAn export is typically broken up into multiple runs which correspond to a specific date partition to export.\nTo list all runs associated with a specific export, use the following cURL command:",
      "language": "unknown"
    },
    {
      "code": "This command fetches all runs related to the specified export, providing details such as run ID, status, creation time, rows exported, etc.\n\n### List All Exports\n\nTo retrieve a list of all export jobs, use the following cURL command:",
      "language": "unknown"
    },
    {
      "code": "This command returns a list of all export jobs along with their current statuses and creation timestamps.\n\n### Stop an Export\n\nTo stop an existing export, use the following cURL command:",
      "language": "unknown"
    },
    {
      "code": "Replace `{export_id}` with the ID of the export you wish to cancel. Note that a job cannot be restarted once it has been cancelled,\nyou will need to create a new export job instead.\n\n## Partitioning Scheme\n\nData will be exported into your bucket into the follow Hive partitioned format:",
      "language": "unknown"
    },
    {
      "code": "## Importing Data into other systems\n\nImporting data from S3 and Parquet format is commonly supported by the majority of analytical systems. See below for documentation links:\n\n### BigQuery\n\nTo import your data into BigQuery, see [Loading Data from Parquet](https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-parquet) and also\n[Hive Partitioned loads](https://cloud.google.com/bigquery/docs/hive-partitioned-loads-gcs).\n\n### Snowflake\n\nYou can load data into Snowflake from S3 by following the [Load from Cloud Document](https://docs.snowflake.com/en/user-guide/tutorials/load-from-cloud-tutorial).\n\n### RedShift\n\nYou can COPY data from S3 or Parquet into Amazon Redshift by following the [AWS COPY command documentation](https://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html).\n\n### Clickhouse\n\nYou can directly query data in S3 / Parquet format in Clickhouse. As an example, if using GCS, you can query the data as follows:",
      "language": "unknown"
    },
    {
      "code": "See [Clickhouse S3 Integration Documentation](https://clickhouse.com/docs/en/engines/table-engines/integrations/s3) for more information.\n\n### DuckDB\n\nYou can query the data from S3 in-memory with SQL using DuckDB. See [S3 import Documentation](https://duckdb.org/docs/guides/network_cloud_storage/s3_import.html).\n\n## Error Handling\n\n### Debugging Destination Errors\n\nThe destinations API endpoint will validate that the destination and credentials are valid and that write access is\nis present for the bucket.\n\nIf you receive an error, and would like to debug this error, you can use the [AWS CLI](https://aws.amazon.com/cli/)\nto test the connectivity to the bucket. You should be able to write a file with the CLI using the same\ndata that you supplied to the destinations API above.\n\n**AWS S3:**",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": "Destinations",
      "id": "destinations"
    },
    {
      "level": "h2",
      "text": "Exporting Data",
      "id": "exporting-data"
    },
    {
      "level": "h3",
      "text": "Destinations - Providing a S3 bucket",
      "id": "destinations---providing-a-s3-bucket"
    },
    {
      "level": "h3",
      "text": "Preparing the Destination",
      "id": "preparing-the-destination"
    },
    {
      "level": "h3",
      "text": "Create an export job",
      "id": "create-an-export-job"
    },
    {
      "level": "h3",
      "text": "Scheduled exports",
      "id": "scheduled-exports"
    },
    {
      "level": "h2",
      "text": "Monitoring the Export Job",
      "id": "monitoring-the-export-job"
    },
    {
      "level": "h3",
      "text": "Monitor Export Status",
      "id": "monitor-export-status"
    },
    {
      "level": "h3",
      "text": "List Runs for an Export",
      "id": "list-runs-for-an-export"
    },
    {
      "level": "h3",
      "text": "List All Exports",
      "id": "list-all-exports"
    },
    {
      "level": "h3",
      "text": "Stop an Export",
      "id": "stop-an-export"
    },
    {
      "level": "h2",
      "text": "Partitioning Scheme",
      "id": "partitioning-scheme"
    },
    {
      "level": "h2",
      "text": "Importing Data into other systems",
      "id": "importing-data-into-other-systems"
    },
    {
      "level": "h3",
      "text": "BigQuery",
      "id": "bigquery"
    },
    {
      "level": "h3",
      "text": "Snowflake",
      "id": "snowflake"
    },
    {
      "level": "h3",
      "text": "RedShift",
      "id": "redshift"
    },
    {
      "level": "h3",
      "text": "Clickhouse",
      "id": "clickhouse"
    },
    {
      "level": "h3",
      "text": "DuckDB",
      "id": "duckdb"
    },
    {
      "level": "h2",
      "text": "Error Handling",
      "id": "error-handling"
    },
    {
      "level": "h3",
      "text": "Debugging Destination Errors",
      "id": "debugging-destination-errors"
    }
  ],
  "url": "llms-txt#bulk-exporting-trace-data",
  "links": []
}