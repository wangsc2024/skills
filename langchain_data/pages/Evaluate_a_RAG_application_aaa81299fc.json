{
  "title": "Evaluate a RAG application",
  "content": "Source: https://docs.langchain.com/langsmith/evaluate-rag-tutorial\n\n<Info>\n  [RAG evaluation](/langsmith/evaluation-concepts#retrieval-augmented-generation-rag) | [Evaluators](/langsmith/evaluation-concepts#evaluators) | [LLM-as-judge evaluators](/langsmith/evaluation-concepts#llm-as-judge)\n</Info>\n\nRetrieval Augmented Generation (RAG) is a technique that enhances Large Language Models (LLMs) by providing them with relevant external knowledge. It has become one of the most widely used approaches for building LLM applications.\n\nThis tutorial will show you how to evaluate your RAG applications using LangSmith. You'll learn:\n\n1. How to create test datasets\n2. How to run your RAG application on those datasets\n3. How to measure your application's performance using different evaluation metrics\n\nA typical RAG evaluation workflow consists of three main steps:\n\n1. Creating a dataset with questions and their expected answers\n\n2. Running your RAG application on those questions\n\n3. Using evaluators to measure how well your application performed, looking at factors like:\n\n* Answer relevance\n   * Answer accuracy\n   * Retrieval quality\n\nFor this tutorial, we'll create and evaluate a bot that answers questions about a few of [Lilian Weng's](https://lilianweng.github.io/) insightful blog posts.\n\nFirst, let's set our environment variables:\n\nAnd install the dependencies we'll need:\n\n<Info>\n  While this tutorial uses LangChain, the evaluation techniques and LangSmith functionality demonstrated here work with any framework. Feel free to use your preferred tools and libraries.\n</Info>\n\nIn this section, we'll build a basic Retrieval-Augmented Generation (RAG) application.\n\nWe'll stick to a simple implementation that:\n\n* Indexing: chunks and indexes a few of Lilian Weng's blogs in a vector store\n* Retrieval: retrieves those chunks based on the user question\n* Generation: passes the question and retrieved docs to an LLM.\n\n#### Indexing and retrieval\n\nFirst, lets load the blog posts we want to build a chatbot for and index them.\n\nWe can now define the generative pipeline.\n\nNow that we've got our application, let's build a dataset to evaluate it. Our dataset will be very simple in this case: we'll have example questions and reference answers.\n\nOne way to think about different types of RAG evaluators is as a tuple of what is being evaluated X what its being evaluated against:\n\n1. **Correctness**: Response vs reference answer\n\n* `Goal`: Measure \"*how similar/correct is the RAG chain answer, relative to a ground-truth answer*\"\n* `Mode`: Requires a ground truth (reference) answer supplied through a dataset\n* `Evaluator`: Use LLM-as-judge to assess answer correctness.\n\n2. **Relevance**: Response vs input\n\n* `Goal`: Measure \"*how well does the generated response address the initial user input*\"\n* `Mode`: Does not require reference answer, because it will compare the answer to the input question\n* `Evaluator`: Use LLM-as-judge to assess answer relevance, helpfulness, etc.\n\n3. **Groundedness**: Response vs retrieved docs\n\n* `Goal`: Measure \"*to what extent does the generated response agree with the retrieved context*\"\n* `Mode`: Does not require reference answer, because it will compare the answer to the retrieved context\n* `Evaluator`: Use LLM-as-judge to assess faithfulness, hallucinations, etc.\n\n4. **Retrieval relevance**: Retrieved docs vs input\n\n* `Goal`: Measure \"*how relevant are my retrieved results for this query*\"\n* `Mode`: Does not require reference answer, because it will compare the question to the retrieved context\n* `Evaluator`: Use LLM-as-judge to assess relevance\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rag-eval-overview.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=6f303c2a284296b42e7d2d2e658f5171\" alt=\"Rag eval overview\" data-og-width=\"1252\" width=\"1252\" data-og-height=\"547\" height=\"547\" data-path=\"langsmith/images/rag-eval-overview.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rag-eval-overview.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=b5531116cdd61ca9e8ea6fcd760643db 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rag-eval-overview.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=8f83816ac849c05dd8d3dee4c9670729 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rag-eval-overview.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=8656d8f8af7ffa7f2684376cf2f70874 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rag-eval-overview.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=4299367332fbefd15e938aefc23ca6fe 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rag-eval-overview.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=0bc19645279f119031a5cb8ca32f2f7d 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rag-eval-overview.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=185b7c4ba4f4127f780d5fa17b96c752 2500w\" />\n\n### Correctness: Response vs reference answer\n\n### Relevance: Response vs input\n\nThe flow is similar to above, but we simply look at the `inputs` and `outputs` without needing the `reference_outputs`. Without a reference answer we can't grade accuracy, but can still grade relevance—as in, did the model address the user's question or not.\n\n### Groundedness: Response vs retrieved docs\n\nAnother useful way to evaluate responses without needing reference answers is to check if the response is justified by (or \"grounded in\") the retrieved documents.\n\n### Retrieval relevance: Retrieved docs vs input\n\nWe can now kick off our evaluation job with all of our different evaluators.\n\nYou can see an example of what these results look like here: [LangSmith link](https://smith.langchain.com/public/302573e2-20bf-4f8c-bdad-e97c20f33f1b/d)\n\n<Accordion title=\"Here's a consolidated script with all the above code:\">\n  <CodeGroup>\n\n</CodeGroup>\n</Accordion>\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/evaluate-rag-tutorial.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "code_samples": [
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</CodeGroup>\n\nAnd install the dependencies we'll need:\n\n<CodeGroup>",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</CodeGroup>\n\n### Application\n\n<Info>\n  While this tutorial uses LangChain, the evaluation techniques and LangSmith functionality demonstrated here work with any framework. Feel free to use your preferred tools and libraries.\n</Info>\n\nIn this section, we'll build a basic Retrieval-Augmented Generation (RAG) application.\n\nWe'll stick to a simple implementation that:\n\n* Indexing: chunks and indexes a few of Lilian Weng's blogs in a vector store\n* Retrieval: retrieves those chunks based on the user question\n* Generation: passes the question and retrieved docs to an LLM.\n\n#### Indexing and retrieval\n\nFirst, lets load the blog posts we want to build a chatbot for and index them.\n\n<CodeGroup>",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</CodeGroup>\n\n#### Generation\n\nWe can now define the generative pipeline.\n\n<CodeGroup>",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</CodeGroup>\n\n## Dataset\n\nNow that we've got our application, let's build a dataset to evaluate it. Our dataset will be very simple in this case: we'll have example questions and reference answers.\n\n<CodeGroup>",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</CodeGroup>\n\n## Evaluators\n\nOne way to think about different types of RAG evaluators is as a tuple of what is being evaluated X what its being evaluated against:\n\n1. **Correctness**: Response vs reference answer\n\n* `Goal`: Measure \"*how similar/correct is the RAG chain answer, relative to a ground-truth answer*\"\n* `Mode`: Requires a ground truth (reference) answer supplied through a dataset\n* `Evaluator`: Use LLM-as-judge to assess answer correctness.\n\n2. **Relevance**: Response vs input\n\n* `Goal`: Measure \"*how well does the generated response address the initial user input*\"\n* `Mode`: Does not require reference answer, because it will compare the answer to the input question\n* `Evaluator`: Use LLM-as-judge to assess answer relevance, helpfulness, etc.\n\n3. **Groundedness**: Response vs retrieved docs\n\n* `Goal`: Measure \"*to what extent does the generated response agree with the retrieved context*\"\n* `Mode`: Does not require reference answer, because it will compare the answer to the retrieved context\n* `Evaluator`: Use LLM-as-judge to assess faithfulness, hallucinations, etc.\n\n4. **Retrieval relevance**: Retrieved docs vs input\n\n* `Goal`: Measure \"*how relevant are my retrieved results for this query*\"\n* `Mode`: Does not require reference answer, because it will compare the question to the retrieved context\n* `Evaluator`: Use LLM-as-judge to assess relevance\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rag-eval-overview.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=6f303c2a284296b42e7d2d2e658f5171\" alt=\"Rag eval overview\" data-og-width=\"1252\" width=\"1252\" data-og-height=\"547\" height=\"547\" data-path=\"langsmith/images/rag-eval-overview.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rag-eval-overview.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=b5531116cdd61ca9e8ea6fcd760643db 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rag-eval-overview.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=8f83816ac849c05dd8d3dee4c9670729 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rag-eval-overview.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=8656d8f8af7ffa7f2684376cf2f70874 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rag-eval-overview.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=4299367332fbefd15e938aefc23ca6fe 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rag-eval-overview.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=0bc19645279f119031a5cb8ca32f2f7d 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rag-eval-overview.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=185b7c4ba4f4127f780d5fa17b96c752 2500w\" />\n\n### Correctness: Response vs reference answer\n\n<CodeGroup>",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</CodeGroup>\n\n### Relevance: Response vs input\n\nThe flow is similar to above, but we simply look at the `inputs` and `outputs` without needing the `reference_outputs`. Without a reference answer we can't grade accuracy, but can still grade relevance—as in, did the model address the user's question or not.\n\n<CodeGroup>",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</CodeGroup>\n\n### Groundedness: Response vs retrieved docs\n\nAnother useful way to evaluate responses without needing reference answers is to check if the response is justified by (or \"grounded in\") the retrieved documents.\n\n<CodeGroup>",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</CodeGroup>\n\n### Retrieval relevance: Retrieved docs vs input\n\n<CodeGroup>",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</CodeGroup>\n\n## Run evaluation\n\nWe can now kick off our evaluation job with all of our different evaluators.\n\n<CodeGroup>",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</CodeGroup>\n\nYou can see an example of what these results look like here: [LangSmith link](https://smith.langchain.com/public/302573e2-20bf-4f8c-bdad-e97c20f33f1b/d)\n\n## Reference code\n\n<Accordion title=\"Here's a consolidated script with all the above code:\">\n  <CodeGroup>",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": "Overview",
      "id": "overview"
    },
    {
      "level": "h2",
      "text": "Setup",
      "id": "setup"
    },
    {
      "level": "h3",
      "text": "Environment",
      "id": "environment"
    },
    {
      "level": "h3",
      "text": "Application",
      "id": "application"
    },
    {
      "level": "h2",
      "text": "Dataset",
      "id": "dataset"
    },
    {
      "level": "h2",
      "text": "Evaluators",
      "id": "evaluators"
    },
    {
      "level": "h3",
      "text": "Correctness: Response vs reference answer",
      "id": "correctness:-response-vs-reference-answer"
    },
    {
      "level": "h3",
      "text": "Relevance: Response vs input",
      "id": "relevance:-response-vs-input"
    },
    {
      "level": "h3",
      "text": "Groundedness: Response vs retrieved docs",
      "id": "groundedness:-response-vs-retrieved-docs"
    },
    {
      "level": "h3",
      "text": "Retrieval relevance: Retrieved docs vs input",
      "id": "retrieval-relevance:-retrieved-docs-vs-input"
    },
    {
      "level": "h2",
      "text": "Run evaluation",
      "id": "run-evaluation"
    },
    {
      "level": "h2",
      "text": "Reference code",
      "id": "reference-code"
    }
  ],
  "url": "llms-txt#evaluate-a-rag-application",
  "links": []
}