{
  "title": "Built-in middleware",
  "content": "Source: https://docs.langchain.com/oss/python/langchain/middleware/built-in\n\nPrebuilt middleware for common agent use cases\n\nLangChain provides prebuilt middleware for common use cases. Each middleware is production-ready and configurable for your specific needs.\n\n## Provider-agnostic middleware\n\nThe following middleware work with any LLM provider:\n\n| Middleware                              | Description                                                                 |\n| --------------------------------------- | --------------------------------------------------------------------------- |\n| [Summarization](#summarization)         | Automatically summarize conversation history when approaching token limits. |\n| [Human-in-the-loop](#human-in-the-loop) | Pause execution for human approval of tool calls.                           |\n| [Model call limit](#model-call-limit)   | Limit the number of model calls to prevent excessive costs.                 |\n| [Tool call limit](#tool-call-limit)     | Control tool execution by limiting call counts.                             |\n| [Model fallback](#model-fallback)       | Automatically fallback to alternative models when primary fails.            |\n| [PII detection](#pii-detection)         | Detect and handle Personally Identifiable Information (PII).                |\n| [To-do list](#to-do-list)               | Equip agents with task planning and tracking capabilities.                  |\n| [LLM tool selector](#llm-tool-selector) | Use an LLM to select relevant tools before calling main model.              |\n| [Tool retry](#tool-retry)               | Automatically retry failed tool calls with exponential backoff.             |\n| [Model retry](#model-retry)             | Automatically retry failed model calls with exponential backoff.            |\n| [LLM tool emulator](#llm-tool-emulator) | Emulate tool execution using an LLM for testing purposes.                   |\n| [Context editing](#context-editing)     | Manage conversation context by trimming or clearing tool uses.              |\n| [Shell tool](#shell-tool)               | Expose a persistent shell session to agents for command execution.          |\n| [File search](#file-search)             | Provide Glob and Grep search tools over filesystem files.                   |\n\nAutomatically summarize conversation history when approaching token limits, preserving recent messages while compressing older context. Summarization is useful for the following:\n\n* Long-running conversations that exceed context windows.\n* Multi-turn dialogues with extensive history.\n* Applications where preserving full conversation context matters.\n\n**API reference:** [`SummarizationMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.SummarizationMiddleware)\n\n<Accordion title=\"Configuration options\">\n  <Tip>\n    The `fraction` conditions for `trigger` and `keep` (shown below) rely on a chat model's [profile data](/oss/python/langchain/models#model-profiles) if using `langchain>=1.1`. If data are not available, use another condition or specify manually:\n\n<ParamField body=\"model\" type=\"string | BaseChatModel\" required>\n    Model for generating summaries. Can be a model identifier string (e.g., `'openai:gpt-4o-mini'`) or a `BaseChatModel` instance. See [`init_chat_model`](https://reference.langchain.com/python/langchain/models/#langchain.chat_models.init_chat_model\\(model\\)) for more information.\n  </ParamField>\n\n<ParamField body=\"trigger\" type=\"ContextSize | list[ContextSize] | None\">\n    Conditions for triggering summarization. Can be:\n\n* A single [`ContextSize`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.summarization.ContextSize) dict (all properties must be met - AND logic)\n    * A list of [`ContextSize`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.summarization.ContextSize) dicts (any condition must be met - OR logic)\n\nEach condition can include:\n\n* `fraction` (float): Fraction of model's context size (0-1)\n    * `tokens` (int): Absolute token count\n    * `messages` (int): Message count\n\nAt least one property must be specified per condition. If not provided, summarization will not trigger automatically.\n\nSee the API reference for [`ContextSize`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.summarization.ContextSize) for more information.\n  </ParamField>\n\n<ParamField body=\"keep\" type=\"ContextSize\" default=\"{messages: 20}\">\n    How much context to preserve after summarization. Specify exactly one of:\n\n* `fraction` (float): Fraction of model's context size to keep (0-1)\n    * `tokens` (int): Absolute token count to keep\n    * `messages` (int): Number of recent messages to keep\n\nSee the API reference for [`ContextSize`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.summarization.ContextSize) for more information.\n  </ParamField>\n\n<ParamField body=\"token_counter\" type=\"function\">\n    Custom token counting function. Defaults to character-based counting.\n  </ParamField>\n\n<ParamField body=\"summary_prompt\" type=\"string\">\n    Custom prompt template for summarization. Uses built-in template if not specified. The template should include `{messages}` placeholder where conversation history will be inserted.\n  </ParamField>\n\n<ParamField body=\"trim_tokens_to_summarize\" type=\"number\" default=\"4000\">\n    Maximum number of tokens to include when generating the summary. Messages will be trimmed to fit this limit before summarization.\n  </ParamField>\n\n<ParamField body=\"summary_prefix\" type=\"string\">\n    Prefix to add to the summary message. If not provided, a default prefix is used.\n  </ParamField>\n\n<ParamField body=\"max_tokens_before_summary\" type=\"number\" deprecated>\n    **Deprecated:** Use `trigger: {\"tokens\": value}` instead. Token threshold for triggering summarization.\n  </ParamField>\n\n<ParamField body=\"messages_to_keep\" type=\"number\" deprecated>\n    **Deprecated:** Use `keep: {\"messages\": value}` instead. Recent messages to preserve.\n  </ParamField>\n</Accordion>\n\n<Accordion title=\"Full example\">\n  The summarization middleware monitors message token counts and automatically summarizes older messages when thresholds are reached.\n\n**Trigger conditions** control when summarization runs:\n\n* Single condition object (all properties must be met - AND logic)\n  * Array of conditions (any condition must be met - OR logic)\n  * Each condition can use `fraction` (of model's context size), `tokens` (absolute count), or `messages` (message count)\n\n**Keep conditions** control how much context to preserve (specify exactly one):\n\n* `fraction` - Fraction of model's context size to keep\n  * `tokens` - Absolute token count to keep\n  * `messages` - Number of recent messages to keep\n\n### Human-in-the-loop\n\nPause agent execution for human approval, editing, or rejection of tool calls before they execute. [Human-in-the-loop](/oss/python/langchain/human-in-the-loop) is useful for the following:\n\n* High-stakes operations requiring human approval (e.g. database writes, financial transactions).\n* Compliance workflows where human oversight is mandatory.\n* Long-running conversations where human feedback guides the agent.\n\n**API reference:** [`HumanInTheLoopMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.HumanInTheLoopMiddleware)\n\n<Warning>\n  Human-in-the-loop middleware requires a [checkpointer](/oss/python/langgraph/persistence#checkpoints) to maintain state across interruptions.\n</Warning>\n\n<Tip>\n  For complete examples, configuration options, and integration patterns, see the [Human-in-the-loop documentation](/oss/python/langchain/human-in-the-loop).\n</Tip>\n\n<Callout icon=\"circle-play\" iconType=\"solid\">\n  Watch this [video guide](https://www.youtube.com/watch?v=SpfT6-YAVPk) demonstrating Human-in-the-loop middleware behavior.\n</Callout>\n\nLimit the number of model calls to prevent infinite loops or excessive costs. Model call limit is useful for the following:\n\n* Preventing runaway agents from making too many API calls.\n* Enforcing cost controls on production deployments.\n* Testing agent behavior within specific call budgets.\n\n**API reference:** [`ModelCallLimitMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.ModelCallLimitMiddleware)\n\n<Callout icon=\"circle-play\" iconType=\"solid\">\n  Watch this [video guide](https://www.youtube.com/watch?v=nJEER0uaNkE) demonstrating Model Call Limit middleware behavior.\n</Callout>\n\n<Accordion title=\"Configuration options\">\n  <ParamField body=\"thread_limit\" type=\"number\">\n    Maximum model calls across all runs in a thread. Defaults to no limit.\n  </ParamField>\n\n<ParamField body=\"run_limit\" type=\"number\">\n    Maximum model calls per single invocation. Defaults to no limit.\n  </ParamField>\n\n<ParamField body=\"exit_behavior\" type=\"string\" default=\"end\">\n    Behavior when limit is reached. Options: `'end'` (graceful termination) or `'error'` (raise exception)\n  </ParamField>\n</Accordion>\n\nControl agent execution by limiting the number of tool calls, either globally across all tools or for specific tools. Tool call limits are useful for the following:\n\n* Preventing excessive calls to expensive external APIs.\n* Limiting web searches or database queries.\n* Enforcing rate limits on specific tool usage.\n* Protecting against runaway agent loops.\n\n**API reference:** [`ToolCallLimitMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.ToolCallLimitMiddleware)\n\n<Callout icon=\"circle-play\" iconType=\"solid\">\n  Watch this [video guide](https://www.youtube.com/watch?v=6gYlaJJ8t0w) demonstrating Tool Call Limit middleware behavior.\n</Callout>\n\n<Accordion title=\"Configuration options\">\n  <ParamField body=\"tool_name\" type=\"string\">\n    Name of specific tool to limit. If not provided, limits apply to **all tools globally**.\n  </ParamField>\n\n<ParamField body=\"thread_limit\" type=\"number\">\n    Maximum tool calls across all runs in a thread (conversation). Persists across multiple invocations with the same thread ID. Requires a checkpointer to maintain state. `None` means no thread limit.\n  </ParamField>\n\n<ParamField body=\"run_limit\" type=\"number\">\n    Maximum tool calls per single invocation (one user message → response cycle). Resets with each new user message. `None` means no run limit.\n\n**Note:** At least one of `thread_limit` or `run_limit` must be specified.\n  </ParamField>\n\n<ParamField body=\"exit_behavior\" type=\"string\" default=\"continue\">\n    Behavior when limit is reached:\n\n* `'continue'` (default) - Block exceeded tool calls with error messages, let other tools and the model continue. The model decides when to end based on the error messages.\n    * `'error'` - Raise a `ToolCallLimitExceededError` exception, stopping execution immediately\n    * `'end'` - Stop execution immediately with a `ToolMessage` and AI message for the exceeded tool call. Only works when limiting a single tool; raises `NotImplementedError` if other tools have pending calls.\n  </ParamField>\n</Accordion>\n\n<Accordion title=\"Full example\">\n  Specify limits with:\n\n* **Thread limit** - Max calls across all runs in a conversation (requires checkpointer)\n  * **Run limit** - Max calls per single invocation (resets each turn)\n\n* `'continue'` (default) - Block exceeded calls with error messages, agent continues\n  * `'error'` - Raise exception immediately\n  * `'end'` - Stop with ToolMessage + AI message (single-tool scenarios only)\n\nAutomatically fallback to alternative models when the primary model fails. Model fallback is useful for the following:\n\n* Building resilient agents that handle model outages.\n* Cost optimization by falling back to cheaper models.\n* Provider redundancy across OpenAI, Anthropic, etc.\n\n**API reference:** [`ModelFallbackMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.ModelFallbackMiddleware)\n\n<Callout icon=\"circle-play\" iconType=\"solid\">\n  Watch this [video guide](https://www.youtube.com/watch?v=8rCRO0DUeIM) demonstrating Model Fallback middleware behavior.\n</Callout>\n\n<Accordion title=\"Configuration options\">\n  <ParamField body=\"first_model\" type=\"string | BaseChatModel\" required>\n    First fallback model to try when the primary model fails. Can be a model identifier string (e.g., `'openai:gpt-4o-mini'`) or a `BaseChatModel` instance.\n  </ParamField>\n\n<ParamField body=\"*additional_models\" type=\"string | BaseChatModel\">\n    Additional fallback models to try in order if previous models fail\n  </ParamField>\n</Accordion>\n\nDetect and handle Personally Identifiable Information (PII) in conversations using configurable strategies. PII detection is useful for the following:\n\n* Healthcare and financial applications with compliance requirements.\n* Customer service agents that need to sanitize logs.\n* Any application handling sensitive user data.\n\n**API reference:** [`PIIMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.PIIMiddleware)\n\n#### Custom PII types\n\nYou can create custom PII types by providing a `detector` parameter. This allows you to detect patterns specific to your use case beyond the built-in types.\n\n**Three ways to create custom detectors:**\n\n1. **Regex pattern string** - Simple pattern matching\n\n2. **Custom function** - Complex detection logic with validation\n\n```python  theme={null}\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import PIIMiddleware\nimport re",
  "code_samples": [
    {
      "code": "<Accordion title=\"Configuration options\">\n  <Tip>\n    The `fraction` conditions for `trigger` and `keep` (shown below) rely on a chat model's [profile data](/oss/python/langchain/models#model-profiles) if using `langchain>=1.1`. If data are not available, use another condition or specify manually:",
      "language": "unknown"
    },
    {
      "code": "</Tip>\n\n  <ParamField body=\"model\" type=\"string | BaseChatModel\" required>\n    Model for generating summaries. Can be a model identifier string (e.g., `'openai:gpt-4o-mini'`) or a `BaseChatModel` instance. See [`init_chat_model`](https://reference.langchain.com/python/langchain/models/#langchain.chat_models.init_chat_model\\(model\\)) for more information.\n  </ParamField>\n\n  <ParamField body=\"trigger\" type=\"ContextSize | list[ContextSize] | None\">\n    Conditions for triggering summarization. Can be:\n\n    * A single [`ContextSize`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.summarization.ContextSize) dict (all properties must be met - AND logic)\n    * A list of [`ContextSize`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.summarization.ContextSize) dicts (any condition must be met - OR logic)\n\n    Each condition can include:\n\n    * `fraction` (float): Fraction of model's context size (0-1)\n    * `tokens` (int): Absolute token count\n    * `messages` (int): Message count\n\n    At least one property must be specified per condition. If not provided, summarization will not trigger automatically.\n\n    See the API reference for [`ContextSize`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.summarization.ContextSize) for more information.\n  </ParamField>\n\n  <ParamField body=\"keep\" type=\"ContextSize\" default=\"{messages: 20}\">\n    How much context to preserve after summarization. Specify exactly one of:\n\n    * `fraction` (float): Fraction of model's context size to keep (0-1)\n    * `tokens` (int): Absolute token count to keep\n    * `messages` (int): Number of recent messages to keep\n\n    See the API reference for [`ContextSize`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.summarization.ContextSize) for more information.\n  </ParamField>\n\n  <ParamField body=\"token_counter\" type=\"function\">\n    Custom token counting function. Defaults to character-based counting.\n  </ParamField>\n\n  <ParamField body=\"summary_prompt\" type=\"string\">\n    Custom prompt template for summarization. Uses built-in template if not specified. The template should include `{messages}` placeholder where conversation history will be inserted.\n  </ParamField>\n\n  <ParamField body=\"trim_tokens_to_summarize\" type=\"number\" default=\"4000\">\n    Maximum number of tokens to include when generating the summary. Messages will be trimmed to fit this limit before summarization.\n  </ParamField>\n\n  <ParamField body=\"summary_prefix\" type=\"string\">\n    Prefix to add to the summary message. If not provided, a default prefix is used.\n  </ParamField>\n\n  <ParamField body=\"max_tokens_before_summary\" type=\"number\" deprecated>\n    **Deprecated:** Use `trigger: {\"tokens\": value}` instead. Token threshold for triggering summarization.\n  </ParamField>\n\n  <ParamField body=\"messages_to_keep\" type=\"number\" deprecated>\n    **Deprecated:** Use `keep: {\"messages\": value}` instead. Recent messages to preserve.\n  </ParamField>\n</Accordion>\n\n<Accordion title=\"Full example\">\n  The summarization middleware monitors message token counts and automatically summarizes older messages when thresholds are reached.\n\n  **Trigger conditions** control when summarization runs:\n\n  * Single condition object (all properties must be met - AND logic)\n  * Array of conditions (any condition must be met - OR logic)\n  * Each condition can use `fraction` (of model's context size), `tokens` (absolute count), or `messages` (message count)\n\n  **Keep conditions** control how much context to preserve (specify exactly one):\n\n  * `fraction` - Fraction of model's context size to keep\n  * `tokens` - Absolute token count to keep\n  * `messages` - Number of recent messages to keep",
      "language": "unknown"
    },
    {
      "code": "</Accordion>\n\n### Human-in-the-loop\n\nPause agent execution for human approval, editing, or rejection of tool calls before they execute. [Human-in-the-loop](/oss/python/langchain/human-in-the-loop) is useful for the following:\n\n* High-stakes operations requiring human approval (e.g. database writes, financial transactions).\n* Compliance workflows where human oversight is mandatory.\n* Long-running conversations where human feedback guides the agent.\n\n**API reference:** [`HumanInTheLoopMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.HumanInTheLoopMiddleware)\n\n<Warning>\n  Human-in-the-loop middleware requires a [checkpointer](/oss/python/langgraph/persistence#checkpoints) to maintain state across interruptions.\n</Warning>",
      "language": "unknown"
    },
    {
      "code": "<Tip>\n  For complete examples, configuration options, and integration patterns, see the [Human-in-the-loop documentation](/oss/python/langchain/human-in-the-loop).\n</Tip>\n\n<Callout icon=\"circle-play\" iconType=\"solid\">\n  Watch this [video guide](https://www.youtube.com/watch?v=SpfT6-YAVPk) demonstrating Human-in-the-loop middleware behavior.\n</Callout>\n\n### Model call limit\n\nLimit the number of model calls to prevent infinite loops or excessive costs. Model call limit is useful for the following:\n\n* Preventing runaway agents from making too many API calls.\n* Enforcing cost controls on production deployments.\n* Testing agent behavior within specific call budgets.\n\n**API reference:** [`ModelCallLimitMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.ModelCallLimitMiddleware)",
      "language": "unknown"
    },
    {
      "code": "<Callout icon=\"circle-play\" iconType=\"solid\">\n  Watch this [video guide](https://www.youtube.com/watch?v=nJEER0uaNkE) demonstrating Model Call Limit middleware behavior.\n</Callout>\n\n<Accordion title=\"Configuration options\">\n  <ParamField body=\"thread_limit\" type=\"number\">\n    Maximum model calls across all runs in a thread. Defaults to no limit.\n  </ParamField>\n\n  <ParamField body=\"run_limit\" type=\"number\">\n    Maximum model calls per single invocation. Defaults to no limit.\n  </ParamField>\n\n  <ParamField body=\"exit_behavior\" type=\"string\" default=\"end\">\n    Behavior when limit is reached. Options: `'end'` (graceful termination) or `'error'` (raise exception)\n  </ParamField>\n</Accordion>\n\n### Tool call limit\n\nControl agent execution by limiting the number of tool calls, either globally across all tools or for specific tools. Tool call limits are useful for the following:\n\n* Preventing excessive calls to expensive external APIs.\n* Limiting web searches or database queries.\n* Enforcing rate limits on specific tool usage.\n* Protecting against runaway agent loops.\n\n**API reference:** [`ToolCallLimitMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.ToolCallLimitMiddleware)",
      "language": "unknown"
    },
    {
      "code": "<Callout icon=\"circle-play\" iconType=\"solid\">\n  Watch this [video guide](https://www.youtube.com/watch?v=6gYlaJJ8t0w) demonstrating Tool Call Limit middleware behavior.\n</Callout>\n\n<Accordion title=\"Configuration options\">\n  <ParamField body=\"tool_name\" type=\"string\">\n    Name of specific tool to limit. If not provided, limits apply to **all tools globally**.\n  </ParamField>\n\n  <ParamField body=\"thread_limit\" type=\"number\">\n    Maximum tool calls across all runs in a thread (conversation). Persists across multiple invocations with the same thread ID. Requires a checkpointer to maintain state. `None` means no thread limit.\n  </ParamField>\n\n  <ParamField body=\"run_limit\" type=\"number\">\n    Maximum tool calls per single invocation (one user message → response cycle). Resets with each new user message. `None` means no run limit.\n\n    **Note:** At least one of `thread_limit` or `run_limit` must be specified.\n  </ParamField>\n\n  <ParamField body=\"exit_behavior\" type=\"string\" default=\"continue\">\n    Behavior when limit is reached:\n\n    * `'continue'` (default) - Block exceeded tool calls with error messages, let other tools and the model continue. The model decides when to end based on the error messages.\n    * `'error'` - Raise a `ToolCallLimitExceededError` exception, stopping execution immediately\n    * `'end'` - Stop execution immediately with a `ToolMessage` and AI message for the exceeded tool call. Only works when limiting a single tool; raises `NotImplementedError` if other tools have pending calls.\n  </ParamField>\n</Accordion>\n\n<Accordion title=\"Full example\">\n  Specify limits with:\n\n  * **Thread limit** - Max calls across all runs in a conversation (requires checkpointer)\n  * **Run limit** - Max calls per single invocation (resets each turn)\n\n  Exit behaviors:\n\n  * `'continue'` (default) - Block exceeded calls with error messages, agent continues\n  * `'error'` - Raise exception immediately\n  * `'end'` - Stop with ToolMessage + AI message (single-tool scenarios only)",
      "language": "unknown"
    },
    {
      "code": "</Accordion>\n\n### Model fallback\n\nAutomatically fallback to alternative models when the primary model fails. Model fallback is useful for the following:\n\n* Building resilient agents that handle model outages.\n* Cost optimization by falling back to cheaper models.\n* Provider redundancy across OpenAI, Anthropic, etc.\n\n**API reference:** [`ModelFallbackMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.ModelFallbackMiddleware)",
      "language": "unknown"
    },
    {
      "code": "<Callout icon=\"circle-play\" iconType=\"solid\">\n  Watch this [video guide](https://www.youtube.com/watch?v=8rCRO0DUeIM) demonstrating Model Fallback middleware behavior.\n</Callout>\n\n<Accordion title=\"Configuration options\">\n  <ParamField body=\"first_model\" type=\"string | BaseChatModel\" required>\n    First fallback model to try when the primary model fails. Can be a model identifier string (e.g., `'openai:gpt-4o-mini'`) or a `BaseChatModel` instance.\n  </ParamField>\n\n  <ParamField body=\"*additional_models\" type=\"string | BaseChatModel\">\n    Additional fallback models to try in order if previous models fail\n  </ParamField>\n</Accordion>\n\n### PII detection\n\nDetect and handle Personally Identifiable Information (PII) in conversations using configurable strategies. PII detection is useful for the following:\n\n* Healthcare and financial applications with compliance requirements.\n* Customer service agents that need to sanitize logs.\n* Any application handling sensitive user data.\n\n**API reference:** [`PIIMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.PIIMiddleware)",
      "language": "unknown"
    },
    {
      "code": "#### Custom PII types\n\nYou can create custom PII types by providing a `detector` parameter. This allows you to detect patterns specific to your use case beyond the built-in types.\n\n**Three ways to create custom detectors:**\n\n1. **Regex pattern string** - Simple pattern matching\n\n2. **Custom function** - Complex detection logic with validation",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": "Provider-agnostic middleware",
      "id": "provider-agnostic-middleware"
    },
    {
      "level": "h3",
      "text": "Summarization",
      "id": "summarization"
    },
    {
      "level": "h3",
      "text": "Human-in-the-loop",
      "id": "human-in-the-loop"
    },
    {
      "level": "h3",
      "text": "Model call limit",
      "id": "model-call-limit"
    },
    {
      "level": "h3",
      "text": "Tool call limit",
      "id": "tool-call-limit"
    },
    {
      "level": "h3",
      "text": "Model fallback",
      "id": "model-fallback"
    },
    {
      "level": "h3",
      "text": "PII detection",
      "id": "pii-detection"
    }
  ],
  "url": "llms-txt#built-in-middleware",
  "links": []
}