{
  "title": "'outputs' will come from your target function.",
  "content": "def evaluator_one(inputs: dict, outputs: dict) -> bool:\n    return outputs[\"foo\"] == 2\n\ndef evaluator_two(inputs: dict, outputs: dict) -> bool:\n    return len(outputs[\"bar\"]) < 3\n\nclient = Client()\nresults = client.evaluate(\n    dummy_target,  # <-- target function\n    data=\"your-dataset-name\",\n    evaluators=[evaluator_one, evaluator_two],\n    ...\n)\npython Python theme={null}\n  from langsmith import wrappers\n  from openai import OpenAI\n\n# Optionally wrap the OpenAI client to automatically\n  # trace all model calls.\n  oai_client = wrappers.wrap_openai(OpenAI())\n\ndef target(inputs: dict) -> dict:\n    # This assumes your dataset has inputs with a 'messages' key.\n    # You can update to match your dataset schema.\n    messages = inputs[\"messages\"]\n    response = oai_client.chat.completions.create(\n        messages=messages,\n        model=\"gpt-4o-mini\",\n    )\n    return {\"answer\": response.choices[0].message.content}\n  typescript TypeScript theme={null}\n  import OpenAI from 'openai';\n  import { wrapOpenAI } from \"langsmith/wrappers\";\n\nconst client = wrapOpenAI(new OpenAI());\n\n// This is the function you will evaluate.\n  const target = async(inputs) => {\n    // This assumes your dataset has inputs with a `messages` key\n    const messages = inputs.messages;\n    const response = await client.chat.completions.create({\n        messages: messages,\n        model: 'gpt-4o-mini',\n    });\n    return { answer: response.choices[0].message.content };\n  }\n  python Python (LangChain) theme={null}\n  from langchain.chat_models import init_chat_model\n\nmodel = init_chat_model(\"gpt-4o-mini\")\n\ndef target(inputs: dict) -> dict:\n    # This assumes your dataset has inputs with a `messages` key\n    messages = inputs[\"messages\"]\n    response = model.invoke(messages)\n    return {\"answer\": response.content}\n  typescript TypeScript (LangChain) theme={null}\n  import { ChatOpenAI } from '@langchain/openai';\n\n// This is the function you will evaluate.\n  const target = async(inputs) => {\n    // This assumes your dataset has inputs with a `messages` key\n    const messages = inputs.messages;\n    const model = new ChatOpenAI({ model: \"gpt-4o-mini\" });\n    const response = await model.invoke(messages);\n    return {\"answer\": response.content};\n  }\n  python Python theme={null}\n  from langsmith import traceable\n\n# Optionally decorate with '@traceable' to trace all invocations of this function.\n  @traceable\n  def calculator_tool(operation: str, number1: float, number2: float) -> str:\n    if operation == \"add\":\n        return str(number1 + number2)\n    elif operation == \"subtract\":\n        return str(number1 - number2)\n    elif operation == \"multiply\":\n        return str(number1 * number2)\n    elif operation == \"divide\":\n        return str(number1 / number2)\n    else:\n        raise ValueError(f\"Unrecognized operation: {operation}.\")\n\n# This is the function you will evaluate.\n  def target(inputs: dict) -> dict:\n    # This assumes your dataset has inputs with `operation`, `num1`, and `num2` keys.\n    operation = inputs[\"operation\"]\n    number1 = inputs[\"num1\"]\n    number2 = inputs[\"num2\"]\n    result = calculator_tool(operation, number1, number2)\n    return {\"result\": result}\n  typescript TypeScript theme={null}\n  import { traceable } from \"langsmith/traceable\";\n\n// Optionally wrap in 'traceable' to trace all invocations of this function.\n  const calculatorTool = traceable(async ({ operation, number1, number2 }) => {\n  // Functions must return strings\n  if (operation === \"add\") {\n    return (number1 + number2).toString();\n  } else if (operation === \"subtract\") {\n    return (number1 - number2).toString();\n  } else if (operation === \"multiply\") {\n    return (number1 * number2).toString();\n  } else if (operation === \"divide\") {\n    return (number1 / number2).toString();\n  } else {\n    throw new Error(\"Invalid operation.\");\n  }\n  });\n\n// This is the function you will evaluate.\n  const target = async (inputs) => {\n  // This assumes your dataset has inputs with `operation`, `num1`, and `num2` keys\n  const result = await calculatorTool.invoke({\n    operation: inputs.operation,\n    number1: inputs.num1,\n    number2: inputs.num2,\n  });\n  return { result };\n  }\n  python Python theme={null}\n  from my_agent import agent\n\n# This is the function you will evaluate.\n  def target(inputs: dict) -> dict:\n    # This assumes your dataset has inputs with a `messages` key\n    messages = inputs[\"messages\"]\n    # Replace `invoke` with whatever you use to call your agent\n    response = agent.invoke({\"messages\": messages})\n    # This assumes your agent output is in the right format\n    return response\n  typescript TypeScript theme={null}\n  import { agent } from 'my_agent';\n\n// This is the function you will evaluate.\n  const target = async(inputs) => {\n  // This assumes your dataset has inputs with a `messages` key\n  const messages = inputs.messages;\n  // Replace `invoke` with whatever you use to call your agent\n  const response = await agent.invoke({ messages });\n  // This assumes your agent output is in the right format\n  return response;\n  }\n  python  theme={null}\n  from my_agent import agent\n  from langsmith import Client\n  client = Client()\n  client.evaluate(agent, ...)\n  ```\n</Check>\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/define-target-function.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "code_samples": [
    {
      "code": "<Check>\n  `evaluate()` will automatically trace your target function. This means that if you run any traceable code within your target function, this will also be traced as child runs of the target trace.\n</Check>\n\n## Example: Single LLM call\n\n<CodeGroup>",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</CodeGroup>\n\n## Example: Non-LLM component\n\n<CodeGroup>",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</CodeGroup>\n\n## Example: Application or agent\n\n<CodeGroup>",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</CodeGroup>\n\n<Check>\n  If you have a LangGraph/LangChain agent that accepts the inputs defined in your dataset and that returns the output format you want to use in your evaluators, you can pass that object in as the target directly:",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": "Example: Single LLM call",
      "id": "example:-single-llm-call"
    },
    {
      "level": "h2",
      "text": "Example: Non-LLM component",
      "id": "example:-non-llm-component"
    },
    {
      "level": "h2",
      "text": "Example: Application or agent",
      "id": "example:-application-or-agent"
    }
  ],
  "url": "llms-txt#'outputs'-will-come-from-your-target-function.",
  "links": []
}