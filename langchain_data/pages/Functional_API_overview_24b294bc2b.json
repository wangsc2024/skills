{
  "title": "Functional API overview",
  "content": "Source: https://docs.langchain.com/oss/python/langgraph/functional-api\n\nThe **Functional API** allows you to add LangGraph's key features — [persistence](/oss/python/langgraph/persistence), [memory](/oss/python/langgraph/add-memory), [human-in-the-loop](/oss/python/langgraph/interrupts), and [streaming](/oss/python/langgraph/streaming) — to your applications with minimal changes to your existing code.\n\nIt is designed to integrate these features into existing code that may use standard language primitives for branching and control flow, such as `if` statements, `for` loops, and function calls. Unlike many data orchestration frameworks that require restructuring code into an explicit pipeline or DAG, the Functional API allows you to incorporate these capabilities without enforcing a rigid execution model.\n\nThe Functional API uses two key building blocks:\n\n* **`@entrypoint`** – Marks a function as the starting point of a workflow, encapsulating logic and managing execution flow, including handling long-running tasks and interrupts.\n* **[`@task`](https://reference.langchain.com/python/langgraph/func/#langgraph.func.task)** – Represents a discrete unit of work, such as an API call or data processing step, that can be executed asynchronously within an entrypoint. Tasks return a future-like object that can be awaited or resolved synchronously.\n\nThis provides a minimal abstraction for building workflows with state management and streaming.\n\n<Tip>\n  For information on how to use the functional API, see [Use Functional API](/oss/python/langgraph/use-functional-api).\n</Tip>\n\n## Functional API vs. Graph API\n\nFor users who prefer a more declarative approach, LangGraph's [Graph API](/oss/python/langgraph/graph-api) allows you to define workflows using a Graph paradigm. Both APIs share the same underlying runtime, so you can use them together in the same application.\n\nHere are some key differences:\n\n* **Control flow**: The Functional API does not require thinking about graph structure. You can use standard Python constructs to define workflows. This will usually trim the amount of code you need to write.\n* **Short-term memory**: The **GraphAPI** requires declaring a [**State**](/oss/python/langgraph/graph-api#state) and may require defining [**reducers**](/oss/python/langgraph/graph-api#reducers) to manage updates to the graph state. `@entrypoint` and `@tasks` do not require explicit state management as their state is scoped to the function and is not shared across functions.\n* **Checkpointing**: Both APIs generate and use checkpoints. In the **Graph API** a new checkpoint is generated after every [superstep](/oss/python/langgraph/graph-api). In the **Functional API**, when tasks are executed, their results are saved to an existing checkpoint associated with the given entrypoint instead of creating a new checkpoint.\n* **Visualization**: The Graph API makes it easy to visualize the workflow as a graph which can be useful for debugging, understanding the workflow, and sharing with others. The Functional API does not support visualization as the graph is dynamically generated during runtime.\n\nBelow we demonstrate a simple application that writes an essay and [interrupts](/oss/python/langgraph/interrupts) to request human review.\n\n<Accordion title=\"Detailed Explanation\">\n  This workflow will write an essay about the topic \"cat\" and then pause to get a review from a human. The workflow can be interrupted for an indefinite amount of time until a review is provided.\n\nWhen the workflow is resumed, it executes from the very start, but because the result of the `writeEssay` task was already saved, the task result will be loaded from the checkpoint instead of being recomputed.\n\nAn essay has been written and is ready for review. Once the review is provided, we can resume the workflow:\n\nThe workflow has been completed and the review has been added to the essay.\n</Accordion>\n\nThe [`@entrypoint`](https://reference.langchain.com/python/langgraph/func/#langgraph.func.entrypoint) decorator can be used to create a workflow from a function. It encapsulates workflow logic and manages execution flow, including handling *long-running tasks* and [interrupts](/oss/python/langgraph/interrupts).\n\nAn **entrypoint** is defined by decorating a function with the `@entrypoint` decorator.\n\nThe function **must accept a single positional argument**, which serves as the workflow input. If you need to pass multiple pieces of data, use a dictionary as the input type for the first argument.\n\nDecorating a function with an `entrypoint` produces a [`Pregel`](https://reference.langchain.com/python/langgraph/pregel/#langgraph.pregel.Pregel.stream) instance which helps to manage the execution of the workflow (e.g., handles streaming, resumption, and checkpointing).\n\nYou will usually want to pass a **checkpointer** to the `@entrypoint` decorator to enable persistence and use features like **human-in-the-loop**.\n\n<Tabs>\n  <Tab title=\"Sync\">\n    \n  </Tab>\n\n<Tab title=\"Async\">\n    \n  </Tab>\n</Tabs>\n\n<Warning>\n  **Serialization**\n  The **inputs** and **outputs** of entrypoints must be JSON-serializable to support checkpointing. Please see the [serialization](#serialization) section for more details.\n</Warning>\n\n### Injectable parameters\n\nWhen declaring an `entrypoint`, you can request access to additional parameters that will be injected automatically at run time. These parameters include:\n\n| Parameter    | Description                                                                                                                                                                 |\n| ------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| **previous** | Access the state associated with the previous `checkpoint` for the given thread. See [short-term-memory](#short-term-memory).                                               |\n| **store**    | An instance of \\[BaseStore]\\[langgraph.store.base.BaseStore]. Useful for [long-term memory](/oss/python/langgraph/use-functional-api#long-term-memory).                     |\n| **writer**   | Use to access the StreamWriter when working with Async Python \\< 3.11. See [streaming with functional API for details](/oss/python/langgraph/use-functional-api#streaming). |\n| **config**   | For accessing run time configuration. See [RunnableConfig](https://python.langchain.com/docs/concepts/runnables/#runnableconfig) for information.                           |\n\n<Warning>\n  Declare the parameters with the appropriate name and type annotation.\n</Warning>\n\n<Accordion title=\"Requesting Injectable Parameters\">\n  \n</Accordion>\n\nUsing the [`@entrypoint`](#entrypoint) yields a [`Pregel`](https://reference.langchain.com/python/langgraph/pregel/#langgraph.pregel.Pregel.stream) object that can be executed using the `invoke`, `ainvoke`, `stream`, and `astream` methods.\n\n<Tabs>\n  <Tab title=\"Invoke\">\n    \n  </Tab>\n\n<Tab title=\"Async Invoke\">\n    \n  </Tab>\n\n<Tab title=\"Stream\">\n    \n  </Tab>\n\n<Tab title=\"Async Stream\">\n    \n  </Tab>\n</Tabs>\n\nResuming an execution after an [interrupt](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) can be done by passing a **resume** value to the [`Command`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.Command) primitive.\n\n<Tabs>\n  <Tab title=\"Invoke\">\n    \n  </Tab>\n\n<Tab title=\"Async Invoke\">\n    \n  </Tab>\n\n<Tab title=\"Stream\">\n    \n  </Tab>\n\n<Tab title=\"Async Stream\">\n    \n  </Tab>\n</Tabs>\n\n**Resuming after an error**\n\nTo resume after an error, run the `entrypoint` with a `None` and the same **thread id** (config).\n\nThis assumes that the underlying **error** has been resolved and execution can proceed successfully.\n\n<Tabs>\n  <Tab title=\"Invoke\">\n    \n  </Tab>\n\n<Tab title=\"Async Invoke\">\n    \n  </Tab>\n\n<Tab title=\"Stream\">\n    \n  </Tab>\n\n<Tab title=\"Async Stream\">\n    \n  </Tab>\n</Tabs>\n\n### Short-term memory\n\nWhen an `entrypoint` is defined with a `checkpointer`, it stores information between successive invocations on the same **thread id** in [checkpoints](/oss/python/langgraph/persistence#checkpoints).\n\nThis allows accessing the state from the previous invocation using the `previous` parameter.\n\nBy default, the `previous` parameter is the return value of the previous invocation.\n\n#### `entrypoint.final`\n\n[`entrypoint.final`](https://reference.langchain.com/python/langgraph/func/#langgraph.func.entrypoint.final) is a special primitive that can be returned from an entrypoint and allows **decoupling** the value that is **saved in the checkpoint** from the **return value of the entrypoint**.\n\nThe first value is the return value of the entrypoint, and the second value is the value that will be saved in the checkpoint. The type annotation is `entrypoint.final[return_type, save_type]`.\n\nA **task** represents a discrete unit of work, such as an API call or data processing step. It has two key characteristics:\n\n* **Asynchronous Execution**: Tasks are designed to be executed asynchronously, allowing multiple operations to run concurrently without blocking.\n* **Checkpointing**: Task results are saved to a checkpoint, enabling resumption of the workflow from the last saved state. (See [persistence](/oss/python/langgraph/persistence) for more details).\n\nTasks are defined using the `@task` decorator, which wraps a regular Python function.\n\n<Warning>\n  **Serialization**\n  The **outputs** of tasks must be JSON-serializable to support checkpointing.\n</Warning>\n\n**Tasks** can only be called from within an **entrypoint**, another **task**, or a [state graph node](/oss/python/langgraph/graph-api#nodes).\n\nTasks *cannot* be called directly from the main application code.\n\nWhen you call a **task**, it returns *immediately* with a future object. A future is a placeholder for a result that will be available later.\n\nTo obtain the result of a **task**, you can either wait for it synchronously (using `result()`) or await it asynchronously (using `await`).\n\n<Tabs>\n  <Tab title=\"Synchronous Invocation\">\n    \n  </Tab>\n\n<Tab title=\"Asynchronous Invocation\">\n    \n  </Tab>\n</Tabs>\n\n## When to use a task\n\n**Tasks** are useful in the following scenarios:\n\n* **Checkpointing**: When you need to save the result of a long-running operation to a checkpoint, so you don't need to recompute it when resuming the workflow.\n* **Human-in-the-loop**: If you're building a workflow that requires human intervention, you MUST use **tasks** to encapsulate any randomness (e.g., API calls) to ensure that the workflow can be resumed correctly. See the [determinism](#determinism) section for more details.\n* **Parallel Execution**: For I/O-bound tasks, **tasks** enable parallel execution, allowing multiple operations to run concurrently without blocking (e.g., calling multiple APIs).\n* **Observability**: Wrapping operations in **tasks** provides a way to track the progress of the workflow and monitor the execution of individual operations using [LangSmith](https://docs.langchain.com/langsmith/home).\n* **Retryable Work**: When work needs to be retried to handle failures or inconsistencies, **tasks** provide a way to encapsulate and manage the retry logic.\n\nThere are two key aspects to serialization in LangGraph:\n\n1. `entrypoint` inputs and outputs must be JSON-serializable.\n2. `task` outputs must be JSON-serializable.\n\nThese requirements are necessary for enabling checkpointing and workflow resumption. Use python primitives like dictionaries, lists, strings, numbers, and booleans to ensure that your inputs and outputs are serializable.\n\nSerialization ensures that workflow state, such as task results and intermediate values, can be reliably saved and restored. This is critical for enabling human-in-the-loop interactions, fault tolerance, and parallel execution.\n\nProviding non-serializable inputs or outputs will result in a runtime error when a workflow is configured with a checkpointer.\n\nTo utilize features like **human-in-the-loop**, any randomness should be encapsulated inside of **tasks**. This guarantees that when execution is halted (e.g., for human in the loop) and then resumed, it will follow the same *sequence of steps*, even if **task** results are non-deterministic.\n\nLangGraph achieves this behavior by persisting **task** and [**subgraph**](/oss/python/langgraph/use-subgraphs) results as they execute. A well-designed workflow ensures that resuming execution follows the *same sequence of steps*, allowing previously computed results to be retrieved correctly without having to re-execute them. This is particularly useful for long-running **tasks** or **tasks** with non-deterministic results, as it avoids repeating previously done work and allows resuming from essentially the same.\n\nWhile different runs of a workflow can produce different results, resuming a **specific** run should always follow the same sequence of recorded steps. This allows LangGraph to efficiently look up **task** and **subgraph** results that were executed prior to the graph being interrupted and avoid recomputing them.\n\nIdempotency ensures that running the same operation multiple times produces the same result. This helps prevent duplicate API calls and redundant processing if a step is rerun due to a failure. Always place API calls inside **tasks** functions for checkpointing, and design them to be idempotent in case of re-execution. Re-execution can occur if a **task** starts, but does not complete successfully. Then, if the workflow is resumed, the **task** will run again. Use idempotency keys or verify existing results to avoid duplication.\n\n### Handling side effects\n\nEncapsulate side effects (e.g., writing to a file, sending an email) in tasks to ensure they are not executed multiple times when resuming a workflow.\n\n<Tabs>\n  <Tab title=\"Incorrect\">\n    In this example, a side effect (writing to a file) is directly included in the workflow, so it will be executed a second time when resuming the workflow.\n\n<Tab title=\"Correct\">\n    In this example, the side effect is encapsulated in a task, ensuring consistent execution upon resumption.\n\n### Non-deterministic control flow\n\nOperations that might give different results each time (like getting current time or random numbers) should be encapsulated in tasks to ensure that on resume, the same result is returned.\n\n* In a task: Get random number (5) → interrupt → resume → (returns 5 again) → ...\n* Not in a task: Get random number (5) → interrupt → resume → get new random number (7) → ...\n\nThis is especially important when using **human-in-the-loop** workflows with multiple interrupts calls. LangGraph keeps a list of resume values for each task/entrypoint. When an interrupt is encountered, it's matched with the corresponding resume value. This matching is strictly **index-based**, so the order of the resume values should match the order of the interrupts.\n\nIf order of execution is not maintained when resuming, one [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) call may be matched with the wrong `resume` value, leading to incorrect results.\n\nPlease read the section on [determinism](#determinism) for more details.\n\n<Tabs>\n  <Tab title=\"Incorrect\">\n    In this example, the workflow uses the current time to determine which task to execute. This is non-deterministic because the result of the workflow depends on the time at which it is executed.\n\n<Tab title=\"Correct\">\n    In this example, the workflow uses the input `t0` to determine which task to execute. This is deterministic because the result of the workflow depends only on the input.\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/functional-api.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "code_samples": [
    {
      "code": "<Accordion title=\"Detailed Explanation\">\n  This workflow will write an essay about the topic \"cat\" and then pause to get a review from a human. The workflow can be interrupted for an indefinite amount of time until a review is provided.\n\n  When the workflow is resumed, it executes from the very start, but because the result of the `writeEssay` task was already saved, the task result will be loaded from the checkpoint instead of being recomputed.",
      "language": "unknown"
    },
    {
      "code": "An essay has been written and is ready for review. Once the review is provided, we can resume the workflow:",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "The workflow has been completed and the review has been added to the essay.\n</Accordion>\n\n## Entrypoint\n\nThe [`@entrypoint`](https://reference.langchain.com/python/langgraph/func/#langgraph.func.entrypoint) decorator can be used to create a workflow from a function. It encapsulates workflow logic and manages execution flow, including handling *long-running tasks* and [interrupts](/oss/python/langgraph/interrupts).\n\n### Definition\n\nAn **entrypoint** is defined by decorating a function with the `@entrypoint` decorator.\n\nThe function **must accept a single positional argument**, which serves as the workflow input. If you need to pass multiple pieces of data, use a dictionary as the input type for the first argument.\n\nDecorating a function with an `entrypoint` produces a [`Pregel`](https://reference.langchain.com/python/langgraph/pregel/#langgraph.pregel.Pregel.stream) instance which helps to manage the execution of the workflow (e.g., handles streaming, resumption, and checkpointing).\n\nYou will usually want to pass a **checkpointer** to the `@entrypoint` decorator to enable persistence and use features like **human-in-the-loop**.\n\n<Tabs>\n  <Tab title=\"Sync\">",
      "language": "unknown"
    },
    {
      "code": "</Tab>\n\n  <Tab title=\"Async\">",
      "language": "unknown"
    },
    {
      "code": "</Tab>\n</Tabs>\n\n<Warning>\n  **Serialization**\n  The **inputs** and **outputs** of entrypoints must be JSON-serializable to support checkpointing. Please see the [serialization](#serialization) section for more details.\n</Warning>\n\n### Injectable parameters\n\nWhen declaring an `entrypoint`, you can request access to additional parameters that will be injected automatically at run time. These parameters include:\n\n| Parameter    | Description                                                                                                                                                                 |\n| ------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| **previous** | Access the state associated with the previous `checkpoint` for the given thread. See [short-term-memory](#short-term-memory).                                               |\n| **store**    | An instance of \\[BaseStore]\\[langgraph.store.base.BaseStore]. Useful for [long-term memory](/oss/python/langgraph/use-functional-api#long-term-memory).                     |\n| **writer**   | Use to access the StreamWriter when working with Async Python \\< 3.11. See [streaming with functional API for details](/oss/python/langgraph/use-functional-api#streaming). |\n| **config**   | For accessing run time configuration. See [RunnableConfig](https://python.langchain.com/docs/concepts/runnables/#runnableconfig) for information.                           |\n\n<Warning>\n  Declare the parameters with the appropriate name and type annotation.\n</Warning>\n\n<Accordion title=\"Requesting Injectable Parameters\">",
      "language": "unknown"
    },
    {
      "code": "</Accordion>\n\n### Executing\n\nUsing the [`@entrypoint`](#entrypoint) yields a [`Pregel`](https://reference.langchain.com/python/langgraph/pregel/#langgraph.pregel.Pregel.stream) object that can be executed using the `invoke`, `ainvoke`, `stream`, and `astream` methods.\n\n<Tabs>\n  <Tab title=\"Invoke\">",
      "language": "unknown"
    },
    {
      "code": "</Tab>\n\n  <Tab title=\"Async Invoke\">",
      "language": "unknown"
    },
    {
      "code": "</Tab>\n\n  <Tab title=\"Stream\">",
      "language": "unknown"
    },
    {
      "code": "</Tab>\n\n  <Tab title=\"Async Stream\">",
      "language": "unknown"
    },
    {
      "code": "</Tab>\n</Tabs>\n\n### Resuming\n\nResuming an execution after an [interrupt](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) can be done by passing a **resume** value to the [`Command`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.Command) primitive.\n\n<Tabs>\n  <Tab title=\"Invoke\">",
      "language": "unknown"
    },
    {
      "code": "</Tab>\n\n  <Tab title=\"Async Invoke\">",
      "language": "unknown"
    },
    {
      "code": "</Tab>\n\n  <Tab title=\"Stream\">",
      "language": "unknown"
    },
    {
      "code": "</Tab>\n\n  <Tab title=\"Async Stream\">",
      "language": "unknown"
    },
    {
      "code": "</Tab>\n</Tabs>\n\n**Resuming after an error**\n\nTo resume after an error, run the `entrypoint` with a `None` and the same **thread id** (config).\n\nThis assumes that the underlying **error** has been resolved and execution can proceed successfully.\n\n<Tabs>\n  <Tab title=\"Invoke\">",
      "language": "unknown"
    },
    {
      "code": "</Tab>\n\n  <Tab title=\"Async Invoke\">",
      "language": "unknown"
    },
    {
      "code": "</Tab>\n\n  <Tab title=\"Stream\">",
      "language": "unknown"
    },
    {
      "code": "</Tab>\n\n  <Tab title=\"Async Stream\">",
      "language": "unknown"
    },
    {
      "code": "</Tab>\n</Tabs>\n\n### Short-term memory\n\nWhen an `entrypoint` is defined with a `checkpointer`, it stores information between successive invocations on the same **thread id** in [checkpoints](/oss/python/langgraph/persistence#checkpoints).\n\nThis allows accessing the state from the previous invocation using the `previous` parameter.\n\nBy default, the `previous` parameter is the return value of the previous invocation.",
      "language": "unknown"
    },
    {
      "code": "#### `entrypoint.final`\n\n[`entrypoint.final`](https://reference.langchain.com/python/langgraph/func/#langgraph.func.entrypoint.final) is a special primitive that can be returned from an entrypoint and allows **decoupling** the value that is **saved in the checkpoint** from the **return value of the entrypoint**.\n\nThe first value is the return value of the entrypoint, and the second value is the value that will be saved in the checkpoint. The type annotation is `entrypoint.final[return_type, save_type]`.",
      "language": "unknown"
    },
    {
      "code": "## Task\n\nA **task** represents a discrete unit of work, such as an API call or data processing step. It has two key characteristics:\n\n* **Asynchronous Execution**: Tasks are designed to be executed asynchronously, allowing multiple operations to run concurrently without blocking.\n* **Checkpointing**: Task results are saved to a checkpoint, enabling resumption of the workflow from the last saved state. (See [persistence](/oss/python/langgraph/persistence) for more details).\n\n### Definition\n\nTasks are defined using the `@task` decorator, which wraps a regular Python function.",
      "language": "unknown"
    },
    {
      "code": "<Warning>\n  **Serialization**\n  The **outputs** of tasks must be JSON-serializable to support checkpointing.\n</Warning>\n\n### Execution\n\n**Tasks** can only be called from within an **entrypoint**, another **task**, or a [state graph node](/oss/python/langgraph/graph-api#nodes).\n\nTasks *cannot* be called directly from the main application code.\n\nWhen you call a **task**, it returns *immediately* with a future object. A future is a placeholder for a result that will be available later.\n\nTo obtain the result of a **task**, you can either wait for it synchronously (using `result()`) or await it asynchronously (using `await`).\n\n<Tabs>\n  <Tab title=\"Synchronous Invocation\">",
      "language": "unknown"
    },
    {
      "code": "</Tab>\n\n  <Tab title=\"Asynchronous Invocation\">",
      "language": "unknown"
    },
    {
      "code": "</Tab>\n</Tabs>\n\n## When to use a task\n\n**Tasks** are useful in the following scenarios:\n\n* **Checkpointing**: When you need to save the result of a long-running operation to a checkpoint, so you don't need to recompute it when resuming the workflow.\n* **Human-in-the-loop**: If you're building a workflow that requires human intervention, you MUST use **tasks** to encapsulate any randomness (e.g., API calls) to ensure that the workflow can be resumed correctly. See the [determinism](#determinism) section for more details.\n* **Parallel Execution**: For I/O-bound tasks, **tasks** enable parallel execution, allowing multiple operations to run concurrently without blocking (e.g., calling multiple APIs).\n* **Observability**: Wrapping operations in **tasks** provides a way to track the progress of the workflow and monitor the execution of individual operations using [LangSmith](https://docs.langchain.com/langsmith/home).\n* **Retryable Work**: When work needs to be retried to handle failures or inconsistencies, **tasks** provide a way to encapsulate and manage the retry logic.\n\n## Serialization\n\nThere are two key aspects to serialization in LangGraph:\n\n1. `entrypoint` inputs and outputs must be JSON-serializable.\n2. `task` outputs must be JSON-serializable.\n\nThese requirements are necessary for enabling checkpointing and workflow resumption. Use python primitives like dictionaries, lists, strings, numbers, and booleans to ensure that your inputs and outputs are serializable.\n\nSerialization ensures that workflow state, such as task results and intermediate values, can be reliably saved and restored. This is critical for enabling human-in-the-loop interactions, fault tolerance, and parallel execution.\n\nProviding non-serializable inputs or outputs will result in a runtime error when a workflow is configured with a checkpointer.\n\n## Determinism\n\nTo utilize features like **human-in-the-loop**, any randomness should be encapsulated inside of **tasks**. This guarantees that when execution is halted (e.g., for human in the loop) and then resumed, it will follow the same *sequence of steps*, even if **task** results are non-deterministic.\n\nLangGraph achieves this behavior by persisting **task** and [**subgraph**](/oss/python/langgraph/use-subgraphs) results as they execute. A well-designed workflow ensures that resuming execution follows the *same sequence of steps*, allowing previously computed results to be retrieved correctly without having to re-execute them. This is particularly useful for long-running **tasks** or **tasks** with non-deterministic results, as it avoids repeating previously done work and allows resuming from essentially the same.\n\nWhile different runs of a workflow can produce different results, resuming a **specific** run should always follow the same sequence of recorded steps. This allows LangGraph to efficiently look up **task** and **subgraph** results that were executed prior to the graph being interrupted and avoid recomputing them.\n\n## Idempotency\n\nIdempotency ensures that running the same operation multiple times produces the same result. This helps prevent duplicate API calls and redundant processing if a step is rerun due to a failure. Always place API calls inside **tasks** functions for checkpointing, and design them to be idempotent in case of re-execution. Re-execution can occur if a **task** starts, but does not complete successfully. Then, if the workflow is resumed, the **task** will run again. Use idempotency keys or verify existing results to avoid duplication.\n\n## Common Pitfalls\n\n### Handling side effects\n\nEncapsulate side effects (e.g., writing to a file, sending an email) in tasks to ensure they are not executed multiple times when resuming a workflow.\n\n<Tabs>\n  <Tab title=\"Incorrect\">\n    In this example, a side effect (writing to a file) is directly included in the workflow, so it will be executed a second time when resuming the workflow.",
      "language": "unknown"
    },
    {
      "code": "</Tab>\n\n  <Tab title=\"Correct\">\n    In this example, the side effect is encapsulated in a task, ensuring consistent execution upon resumption.",
      "language": "unknown"
    },
    {
      "code": "</Tab>\n</Tabs>\n\n### Non-deterministic control flow\n\nOperations that might give different results each time (like getting current time or random numbers) should be encapsulated in tasks to ensure that on resume, the same result is returned.\n\n* In a task: Get random number (5) → interrupt → resume → (returns 5 again) → ...\n* Not in a task: Get random number (5) → interrupt → resume → get new random number (7) → ...\n\nThis is especially important when using **human-in-the-loop** workflows with multiple interrupts calls. LangGraph keeps a list of resume values for each task/entrypoint. When an interrupt is encountered, it's matched with the corresponding resume value. This matching is strictly **index-based**, so the order of the resume values should match the order of the interrupts.\n\nIf order of execution is not maintained when resuming, one [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) call may be matched with the wrong `resume` value, leading to incorrect results.\n\nPlease read the section on [determinism](#determinism) for more details.\n\n<Tabs>\n  <Tab title=\"Incorrect\">\n    In this example, the workflow uses the current time to determine which task to execute. This is non-deterministic because the result of the workflow depends on the time at which it is executed.",
      "language": "unknown"
    },
    {
      "code": "</Tab>\n\n  <Tab title=\"Correct\">\n    In this example, the workflow uses the input `t0` to determine which task to execute. This is deterministic because the result of the workflow depends only on the input.",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": "Functional API vs. Graph API",
      "id": "functional-api-vs.-graph-api"
    },
    {
      "level": "h2",
      "text": "Example",
      "id": "example"
    },
    {
      "level": "h2",
      "text": "Entrypoint",
      "id": "entrypoint"
    },
    {
      "level": "h3",
      "text": "Definition",
      "id": "definition"
    },
    {
      "level": "h3",
      "text": "Injectable parameters",
      "id": "injectable-parameters"
    },
    {
      "level": "h3",
      "text": "Executing",
      "id": "executing"
    },
    {
      "level": "h3",
      "text": "Resuming",
      "id": "resuming"
    },
    {
      "level": "h3",
      "text": "Short-term memory",
      "id": "short-term-memory"
    },
    {
      "level": "h2",
      "text": "Task",
      "id": "task"
    },
    {
      "level": "h3",
      "text": "Definition",
      "id": "definition"
    },
    {
      "level": "h3",
      "text": "Execution",
      "id": "execution"
    },
    {
      "level": "h2",
      "text": "When to use a task",
      "id": "when-to-use-a-task"
    },
    {
      "level": "h2",
      "text": "Serialization",
      "id": "serialization"
    },
    {
      "level": "h2",
      "text": "Determinism",
      "id": "determinism"
    },
    {
      "level": "h2",
      "text": "Idempotency",
      "id": "idempotency"
    },
    {
      "level": "h2",
      "text": "Common Pitfalls",
      "id": "common-pitfalls"
    },
    {
      "level": "h3",
      "text": "Handling side effects",
      "id": "handling-side-effects"
    },
    {
      "level": "h3",
      "text": "Non-deterministic control flow",
      "id": "non-deterministic-control-flow"
    }
  ],
  "url": "llms-txt#functional-api-overview",
  "links": []
}