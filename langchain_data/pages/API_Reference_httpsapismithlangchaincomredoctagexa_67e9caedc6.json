{
  "title": "API Reference: https://api.smith.langchain.com/redoc#tag/examples/operation/read_examples_api_v1_examples_get",
  "content": "dataset_id = dataset.id\nparams = { \"dataset\": dataset_id }\n\nresp = requests.get(\n    \"https://api.smith.langchain.com/api/v1/examples\",\n    params=params,\n    headers={\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]}\n)\n\nexamples = resp.json()\npython  theme={null}\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n\ndef run_completion_on_example(example, model_name, experiment_id):\n    \"\"\"Run completions on a list of examples.\"\"\"\n    # We are using the OpenAI API here, but you can use any model you like\n\ndef _post_run(run_id, name, run_type, inputs, parent_id=None):\n        \"\"\"Function to post a new run to the API.\n        API Reference: https://api.smith.langchain.com/redoc#tag/run/operation/create_run_api_v1_runs_post\n        \"\"\"\n        data = {\n            \"id\": run_id.hex,\n            \"name\": name,\n            \"run_type\": run_type,\n            \"inputs\": inputs,\n            \"start_time\": datetime.utcnow().isoformat(),\n            \"reference_example_id\": example[\"id\"],\n            \"session_id\": experiment_id,\n        }\n        if parent_id:\n            data[\"parent_run_id\"] = parent_id.hex\n        resp = requests.post(\n            \"https://api.smith.langchain.com/api/v1/runs\", # Update appropriately for self-hosted installations or the EU region\n            json=data,\n            headers=headers\n        )\n        resp.raise_for_status()\n\ndef _patch_run(run_id, outputs):\n        \"\"\"Function to patch a run with outputs.\n        API Reference: https://api.smith.langchain.com/redoc#tag/run/operation/update_run_api_v1_runs__run_id__patch\n        \"\"\"\n        resp = requests.patch(\n            f\"https://api.smith.langchain.com/api/v1/runs/{run_id}\",\n            json={\n                \"outputs\": outputs,\n                \"end_time\": datetime.utcnow().isoformat(),\n            },\n            headers=headers,\n        )\n        resp.raise_for_status()\n\n# Send your API Key in the request headers\n    headers = {\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]}\n\ntext = example[\"inputs\"][\"text\"]\n\nmessages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\",\n        },\n        {\"role\": \"user\", \"content\": text},\n    ]\n\n# Create parent run\n    parent_run_id = uuid7()\n    _post_run(parent_run_id, \"LLM Pipeline\", \"chain\", {\"text\": text})\n\n# Create child run\n    child_run_id = uuid7()\n    _post_run(child_run_id, \"OpenAI Call\", \"llm\", {\"messages\": messages}, parent_run_id)\n\n# Generate completion\n    chat_completion = oa_client.chat.completions.create(model=model_name, messages=messages)\n    output_text = chat_completion.choices[0].message.content\n\n# End run\n    _patch_run(child_run_id, {\n    \"messages\": messages,\n        \"output\": output_text,\n        \"model\": model_name\n    })\n\n_patch_run(parent_run_id, {\"label\": output_text})\npython  theme={null}",
  "code_samples": [
    {
      "code": "from langsmith import uuid7\n\nNext, define a function that will run your model on a single example and log the results to LangSmith. When using the API directly, you're responsible for:\n\n* Creating run objects via POST to `/runs` with `reference_example_id` and `session_id` set.\n* Tracking parent-child relationships between runs (e.g., a parent \"chain\" run containing a child \"llm\" run).\n* Updating runs with outputs via PATCH to `/runs/{run_id}`.",
      "language": "unknown"
    },
    {
      "code": "Now create the experiments and run completions on all examples. In the API, an \"experiment\" is represented as a session (or \"tracer session\") that references a dataset via `reference_dataset_id`. The key difference from regular tracing is that runs in an experiment must have a `reference_example_id` that links each run to a specific example in the dataset.",
      "language": "unknown"
    }
  ],
  "headings": [],
  "url": "llms-txt#api-reference:-https://api.smith.langchain.com/redoc#tag/examples/operation/read_examples_api_v1_examples_get",
  "links": []
}