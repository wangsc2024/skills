{
  "title": "How to define a summary evaluator",
  "content": "Source: https://docs.langchain.com/langsmith/summary\n\nSome metrics can only be defined on the entire experiment level as opposed to the individual runs of the experiment. For example, you may want to compute the overall pass rate or f1 score of your evaluation target across all examples in the dataset. These are called summary evaluators.\n\nHere, we'll compute the f1-score, which is a combination of precision and recall.\n\nThis sort of metric can only be computed over all of the examples in our experiment, so our evaluator takes in a list of outputs, and a list of reference\\_outputs.\n\nYou can then pass this evaluator to the `evaluate` method as follows:\n\nIn the LangSmith UI, you'll the summary evaluator's score displayed with the corresponding key.\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/summary-eval.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=d0f259baa7d7467bf172ef8197c3bb17\" alt=\"summary_eval.png\" data-og-width=\"1535\" width=\"1535\" data-og-height=\"122\" height=\"122\" data-path=\"langsmith/images/summary-eval.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/summary-eval.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=076d830ea3952a4a598d25a2830232e0 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/summary-eval.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=a5f96c7cf258a92be14f489bc1a05f8c 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/summary-eval.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=4a2a77c6e0ae855a7027888591733e13 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/summary-eval.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=62eee3f7f104ae12e97ba22828a8bb2c 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/summary-eval.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=48d6045a20e1021aceadc98554e39e9e 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/summary-eval.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=d72ea1cfc84b0cb33461eb36ea47c696 2500w\" />\n\n## Summary evaluator args\n\nSummary evaluator functions must have specific argument names. They can take any subset of the following arguments:\n\n* `inputs: list[dict]`: A list of the inputs corresponding to a single example in a dataset.\n* `outputs: list[dict]`: A list of the dict outputs produced by each experiment on the given inputs.\n* `reference_outputs/referenceOutputs: list[dict]`: A list of the reference outputs associated with the example, if available.\n* `runs: list[Run]`: A list of the full [Run](/langsmith/run-data-format) objects generated by the two experiments on the given example. Use this if you need access to intermediate steps or metadata about each run.\n* `examples: list[Example]`: All of the dataset [Example](/langsmith/example-data-format) objects, including the example inputs, outputs (if available), and metdata (if available).\n\n## Summary evaluator output\n\nSummary evaluators are expected to return one of the following types:\n\n* `dict`: dicts of the form `{\"score\": ..., \"name\": ...}` allow you to pass a numeric or boolean score and metric name.\n\nCurrently Python only\n\n* `int | float | bool`: this is interepreted as an continuous metric that can be averaged, sorted, etc. The function name is used as the name of the metric.\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/summary.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "code_samples": [
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</CodeGroup>\n\nYou can then pass this evaluator to the `evaluate` method as follows:\n\n<CodeGroup>",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": "Basic example",
      "id": "basic-example"
    },
    {
      "level": "h2",
      "text": "Summary evaluator args",
      "id": "summary-evaluator-args"
    },
    {
      "level": "h2",
      "text": "Summary evaluator output",
      "id": "summary-evaluator-output"
    }
  ],
  "url": "llms-txt#how-to-define-a-summary-evaluator",
  "links": []
}