{
  "title": "Run an evaluation from the prompt playground",
  "content": "Source: https://docs.langchain.com/langsmith/run-evaluation-from-prompt-playground\n\nLangSmith allows you to run evaluations directly in the UI. The [**Prompt Playground**](/langsmith/prompt-engineering#prompt-playground) allows you to test your prompt or model configuration over a series of inputs to see how well it scores across different contexts or scenarios, without having to write any code.\n\nBefore you run an evaluation, you need to have an [existing dataset](/langsmith/evaluation-concepts#datasets). Learn how to [create a dataset from the UI](/langsmith/manage-datasets-in-application#set-up-your-dataset).\n\nIf you prefer to run experiments in code, visit [run an evaluation using the SDK](/langsmith/evaluate-llm-application).\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/playground-experiment.gif?s=05ec3d2b1aa6590c443a033924fc6141\" alt=\"Playground experiment\" data-og-width=\"1358\" width=\"1358\" data-og-height=\"720\" height=\"720\" data-path=\"langsmith/images/playground-experiment.gif\" data-optimize=\"true\" data-opv=\"3\" />\n\n<Callout type=\"info\" icon=\"bird\">\n  **[Polly](/langsmith/polly)** is available in the Playground to help you optimize prompts before running evaluations.\n</Callout>\n\n## Create an experiment in the prompt playground[​](#create-an-experiment-in-the-prompt-playground \"Direct link to Create an experiment in the prompt playground\")\n\n1. **Navigate to the playground** by clicking **Playground** in the sidebar.\n2. **Add a prompt** by selecting an existing saved a prompt or creating a new one.\n3. **Select a dataset** from the **Test over dataset** dropdown\n\n* Note that the keys in the dataset input must match the input variables of the prompt. For example, in the above video the selected dataset has inputs with the key \"blog\", which correctly match the input variable of the prompt.\n* There is a maximum of 15 input variables allowed in the prompt playground.\n\n4. **Start the experiment** by clicking on the **Start** or CMD+Enter. This will run the prompt over all the examples in the dataset and create an entry for the experiment in the dataset details page. We recommend committing the prompt to the prompt hub before starting the experiment so that it can be easily referenced later when reviewing your experiment.\n5. **View the full results** by clicking **View full experiment**. This will take you to the experiment details page where you can see the results of the experiment.\n\n## Add evaluation scores to the experiment[​](#add-evaluation-scores-to-the-experiment \"Direct link to Add evaluation scores to the experiment\")\n\nEvaluate your experiment over specific critera by adding evaluators. Add LLM-as-a-judge or custom code evaluators in the playground using the **+Evaluator** button.\n\nTo learn more about adding evaluators in via UI, visit [how to define an LLM-as-a-judge evaluator](/langsmith/llm-as-judge).\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/run-evaluation-from-prompt-playground.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "code_samples": [],
  "headings": [
    {
      "level": "h2",
      "text": "Create an experiment in the prompt playground[​](#create-an-experiment-in-the-prompt-playground \"Direct link to Create an experiment in the prompt playground\")",
      "id": "create-an-experiment-in-the-prompt-playground[​](#create-an-experiment-in-the-prompt-playground-\"direct-link-to-create-an-experiment-in-the-prompt-playground\")"
    },
    {
      "level": "h2",
      "text": "Add evaluation scores to the experiment[​](#add-evaluation-scores-to-the-experiment \"Direct link to Add evaluation scores to the experiment\")",
      "id": "add-evaluation-scores-to-the-experiment[​](#add-evaluation-scores-to-the-experiment-\"direct-link-to-add-evaluation-scores-to-the-experiment\")"
    }
  ],
  "url": "llms-txt#run-an-evaluation-from-the-prompt-playground",
  "links": []
}