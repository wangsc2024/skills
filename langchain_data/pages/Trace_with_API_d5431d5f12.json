{
  "title": "Trace with API",
  "content": "Source: https://docs.langchain.com/langsmith/trace-with-api\n\nLearn how to trace your LLM applications using the LangSmith API directly.\n\nIt is **highly** recommended to use our Python or TypeScript SDKs to send traces to LangSmith. We have designed these SDKs with optimizations like batching and backgrounding to ensure that your application's performance is not impacted by sending traces to LangSmith. However, if you are unable to use our SDKs, you can use the LangSmith REST API to send traces. Performance may be impacted if you send traces synchronously in your application. This guide will show you how to trace a request using the LangSmith REST API. Please view our API documentation  for a full list of endpoints and request/response schemas.\n\nThe simplest way to log runs is via the POST and PATCH `/runs` endpoint. These routes expect minimal contextual information about the tree structure to\n\n<Note>\n  When using the LangSmith REST API, you will need to provide your API key in the request headers as `\"x-api-key\"`.\n\nIf your API key is linked to multiple workspaces, you will need to specify the workspace being used in the header with `\"x-tenant-id\"`.\n\nIn the simple example, you do not need to set the `dotted_order` opr `trace_id` fields in the request body. These fields will be automatically generated by the system. Though this is simpler, it is slower and has a lower rate limit in LangSmith.\n</Note>\n\nThe following example shows how you might leverage our API directly in Python. The same principles apply to other languages.\n\n```python  theme={null}\nimport openai\nimport os\nimport requests\nfrom datetime import datetime, timezone\nfrom langsmith import uuid7",
  "code_samples": [],
  "headings": [
    {
      "level": "h2",
      "text": "Basic tracing",
      "id": "basic-tracing"
    }
  ],
  "url": "llms-txt#trace-with-api",
  "links": []
}