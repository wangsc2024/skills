{
  "title": "Get the tool call",
  "content": "msg.tool_calls\npython Graph API theme={null}\n  from typing_extensions import TypedDict\n  from langgraph.graph import StateGraph, START, END\n  from IPython.display import Image, display\n\n# Graph state\n  class State(TypedDict):\n      topic: str\n      joke: str\n      improved_joke: str\n      final_joke: str\n\n# Nodes\n  def generate_joke(state: State):\n      \"\"\"First LLM call to generate initial joke\"\"\"\n\nmsg = llm.invoke(f\"Write a short joke about {state['topic']}\")\n      return {\"joke\": msg.content}\n\ndef check_punchline(state: State):\n      \"\"\"Gate function to check if the joke has a punchline\"\"\"\n\n# Simple check - does the joke contain \"?\" or \"!\"\n      if \"?\" in state[\"joke\"] or \"!\" in state[\"joke\"]:\n          return \"Pass\"\n      return \"Fail\"\n\ndef improve_joke(state: State):\n      \"\"\"Second LLM call to improve the joke\"\"\"\n\nmsg = llm.invoke(f\"Make this joke funnier by adding wordplay: {state['joke']}\")\n      return {\"improved_joke\": msg.content}\n\ndef polish_joke(state: State):\n      \"\"\"Third LLM call for final polish\"\"\"\n      msg = llm.invoke(f\"Add a surprising twist to this joke: {state['improved_joke']}\")\n      return {\"final_joke\": msg.content}\n\n# Build workflow\n  workflow = StateGraph(State)\n\n# Add nodes\n  workflow.add_node(\"generate_joke\", generate_joke)\n  workflow.add_node(\"improve_joke\", improve_joke)\n  workflow.add_node(\"polish_joke\", polish_joke)\n\n# Add edges to connect nodes\n  workflow.add_edge(START, \"generate_joke\")\n  workflow.add_conditional_edges(\n      \"generate_joke\", check_punchline, {\"Fail\": \"improve_joke\", \"Pass\": END}\n  )\n  workflow.add_edge(\"improve_joke\", \"polish_joke\")\n  workflow.add_edge(\"polish_joke\", END)\n\n# Compile\n  chain = workflow.compile()\n\n# Show workflow\n  display(Image(chain.get_graph().draw_mermaid_png()))\n\n# Invoke\n  state = chain.invoke({\"topic\": \"cats\"})\n  print(\"Initial joke:\")\n  print(state[\"joke\"])\n  print(\"\\n--- --- ---\\n\")\n  if \"improved_joke\" in state:\n      print(\"Improved joke:\")\n      print(state[\"improved_joke\"])\n      print(\"\\n--- --- ---\\n\")\n\nprint(\"Final joke:\")\n      print(state[\"final_joke\"])\n  else:\n      print(\"Final joke:\")\n      print(state[\"joke\"])\n  python Functional API theme={null}\n  from langgraph.func import entrypoint, task\n\n# Tasks\n  @task\n  def generate_joke(topic: str):\n      \"\"\"First LLM call to generate initial joke\"\"\"\n      msg = llm.invoke(f\"Write a short joke about {topic}\")\n      return msg.content\n\ndef check_punchline(joke: str):\n      \"\"\"Gate function to check if the joke has a punchline\"\"\"\n      # Simple check - does the joke contain \"?\" or \"!\"\n      if \"?\" in joke or \"!\" in joke:\n          return \"Fail\"\n\n@task\n  def improve_joke(joke: str):\n      \"\"\"Second LLM call to improve the joke\"\"\"\n      msg = llm.invoke(f\"Make this joke funnier by adding wordplay: {joke}\")\n      return msg.content\n\n@task\n  def polish_joke(joke: str):\n      \"\"\"Third LLM call for final polish\"\"\"\n      msg = llm.invoke(f\"Add a surprising twist to this joke: {joke}\")\n      return msg.content\n\n@entrypoint()\n  def prompt_chaining_workflow(topic: str):\n      original_joke = generate_joke(topic).result()\n      if check_punchline(original_joke) == \"Pass\":\n          return original_joke\n\nimproved_joke = improve_joke(original_joke).result()\n      return polish_joke(improved_joke).result()\n\n# Invoke\n  for step in prompt_chaining_workflow.stream(\"cats\", stream_mode=\"updates\"):\n      print(step)\n      print(\"\\n\")\n  python Graph API theme={null}\n  # Graph state\n  class State(TypedDict):\n      topic: str\n      joke: str\n      story: str\n      poem: str\n      combined_output: str\n\n# Nodes\n  def call_llm_1(state: State):\n      \"\"\"First LLM call to generate initial joke\"\"\"\n\nmsg = llm.invoke(f\"Write a joke about {state['topic']}\")\n      return {\"joke\": msg.content}\n\ndef call_llm_2(state: State):\n      \"\"\"Second LLM call to generate story\"\"\"\n\nmsg = llm.invoke(f\"Write a story about {state['topic']}\")\n      return {\"story\": msg.content}\n\ndef call_llm_3(state: State):\n      \"\"\"Third LLM call to generate poem\"\"\"\n\nmsg = llm.invoke(f\"Write a poem about {state['topic']}\")\n      return {\"poem\": msg.content}\n\ndef aggregator(state: State):\n      \"\"\"Combine the joke and story into a single output\"\"\"\n\ncombined = f\"Here's a story, joke, and poem about {state['topic']}!\\n\\n\"\n      combined += f\"STORY:\\n{state['story']}\\n\\n\"\n      combined += f\"JOKE:\\n{state['joke']}\\n\\n\"\n      combined += f\"POEM:\\n{state['poem']}\"\n      return {\"combined_output\": combined}\n\n# Build workflow\n  parallel_builder = StateGraph(State)\n\n# Add nodes\n  parallel_builder.add_node(\"call_llm_1\", call_llm_1)\n  parallel_builder.add_node(\"call_llm_2\", call_llm_2)\n  parallel_builder.add_node(\"call_llm_3\", call_llm_3)\n  parallel_builder.add_node(\"aggregator\", aggregator)\n\n# Add edges to connect nodes\n  parallel_builder.add_edge(START, \"call_llm_1\")\n  parallel_builder.add_edge(START, \"call_llm_2\")\n  parallel_builder.add_edge(START, \"call_llm_3\")\n  parallel_builder.add_edge(\"call_llm_1\", \"aggregator\")\n  parallel_builder.add_edge(\"call_llm_2\", \"aggregator\")\n  parallel_builder.add_edge(\"call_llm_3\", \"aggregator\")\n  parallel_builder.add_edge(\"aggregator\", END)\n  parallel_workflow = parallel_builder.compile()\n\n# Show workflow\n  display(Image(parallel_workflow.get_graph().draw_mermaid_png()))\n\n# Invoke\n  state = parallel_workflow.invoke({\"topic\": \"cats\"})\n  print(state[\"combined_output\"])\n  python Functional API theme={null}\n  @task\n  def call_llm_1(topic: str):\n      \"\"\"First LLM call to generate initial joke\"\"\"\n      msg = llm.invoke(f\"Write a joke about {topic}\")\n      return msg.content\n\n@task\n  def call_llm_2(topic: str):\n      \"\"\"Second LLM call to generate story\"\"\"\n      msg = llm.invoke(f\"Write a story about {topic}\")\n      return msg.content\n\n@task\n  def call_llm_3(topic):\n      \"\"\"Third LLM call to generate poem\"\"\"\n      msg = llm.invoke(f\"Write a poem about {topic}\")\n      return msg.content\n\n@task\n  def aggregator(topic, joke, story, poem):\n      \"\"\"Combine the joke and story into a single output\"\"\"\n\ncombined = f\"Here's a story, joke, and poem about {topic}!\\n\\n\"\n      combined += f\"STORY:\\n{story}\\n\\n\"\n      combined += f\"JOKE:\\n{joke}\\n\\n\"\n      combined += f\"POEM:\\n{poem}\"\n      return combined\n\n# Build workflow\n  @entrypoint()\n  def parallel_workflow(topic: str):\n      joke_fut = call_llm_1(topic)\n      story_fut = call_llm_2(topic)\n      poem_fut = call_llm_3(topic)\n      return aggregator(\n          topic, joke_fut.result(), story_fut.result(), poem_fut.result()\n      ).result()\n\n# Invoke\n  for step in parallel_workflow.stream(\"cats\", stream_mode=\"updates\"):\n      print(step)\n      print(\"\\n\")\n  python Graph API theme={null}\n  from typing_extensions import Literal\n  from langchain.messages import HumanMessage, SystemMessage\n\n# Schema for structured output to use as routing logic\n  class Route(BaseModel):\n      step: Literal[\"poem\", \"story\", \"joke\"] = Field(\n          None, description=\"The next step in the routing process\"\n      )\n\n# Augment the LLM with schema for structured output\n  router = llm.with_structured_output(Route)\n\n# State\n  class State(TypedDict):\n      input: str\n      decision: str\n      output: str\n\n# Nodes\n  def llm_call_1(state: State):\n      \"\"\"Write a story\"\"\"\n\nresult = llm.invoke(state[\"input\"])\n      return {\"output\": result.content}\n\ndef llm_call_2(state: State):\n      \"\"\"Write a joke\"\"\"\n\nresult = llm.invoke(state[\"input\"])\n      return {\"output\": result.content}\n\ndef llm_call_3(state: State):\n      \"\"\"Write a poem\"\"\"\n\nresult = llm.invoke(state[\"input\"])\n      return {\"output\": result.content}\n\ndef llm_call_router(state: State):\n      \"\"\"Route the input to the appropriate node\"\"\"\n\n# Run the augmented LLM with structured output to serve as routing logic\n      decision = router.invoke(\n          [\n              SystemMessage(\n                  content=\"Route the input to story, joke, or poem based on the user's request.\"\n              ),\n              HumanMessage(content=state[\"input\"]),\n          ]\n      )\n\nreturn {\"decision\": decision.step}\n\n# Conditional edge function to route to the appropriate node\n  def route_decision(state: State):\n      # Return the node name you want to visit next\n      if state[\"decision\"] == \"story\":\n          return \"llm_call_1\"\n      elif state[\"decision\"] == \"joke\":\n          return \"llm_call_2\"\n      elif state[\"decision\"] == \"poem\":\n          return \"llm_call_3\"\n\n# Build workflow\n  router_builder = StateGraph(State)\n\n# Add nodes\n  router_builder.add_node(\"llm_call_1\", llm_call_1)\n  router_builder.add_node(\"llm_call_2\", llm_call_2)\n  router_builder.add_node(\"llm_call_3\", llm_call_3)\n  router_builder.add_node(\"llm_call_router\", llm_call_router)\n\n# Add edges to connect nodes\n  router_builder.add_edge(START, \"llm_call_router\")\n  router_builder.add_conditional_edges(\n      \"llm_call_router\",\n      route_decision,\n      {  # Name returned by route_decision : Name of next node to visit\n          \"llm_call_1\": \"llm_call_1\",\n          \"llm_call_2\": \"llm_call_2\",\n          \"llm_call_3\": \"llm_call_3\",\n      },\n  )\n  router_builder.add_edge(\"llm_call_1\", END)\n  router_builder.add_edge(\"llm_call_2\", END)\n  router_builder.add_edge(\"llm_call_3\", END)\n\n# Compile workflow\n  router_workflow = router_builder.compile()\n\n# Show the workflow\n  display(Image(router_workflow.get_graph().draw_mermaid_png()))\n\n# Invoke\n  state = router_workflow.invoke({\"input\": \"Write me a joke about cats\"})\n  print(state[\"output\"])\n  python Functional API theme={null}\n  from typing_extensions import Literal\n  from pydantic import BaseModel\n  from langchain.messages import HumanMessage, SystemMessage\n\n# Schema for structured output to use as routing logic\n  class Route(BaseModel):\n      step: Literal[\"poem\", \"story\", \"joke\"] = Field(\n          None, description=\"The next step in the routing process\"\n      )\n\n# Augment the LLM with schema for structured output\n  router = llm.with_structured_output(Route)\n\n@task\n  def llm_call_1(input_: str):\n      \"\"\"Write a story\"\"\"\n      result = llm.invoke(input_)\n      return result.content\n\n@task\n  def llm_call_2(input_: str):\n      \"\"\"Write a joke\"\"\"\n      result = llm.invoke(input_)\n      return result.content\n\n@task\n  def llm_call_3(input_: str):\n      \"\"\"Write a poem\"\"\"\n      result = llm.invoke(input_)\n      return result.content\n\ndef llm_call_router(input_: str):\n      \"\"\"Route the input to the appropriate node\"\"\"\n      # Run the augmented LLM with structured output to serve as routing logic\n      decision = router.invoke(\n          [\n              SystemMessage(\n                  content=\"Route the input to story, joke, or poem based on the user's request.\"\n              ),\n              HumanMessage(content=input_),\n          ]\n      )\n      return decision.step\n\n# Create workflow\n  @entrypoint()\n  def router_workflow(input_: str):\n      next_step = llm_call_router(input_)\n      if next_step == \"story\":\n          llm_call = llm_call_1\n      elif next_step == \"joke\":\n          llm_call = llm_call_2\n      elif next_step == \"poem\":\n          llm_call = llm_call_3\n\nreturn llm_call(input_).result()\n\n# Invoke\n  for step in router_workflow.stream(\"Write me a joke about cats\", stream_mode=\"updates\"):\n      print(step)\n      print(\"\\n\")\n  python Graph API theme={null}\n  from typing import Annotated, List\n  import operator\n\n# Schema for structured output to use in planning\n  class Section(BaseModel):\n      name: str = Field(\n          description=\"Name for this section of the report.\",\n      )\n      description: str = Field(\n          description=\"Brief overview of the main topics and concepts to be covered in this section.\",\n      )\n\nclass Sections(BaseModel):\n      sections: List[Section] = Field(\n          description=\"Sections of the report.\",\n      )\n\n# Augment the LLM with schema for structured output\n  planner = llm.with_structured_output(Sections)\n  python Functional API theme={null}\n  from typing import List\n\n# Schema for structured output to use in planning\n  class Section(BaseModel):\n      name: str = Field(\n          description=\"Name for this section of the report.\",\n      )\n      description: str = Field(\n          description=\"Brief overview of the main topics and concepts to be covered in this section.\",\n      )\n\nclass Sections(BaseModel):\n      sections: List[Section] = Field(\n          description=\"Sections of the report.\",\n      )\n\n# Augment the LLM with schema for structured output\n  planner = llm.with_structured_output(Sections)\n\n@task\n  def orchestrator(topic: str):\n      \"\"\"Orchestrator that generates a plan for the report\"\"\"\n      # Generate queries\n      report_sections = planner.invoke(\n          [\n              SystemMessage(content=\"Generate a plan for the report.\"),\n              HumanMessage(content=f\"Here is the report topic: {topic}\"),\n          ]\n      )\n\nreturn report_sections.sections\n\n@task\n  def llm_call(section: Section):\n      \"\"\"Worker writes a section of the report\"\"\"\n\n# Generate section\n      result = llm.invoke(\n          [\n              SystemMessage(content=\"Write a report section.\"),\n              HumanMessage(\n                  content=f\"Here is the section name: {section.name} and description: {section.description}\"\n              ),\n          ]\n      )\n\n# Write the updated section to completed sections\n      return result.content\n\n@task\n  def synthesizer(completed_sections: list[str]):\n      \"\"\"Synthesize full report from sections\"\"\"\n      final_report = \"\\n\\n---\\n\\n\".join(completed_sections)\n      return final_report\n\n@entrypoint()\n  def orchestrator_worker(topic: str):\n      sections = orchestrator(topic).result()\n      section_futures = [llm_call(section) for section in sections]\n      final_report = synthesizer(\n          [section_fut.result() for section_fut in section_futures]\n      ).result()\n      return final_report\n\n# Invoke\n  report = orchestrator_worker.invoke(\"Create a report on LLM scaling laws\")\n  from IPython.display import Markdown\n  Markdown(report)\n  python  theme={null}\nfrom langgraph.types import Send",
  "code_samples": [
    {
      "code": "## Prompt chaining\n\nPrompt chaining is when each LLM call processes the output of the previous call. It's often used for performing well-defined tasks that can be broken down into smaller, verifiable steps. Some examples include:\n\n* Translating documents into different languages\n* Verifying generated content for consistency\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/prompt_chain.png?fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=762dec147c31b8dc6ebb0857e236fc1f\" alt=\"Prompt chaining\" data-og-width=\"1412\" width=\"1412\" data-og-height=\"444\" height=\"444\" data-path=\"oss/images/prompt_chain.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/prompt_chain.png?w=280&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=fda27cf4f997e350d4ce48be16049c47 280w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/prompt_chain.png?w=560&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=1374b6de11900d394fc73722a3a6040e 560w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/prompt_chain.png?w=840&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=25246c7111a87b5df5a2af24a0181efe 840w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/prompt_chain.png?w=1100&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=0c57da86a49cf966cc090497ade347f1 1100w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/prompt_chain.png?w=1650&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=a1b5c8fc644d7a80c0792b71769c97da 1650w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/prompt_chain.png?w=2500&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=8a3f66f0e365e503a85b30be48bc1a76 2500w\" />\n\n<CodeGroup>",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</CodeGroup>\n\n## Parallelization\n\nWith parallelization, LLMs work simultaneously on a task. This is either done by running multiple independent subtasks at the same time, or running the same task multiple times to check for different outputs. Parallelization is commonly used to:\n\n* Split up subtasks and run them in parallel, which increases speed\n* Run tasks multiple times to check for different outputs, which increases confidence\n\nSome examples include:\n\n* Running one subtask that processes a document for keywords, and a second subtask to check for formatting errors\n* Running a task multiple times that scores a document for accuracy based on different criteria, like the number of citations, the number of sources used, and the quality of the sources\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/parallelization.png?fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=8afe3c427d8cede6fed1e4b2a5107b71\" alt=\"parallelization.png\" data-og-width=\"1020\" width=\"1020\" data-og-height=\"684\" height=\"684\" data-path=\"oss/images/parallelization.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/parallelization.png?w=280&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=88e51062b14d9186a6f0ea246bc48635 280w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/parallelization.png?w=560&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=934941ca52019b7cbce7fbdd31d00f0f 560w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/parallelization.png?w=840&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=30b5c86c545d0e34878ff0a2c367dd0a 840w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/parallelization.png?w=1100&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=6227d2c39f332eaeda23f7db66871dd7 1100w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/parallelization.png?w=1650&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=283f3ee2924a385ab88f2cbfd9c9c48c 1650w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/parallelization.png?w=2500&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=69f6a97716b38998b7b399c3d8ac7d9c 2500w\" />\n\n<CodeGroup>",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</CodeGroup>\n\n## Routing\n\nRouting workflows process inputs and then directs them to context-specific tasks. This allows you to define specialized flows for complex tasks. For example, a workflow built to answer product related questions might process the type of question first, and then route the request to specific processes for pricing, refunds, returns, etc.\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/routing.png?fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=272e0e9b681b89cd7d35d5c812c50ee6\" alt=\"routing.png\" data-og-width=\"1214\" width=\"1214\" data-og-height=\"678\" height=\"678\" data-path=\"oss/images/routing.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/routing.png?w=280&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=ab85efe91d20c816f9a4e491e92a61f7 280w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/routing.png?w=560&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=769e29f9be058a47ee85e0c9228e6e44 560w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/routing.png?w=840&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=3711ee40746670731a0ce3e96b7cfeb1 840w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/routing.png?w=1100&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=9aaa28410da7643f4a2587f7bfae0f21 1100w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/routing.png?w=1650&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=6706326c7fef0511805c684d1e4f7082 1650w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/routing.png?w=2500&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=f6d603145ca33791b18c8c8afec0bb4d 2500w\" />\n\n<CodeGroup>",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</CodeGroup>\n\n## Orchestrator-worker\n\nIn an orchestrator-worker configuration, the orchestrator:\n\n* Breaks down tasks into subtasks\n* Delegates subtasks to workers\n* Synthesizes worker outputs into a final result\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/worker.png?fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=2e423c67cd4f12e049cea9c169ff0676\" alt=\"worker.png\" data-og-width=\"1486\" width=\"1486\" data-og-height=\"548\" height=\"548\" data-path=\"oss/images/worker.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/worker.png?w=280&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=037222991ea08f889306be035c4730b6 280w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/worker.png?w=560&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=081f3ff05cc1fe50770c864d74084b5b 560w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/worker.png?w=840&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=0ef6c1b9ceb5159030aa34d0f05f1ada 840w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/worker.png?w=1100&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=92ec7353a89ae96e221a5a8f65c88adf 1100w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/worker.png?w=1650&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=71b201dd99fa234ebfb918915aac3295 1650w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/worker.png?w=2500&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=4f7b6e2064db575027932394a3658fbd 2500w\" />\n\nOrchestrator-worker workflows provide more flexibility and are often used when subtasks cannot be predefined the way they can with [parallelization](#parallelization). This is common with workflows that write code or need to update content across multiple files. For example, a workflow that needs to update installation instructions for multiple Python libraries across an unknown number of documents might use this pattern.\n\n<CodeGroup>",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</CodeGroup>\n\n### Creating workers in LangGraph\n\nOrchestrator-worker workflows are common and LangGraph has built-in support for them. The `Send` API lets you dynamically create worker nodes and send them specific inputs. Each worker has its own state, and all worker outputs are written to a shared state key that is accessible to the orchestrator graph. This gives the orchestrator access to all worker output and allows it to synthesize them into a final output. The example below iterates over a list of sections and uses the `Send` API to send a section to each worker.",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": "Prompt chaining",
      "id": "prompt-chaining"
    },
    {
      "level": "h2",
      "text": "Parallelization",
      "id": "parallelization"
    },
    {
      "level": "h2",
      "text": "Routing",
      "id": "routing"
    },
    {
      "level": "h2",
      "text": "Orchestrator-worker",
      "id": "orchestrator-worker"
    },
    {
      "level": "h3",
      "text": "Creating workers in LangGraph",
      "id": "creating-workers-in-langgraph"
    }
  ],
  "url": "llms-txt#get-the-tool-call",
  "links": []
}