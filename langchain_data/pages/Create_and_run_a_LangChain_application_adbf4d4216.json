{
  "title": "Create and run a LangChain application",
  "content": "prompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\nmodel = ChatOpenAI()\nchain = prompt | model\nresult = chain.invoke({\"topic\": \"programming\"})\nprint(result.content)\npython  theme={null}\nimport asyncio\nfrom langsmith.integrations.otel import configure\nfrom google.adk import Runner\nfrom google.adk.agents import LlmAgent\nfrom google.adk.sessions import InMemorySessionService\nfrom google.genai import types",
  "code_samples": [
    {
      "code": "<Info>\n  Hybrid tracing is available in version **≥ 0.4.1**. To send traces **only** to your OTEL endpoint, set:\n\n  `LANGSMITH_OTEL_ONLY=\"true\"`\n  (Recommendation: use **langsmith ≥ 0.4.25**.)\n</Info>\n\n## Supported OpenTelemetry attribute and event mapping\n\nWhen sending traces to LangSmith via OpenTelemetry, the following attributes are mapped to LangSmith fields:\n\n### Core LangSmith attributes\n\n| OpenTelemetry attribute        | LangSmith field  | Notes                                                                        |\n| ------------------------------ | ---------------- | ---------------------------------------------------------------------------- |\n| `langsmith.trace.name`         | Run name         | Overrides the span name for the run                                          |\n| `langsmith.span.kind`          | Run type         | Values: `llm`, `chain`, `tool`, `retriever`, `embedding`, `prompt`, `parser` |\n| `langsmith.trace.session_id`   | Session ID       | Session identifier for related traces                                        |\n| `langsmith.trace.session_name` | Session name     | Name of the session                                                          |\n| `langsmith.span.tags`          | Tags             | Custom tags attached to the span (comma-separated)                           |\n| `langsmith.metadata.{key}`     | `metadata.{key}` | Custom metadata with langsmith prefix                                        |\n\n### GenAI standard attributes\n\n| OpenTelemetry attribute                 | LangSmith field               | Notes                                                         |\n| --------------------------------------- | ----------------------------- | ------------------------------------------------------------- |\n| `gen_ai.system`                         | `metadata.ls_provider`        | The GenAI system (e.g., \"openai\", \"anthropic\")                |\n| `gen_ai.operation.name`                 | Run type                      | Maps \"chat\"/\"completion\" to \"llm\", \"embedding\" to \"embedding\" |\n| `gen_ai.prompt`                         | `inputs`                      | The input prompt sent to the model                            |\n| `gen_ai.completion`                     | `outputs`                     | The output generated by the model                             |\n| `gen_ai.prompt.{n}.role`                | `inputs.messages[n].role`     | Role for the nth input message                                |\n| `gen_ai.prompt.{n}.content`             | `inputs.messages[n].content`  | Content for the nth input message                             |\n| `gen_ai.prompt.{n}.message.role`        | `inputs.messages[n].role`     | Alternative format for role                                   |\n| `gen_ai.prompt.{n}.message.content`     | `inputs.messages[n].content`  | Alternative format for content                                |\n| `gen_ai.completion.{n}.role`            | `outputs.messages[n].role`    | Role for the nth output message                               |\n| `gen_ai.completion.{n}.content`         | `outputs.messages[n].content` | Content for the nth output message                            |\n| `gen_ai.completion.{n}.message.role`    | `outputs.messages[n].role`    | Alternative format for role                                   |\n| `gen_ai.completion.{n}.message.content` | `outputs.messages[n].content` | Alternative format for content                                |\n| `gen_ai.input.messages`                 | `inputs.messages`             | Array of input messages                                       |\n| `gen_ai.output.messages`                | `outputs.messages`            | Array of output messages                                      |\n| `gen_ai.tool.name`                      | `invocation_params.tool_name` | Tool name, also sets run type to \"tool\"                       |\n\n### GenAI request parameters\n\n| OpenTelemetry attribute            | LangSmith field                       | Notes                                   |\n| ---------------------------------- | ------------------------------------- | --------------------------------------- |\n| `gen_ai.request.model`             | `invocation_params.model`             | The model name used for the request     |\n| `gen_ai.response.model`            | `invocation_params.model`             | The model name returned in the response |\n| `gen_ai.request.temperature`       | `invocation_params.temperature`       | Temperature setting                     |\n| `gen_ai.request.top_p`             | `invocation_params.top_p`             | Top-p sampling setting                  |\n| `gen_ai.request.max_tokens`        | `invocation_params.max_tokens`        | Maximum tokens setting                  |\n| `gen_ai.request.frequency_penalty` | `invocation_params.frequency_penalty` | Frequency penalty setting               |\n| `gen_ai.request.presence_penalty`  | `invocation_params.presence_penalty`  | Presence penalty setting                |\n| `gen_ai.request.seed`              | `invocation_params.seed`              | Random seed used for generation         |\n| `gen_ai.request.stop_sequences`    | `invocation_params.stop`              | Sequences that stop generation          |\n| `gen_ai.request.top_k`             | `invocation_params.top_k`             | Top-k sampling parameter                |\n| `gen_ai.request.encoding_formats`  | `invocation_params.encoding_formats`  | Output encoding formats                 |\n\n### GenAI usage metrics\n\n| OpenTelemetry attribute                 | LangSmith field                   | Notes                                     |\n| --------------------------------------- | --------------------------------- | ----------------------------------------- |\n| `gen_ai.usage.input_tokens`             | `usage_metadata.input_tokens`     | Number of input tokens used               |\n| `gen_ai.usage.output_tokens`            | `usage_metadata.output_tokens`    | Number of output tokens used              |\n| `gen_ai.usage.total_tokens`             | `usage_metadata.total_tokens`     | Total number of tokens used               |\n| `gen_ai.usage.prompt_tokens`            | `usage_metadata.input_tokens`     | Number of input tokens used (deprecated)  |\n| `gen_ai.usage.completion_tokens`        | `usage_metadata.output_tokens`    | Number of output tokens used (deprecated) |\n| `gen_ai.usage.details.reasoning_tokens` | `usage_metadata.reasoning_tokens` | Number of reasoning tokens used           |\n\n### TraceLoop attributes\n\n| OpenTelemetry attribute                  | LangSmith field  | Notes                                            |\n| ---------------------------------------- | ---------------- | ------------------------------------------------ |\n| `traceloop.entity.input`                 | `inputs`         | Full input value from TraceLoop                  |\n| `traceloop.entity.output`                | `outputs`        | Full output value from TraceLoop                 |\n| `traceloop.entity.name`                  | Run name         | Entity name from TraceLoop                       |\n| `traceloop.span.kind`                    | Run type         | Maps to LangSmith run types                      |\n| `traceloop.llm.request.type`             | Run type         | \"embedding\" maps to \"embedding\", others to \"llm\" |\n| `traceloop.association.properties.{key}` | `metadata.{key}` | Custom metadata with traceloop prefix            |\n\n### OpenInference attributes\n\n| OpenTelemetry attribute   | LangSmith field          | Notes                                     |\n| ------------------------- | ------------------------ | ----------------------------------------- |\n| `input.value`             | `inputs`                 | Full input value, can be string or JSON   |\n| `output.value`            | `outputs`                | Full output value, can be string or JSON  |\n| `openinference.span.kind` | Run type                 | Maps various kinds to LangSmith run types |\n| `llm.system`              | `metadata.ls_provider`   | LLM system provider                       |\n| `llm.model_name`          | `metadata.ls_model_name` | Model name from OpenInference             |\n| `tool.name`               | Run name                 | Tool name when span kind is \"TOOL\"        |\n| `metadata`                | `metadata.*`             | JSON string of metadata to be merged      |\n\n### LLM attributes\n\n| OpenTelemetry attribute      | LangSmith field                       | Notes                                |\n| ---------------------------- | ------------------------------------- | ------------------------------------ |\n| `llm.input_messages`         | `inputs.messages`                     | Input messages                       |\n| `llm.output_messages`        | `outputs.messages`                    | Output messages                      |\n| `llm.token_count.prompt`     | `usage_metadata.input_tokens`         | Prompt token count                   |\n| `llm.token_count.completion` | `usage_metadata.output_tokens`        | Completion token count               |\n| `llm.token_count.total`      | `usage_metadata.total_tokens`         | Total token count                    |\n| `llm.usage.total_tokens`     | `usage_metadata.total_tokens`         | Alternative total token count        |\n| `llm.invocation_parameters`  | `invocation_params.*`                 | JSON string of invocation parameters |\n| `llm.presence_penalty`       | `invocation_params.presence_penalty`  | Presence penalty                     |\n| `llm.frequency_penalty`      | `invocation_params.frequency_penalty` | Frequency penalty                    |\n| `llm.request.functions`      | `invocation_params.functions`         | Function definitions                 |\n\n### Prompt template attributes\n\n| OpenTelemetry attribute         | LangSmith field | Notes                                            |\n| ------------------------------- | --------------- | ------------------------------------------------ |\n| `llm.prompt_template.variables` | Run type        | Sets run type to \"prompt\", used with input.value |\n\n### Retriever attributes\n\n| OpenTelemetry attribute                     | LangSmith field                     | Notes                                         |\n| ------------------------------------------- | ----------------------------------- | --------------------------------------------- |\n| `retrieval.documents.{n}.document.content`  | `outputs.documents[n].page_content` | Content of the nth retrieved document         |\n| `retrieval.documents.{n}.document.metadata` | `outputs.documents[n].metadata`     | Metadata of the nth retrieved document (JSON) |\n\n### Tool attributes\n\n| OpenTelemetry attribute | LangSmith field                    | Notes                                     |\n| ----------------------- | ---------------------------------- | ----------------------------------------- |\n| `tools`                 | `invocation_params.tools`          | Array of tool definitions                 |\n| `tool_arguments`        | `invocation_params.tool_arguments` | Tool arguments as JSON or key-value pairs |\n\n### Logfire attributes\n\n| OpenTelemetry attribute | LangSmith field    | Notes                                            |\n| ----------------------- | ------------------ | ------------------------------------------------ |\n| `prompt`                | `inputs`           | Logfire prompt input                             |\n| `all_messages_events`   | `outputs`          | Logfire message events output                    |\n| `events`                | `inputs`/`outputs` | Logfire events array, splits input/choice events |\n\n### OpenTelemetry event mapping\n\n| Event name                  | LangSmith field      | Notes                                                            |\n| --------------------------- | -------------------- | ---------------------------------------------------------------- |\n| `gen_ai.content.prompt`     | `inputs`             | Extracts prompt content from event attributes                    |\n| `gen_ai.content.completion` | `outputs`            | Extracts completion content from event attributes                |\n| `gen_ai.system.message`     | `inputs.messages[]`  | System message in conversation                                   |\n| `gen_ai.user.message`       | `inputs.messages[]`  | User message in conversation                                     |\n| `gen_ai.assistant.message`  | `outputs.messages[]` | Assistant message in conversation                                |\n| `gen_ai.tool.message`       | `outputs.messages[]` | Tool response message                                            |\n| `gen_ai.choice`             | `outputs`            | Model choice/response with finish reason                         |\n| `exception`                 | `status`, `error`    | Sets status to \"error\" and extracts exception message/stacktrace |\n\n#### Event attribute extraction\n\nFor message events, the following attributes are extracted:\n\n* `content` → message content\n* `role` → message role\n* `id` → tool\\_call\\_id (for tool messages)\n* `gen_ai.event.content` → full message JSON\n\nFor choice events:\n\n* `finish_reason` → choice finish reason\n* `message.content` → choice message content\n* `message.role` → choice message role\n* `tool_calls.{n}.id` → tool call ID\n* `tool_calls.{n}.function.name` → tool function name\n* `tool_calls.{n}.function.arguments` → tool function arguments\n* `tool_calls.{n}.type` → tool call type\n\nFor exception events:\n\n* `exception.message` → error message\n* `exception.stacktrace` → error stacktrace (appended to message)\n\n## Implementation examples\n\n### Trace using the LangSmith SDK\n\nUse the LangSmith SDK's OpenTelemetry helper to configure export. The following example [traces a Google ADK agent](/langsmith/trace-with-google-adk):",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": "Supported OpenTelemetry attribute and event mapping",
      "id": "supported-opentelemetry-attribute-and-event-mapping"
    },
    {
      "level": "h3",
      "text": "Core LangSmith attributes",
      "id": "core-langsmith-attributes"
    },
    {
      "level": "h3",
      "text": "GenAI standard attributes",
      "id": "genai-standard-attributes"
    },
    {
      "level": "h3",
      "text": "GenAI request parameters",
      "id": "genai-request-parameters"
    },
    {
      "level": "h3",
      "text": "GenAI usage metrics",
      "id": "genai-usage-metrics"
    },
    {
      "level": "h3",
      "text": "TraceLoop attributes",
      "id": "traceloop-attributes"
    },
    {
      "level": "h3",
      "text": "OpenInference attributes",
      "id": "openinference-attributes"
    },
    {
      "level": "h3",
      "text": "LLM attributes",
      "id": "llm-attributes"
    },
    {
      "level": "h3",
      "text": "Prompt template attributes",
      "id": "prompt-template-attributes"
    },
    {
      "level": "h3",
      "text": "Retriever attributes",
      "id": "retriever-attributes"
    },
    {
      "level": "h3",
      "text": "Tool attributes",
      "id": "tool-attributes"
    },
    {
      "level": "h3",
      "text": "Logfire attributes",
      "id": "logfire-attributes"
    },
    {
      "level": "h3",
      "text": "OpenTelemetry event mapping",
      "id": "opentelemetry-event-mapping"
    },
    {
      "level": "h2",
      "text": "Implementation examples",
      "id": "implementation-examples"
    },
    {
      "level": "h3",
      "text": "Trace using the LangSmith SDK",
      "id": "trace-using-the-langsmith-sdk"
    }
  ],
  "url": "llms-txt#create-and-run-a-langchain-application",
  "links": []
}