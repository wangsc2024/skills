{
  "title": "Evaluate a chatbot",
  "content": "Source: https://docs.langchain.com/langsmith/evaluate-chatbot-tutorial\n\nIn this guide we will set up evaluations for a chatbot. These allow you to measure how well your application is performing over a set of data. Being able to get this insight quickly and reliably will allow you to iterate with confidence.\n\nAt a high level, in this tutorial we will:\n\n* *Create an initial golden dataset to measure performance*\n* *Define metrics to use to measure performance*\n* *Run evaluations on a few different prompts or models*\n* *Compare results manually*\n* *Track results over time*\n* *Set up automated testing to run in CI/CD*\n\nFor more information on the evaluation workflows LangSmith supports, check out the [how-to guides](/langsmith/evaluation), or see the reference docs for [evaluate](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate) and its asynchronous [aevaluate](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._arunner.aevaluate) counterpart.\n\nLots to cover, let's dive in!\n\nFirst install the required dependencies for this tutorial. We happen to use OpenAI, but LangSmith can be used with any model:\n\nAnd set environment variables to enable LangSmith tracing:\n\nThe first step when getting ready to test and evaluate your application is to define the datapoints you want to evaluate. There are a few aspects to consider here:\n\n* What should the schema of each datapoint be?\n* How many datapoints should I gather?\n* How should I gather those datapoints?\n\n**Schema:** Each datapoint should consist of, at the very least, the inputs to the application. If you are able, it is also very helpful to define the expected outputs - these represent what you would expect a properly functioning application to output. Often times you cannot define the perfect output - that's okay! Evaluation is an iterative process. Sometimes you may also want to define more information for each example - like the expected documents to fetch in RAG, or the expected steps to take as an agent. LangSmith datasets are very flexible and allow you to define arbitrary schemas.\n\n**How many:** There's no hard and fast rule for how many you should gather. The main thing is to make sure you have proper coverage of edge cases you may want to guard against. Even 10-50 examples can provide a lot of value! Don't worry about getting a large number to start - you can (and should) always add over time!\n\n**How to get:** This is maybe the trickiest part. Once you know you want to gather a dataset... how do you actually go about it? For most teams that are starting a new project, we generally see them start by collecting the first 10-20 datapoints by hand. After starting with these datapoints, these datasets are generally *living* constructs and grow over time. They generally grow after seeing how real users will use your application, seeing the pain points that exist, and then moving a few of those datapoints into this set. There are also methods like synthetically generating data that can be used to augment your dataset. To start, we recommend not worrying about those and just hand labeling \\~10-20 examples.\n\nOnce you've got your dataset, there are a few different ways to upload them to LangSmith. For this tutorial, we will use the client, but you can also upload via the UI (or even create them in the UI).\n\nFor this tutorial, we will create 5 datapoints to evaluate on. We will be evaluating a question-answering application. The input will be a question, and the output will be an answer. Since this is a question-answering application, we can define the expected answer. Let's show how to create and upload this dataset to LangSmith!\n\n```python  theme={null}\nfrom langsmith import Client",
  "code_samples": [
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</CodeGroup>\n\nAnd set environment variables to enable LangSmith tracing:",
      "language": "unknown"
    },
    {
      "code": "## Create a dataset\n\nThe first step when getting ready to test and evaluate your application is to define the datapoints you want to evaluate. There are a few aspects to consider here:\n\n* What should the schema of each datapoint be?\n* How many datapoints should I gather?\n* How should I gather those datapoints?\n\n**Schema:** Each datapoint should consist of, at the very least, the inputs to the application. If you are able, it is also very helpful to define the expected outputs - these represent what you would expect a properly functioning application to output. Often times you cannot define the perfect output - that's okay! Evaluation is an iterative process. Sometimes you may also want to define more information for each example - like the expected documents to fetch in RAG, or the expected steps to take as an agent. LangSmith datasets are very flexible and allow you to define arbitrary schemas.\n\n**How many:** There's no hard and fast rule for how many you should gather. The main thing is to make sure you have proper coverage of edge cases you may want to guard against. Even 10-50 examples can provide a lot of value! Don't worry about getting a large number to start - you can (and should) always add over time!\n\n**How to get:** This is maybe the trickiest part. Once you know you want to gather a dataset... how do you actually go about it? For most teams that are starting a new project, we generally see them start by collecting the first 10-20 datapoints by hand. After starting with these datapoints, these datasets are generally *living* constructs and grow over time. They generally grow after seeing how real users will use your application, seeing the pain points that exist, and then moving a few of those datapoints into this set. There are also methods like synthetically generating data that can be used to augment your dataset. To start, we recommend not worrying about those and just hand labeling \\~10-20 examples.\n\nOnce you've got your dataset, there are a few different ways to upload them to LangSmith. For this tutorial, we will use the client, but you can also upload via the UI (or even create them in the UI).\n\nFor this tutorial, we will create 5 datapoints to evaluate on. We will be evaluating a question-answering application. The input will be a question, and the output will be an answer. Since this is a question-answering application, we can define the expected answer. Let's show how to create and upload this dataset to LangSmith!",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": "Setup",
      "id": "setup"
    },
    {
      "level": "h2",
      "text": "Create a dataset",
      "id": "create-a-dataset"
    }
  ],
  "url": "llms-txt#evaluate-a-chatbot",
  "links": []
}