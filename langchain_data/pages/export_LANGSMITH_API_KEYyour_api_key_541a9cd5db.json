{
  "title": "export LANGSMITH_API_KEY=\"your-api-key\"",
  "content": "bash npm theme={null}\n  npm install @langchain/openai @langchain/core\n  bash yarn theme={null}\n  yarn add @langchain/openai @langchain/core\n  bash pnpm theme={null}\n  pnpm add @langchain/openai @langchain/core\n  typescript  theme={null}\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\n\nconst embeddings = new OpenAIEmbeddings({\n  apiKey: \"YOUR-API-KEY\", // In Node.js defaults to process.env.OPENAI_API_KEY\n  batchSize: 512, // Default value if omitted is 512. Max is 2048\n  model: \"text-embedding-3-large\",\n});\ntypescript  theme={null}\n// Create a vector store with a sample text\nimport { MemoryVectorStore } from \"@langchain/classic/vectorstores/memory\";\n\nconst text = \"LangChain is the framework for building context-aware reasoning applications\";\n\nconst vectorstore = await MemoryVectorStore.fromDocuments(\n  [{ pageContent: text, metadata: {} }],\n  embeddings,\n);\n\n// Use the vector store as a retriever that returns a single document\nconst retriever = vectorstore.asRetriever(1);\n\n// Retrieve the most similar text\nconst retrievedDocuments = await retriever.invoke(\"What is LangChain?\");\n\nretrievedDocuments[0].pageContent;\ntext  theme={null}\nLangChain is the framework for building context-aware reasoning applications\ntypescript  theme={null}\nconst singleVector = await embeddings.embedQuery(text);\n\nconsole.log(singleVector.slice(0, 100));\ntext  theme={null}\n[\n    -0.01927683,  0.0037708976,  -0.032942563,  0.0037671267,  0.008175306,\n   -0.012511838,  -0.009713832,   0.021403614,  -0.015377721, 0.0018684798,\n    0.020574018,   0.022399133,   -0.02322873,   -0.01524951,  -0.00504169,\n   -0.007375876,   -0.03448109, 0.00015130726,   0.021388533, -0.012564631,\n   -0.020031009,   0.027406884,  -0.039217334,    0.03036327,  0.030393435,\n   -0.021750538,   0.032610722,  -0.021162277,  -0.025898525,  0.018869571,\n    0.034179416,  -0.013371604,  0.0037652412,   -0.02146395, 0.0012641934,\n   -0.055688616,    0.05104287,  0.0024982197,  -0.019095825, 0.0037369595,\n  0.00088757504,   0.025189597,  -0.018779071,   0.024978427,  0.016833287,\n  -0.0025868358,  -0.011727491, -0.0021154736,  -0.017738303, 0.0013839195,\n  -0.0131151825,   -0.05405959,   0.029729757,  -0.003393808,  0.019774588,\n    0.028885076,   0.004355387,   0.026094612,    0.06479911,  0.038040817,\n    -0.03478276,  -0.012594799,  -0.024767255, -0.0031430433,  0.017874055,\n   -0.015294761,   0.005709139,   0.025355516,   0.044798266,   0.02549127,\n    -0.02524993, 0.00014553308,  -0.019427665,  -0.023545485,  0.008748483,\n    0.019850006,  -0.028417485,  -0.001860938,   -0.02318348, -0.010799851,\n     0.04793565, -0.0048983963,    0.02193154,  -0.026411368,  0.026426451,\n   -0.012149832,   0.035355937,  -0.047814984,  -0.027165547, -0.008228099,\n   -0.007737882,   0.023726488,  -0.046487626,  -0.007783133, -0.019638835,\n     0.01793439,  -0.018024892,  0.0030336871,  -0.019578502, 0.0042837397\n]\ntypescript  theme={null}\nconst text2 = \"LangGraph is a library for building stateful, multi-actor applications with LLMs\";\n\nconst vectors = await embeddings.embedDocuments([text, text2]);\n\nconsole.log(vectors[0].slice(0, 100));\nconsole.log(vectors[1].slice(0, 100));\ntext  theme={null}\n[\n    -0.01927683,  0.0037708976,  -0.032942563,  0.0037671267,  0.008175306,\n   -0.012511838,  -0.009713832,   0.021403614,  -0.015377721, 0.0018684798,\n    0.020574018,   0.022399133,   -0.02322873,   -0.01524951,  -0.00504169,\n   -0.007375876,   -0.03448109, 0.00015130726,   0.021388533, -0.012564631,\n   -0.020031009,   0.027406884,  -0.039217334,    0.03036327,  0.030393435,\n   -0.021750538,   0.032610722,  -0.021162277,  -0.025898525,  0.018869571,\n    0.034179416,  -0.013371604,  0.0037652412,   -0.02146395, 0.0012641934,\n   -0.055688616,    0.05104287,  0.0024982197,  -0.019095825, 0.0037369595,\n  0.00088757504,   0.025189597,  -0.018779071,   0.024978427,  0.016833287,\n  -0.0025868358,  -0.011727491, -0.0021154736,  -0.017738303, 0.0013839195,\n  -0.0131151825,   -0.05405959,   0.029729757,  -0.003393808,  0.019774588,\n    0.028885076,   0.004355387,   0.026094612,    0.06479911,  0.038040817,\n    -0.03478276,  -0.012594799,  -0.024767255, -0.0031430433,  0.017874055,\n   -0.015294761,   0.005709139,   0.025355516,   0.044798266,   0.02549127,\n    -0.02524993, 0.00014553308,  -0.019427665,  -0.023545485,  0.008748483,\n    0.019850006,  -0.028417485,  -0.001860938,   -0.02318348, -0.010799851,\n     0.04793565, -0.0048983963,    0.02193154,  -0.026411368,  0.026426451,\n   -0.012149832,   0.035355937,  -0.047814984,  -0.027165547, -0.008228099,\n   -0.007737882,   0.023726488,  -0.046487626,  -0.007783133, -0.019638835,\n     0.01793439,  -0.018024892,  0.0030336871,  -0.019578502, 0.0042837397\n]\n[\n   -0.010181213,   0.023419594,   -0.04215527, -0.0015320902,  -0.023573855,\n  -0.0091644935,  -0.014893179,   0.019016149,  -0.023475688,  0.0010219777,\n    0.009255648,    0.03996757,   -0.04366983,   -0.01640774,  -0.020194141,\n    0.019408813,  -0.027977299,  -0.022017224,   0.013539891,  -0.007769135,\n    0.032647192,  -0.015089511,  -0.022900717,   0.023798235,   0.026084099,\n   -0.024625633,   0.035003178,  -0.017978394,  -0.049615882,   0.013364594,\n    0.031132633,   0.019142363,   0.023195215,  -0.038396914,   0.005584942,\n   -0.031946007,   0.053682756, -0.0036356465,   0.011240003,  0.0056690844,\n  -0.0062791156,   0.044146635,  -0.037387207,    0.01300699,   0.018946031,\n   0.0050415234,   0.029618073,  -0.021750772,  -0.000649473, 0.00026951815,\n   -0.014710871,  -0.029814405,    0.04204308,  -0.014710871,  0.0039616977,\n   -0.021512369,   0.054608323,   0.021484323,    0.02790718,  -0.010573876,\n   -0.023952495,  -0.035143413,  -0.048802506, -0.0075798146,   0.023279356,\n   -0.022690361,  -0.016590048,  0.0060477243,   0.014100839,   0.005476258,\n   -0.017221114, -0.0100059165,  -0.017922299,  -0.021989176,    0.01830094,\n     0.05516927,   0.001033372,  0.0017310516,   -0.00960624,  -0.037864015,\n    0.013063084,   0.006591143,  -0.010160177,  0.0011394264,    0.04953174,\n    0.004806626,   0.029421741,  -0.037751824,   0.003618117,   0.007162609,\n    0.027696826, -0.0021070621,  -0.024485396, -0.0042141243,   -0.02801937,\n   -0.019605145,   0.016281527,  -0.035143413,    0.01640774,   0.042323552\n]\ntypescript  theme={null}\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\n\nconst embeddingsDefaultDimensions = new OpenAIEmbeddings({\n  model: \"text-embedding-3-large\",\n});\n\nconst vectorsDefaultDimensions = await embeddingsDefaultDimensions.embedDocuments([\"some text\"]);\nconsole.log(vectorsDefaultDimensions[0].length);\ntext  theme={null}\n3072\ntypescript  theme={null}\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\n\nconst embeddings1024 = new OpenAIEmbeddings({\n  model: \"text-embedding-3-large\",\n  dimensions: 1024,\n});\n\nconst vectors1024 = await embeddings1024.embedDocuments([\"some text\"]);\nconsole.log(vectors1024[0].length);\ntext  theme={null}\n1024\ntypescript  theme={null}\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\n\nconst model = new OpenAIEmbeddings({\n  configuration: {\n    baseURL: \"https://your_custom_url.com\",\n  },\n});\n```\n\nYou can also pass other `ClientOptions` parameters accepted by the official SDK.\n\nIf you are hosting on Azure OpenAI, see the [dedicated page instead](/oss/javascript/integrations/text_embedding/azure_openai).\n\nFor detailed documentation of all OpenAIEmbeddings features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_openai.OpenAIEmbeddings.html).\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/integrations/text_embedding/openai.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>",
  "code_samples": [
    {
      "code": "### Installation\n\nThe LangChain OpenAIEmbeddings integration lives in the `@langchain/openai` package:\n\n<CodeGroup>",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</CodeGroup>\n\n## Instantiation\n\nNow we can instantiate our model object and generate chat completions:",
      "language": "unknown"
    },
    {
      "code": "If you're part of an organization, you can set `process.env.OPENAI_ORGANIZATION` to your OpenAI organization id, or pass it in as `organization` when\ninitializing the model.\n\n## Indexing and Retrieval\n\nEmbedding models are often used in retrieval-augmented generation (RAG) flows, both as part of indexing data as well as later retrieving it. For more detailed instructions, please see our RAG tutorials under the [**Learn** tab](/oss/javascript/learn/).\n\nBelow, see how to index and retrieve data using the `embeddings` object we initialized above. In this example, we will index and retrieve a sample document using the demo [`MemoryVectorStore`](/oss/javascript/integrations/vectorstores/memory).",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "## Direct Usage\n\nUnder the hood, the vectorstore and retriever implementations are calling `embeddings.embedDocument(...)` and `embeddings.embedQuery(...)` to create embeddings for the text(s) used in `fromDocuments` and the retriever's `invoke` operations, respectively.\n\nYou can directly call these methods to get embeddings for your own use cases.\n\n### Embed single texts\n\nYou can embed queries for search with `embedQuery`. This generates a vector representation specific to the query:",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "### Embed multiple texts\n\nYou can embed multiple texts for indexing with `embedDocuments`. The internals used for this method may (but do not have to) differ from embedding queries:",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "## Specifying dimensions\n\nWith the `text-embedding-3` class of models, you can specify the size of the embeddings you want returned. For example by default `text-embedding-3-large` returns embeddings of dimension 3072:",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "But by passing in `dimensions: 1024` we can reduce the size of our embeddings to 1024:",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "## Custom URLs\n\nYou can customize the base URL the SDK sends requests to by passing a `configuration` parameter like this:",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h3",
      "text": "Installation",
      "id": "installation"
    },
    {
      "level": "h2",
      "text": "Instantiation",
      "id": "instantiation"
    },
    {
      "level": "h2",
      "text": "Indexing and Retrieval",
      "id": "indexing-and-retrieval"
    },
    {
      "level": "h2",
      "text": "Direct Usage",
      "id": "direct-usage"
    },
    {
      "level": "h3",
      "text": "Embed single texts",
      "id": "embed-single-texts"
    },
    {
      "level": "h3",
      "text": "Embed multiple texts",
      "id": "embed-multiple-texts"
    },
    {
      "level": "h2",
      "text": "Specifying dimensions",
      "id": "specifying-dimensions"
    },
    {
      "level": "h2",
      "text": "Custom URLs",
      "id": "custom-urls"
    },
    {
      "level": "h2",
      "text": "API reference",
      "id": "api-reference"
    }
  ],
  "url": "llms-txt#export-langsmith_api_key=\"your-api-key\"",
  "links": []
}