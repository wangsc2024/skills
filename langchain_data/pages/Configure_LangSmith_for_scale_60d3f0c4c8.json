{
  "title": "Configure LangSmith for scale",
  "content": "Source: https://docs.langchain.com/langsmith/self-host-scale\n\nA self-hosted LangSmith instance can handle a large number of traces and users. The default configuration for the self-hosted deployment can handle substantial load, and you can configure your deployment to be able to achieve higher scale. This page describes scaling considerations and provides some examples to help configure your self-hosted instance.\n\nFor example configurations, refer to [Example LangSmith configurations for scale](#example-langsmith-configurations-for-scale).\n\nThe table below provides an overview comparing different LangSmith configurations for various load patterns (reads / writes):\n\n|                                                                                                                                                                  | **[Low / low](#low-reads-low-writes)**              | **[Low / high](#low-reads-high-writes)**            | **[High / low](#high-reads-low-writes)**                                                                                                                                                                                                 | [Medium / medium](#medium-reads-medium-writes)      | [High / high](#high-reads-high-writes)                                                                                                                                                                                                   |\n| :--------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------------------------------------------- | :-------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| <Tooltip tip=\"Number of users actively viewing traces on the frontend\">Concurrent frontend users</Tooltip>                                                       | 5                                                   | 5                                                   | 50                                                                                                                                                                                                                                       | 20                                                  | 50                                                                                                                                                                                                                                       |\n| <Tooltip tip=\"Number of traces being ingested via SDKs or API endpoints\">Traces submitted per second</Tooltip>                                                   | 10                                                  | 1000                                                | 10                                                                                                                                                                                                                                       | 100                                                 | 1000                                                                                                                                                                                                                                     |\n| **Frontend replicas**<br />(500m CPU, 1Gi per replica)                                                                                                           | 1 (default)                                         | 4                                                   | 2                                                                                                                                                                                                                                        | 2                                                   | 4                                                                                                                                                                                                                                        |\n| **Platform backend replicas**<br />(1 CPU, 2Gi per replica)                                                                                                      | 3 (default)                                         | 20                                                  | 3 (default)                                                                                                                                                                                                                              | 3 (default)                                         | 20                                                                                                                                                                                                                                       |\n| **Queue replicas**<br />(1 CPU, 2Gi per replica)                                                                                                                 | 3 (default)                                         | 160                                                 | 6                                                                                                                                                                                                                                        | 10                                                  | 160                                                                                                                                                                                                                                      |\n| **Backend replicas**<br />(1 CPU, 2Gi per replica)                                                                                                               | 2 (default)                                         | 5                                                   | 40                                                                                                                                                                                                                                       | 16                                                  | 50                                                                                                                                                                                                                                       |\n| **Redis resources**                                                                                                                                              | 8 Gi (default)                                      | 200 Gi external                                     | 8 Gi (default)                                                                                                                                                                                                                           | 13Gi external                                       | 200 Gi external                                                                                                                                                                                                                          |\n| **ClickHouse resources**                                                                                                                                         | 4 CPU<br />16 Gi (default)                          | 10 CPU<br />32Gi memory                             | 8 CPU<br />16 Gi per replica                                                                                                                                                                                                             | 16 CPU<br />24Gi memory                             | 14 CPU<br />24 Gi per replica                                                                                                                                                                                                            |\n| **ClickHouse setup**                                                                                                                                             | Single instance                                     | Single instance                                     | 3-node <Tooltip tip=\"Recommended for high read loads to prevent degraded performance. Another option would be [managed clickhouse](/langsmith/self-host-external-clickhouse#langsmith-managed-clickhouse).\">replicated cluster</Tooltip> | Single instance                                     | 3-node <Tooltip tip=\"Recommended for high read loads to prevent degraded performance. Another option would be [managed clickhouse](/langsmith/self-host-external-clickhouse#langsmith-managed-clickhouse).\">replicated cluster</Tooltip> |\n| <Tooltip tip=\"We recommend using an external instance and enabling autoexpansion for the disk to handle growing data requirements.\">Postgres resources</Tooltip> | 2 CPU<br />8 GB memory<br />10GB storage (external) | 2 CPU<br />8 GB memory<br />10GB storage (external) | 2 CPU<br />8 GB memory<br />10GB storage (external)                                                                                                                                                                                      | 2 CPU<br />8 GB memory<br />10GB storage (external) | 2 CPU<br />8 GB memory<br />10GB storage (external)                                                                                                                                                                                      |\n| **Blob storage**                                                                                                                                                 | Disabled                                            | Enabled                                             | Enabled                                                                                                                                                                                                                                  | Enabled                                             | Enabled                                                                                                                                                                                                                                  |\n\nBelow we go into more details about the read and write paths as well as provide a `values.yaml` snippet for you to start with for your self-hosted LangSmith instance.\n\n## Trace ingestion (write path)\n\nCommon usage that put load on the write path:\n\n* Ingesting traces via the Python or JavaScript LangSmith SDK\n* Ingesting traces via the `@traceable` wrapper\n* Submitting traces via the `/runs/multipart` endpoint\n\nServices that play a large role in trace ingestion:\n\n* Platform backend service: Receives initial request to ingest traces and places traces on a Redis queue\n* Redis cache: Used to queue traces that need to be persisted\n* Queue service: Persists traces for querying\n* ClickHouse: Persistent storage used for traces\n\nWhen scaling up the write path (trace ingestion), it is helpful to monitor the four services/resources listed above. Here are some typical changes that can help increase performance of trace ingestion:\n\n* Give ClickHouse more resources (CPU and memory) if it is approaching resource limits.\n* Increase the number of platform-backend pods if ingest requests are taking long to respond.\n* Increase queue service pod replicas if traces are not being processed from Redis fast enough.\n* Use a larger Redis cache if you notice that the current Redis instance is reaching resource limits. This could also be a reason why ingest requests take a long time.\n\n## Trace querying (read path)\n\nCommon usage that puts load on the read path:\n\n* Users on the frontend looking at tracing projects or individual traces\n* Scripts used to query for trace info\n* Hitting either the `/runs/query` or `/runs/<run-id>` api endpoints\n\nServices that play a large role in querying traces:\n\n* Backend service: Receives the request and submits a query to ClickHouse to then respond to the request\n* ClickHouse: Persistent storage for traces. This is the main database that is queried when requesting trace info.\n\nWhen scaling up the read path (trace querying), it is helpful to monitor the two services/resources listed above. Here are some typical changes that can help improve performance of trace querying:\n\n* Increase the number of backend service pods. This would be most impactful if backend service pods are reaching 1 core CPU usage.\n* Give ClickHouse more resources (CPU or Memory). ClickHouse can be very resource intensive, but it should lead to better performance.\n* Move to a [replicated ClickHouse cluster](/langsmith/self-host-external-clickhouse#ha-replicated-clickhouse-cluster). Adding replicas of ClickHouse helps with read performance, but we recommend staying below 5 replicas (start with 3).\n\nFor more precise guidance on how this translates to helm chart values, refer to the examples the following [section](#example-langsmith-configurations-for-scale). If you are unsure why your LangSmith instance cannot handle a certain load pattern, contact the LangChain team.\n\n## Example LangSmith configurations for scale\n\nBelow we provide some example LangSmith configurations based on expected read and write loads.\n\nFor read load (trace querying):\n\n* Low means roughly 5 users looking at traces at a time (about 10 requests per second)\n* Medium means roughly 20 users looking at traces at a time (about 40 requests per second)\n* High means roughly 50 users looking at traces at a time (about 100 requests per second)\n\nFor write load (trace ingestion):\n\n* Low means up to 10 traces submitted per second\n* Medium means up to 100 traces submitted per second\n* High means up to 1000 traces submitted per second\n\n<Note>\n  The exact optimal configuration depends on your usage and trace payloads. Use the examples below in combination with the information above and your specific usage to update your LangSmith configuration as you see fit. If you have any questions, please reach out to the LangChain team.\n</Note>\n\n### Low reads, low writes <a name=\"low-reads-low-writes\" />\n\nThe default LangSmith configuration will handle this load. No custom resource configuration is needed here.\n\n### Low reads, high writes <a name=\"low-reads-high-writes\" />\n\nYou have a very high scale of trace ingestions, but single digit number of users on the frontend querying traces at any one time.\n\nFor this, we recommend a configuration like this:\n\n```yaml  theme={null}\nconfig:\n  blobStorage:\n    # Please also set the other keys to connect to your blob storage. See configuration section.\n    enabled: true\n  settings:\n    redisRunsExpirySeconds: \"3600\"",
  "code_samples": [],
  "headings": [
    {
      "level": "h2",
      "text": "Summary",
      "id": "summary"
    },
    {
      "level": "h2",
      "text": "Trace ingestion (write path)",
      "id": "trace-ingestion-(write-path)"
    },
    {
      "level": "h2",
      "text": "Trace querying (read path)",
      "id": "trace-querying-(read-path)"
    },
    {
      "level": "h2",
      "text": "Example LangSmith configurations for scale",
      "id": "example-langsmith-configurations-for-scale"
    },
    {
      "level": "h3",
      "text": "Low reads, low writes <a name=\"low-reads-low-writes\" />",
      "id": "low-reads,-low-writes-<a-name=\"low-reads-low-writes\"-/>"
    },
    {
      "level": "h3",
      "text": "Low reads, high writes <a name=\"low-reads-high-writes\" />",
      "id": "low-reads,-high-writes-<a-name=\"low-reads-high-writes\"-/>"
    }
  ],
  "url": "llms-txt#configure-langsmith-for-scale",
  "links": []
}