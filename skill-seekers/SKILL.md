---
name: skill-seekers
description: |
  Skill Seekers æ–‡æª”è½‰æ›å·¥å…· - å¾æ–‡æª”ç¶²ç«™ã€GitHub å€‰åº«ã€PDF è‡ªå‹•ç”Ÿæˆ Claude AI skillsã€‚
  æ”¯æ´äº’å‹•å¼å·¥ä½œæµç¨‹ï¼ˆClaude Chatï¼‰å’Œè‡ªå‹•åŒ–è…³æœ¬ï¼ˆClaude Codeï¼‰å…©ç¨®æ¨¡å¼ã€‚
  Use when: å»ºç«‹ skillã€çˆ¬å–æ–‡æª”ã€è½‰æ› GitHub å°ˆæ¡ˆç‚º skillã€åˆ†ææŠ€è¡“æ–‡ä»¶ï¼Œor when user mentions Skill Seekers, å»ºç«‹ skill, æ–‡æª”è½‰æ›, æŠ“å–æ–‡ä»¶, çŸ¥è­˜åº«.
  Triggers: "Skill Seekers", "skill-seekers", "å»ºç«‹ skill", "æ–‡æª”è½‰æ›", "scrape docs", "GitHub to skill", "æŠ“å–æ–‡ä»¶", "çŸ¥è­˜åº«"
version: 2.0.0
---

# Skill Seekers

å°‡æŠ€è¡“æ–‡ä»¶ç¶²ç«™ã€GitHub å€‰åº«ã€PDF æª”æ¡ˆè½‰æ›ç‚ºçµæ§‹åŒ– AI çŸ¥è­˜åº«ã€‚

## Quick Reference

| ä»»å‹™ | æ–¹æ³• |
|------|------|
| æŠ“å–æ–‡ä»¶ç¶²ç«™ | `scrape_documentation(url)` |
| æŠ“å– GitHub å€‰åº« | `scrape_github_repo(repo)` |
| è™•ç† PDF | `pdf_extractor.py` |
| ç”Ÿæˆ Skill | `generate_skill(content, name)` |

### æ ¸å¿ƒå·¥ä½œæµç¨‹

```
1. åˆ†æä¾†æº â†’ æª¢æŸ¥ llms.txtã€sitemapã€å°èˆªçµæ§‹
2. æŠ“å–å…§å®¹ â†’ å¿«é€Ÿ(20é ) / å®Œæ•´(100+é ) / è‡ªè¨‚
3. å…§å®¹åˆ†é¡ â†’ è‡ªå‹•åˆ†é¡ç‚º 8 å¤§é¡åˆ¥
4. ç”Ÿæˆè¼¸å‡º â†’ SKILL.md + references/*.md + metadata.json
```

---

## å·¥ä½œæµç¨‹

### Phase 1: ä¾†æºåˆ†æ

ç•¶æä¾› URL æ™‚ï¼š

1. è¨ªå• URLï¼Œè­˜åˆ¥æ–‡ä»¶é¡å‹å’Œçµæ§‹
2. æª¢æŸ¥ `/llms.txt` æˆ– `/llms-full.txt`ï¼ˆå„ªå…ˆä½¿ç”¨ï¼‰
3. åˆ†æå°èˆªçµæ§‹ï¼Œä¼°è¨ˆé é¢æ•¸é‡
4. é¡¯ç¤ºåˆ†æçµæœ

åˆ†æçµæœæ ¼å¼ï¼š
```
ğŸ“Š **ä¾†æºåˆ†æçµæœ**
| é …ç›® | å…§å®¹ |
|------|------|
| é¡å‹ | [æ–‡ä»¶ç¶²ç«™/GitHub/PDF] |
| æ¡†æ¶ | [MkDocs/Docusaurus/Sphinx/...] |
| é ä¼°é æ•¸ | [æ•¸å­—] |
| ä¸»è¦ç« ç¯€ | [ç« ç¯€åˆ—è¡¨] |
| ç¨‹å¼èªè¨€ | [èªè¨€] |
| llms.txt | [âœ…/âŒ] |
```

### Phase 2: å…§å®¹æŠ“å–

**å¿«é€Ÿæ¨¡å¼** (é è¨­)ï¼š
- æŠ“å– 10-15 é æ ¸å¿ƒå…§å®¹
- å„ªå…ˆï¼šé¦–é ã€å…¥é–€ã€æ ¸å¿ƒ APIã€å¸¸è¦‹ç¯„ä¾‹

**å®Œæ•´æ¨¡å¼**ï¼š
- ç›¡å¯èƒ½æŠ“å–æ‰€æœ‰é é¢
- æŒ‰ç« ç¯€é€ä¸€è®€å–

**è‡ªè¨‚æ¨¡å¼**ï¼š
- ä½¿ç”¨è€…æŒ‡å®šè¦æŠ“å–çš„ç« ç¯€

### Phase 3: å…§å®¹åˆ†é¡

| é¡åˆ¥ | é—œéµå­— |
|------|--------|
| `getting_started` | intro, quickstart, installation, setup |
| `core_concepts` | concepts, fundamentals, basics, overview |
| `api_reference` | api, reference, methods, functions |
| `guides` | guide, tutorial, how-to, walkthrough |
| `examples` | example, sample, demo, cookbook |
| `configuration` | config, settings, options |
| `troubleshooting` | error, debug, faq, troubleshoot |
| `advanced` | advanced, deep-dive, internals |

### Phase 4: è¼¸å‡ºç”Ÿæˆ

ç”Ÿæˆæ¨™æº– SKILL.mdï¼ŒåŒ…å«ï¼š

1. **Description** - ä¸€å¥è©±æè¿°
2. **When to Use** - 4-6 å€‹ä½¿ç”¨å ´æ™¯
3. **Core Concepts** - 3-5 å€‹æ ¸å¿ƒæ¦‚å¿µ + ç¨‹å¼ç¢¼
4. **Quick Reference** - API è¡¨æ ¼ + å¸¸è¦‹æ¨¡å¼
5. **Code Examples** - è‡³å°‘ 5 å€‹å®Œæ•´ç¯„ä¾‹
6. **Common Pitfalls** - 2-3 å€‹å¸¸è¦‹éŒ¯èª¤
7. **Related Resources** - ç›¸é—œé€£çµ

---

## Python å¯¦ä½œ

### æŠ“å–æ–‡ä»¶ç¶²ç«™

```python
import asyncio
import httpx
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import re

async def scrape_documentation(base_url: str, max_pages: int = 50):
    """æŠ“å–æ–‡ä»¶ç¶²ç«™"""

    visited = set()
    pages = []
    to_visit = [base_url]

    parsed_base = urlparse(base_url)
    base_domain = parsed_base.netloc

    # æ’é™¤è·¯å¾‘
    exclude_patterns = ['/blog', '/changelog', '/about', '/community']

    async with httpx.AsyncClient(timeout=30.0) as client:
        # å…ˆæª¢æŸ¥ llms.txt
        llms_content = await check_llms_txt(client, base_url)
        if llms_content:
            print("âœ… ç™¼ç¾ llms.txtï¼Œä½¿ç”¨å„ªåŒ–ç‰ˆæœ¬")
            return {"source": "llms.txt", "content": llms_content}

        while to_visit and len(pages) < max_pages:
            url = to_visit.pop(0)
            if url in visited:
                continue

            visited.add(url)

            try:
                response = await client.get(url, follow_redirects=True)
                if response.status_code != 200:
                    continue

                soup = BeautifulSoup(response.text, 'html.parser')

                # æå–é é¢å…§å®¹
                page_data = extract_page_content(soup, url)
                pages.append(page_data)

                # æ¢ç´¢é€£çµ
                for a in soup.select('a[href]'):
                    href = a.get('href', '')
                    full_url = urljoin(url, href)
                    parsed = urlparse(full_url)

                    if parsed.netloc == base_domain:
                        clean_url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
                        if not any(p in clean_url.lower() for p in exclude_patterns):
                            if clean_url not in visited:
                                to_visit.append(clean_url)

                await asyncio.sleep(0.5)  # é€Ÿç‡é™åˆ¶

            except Exception as e:
                print(f"éŒ¯èª¤: {url} - {e}")

    return categorize_and_build_result(pages)

async def check_llms_txt(client, base_url: str):
    """æª¢æŸ¥ llms.txt"""
    parsed = urlparse(base_url)
    base = f"{parsed.scheme}://{parsed.netloc}"

    for path in ['/llms-full.txt', '/llms.txt', '/llms-small.txt']:
        try:
            response = await client.get(f"{base}{path}")
            if response.status_code == 200:
                return response.text
        except:
            pass
    return None

def extract_page_content(soup, url: str):
    """æå–é é¢å…§å®¹"""
    # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
    for tag in soup.select('nav, footer, header, script, style, .sidebar'):
        tag.decompose()

    # æ¨™é¡Œ
    title = ""
    if soup.h1:
        title = soup.h1.get_text(strip=True)
    elif soup.title:
        title = soup.title.string or ""

    # ä¸»è¦å…§å®¹
    main = soup.select_one('article, main, .content, [role="main"]')
    content = main.get_text(separator='\n', strip=True) if main else ""

    # ç¨‹å¼ç¢¼å€å¡Š
    code_blocks = []
    for code in soup.select('pre code, pre'):
        code_text = code.get_text(strip=True)
        if len(code_text) > 20:
            lang = detect_language(code_text, code.get('class', []))
            code_blocks.append({"language": lang, "code": code_text[:2000]})

    # åˆ†é¡
    category = categorize_page(url, title)

    return {
        "url": url,
        "title": title,
        "content": content[:10000],
        "code_blocks": code_blocks[:10],
        "category": category
    }

def detect_language(code: str, classes: list) -> str:
    """åµæ¸¬ç¨‹å¼ç¢¼èªè¨€"""
    for cls in classes:
        if isinstance(cls, str) and 'language-' in cls:
            return cls.replace('language-', '')

    patterns = {
        "python": [r"\bdef\s+\w+\(", r"\bimport\s+\w+"],
        "javascript": [r"\bconst\s+\w+\s*=", r"=>\s*{"],
        "typescript": [r":\s*(string|number|boolean)"],
        "jsx": [r"useState\s*\(", r"useEffect\s*\("],
        "bash": [r"#!/bin/bash", r"\$\w+"],
    }

    for lang, pats in patterns.items():
        for p in pats:
            if re.search(p, code):
                return lang
    return "text"

def categorize_page(url: str, title: str) -> str:
    """åˆ†é¡é é¢"""
    keywords = {
        "getting_started": ["intro", "quickstart", "installation", "setup"],
        "core_concepts": ["concepts", "fundamentals", "basics", "overview"],
        "api_reference": ["api", "reference", "methods", "functions"],
        "guides": ["guide", "tutorial", "how-to"],
        "examples": ["example", "sample", "demo"],
        "configuration": ["config", "settings", "options"],
        "troubleshooting": ["error", "debug", "faq"],
        "advanced": ["advanced", "deep-dive", "internals"],
    }

    text = (url + title).lower()
    for cat, kws in keywords.items():
        if any(kw in text for kw in kws):
            return cat
    return "general"

def categorize_and_build_result(pages: list) -> dict:
    """æ•´ç†çµæœ"""
    categories = {}
    languages = set()
    code_count = 0

    for page in pages:
        cat = page.get("category", "general")
        categories[cat] = categories.get(cat, 0) + 1
        for block in page.get("code_blocks", []):
            code_count += 1
            if block.get("language"):
                languages.add(block["language"])

    return {
        "pages": pages,
        "categories": categories,
        "statistics": {
            "total_pages": len(pages),
            "total_code_blocks": code_count,
            "languages_detected": list(languages)
        }
    }
```

### æŠ“å– GitHub å€‰åº«

```python
import httpx
import base64

async def scrape_github_repo(repo: str, include_code: bool = False):
    """æŠ“å– GitHub å€‰åº«"""

    async with httpx.AsyncClient(timeout=30.0) as client:
        headers = {}
        # å¦‚æœ‰ token: headers["Authorization"] = f"token {os.environ.get('GITHUB_TOKEN')}"

        api_base = f"https://api.github.com/repos/{repo}"

        # å€‰åº«è³‡è¨Š
        repo_resp = await client.get(api_base, headers=headers)
        repo_data = repo_resp.json()

        # README
        readme_content = ""
        readme_resp = await client.get(f"{api_base}/readme", headers=headers)
        if readme_resp.status_code == 200:
            readme_data = readme_resp.json()
            readme_content = base64.b64decode(readme_data.get("content", "")).decode("utf-8")

        # ç›®éŒ„çµæ§‹
        structure = []
        contents_resp = await client.get(f"{api_base}/contents", headers=headers)
        if contents_resp.status_code == 200:
            for item in contents_resp.json():
                structure.append({
                    "name": item.get("name"),
                    "type": item.get("type"),
                    "path": item.get("path")
                })

        # docs ç›®éŒ„
        docs = []
        docs_resp = await client.get(f"{api_base}/contents/docs", headers=headers)
        if docs_resp.status_code == 200:
            for item in docs_resp.json()[:20]:
                if item.get("name", "").endswith(".md"):
                    docs.append({
                        "name": item.get("name"),
                        "path": item.get("path")
                    })

        return {
            "repo": repo,
            "description": repo_data.get("description", ""),
            "stars": repo_data.get("stargazers_count", 0),
            "language": repo_data.get("language", ""),
            "topics": repo_data.get("topics", []),
            "readme": readme_content[:10000],
            "structure": structure,
            "docs": docs
        }

# åŸ·è¡Œ
result = asyncio.run(scrape_github_repo("facebook/react"))
print(f"â­ Stars: {result['stars']}")
```

### ç”Ÿæˆ Skill

```python
import json
import os
from datetime import datetime

def generate_skill(content: dict, name: str, output_dir: str):
    """ç”Ÿæˆå®Œæ•´ Skill"""

    os.makedirs(output_dir, exist_ok=True)
    os.makedirs(f"{output_dir}/references", exist_ok=True)

    pages = content.get("pages", [])
    stats = content.get("statistics", {})
    categories = content.get("categories", {})

    # æ”¶é›†ç¨‹å¼ç¢¼ç¯„ä¾‹
    examples = []
    for page in pages:
        for block in page.get("code_blocks", [])[:2]:
            if len(block.get("code", "")) > 50:
                examples.append({
                    "source": page.get("title", ""),
                    "language": block.get("language", ""),
                    "code": block.get("code", "")
                })

    # ç”Ÿæˆ SKILL.md
    skill_md = f"""# {name} Skill

## Description

{name} æŠ€è¡“æ–‡ä»¶çŸ¥è­˜åº«ï¼ŒåŒ…å«æ ¸å¿ƒæ¦‚å¿µã€API åƒè€ƒèˆ‡å¯¦ç”¨ç¨‹å¼ç¢¼ç¯„ä¾‹ã€‚

## When to Use

- éœ€è¦æŸ¥è©¢ {name} ç›¸é—œæŠ€è¡“å•é¡Œ
- å°‹æ‰¾ç¨‹å¼ç¢¼ç¯„ä¾‹èˆ‡æœ€ä½³å¯¦è¸
- äº†è§£ API ç”¨æ³•èˆ‡è¨­å®šæ–¹å¼

## Statistics

| é …ç›® | æ•¸å€¼ |
|------|------|
| ç¸½é æ•¸ | {stats.get('total_pages', 0)} |
| ç¨‹å¼ç¢¼å€å¡Š | {stats.get('total_code_blocks', 0)} |
| åµæ¸¬èªè¨€ | {', '.join(stats.get('languages_detected', []))} |

## Categories

"""

    for cat, count in sorted(categories.items(), key=lambda x: -x[1]):
        skill_md += f"- **{cat.replace('_', ' ').title()}**: {count} é \n"

    skill_md += "\n## Core Concepts\n\n"

    concept_pages = [p for p in pages if p.get("category") in ["core_concepts", "getting_started"]][:5]
    for page in concept_pages:
        skill_md += f"### {page.get('title', 'Untitled')}\n\n"
        skill_md += f"{page.get('content', '')[:500]}...\n\n"

    skill_md += "## Code Examples\n\n"

    for i, ex in enumerate(examples[:5], 1):
        skill_md += f"### Example {i}: {ex.get('source', '')}\n\n"
        skill_md += f"```{ex.get('language', '')}\n{ex.get('code', '')}\n```\n\n"

    skill_md += """## Related Resources

- åŸå§‹æ–‡ä»¶ä¾†æº
- GitHub å€‰åº«ï¼ˆå¦‚é©ç”¨ï¼‰
"""

    # å„²å­˜ SKILL.md
    with open(f"{output_dir}/SKILL.md", "w", encoding="utf-8") as f:
        f.write(skill_md)

    # å„²å­˜åˆ†é¡æ–‡ä»¶
    for cat in categories.keys():
        cat_pages = [p for p in pages if p.get("category") == cat]
        cat_md = f"# {cat.replace('_', ' ').title()}\n\n"

        for page in cat_pages[:20]:
            cat_md += f"## {page.get('title', 'Untitled')}\n\n"
            cat_md += f"{page.get('content', '')[:2000]}\n\n"

        with open(f"{output_dir}/references/{cat}.md", "w", encoding="utf-8") as f:
            f.write(cat_md)

    # å„²å­˜ metadata.json
    metadata = {
        "skill_name": name.lower().replace(" ", "-"),
        "version": "1.0.0",
        "generated_at": datetime.now().isoformat(),
        "statistics": stats,
        "categories": list(categories.keys())
    }

    with open(f"{output_dir}/metadata.json", "w", encoding="utf-8") as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)

    print(f"âœ… Skill å·²å„²å­˜è‡³ {output_dir}/")
    return skill_md
```

### å®Œæ•´å·¥ä½œæµç¨‹ç¯„ä¾‹

```python
import asyncio

async def create_skill_from_url(url: str, name: str):
    """å®Œæ•´å·¥ä½œæµç¨‹ï¼šå¾ URL å»ºç«‹ Skill"""

    print(f"ğŸ”„ é–‹å§‹æŠ“å– {url}...")

    # 1. æŠ“å–å…§å®¹
    content = await scrape_documentation(url, max_pages=50)

    print(f"âœ… æŠ“å–å®Œæˆ: {content['statistics']['total_pages']} é ")
    print(f"ğŸ“ åˆ†é¡: {list(content['categories'].keys())}")

    # 2. ç”Ÿæˆ Skill
    output_dir = f"output/{name.lower()}"
    generate_skill(content, name, output_dir)

    print(f"ğŸ‰ å®Œæˆï¼Skill ä½æ–¼ {output_dir}/")
    return output_dir

# åŸ·è¡Œ
asyncio.run(create_skill_from_url(
    url="https://fastapi.tiangolo.com/",
    name="FastAPI"
))
```

---

## è¼¸å‡ºå“è³ªæ¨™æº–

- âœ… æ‰€æœ‰ç¨‹å¼ç¢¼å€å¡Šæ¨™è¨˜èªè¨€
- âœ… ç¯„ä¾‹ç¨‹å¼ç¢¼å¯ç›´æ¥åŸ·è¡Œ
- âœ… åŒ…å«å®Œæ•´çš„éŒ¯èª¤è™•ç†ç¯„ä¾‹
- âœ… è¡¨æ ¼æ ¼å¼å°é½Š
- âœ… ä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼ˆé™¤éä½¿ç”¨è€…ç”¨è‹±æ–‡ï¼‰

## ä¾è³´å®‰è£

```bash
pip install httpx beautifulsoup4
```

## é™åˆ¶èªªæ˜

1. **éœ€è¦ç™»å…¥çš„é é¢** - ç„¡æ³•æŠ“å–ï¼Œå»ºè­°ä½¿ç”¨è€…è²¼ä¸Šå…§å®¹
2. **JavaScript å‹•æ…‹è¼‰å…¥** - å¯èƒ½æŠ“å–ä¸å®Œæ•´ï¼Œå»ºè­°ä½¿ç”¨ llms.txt
3. **å¤§å‹ç¶²ç«™ (500+ é )** - å»ºè­°åˆ†æ‰¹è™•ç†
4. **é€Ÿç‡é™åˆ¶** - å…§å»º 0.5 ç§’å»¶é²

---

## ç›¸é—œåƒè€ƒ

- `references/README.md` - å®Œæ•´ README æ–‡æª”
- `references/CHANGELOG.md` - ç‰ˆæœ¬æ­·å²
- `references/issues.md` - GitHub Issues
- `references/releases.md` - ç‰ˆæœ¬ç™¼å¸ƒ

**Repository:** [yusufkaraaslan/Skill_Seekers](https://github.com/yusufkaraaslan/Skill_Seekers)
